% !TeX spellcheck = en_GB

\section*{Lists of Symbols}
%\label{ch_list_of_symbols}

\vspace*{-2mm}
\section*{Conjuntos y Funciones} 

\begin{align} 
	&a \in \mathcal{A} & \quad & \parbox{.75\textwidth}{El objeto $a$ es un elemento del conjunto $\mathcal{A}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&a \defeq b & \quad & \mbox{Utilizamos $a$ como una abreviatura para $b$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&|\mathcal{A}| & \quad & \mbox{La cardinalidad (es decir, el número de elementos) de un conjunto finito $\mathcal{A}$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathcal{A} \subseteq \mathcal{B}& \quad & \mbox{$\mathcal{A}$ es un subconjunto de $\mathcal{B}$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathcal{A} \subset \mathcal{B}& \quad & \mbox{$\mathcal{A}$ es un subconjunto propio de $\mathcal{B}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{N} & \quad & \mbox{Los números naturales $1,2,\ldots$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}  &\quad &\mbox{Los números reales $x$ \cite{RudinBook}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}_{+}  &\quad &\mbox{Los números reales no negativos $x\geq0$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}_{++}  &\quad &\mbox{Los números reales positivos $x> 0$.} \nonumber
\end{align} 

\newpage
\begin{align}
	&\{0,1\}& \quad & \mbox{El conjunto que consta de los dos números reales $0$ y $1$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
&[0,1] &\quad &\mbox{El intervalo cerrado de números reales $x$ con $0 \leq x \leq 1$. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
&\argmin_{\weights} f(\weights) &\quad &\mbox{El conjunto de minimizadores para una función de valor real $f(\weights)$.  } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
&\sphere{\nrnodes} &\quad &\mbox{El conjunto de vectores de norma unitaria en $\mathbb{R}^{\nrnodes+1}$.  }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
 &\log a &\quad &\mbox{El logaritmo del número positivo $a \in \mathbb{R}_{++}$.  } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
 &\hypothesis(\cdot)\!:\!\mathcal{A}\!\rightarrow\!\mathcal{B} :  a \!\mapsto\!h(a) &\quad &\parbox{.75\textwidth}{
	 Una función (aplicación) que acepta cualquier elemento $a \in \mathcal{A}$ de un conjunto $\mathcal{A}$ 
	 como entrada y entrega un elemento bien definido $h(a) \in \mathcal{B}$ de un conjunto $\mathcal{B}$. 
	 El conjunto $\mathcal{A}$ es el dominio de la función $h$ y el conjunto $\mathcal{B}$ es el 
	 codominio de $\hypothesis$. El \gls{ml} (aprendizaje automático) tiene como objetivo encontrar (o aprender) 
	 una función $\hypothesis$ (es decir, una \gls{hypothesis}) que lea las \gls{feature}s $\featurevec$ de un \gls{datapoint} 
	 y entregue una \gls{prediction} $\hypothesis(\featurevec)$ para su \gls{label} $\truelabel$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	 &\nabla f(\weights) & \quad & \parbox{.75\textwidth}{El \gls{gradient} de una función \gls{differentiable} de valor real 
	 $f: \mathbb{R}^{\featuredim}\rightarrow \mathbb{R}$ es el vector 
	 $\nabla f(\weights) = \big( \frac{\partial f}{\partial \weight_{1}},\ldots,\frac{\partial f}{\partial \weight_{\featuredim}}  \big)^{T} \in \mathbb{R}^{\featuredim}$ \cite[Ch. 9]{RudinBookPrinciplesMatheAnalysis}.}   \nonumber
\end{align} 

\section*{Matrices y Vectores} 

\begin{align} 
	 &\featurevec=\big(\feature_{1},\ldots,\feature_{\featuredim})^{T} &\quad & \parbox{.75\textwidth}{Un vector de longitud $\featuredim$, con su 
		$\featureidx$-ésima entrada siendo $\feature_{\featureidx}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}^{\featuredim} & \quad &  \parbox{.75\textwidth}{El conjunto de vectores $\featurevec=\big(\feature_{1},\ldots,\feature_{\featurelen}\big)^{T}$ que consiste en $\featuredim$ entradas de valor real $\feature_{1},\ldots,\feature_{\featurelen} \in \mathbb{R}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{I}_{\modelidx \times \featuredim}  & \quad &  \parbox{.75\textwidth}{Una matriz identidad generalizada 
		con $\modelidx$ filas y $\featuredim$ columnas. Los elementos de $\mathbf{I}_{\modelidx \times \featuredim} \in \mathbb{R}^{\modelidx \times \featuredim}$ 
		son iguales a $1$ en la diagonal principal y iguales a $0$ en los demás casos. }\nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	&\mathbf{I}_{\dimlocalmodel}, \mathbf{I} & \quad &  \parbox{.75\textwidth}{Una matriz identidad cuadrada 
		de tamaño $\dimlocalmodel \times \dimlocalmodel$. Si el tamaño es claro por el contexto, omitimos el subíndice.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\normgeneric{\featurevec}{2}  &\quad & \parbox{.75\textwidth}{La \gls{norm} euclidiana (o $\ell_{2}$) del vector 
		$\featurevec=\big(\feature_{1},\ldots,\feature_{\featurelen}\big)^{T} \in \mathbb{R}^{\featuredim}$ definida como $ \| \featurevec \|_{2} \defeq \sqrt{\sum_{\featureidx=1}^{\featuredim} \feature_{\featureidx}^{2}}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	&\normgeneric{\featurevec}{}  & \quad &  \parbox{.75\textwidth}{Alguna \gls{norm} del vector $\featurevec \in \mathbb{R}^{\featuredim}$ \cite{GolubVanLoanBook}. A menos que se especifique lo contrario, nos referimos a la \gls{norm} euclidiana $\normgeneric{\featurevec}{2}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\featurevec^{T} &\quad & \parbox{.75\textwidth}{La traspuesta de una matriz que tiene el vector 
		$\featurevec \in \mathbb{R}^{\dimlocalmodel}$ como su única columna.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{X}^{T} &\quad & \parbox{.75\textwidth}{La traspuesta de una matriz $\mathbf{X} \in \mathbb{R}^{\samplesize \times \featurelen}$. 
		Una matriz cuadrada de valores reales $\mathbf{X} \in \mathbb{R}^{\samplesize \times \samplesize}$ 
		se denomina simétrica si $\mathbf{X} = \mathbf{X}^{T}$. }  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{0}= \big(0,\ldots,0\big)^{T}  & \quad &  \parbox{.75\textwidth}{El vector en $\mathbb{R}^{\dimlocalmodel}$ con cada entrada igual a cero.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{1}= \big(1,\ldots,1\big)^{T}  & \quad &  \parbox{.75\textwidth}{El vector en $\mathbb{R}^{\dimlocalmodel}$ con cada entrada igual a uno.} \nonumber
\end{align} 

\newpage
\begin{align} 
	&\big(\vv^{T},\vw^{T} \big)^{T}  & \quad &  \parbox{.75\textwidth}{El vector de longitud $\featurelen+\featurelen'$ 
		obtenido al concatenar las entradas del vector $\vv \in \mathbb{R}^{\featurelen}$ con las entradas de $\vw \in \mathbb{R}^{\featurelen'}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&	{\rm span}\{ \mathbf{B} \}  & \quad &  \parbox{.75\textwidth}{El espacio generado (span) por una matriz $\mathbf{B} \in \mathbb{R}^{a \times b}$, 
		que es el subespacio de todas las combinaciones lineales de las columnas de $\mathbf{B}$, 
		${\rm span}\{ \mathbf{B} \} = \big\{  \mathbf{B} \va : \va \in \mathbb{R}^{b} \big\} \subseteq \mathbb{R}^{a}$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\determinant{\mC} &\quad & \parbox{.75\textwidth}{El determinante de la matriz $\mC$. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{A} \otimes \mathbf{B} &\quad & \parbox{.75\textwidth}{El producto de Kronecker de $\mathbf{A}$ y $\mathbf{B}$ \cite{Golub1980}. }  \nonumber
\end{align} 


\newpage
\section*{Teoría de la Probabilidad} 
\begin{align}
	\expect_{p} \{ f(\datapoint) \}  \quad\quad & \parbox{.75\textwidth}{La \gls{expectation} de una función $f(\datapoint)$ de una \gls{rv} 
		$\datapoint$ cuya \gls{probdist} es $\prob{\datapoint}$. Si la \gls{probdist} es clara por el contexto, 
		simplemente escribimos $\expect \{ f(\datapoint) \}$. }  \nonumber \\[2mm] \hline \nonumber\\[-5mm]    
	\prob{\featurevec,\truelabel} \quad\quad & \parbox{.75\textwidth}{Una \gls{probdist} conjunta de una \gls{rv} 
		cuyas \gls{realization}es son \gls{datapoint}s con \gls{feature}s $\featurevec$ y \gls{label} $\truelabel$.} \nonumber        \nonumber \\[2mm] \hline \nonumber\\[-5mm]        
	\prob{\featurevec|\truelabel} \quad\quad & \parbox{.75\textwidth}{Una \gls{probdist} condicional de una \gls{rv} 
		$\featurevec$ dado el valor de otra \gls{rv} $\truelabel$ \cite[Sec.\ 3.5]{BertsekasProb}. } \nonumber       \nonumber \\[2mm] \hline \nonumber\\[-5mm]           
	\prob{\featurevec;\weights} \quad\quad & \parbox{.75\textwidth}{Una \gls{probdist} parametrizada de una \gls{rv} $\featurevec$. 
		La \gls{probdist} depende de un vector de parámetros $\weights$. Por ejemplo, $\prob{\featurevec;\weights}$ podría ser una 
		\gls{mvndist} con el vector de parámetros $\weights$ dado por las entradas del vector de \gls{mean} $\expect \{ \featurevec \}$ 
		y la \gls{covmtx} $\expect \bigg \{ \big( \featurevec - \expect \{ \featurevec \}\big) \big( \featurevec - \expect \{ \featurevec \}\big)^{T}  \bigg\}$.} \nonumber           \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\mathcal{N}(\mu, \sigma^{2}) \quad\quad & \parbox{.75\textwidth}{La \gls{probdist} de una 
		\gls{gaussrv} $\feature \in \mathbb{R}$ con \gls{mean} (o \gls{expectation}) $\mu= \expect \{ \feature \}$ 
		y \gls{variance} $\sigma^{2} =   \expect \big\{  (  \feature - \mu )^2 \big\}$.} \nonumber    \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\mathcal{N}(\clustermean, \mathbf{C}) \quad\quad & \parbox{.75\textwidth}{La \gls{mvndist} de un vector de valores 
		\gls{gaussrv} $\featurevec \in \mathbb{R}^{\featuredim}$ con \gls{mean} (o \gls{expectation}) $\clustermean= \expect \{ \featurevec \}$ 
		y \gls{covmtx} $\mathbf{C} =  \expect \big\{ \big( \featurevec - \clustermean \big)\big( \featurevec - \clustermean \big)^{T} \big\}$.} \nonumber                                             
\end{align}






\newpage
\section*{Aprendizaje Automático}

\begin{align}
	\sampleidx \quad\quad & \parbox{.75\textwidth}{Un índice $\sampleidx=1,2,\ldots$ que 
		enumeran los \gls{datapoint}s.}   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\samplesize \quad\quad &\parbox{.75\textwidth}{El número de \gls{datapoint}s en (es decir, el tamaño de) un \gls{dataset}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	\dataset \quad\quad & \parbox{.75\textwidth}{Un \gls{dataset} $\dataset = \{ \datapoint^{(1)},\ldots, \datapoint^{(\samplesize)} \}$ 
		es una lista de \gls{datapoint}s individuales $\datapoint^{(\sampleidx)}$, para $\sampleidx=1,\ldots,\samplesize$.}   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\featurelen \quad\quad &\parbox{.75\textwidth}{El número de \gls{feature}s que caracterizan un \gls{datapoint}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\feature_{\featureidx} \quad\quad &\parbox{.75\textwidth}{La $\featureidx$-ésima característica (feature) de un \gls{datapoint}. La primera \gls{feature} 
		se denota como $\feature_{1}$, la segunda como $\feature_{2}$, y así sucesivamente. } \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	\featurevec \quad\quad &\parbox{.75\textwidth}{El vector de características (feature vector) $\featurevec=\big(\feature_{1},\ldots,\feature_{\featuredim}\big)^{T}$ de un \gls{datapoint} 
		cuyas entradas son las características individuales de un \gls{datapoint}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\featurespace \quad\quad & \parbox{.75\textwidth}{El \gls{featurespace} $\featurespace$ es 
		el conjunto de todos los valores posibles que las \gls{feature}s $\featurevec$ de un \gls{datapoint} pueden tomar.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\rawfeaturevec \quad\quad &\parbox{.75\textwidth}{En lugar del símbolo $\featurevec$, a veces usamos $\rawfeaturevec$ 
		como otro símbolo para denotar un vector cuyas entradas son las \gls{feature}s individuales de un \gls{datapoint}. 
		Necesitamos dos símbolos diferentes para distinguir entre características (features) crudas y características aprendidas \cite[Ch. 9]{MLBasics}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\featurevec^{(\sampleidx)} \quad\quad &\parbox{.75\textwidth}{El vector de características de 
		el $\sampleidx$-ésimo \gls{datapoint} dentro de un \gls{dataset}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\feature_{\featureidx}^{(\sampleidx)}\quad\quad &\parbox{.75\textwidth}{La $\featureidx$-ésima \gls{feature} del 
		$\sampleidx$-ésimo \gls{datapoint} dentro de un \gls{dataset}.} \nonumber
\end{align}  
      

\begin{align}
	\batch \quad\quad &\parbox{.75\textwidth}{Un mini-\gls{batch} (o subconjunto) de \gls{datapoint}s seleccionados aleatoriamente.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\batchsize \quad\quad &\parbox{.75\textwidth}{El tamaño de (es decir, el número de \gls{datapoint}s en) un mini-\gls{batch}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\truelabel \quad\quad &\parbox{.75\textwidth}{La \gls{label} (o cantidad de interés) de un \gls{datapoint}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\truelabel^{(\sampleidx)} \quad\quad &\parbox{.75\textwidth}{La \gls{label} del $\sampleidx$-ésimo \gls{datapoint}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\big(\featurevec^{(\sampleidx)},\truelabel^{(\sampleidx)}\big)  \quad\quad &\parbox{.75\textwidth}{Las \gls{feature}s y la \gls{label} del $\sampleidx$-ésimo \gls{datapoint}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\labelspace  \quad\quad & \parbox{.75\textwidth}{El \gls{labelspace} $\labelspace$ de un método de \gls{ml} 
		consiste en todos los valores potenciales de \gls{label} que un \gls{datapoint} puede tener. 
		El \gls{labelspace} nominal puede ser más grande que el conjunto de diferentes valores de \gls{label} que surgen en un \gls{dataset} dado (por ejemplo, un \gls{trainset}). 
		Los problemas (o métodos) de \gls{ml} que utilizan un \gls{labelspace} numérico, como $\labelspace=\mathbb{R}$ 
		o $\labelspace=\mathbb{R}^{3}$, se conocen como problemas de \gls{regression}. 
		Los problemas (o métodos) de \gls{ml} que utilizan un \gls{labelspace} discreto, como $\labelspace=\{0,1\}$ 
		o $\labelspace=\{\mbox{\emph{gato}},\mbox{\emph{perro}},\mbox{\emph{ratón}}\}$, se conocen como problemas de \gls{classification}.}  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\lrate  \quad\quad & \parbox{.75\textwidth}{La \gls{learnrate} (o \gls{stepsize}) utilizada por los métodos de \gls{gdmethods}.}  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\hypothesis(\cdot)  \quad\quad &\parbox{.75\textwidth}{Un mapa de \gls{hypothesis} que toma como entrada las \gls{feature}s $\featurevec$ de un \gls{datapoint} 
		y entrega una \gls{prediction} $\hat{\truelabel}=\hypothesis(\featurevec)$ para su \gls{label} $\truelabel$.}  	 \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	 \labelspace^{\featurespace} \quad\quad & \parbox{.75\textwidth}{Dado dos conjuntos $\featurespace$ y $\labelspace$, denotamos por $\labelspace^{\featurespace}$ el conjunto de todos los posibles mapas de \gls{hypothesis} $\hypothesis: \featurespace \rightarrow \labelspace$.} 	 \nonumber 
\end{align}                  


\begin{align}
	\hypospace  \quad\quad & \parbox{.75\textwidth}{Un \gls{hypospace} o \gls{model} utilizado por un método de \gls{ml}. 
		El \gls{hypospace} consiste en diferentes mapas de \gls{hypothesis} $\hypothesis: \featurespace \rightarrow \labelspace$, entre los cuales 
		el método de \gls{ml} debe elegir.}   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\effdim{\hypospace}  \quad\quad & \parbox{.75\textwidth}{La \gls{effdim} de un \gls{hypospace} $\hypospace$.}   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\biasterm^2 \quad\quad &\parbox{.75\textwidth}{
		El \gls{bias} cuadrado de una \gls{hypothesis} aprendida $\learnthypothesis$ 
		producida por un método de \gls{ml}. El método se entrena con \gls{datapoint}s 
		que se modelan como las \gls{realization}es de \gls{rv}s. Dado que los \gls{data} son una \gls{realization} 
		de \gls{rv}s, la \gls{hypothesis} aprendida $\learnthypothesis$ también es una \gls{realization} 
		de una \gls{rv}.} \nonumber  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\varianceterm \quad\quad &\parbox{.75\textwidth}{La \gls{variance} de los (\gls{parameters} de la) \gls{hypothesis} aprendida 
		producida por un método de \gls{ml}. El método se entrena con \gls{datapoint}s que se modelan como las \gls{realization}es 
		de \gls{rv}s. Dado que los \gls{data} son una \gls{realization} de \gls{rv}s, la \gls{hypothesis} aprendida $\learnthypothesis$ 
		también es una \gls{realization} de una \gls{rv}.} \nonumber \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\lossfunc{(\featurevec,\truelabel)}{\hypothesis}  \quad\quad & \parbox{.75\textwidth}{La \gls{loss} incurrida al predecir la 
		\gls{label} $\truelabel$ de un \gls{datapoint} utilizando la \gls{prediction} $\hat{\truelabel}=h(\featurevec)$. 
		La \gls{prediction} $\hat{\truelabel}$ se obtiene al evaluar la \gls{hypothesis} $\hypothesis \in \hypospace$ para 
		la \gls{featurevec} $\featurevec$ del \gls{datapoint}.}    \nonumber  \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	\valerror \quad\quad &\parbox{.75\textwidth}{El \gls{valerr} de una \gls{hypothesis} $\hypothesis$, que es su 
		\gls{loss} promedio incurrida en un \gls{valset}.}  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\emperror\big(\hypothesis| \dataset \big) \quad\quad &\parbox{.75\textwidth}{El \gls{emprisk} o \gls{loss} promedio 
		incurrido por la \gls{hypothesis} $\hypothesis$ en un \gls{dataset} $\dataset$.} \nonumber                           
\end{align}     


\begin{align}
	\trainerror \quad\quad &\parbox{.75\textwidth}{El \gls{trainerr} de una \gls{hypothesis} $\hypothesis$, que es su 
		\gls{loss} promedio incurrido en un \gls{trainset}. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\timeidx \quad\quad &\parbox{.75\textwidth}{Un índice de tiempo discreto $\timeidx=0,1,\ldots$ utilizado para 
		enumerar eventos secuenciales (o instantes de tiempo). }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\taskidx \quad\quad &\parbox{.75\textwidth}{Un índice que enumera las \gls{learningtask}s dentro de un problema de \gls{multitask learning}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\regparam \quad\quad &\parbox{.75\textwidth}{Un parámetro de \gls{regularization} que controla 
		la cantidad de \gls{regularization}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\eigval{\featureidx}\big( \mathbf{Q} \big) \quad\quad &\parbox{.75\textwidth}{El $\featureidx$-ésimo 
		\gls{eigenvalue} (ordenado en forma ascendente o descendente) de una matriz \gls{psd} $\mathbf{Q}$. 
		También usamos la abreviatura $\eigval{\featureidx}$ si la matriz correspondiente es clara por el contexto. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\actfun(\cdot) \quad\quad &\parbox{.75\textwidth}{La \gls{actfun} utilizada por una neurona artificial dentro de una \gls{ann}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\decreg{\hat{\truelabel}} \quad\quad &\parbox{.75\textwidth}{Una \gls{decisionregion} dentro de un \gls{featurespace}.  }\nonumber \\[2mm] \hline \nonumber\\[-5mm]  
	\weights  \quad\quad & \parbox{.75\textwidth}{Un vector de parámetros $\weights = \big(\weight_{1},\ldots,\weight_{\featuredim}\big)^{T}$ 
		de un \gls{model}, por ejemplo, los \gls{weights} de un \gls{linmodel} o en una \gls{ann}.}     \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\hypothesis^{(\weights)}(\cdot)  \quad\quad &\parbox{.75\textwidth}{Un mapa de \gls{hypothesis} que involucra \gls{modelparams} ajustables $\weight_{1},\ldots,\weight_{\featuredim}$ apilados en el vector $\weights=\big(\weight_{1},\ldots,\weight_{\featuredim} \big)^{T}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\featuremap(\cdot)  \quad\quad & \parbox{.75\textwidth}{Un \gls{featuremap} $\featuremap: \featurespace \rightarrow \featurespace' : \featurevec \mapsto \featurevec' \defeq \featuremap\big( \featurevec \big) \in \featurespace'$.}   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\kernelmap{\cdot}{\cdot} \quad\quad & \parbox{.75\textwidth}{Dado un \gls{featurespace} $\featurespace$, 
		un \gls{kernel} es un mapa $\kernel: \featurespace \times \featurespace \rightarrow \mathbb{C}$ que es \gls{psd}.}    \nonumber                                                                                                                                                     
\end{align}               






\newpage
\section*{Aprendizaje Federado}

\begin{align}
 	&\graph = \pair{\nodes}{\edges} & \quad & \parbox{.75\textwidth}{Un \gls{graph} no dirigido cuyos nodos $\nodeidx \in \nodes$ representan 
	\gls{device}s dentro de un \gls{empgraph}. Los bordes ponderados no dirigidos $\edges$ representan conectividad entre 
	\gls{device}s y similitudes estadísticas entre sus \gls{dataset}s y \gls{learningtask}s.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
&\nodeidx \in \nodes& \quad & \parbox{.75\textwidth}{Un nodo que representa un 
	\gls{device} dentro de un \gls{empgraph}. El dispositivo puede acceder a un \gls{localdataset} y entrenar un \gls{localmodel}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\indsubgraph{\graph}{\cluster}& \quad & \parbox{.75\textwidth}{El subgrafo inducido de $\graph$ utilizando los nodos en $\cluster \subseteq \nodes$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\LapMat{\graph}   & \quad & \parbox{.75\textwidth}{La \gls{LapMat} de un \gls{graph} $\graph$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
		&\LapMat{\cluster}   & \quad & \parbox{.75\textwidth}{La \gls{LapMat} del \gls{graph} inducido $\indsubgraph{\graph}{\cluster}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	 &		\neighbourhood{\nodeidx}  & \quad & \parbox{.75\textwidth}{La \gls{neighborhood} de un nodo $\nodeidx$ en un \gls{graph} $\graph$.}   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\nodedegree{\nodeidx} & \quad & \parbox{.75\textwidth}{El grado ponderado $\nodedegree{\nodeidx}\defeq \sum_{\nodeidx' \in \neighbourhood{\nodeidx}} \edgeweight_{\nodeidx,\nodeidx'}$ de un nodo $\nodeidx$ en un \gls{graph} $\graph$.}  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\maxnodedegree^{(\graph)} & \quad & \parbox{.75\textwidth}{El máximo grado ponderado de nodo en un \gls{graph} $\graph$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
&\localdataset{\nodeidx} & \quad & \parbox{.75\textwidth}{El \gls{localdataset} $\localdataset{\nodeidx}$ que posee el 
			nodo $\nodeidx\in \nodes$ de un \gls{empgraph}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
&\localsamplesize{\nodeidx} & \quad & \parbox{.75\textwidth}{El número de \gls{datapoint}s (es decir, el \gls{samplesize}) contenidos en el 
			\gls{localdataset} $\localdataset{\nodeidx}$ en el nodo $\nodeidx\in \nodes$.} \nonumber 
\end{align} 

\begin{align} 
	&\featurevec^{(\nodeidx,\sampleidx)} & \quad & \parbox{.75\textwidth}{Las \gls{feature}s del $\sampleidx$-ésimo \gls{datapoint} en 
	el \gls{localdataset} $\localdataset{\nodeidx}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
&\truelabel^{(\nodeidx,\sampleidx)} & \quad & \parbox{.75\textwidth}{La \gls{label} del $\sampleidx$-ésimo \gls{datapoint} en 
	el \gls{localdataset} $\localdataset{\nodeidx}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\localparams{\nodeidx} & \quad & \parbox{.75\textwidth}{Los \gls{modelparams} locales del \gls{device} $\nodeidx$ dentro de un \gls{empgraph}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\locallossfunc{\nodeidx}{\weights} & \quad & \parbox{.75\textwidth}{La \gls{lossfunc} local utilizada por el \gls{device} $\nodeidx$ 
	para medir la utilidad de alguna elección $\weights$ para los \gls{modelparams} locales.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
& \gtvloss{\featurevec}{\hypothesis\big(\featurevec\big)}{\hypothesis'\big(\featurevec\big)}& \quad & \parbox{.75\textwidth}{La \gls{loss} 
	incurrida por una \gls{hypothesis} $\hypothesis'$ en un \gls{datapoint} con \gls{feature}s $\featurevec$ y \gls{label} 
	$\hypothesis\big( \featurevec\big)$ obtenida de otra \gls{hypothesis}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	& 	{\rm stack} \big\{ \weights^{(\nodeidx)} \big\}_{\nodeidx=1}^{\nrnodes} & \quad & \parbox{.75\textwidth}{El vector $\bigg( \big(\weights^{(1)}  \big)^{T}, \ldots, \big(\weights^{(\nrnodes)}  \big)^{T} \bigg)^{T} \in \mathbb{R}^{\dimlocalmodel\nrnodes}$ que 
		se obtiene apilando verticalmente los \gls{modelparams} locales $\weights^{(\nodeidx)} \in \mathbb{R}^{\dimlocalmodel}$.} \nonumber  
\end{align}        


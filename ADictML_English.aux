\relax 
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\ifx\qr@savematrix\@undefined\def\qr@savematrix{\begingroup\let\do\@makeother\dospecials\catcode`\{=1\catcode`\}=2\relax \qr@savematrix@int}\def\qr@savematrix@int#1#2#3#4{\endgroup}\fi
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\providecommand\@glsxtr@savepreloctag[2]{}
\providecommand\@glsorder[1]{}
\providecommand\@istfilename[1]{}
\@istfilename{ADictML_English.ist}
\@glsorder{word}
\babel@aux{english}{}
\qr@savematrix{https://github.com/aaltodictionaryofml/aaltodictionaryofml.github.io/}{5}{0}{1111111001011100111011010000001111111100000100000100110110110010100100000110111010101110111001110010111010111011011101010111000110110111110001011101101110101100111100111001010010101110110000010110011111010001001110010000011111111010101010101010101010101111111000000001001010110010111000010000000010111110000110010010100111100011111000100110000101001110000010100100100010110000101100110001001100111111101101111111001111000001000011100111101100010011001011100100010010111110011011111011100010110101011111001000000010000011000111010010011100101011110011110111101000011010111011001100010011110010101000110100101110000100111101101011011101000010010110010110111001101011101001101010010111110100001011000110011011001001101110100111111101011111000100001010111010010101001001010110111000000000110110110101110110100110001000011000100011101110100010100110111101101001001001011010001100000111001100010001101001000001001111111111011010111100111001010000111000101011011010100010000011011110000000111001111111001111010010010101100100001000010101010011101001110010001001000010111011111011100000000101000101111110100001000110101111111000000111110011000010101010111100000101111100011111110100010001100110111010100000001001010001111111101101011101011001111011010110001011010001101110101010000110010100100110000101110000010010011110011111010111000100011111111010101101011100111110011011111}
\citation{RudinBook}
\citation{RudinBook}
\citation{RudinBookPrinciplesMatheAnalysis}
\citation{RudinBookPrinciplesMatheAnalysis}
\citation{GolubVanLoanBook}
\citation{GolubVanLoanBook}
\citation{Golub1980}
\citation{Golub1980}
\citation{BertsekasProb}
\citation{BertsekasProb}
\citation{MLBasics}
\citation{MLBasics}
\citation{MLBasics}
\@writefile{toc}{\contentsline {section}{Glossary}{14}{section*.7}\protected@file@percent }
\citation{LC}
\citation{LC}
\citation{LC,GrayProbBook}
\citation{LC}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces If \gls {feature}s and \gls {label} of a \gls {datapoint} are drawn from a \gls {mvndist}, we can achieve minimum \gls {risk} (under \gls {sqerrloss}) by using the \gls {bayesestimator} $\mu _{y|{\bf  x}}$ to predict the \gls {label} $y$ of a \gls {datapoint} with \gls {feature}s ${\bf  x}$. The corresponding minimum \gls {risk} is given by the posterior \gls {variance} $\sigma ^{2}_{y|{\bf  x}}$. We can use this quantity as a baseline for the average \gls {loss} of a trained \gls {model} $\hat  {h}$. }}{18}{figure.1}\protected@file@percent }
\newlabel{fig_post_baseline}{{1}{18}{If \gls {feature}s and \gls {label} of a \gls {datapoint} are drawn from a \gls {mvndist}, we can achieve minimum \gls {risk} (under \gls {sqerrloss}) by using the \gls {bayesestimator} $\mu _{y|{\protect \bf  x}}$ to predict the \gls {label} $y$ of a \gls {datapoint} with \gls {feature}s ${\protect \bf  x}$. The corresponding minimum \gls {risk} is given by the posterior \gls {variance} $\sigma ^{2}_{y|{\protect \bf  x}}$. We can use this quantity as a baseline for the average \gls {loss} of a trained \gls {model} $\protect \hat  {h}$. }{figure.1}{}}
\citation{LC}
\newlabel{labelspace}{{}{20}{\glossarytitle }{figure.1}{}}
\newlabel{equ_def_threshold_bin_classifier}{{1}{20}{\glossarytitle }{equation.0.1}{}}
\citation{SemiSupervisedBook}
\citation{BoydConvexBook}
\citation{JMLR:v22:18-694,Pelckmans2005}
\citation{GolubVanLoanBook}
\citation{GDPR2016}
\citation{EURegulation2018}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Data augmentation exploits intrinsic symmetries of \gls {datapoint}s in some \gls {featurespace} $\mathcal  {X}$. We can represent a symmetry by an operator $\mathcal  {T}^{(\eta )}: \mathcal  {X}\rightarrow \mathcal  {X}$, parametrized by some number $\eta \in \mathbb  {R}$. For example, $\mathcal  {T}^{(\eta )}$ might represent the effect of rotating a cat image by $\eta $ degrees. A \gls {datapoint} with \gls {featurevec} ${\bf  x}^{(2)} = \mathcal  {T}^{(\eta )} \big  ({\bf  x}^{(1)} \big  )$ must have the same \gls {label} $y^{(2)}=y^{(1)}$ as a \gls {datapoint} with \gls {featurevec} ${\bf  x}^{(1)}$.}}{24}{figure.2}\protected@file@percent }
\newlabel{fig_symmetry_dataaug}{{2}{24}{Data augmentation exploits intrinsic symmetries of \gls {datapoint}s in some \gls {featurespace} $\protect \mathcal  {X}$. We can represent a symmetry by an operator $\protect \mathcal  {T}^{(\eta )}: \protect \mathcal  {X}\rightarrow \protect \mathcal  {X}$, parametrized by some number $\eta \in \protect \mathbb  {R}$. For example, $\protect \mathcal  {T}^{(\eta )}$ might represent the effect of rotating a cat image by $\eta $ degrees. A \gls {datapoint} with \gls {featurevec} ${\protect \bf  x}^{(2)} = \protect \mathcal  {T}^{(\eta )} \protect \big  ({\protect \bf  x}^{(1)} \protect \big  )$ must have the same \gls {label} $y^{(2)}=y^{(1)}$ as a \gls {datapoint} with \gls {featurevec} ${\protect \bf  x}^{(1)}$}{figure.2}{}}
\citation{coverthomas}
\citation{Liu2021,PoisonGAN}
\citation{silberschatz2019database,abiteboul1995foundations,hoberman2009data,ramakrishnan2002database}
\citation{silberschatz2019database}
\citation{DatasheetData2021}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Temperature observations at FMI station Kustavi Isokari.}}{27}{table.1}\protected@file@percent }
\newlabel{tab:temperature}{{1}{27}{Temperature observations at FMI station Kustavi Isokari}{table.1}{}}
\citation{Goodfellow-et-al-2016}
\citation{MLBasics}
\citation{RudinBookPrinciplesMatheAnalysis}
\citation{MLBasics}
\citation{kay,hastie01statisticallearning}
\citation{RudinBookPrinciplesMatheAnalysis}
\citation{HalmosMeasure,BillingsleyProbMeasure,RudinBookPrinciplesMatheAnalysis}
\citation{BishopBook,hastie01statisticallearning,GraphModExpFamVarInfWainJor}
\citation{PredictionLearningGames}
\citation{PredictionLearningGames,HazanOCO}
\citation{Colin:2022aa}
\citation{Zhang:2024aa,Colin:2022aa}
\citation{JunXML2020,Chen2018}
\citation{Zhang:2024aa}
\citation{rudin2019stop}
\citation{Molnar2019}
\citation{GradCamPaper}
\citation{Gujarati2021,Dodge2003,Everitt2022}
\citation{LearningKernelsBook}
\citation{Ribeiro2016}
\citation{pmlr-v54-mcmahan17a}
\citation{FedProx2020}
\citation{FlowSpecClustering2021}
\citation{papoulis,BertsekasProb,GrayProbBook}
\citation{Rasmussen2006Gaussian}
\citation{ross2013first}
\citation{GDPR2016}
\citation{ShalevMLBook}
\citation{OnePixelAttack}
\citation{RudinBookPrinciplesMatheAnalysis}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Two \gls {datapoint}s ${\bf  z}^{(1)},{\bf  z}^{(2)}$ that are used as a \gls {trainset} to learn a \gls {hypothesis} $\hat  {h}$ via \gls {erm}. We can evaluate $\hat  {h}$ \emph  {outside} $\mathcal  {D}^{(\rm  train)}$ either by an \gls {iidasspt} with underlying \gls {probdist} $p({\bf  z})$ or by perturbing the \gls {datapoint}s.}}{41}{figure.3}\protected@file@percent }
\newlabel{fig:polynomial_fit}{{3}{41}{Two \gls {datapoint}s ${\protect \bf  z}^{(1)},{\protect \bf  z}^{(2)}$ that are used as a \gls {trainset} to learn a \gls {hypothesis} $\protect \hat  {h}$ via \gls {erm}. We can evaluate $\protect \hat  {h}$ \protect \emph  {outside} $\protect \mathcal  {D}^{(\protect \rm  train)}$ either by an \gls {iidasspt} with underlying \gls {probdist} $p({\protect \bf  z})$ or by perturbing the \gls {datapoint}s}{figure.3}{}}
\newlabel{equ_def_GD_step}{{2}{42}{\glossarytitle }{equation.0.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces A single gradient step \eqref  {equ_def_GD_step} towards the minimizer $\overline  {{\bf  w}}$ of $f({\bf  w})$.}}{42}{figure.4}\protected@file@percent }
\newlabel{fig_basic_GD_step}{{4}{42}{A single gradient step \protect \eqref  {equ_def_GD_step} towards the minimizer $\protect \overline  {{\protect \bf  w}}$ of $f({\protect \bf  w})$}{figure.4}{}}
\newlabel{equ_def_gd_basic}{{3}{42}{\glossarytitle }{equation.0.3}{}}
\citation{ProximalMethods}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The basic gradient step \eqref  {equ_def_gd_basic} maps a given vector ${\bf  w}$ to the updated vector ${\bf  w}'$. It defines an operator $\mathcal  {T}^{(f,\eta )}(\cdot ): \mathbb  {R}^{d} \rightarrow \mathbb  {R}^{d}: {\bf  w}\DOTSB \mapstochar \rightarrow \setbox \z@ \hbox {\mathsurround \z@ $\textstyle {\bf  w}$}\mathaccent "0362{{\bf  w}}$.}}{43}{figure.5}\protected@file@percent }
\newlabel{fig_basic_GD_step_single}{{5}{43}{The basic gradient step \protect \eqref  {equ_def_gd_basic} maps a given vector ${\protect \bf  w}$ to the updated vector ${\protect \bf  w}'$. It defines an operator $\protect \mathcal  {T}^{(f,\eta )}(\cdot ): \protect \mathbb  {R}^{d} \rightarrow \protect \mathbb  {R}^{d}: {\protect \bf  w}\DOTSB \mapstochar \rightarrow \setbox \z@ \hbox {\mathsurround \z@ $\textstyle {\protect \bf  w}$}\mathaccent "0362{{\protect \bf  w}}$}{figure.5}{}}
\newlabel{equ_approx_gd_step}{{4}{43}{\glossarytitle }{equation.0.4}{}}
\citation{RockNetworks}
\citation{Luxburg2007,FlowSpecClustering2021}
\citation{ClusteredFLTVMinTSP}
\citation{Wain2019,BuhlGeerBook}
\newlabel{equ_hinge_loss_gls}{{5}{45}{\glossarytitle }{equation.0.5}{}}
\citation{LampertNowKernel}
\citation{HFLChapter2020}
\citation{JunXML2020}
\citation{LampertNowKernel,LearningKernelsBook}
\citation{LampertNowKernel,LearningKernelsBook}
\citation{coverthomas}
\citation{Gujarati2021,Dodge2003,Everitt2022}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  Five \gls {datapoint}s characterized by \gls {featurevec}s ${\bf  x}^{(r)}$ and \gls {label}s $y^{(r)} \in \{ \circ , \square \}$, for $r=1,\ldots  ,5$. With these \gls {featurevec}s, there is no way to separate the two classes by a straight line (representing the \gls {decisionboundary} of a \gls {linclass}). In contrast, the transformed \gls {featurevec}s ${\bf  z}^{(r)} = K\big  ({\bf  x}^{(r)},\cdot \big  )$ allow to separate the \gls {datapoint}s using a \gls {linclass}. }}{49}{figure.6}\protected@file@percent }
\newlabel{fig_linsep_kernel}{{6}{49}{ Five \gls {datapoint}s characterized by \gls {featurevec}s ${\protect \bf  x}^{(r)}$ and \gls {label}s $y^{(r)} \in \{ \circ , \square \}$, for $r=1,\protect \ldots  ,5$. With these \gls {featurevec}s, there is no way to separate the two classes by a straight line (representing the \gls {decisionboundary} of a \gls {linclass}). In contrast, the transformed \gls {featurevec}s ${\protect \bf  z}^{(r)} = K\protect \big  ({\protect \bf  x}^{(r)},\cdot \protect \big  )$ allow to separate the \gls {datapoint}s using a \gls {linclass}. }{figure.6}{}}
\citation{Luxburg2007,Ng2001}
\citation{vaswani2017attention}
\citation{papoulis}
\citation{MLBasics}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  Left: Some undirected graph $\mathcal  {G}$ with three nodes $i=1,2,3$. Right: Laplacian matrix ${\bf  L}^{(\mathcal  {G})} \in \mathbb  {R}^{3 \times 3}$ of $\mathcal  {G}$.}}{51}{figure.7}\protected@file@percent }
\newlabel{fig_lap_mtx}{{7}{51}{ Left: Some undirected graph $\protect \mathcal  {G}$ with three nodes $i=1,2,3$. Right: Laplacian matrix ${\protect \bf  L}^{(\protect \mathcal  {G})} \in \protect \mathbb  {R}^{3 \times 3}$ of $\protect \mathcal  {G}$}{figure.7}{}}
\citation{Caruana:1997wk,JungGaphLassoSPL,CSGraphSelJournal}
\citation{MLBasics}
\citation{Ribeiro2016,rudin2019stop}
\newlabel{equ_def_lin_model_hypspace}{{7}{53}{\glossarytitle }{equation.0.7}{}}
\newlabel{equ_log_loss_gls}{{8}{54}{\glossarytitle }{equation.0.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Some \gls {lossfunc} $L\left (({\bf  x},y),h \right )$ for a fixed \gls {datapoint}, with \gls {feature} vector ${\bf  x}$ and \gls {label} $y$, and varying \gls {hypothesis} $h$. ML methods try to find (learn) a \gls {hypothesis} that incurs minimal \gls {loss}.}}{55}{figure.8}\protected@file@percent }
\newlabel{fig_loss_function_gls}{{8}{55}{Some \gls {lossfunc} $L\left (({\protect \bf  x},y),h \right )$ for a fixed \gls {datapoint}, with \gls {feature} vector ${\protect \bf  x}$ and \gls {label} $y$, and varying \gls {hypothesis} $h$. ML methods try to find (learn) a \gls {hypothesis} that incurs minimal \gls {loss}}{figure.8}{}}
\citation{LC,kay}
\citation{Abayomi2008DiagnosticsFM}
\citation{ShalevMLBook,MLBasics}
\citation{MLBasics}
\citation{BertsekasProb,GrayProbBook,Lapidoth09}
\citation{coverthomas}
\citation{JungNetExp2020}
\citation{nesterov04}
\citation{HornMatAnalysis}
\citation{HazanOCO,GDOptimalRakhlin2012}
\newlabel{equ_def_ogd}{{9}{60}{\glossarytitle }{equation.0.9}{}}
\citation{ShalevMLBook}
\citation{Bubeck2012}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces An instance of online GD that updates the \gls {modelparams} $w^{(t)}$ using the \gls {datapoint} ${\bf  z}^{(t)} = x^{(t)}$ arriving at time $t$. This instance uses the \gls {sqerrloss} $L\left ({\bf  z}^{(t)},w \right ) = (x^{(t)} - w)^{2}$. }}{61}{figure.9}\protected@file@percent }
\citation{Brockwell91}
\citation{doi:10.1137/0222052,10.1214/20-AOS1961}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces ML methods learn \gls {modelparams} ${\bf  w}$ by using some estimate $f({\bf  w})$ for the ultimate performance criterion $\bar  {f}({\bf  w})$. Using a \gls {probmodel}, one can use $f({\bf  w})$ to construct confidence intervals $\big  [ l^{({\bf  w})}, u^{({\bf  w})} \big  ]$ which contain $\bar  {f}({\bf  w})$ with high probability. The best plausible performance measure for a specific choice ${\bf  w}$ of \gls {modelparams} is $\tilde  {f}({\bf  w}) :=l^{({\bf  w})}$.}}{62}{figure.10}\protected@file@percent }
\citation{LearningKernelsBook}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces The parameter space $\mathcal  {W}$ of an ML \gls {model} $\mathcal  {H}$ consists of all feasible choices for the \gls {modelparams}. Each choice ${\bf  w}$ for the \gls {modelparams} selects a \gls {hypothesis} map $h^{({\bf  w})} \in  \mathcal  {H}$. }}{64}{figure.11}\protected@file@percent }
\newlabel{fig_param_space}{{11}{64}{The parameter space $\protect \mathcal  {W}$ of an ML \gls {model} $\protect \mathcal  {H}$ consists of all feasible choices for the \gls {modelparams}. Each choice ${\protect \bf  w}$ for the \gls {modelparams} selects a \gls {hypothesis} map $h^{({\protect \bf  w})} \in  \protect \mathcal  {H}$. }{figure.11}{}}
\citation{MLBasics}
\citation{PrivacyFunnel}
\citation{InfThDiffPriv}
\citation{LC}
\citation{KallenbergBook,BertsekasProb,BillingsleyProbMeasure,HalmosMeasure}
\citation{BertsekasProb}
\citation{BertsekasProb}
\citation{GrayProbBook,BillingsleyProbMeasure}
\citation{BoydConvexBook}
\citation{Condat2013}
\citation{ProximalMethods,Bauschke:2017}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces \Gls {projgd} augments a basic \gls {gradstep} with a projection back onto the constraint set $\mathcal  {W}$.}}{68}{figure.12}\protected@file@percent }
\newlabel{fig_projected_GD}{{12}{68}{\Gls {projgd} augments a basic \gls {gradstep} with a projection back onto the constraint set $\protect \mathcal  {W}$}{figure.12}{}}
\newlabel{equ_def_proj_generic}{{10}{68}{\glossarytitle }{equation.0.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces A generalized gradient step updates a vector ${\bf  w}$ by minimizing a penalized version of the function $f(\cdot )$. The penalty term is the squared Euclidean distance between the optimization variable ${\bf  w}'$ and the given vector ${\bf  w}$. }}{69}{figure.13}\protected@file@percent }
\newlabel{fig_proxoperator_opt}{{13}{69}{A generalized gradient step updates a vector ${\protect \bf  w}$ by minimizing a penalized version of the function $f(\cdot )$. The penalty term is the squared Euclidean distance between the optimization variable ${\protect \bf  w}'$ and the given vector ${\protect \bf  w}$. }{figure.13}{}}
\citation{RenyiInfo95}
\citation{BillingsleyProbMeasure}
\citation{GrayProbBook,BillingsleyProbMeasure}
\citation{BillingsleyProbMeasure,RudinBookPrinciplesMatheAnalysis,HalmosMeasure}
\citation{MLBasics}
\citation{PredictionLearningGames}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Three approaches to regularization: \gls {dataaug}, \gls {loss} penalization, and \gls {model} pruning (via constraints on \gls {modelparams}).  }}{72}{figure.14}\protected@file@percent }
\newlabel{fig_equiv_dataaug_penal}{{14}{72}{Three approaches to regularization: \gls {dataaug}, \gls {loss} penalization, and \gls {model} pruning (via constraints on \gls {modelparams}).  }{figure.14}{}}
\citation{MLBasics}
\citation{MLBasics}
\citation{SemiSupervisedBook}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces A scatterplot of some \gls {datapoint}s representing daily weather conditions in Finland. Each \gls {datapoint} is characterized by its minimum daytime temperature $x$ as the \gls {feature} and its maximum daytime temperature $y$ as the \gls {label}. The temperatures have been measured at the \gls {fmi} weather station \emph  {Helsinki Kaisaniemi} during 1.9.2024 - 28.10.2024.}}{75}{figure.15}\protected@file@percent }
\newlabel{fig_scatterplot_temp_FMI}{{15}{75}{A scatterplot of some \gls {datapoint}s representing daily weather conditions in Finland. Each \gls {datapoint} is characterized by its minimum daytime temperature $x$ as the \gls {feature} and its maximum daytime temperature $y$ as the \gls {label}. The temperatures have been measured at the \gls {fmi} weather station \protect \emph  {Helsinki Kaisaniemi} during 1.9.2024 - 28.10.2024}{figure.15}{}}
\citation{GolubVanLoanBook}
\citation{nesterov04,CvxBubeck2015}
\citation{nesterov04,CvxAlgBertsekas,CvxBubeck2015}
\citation{nesterov04,CvxAlgBertsekas,CvxBubeck2015}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Consider an \gls {objfunc} $f({\bf  w})$ that is $\beta $-smooth. Taking a \gls {gradstep}, with \gls {stepsize} $\eta = 1/\beta $ decreases the objective by at least $\frac  {1}{2\beta }\left \Vert {\nabla f({\bf  w}^{(k)})} \right \Vert _{2}^{2}$ \cite  {nesterov04,CvxAlgBertsekas,CvxBubeck2015}. Note that the \gls {stepsize} $\eta = 1/\beta $ becomes larger for smaller $\beta $. Thus, for smoother \gls {objfunc}s (those who are $\beta $-smooth with smaller $\beta $), we can take larger steps. }}{77}{figure.16}\protected@file@percent }
\newlabel{fig_gd_smooth}{{16}{77}{Consider an \gls {objfunc} $f({\protect \bf  w})$ that is $\beta $-smooth. Taking a \gls {gradstep}, with \gls {stepsize} $\eta = 1/\beta $ decreases the objective by at least $\protect \frac  {1}{2\beta }\left \Vert {\nabla f({\protect \bf  w}^{(k)})} \right \Vert _{2}^{2}$ \protect \cite  {nesterov04,CvxAlgBertsekas,CvxBubeck2015}. Note that the \gls {stepsize} $\eta = 1/\beta $ becomes larger for smaller $\beta $. Thus, for smoother \gls {objfunc}s (those who are $\beta $-smooth with smaller $\beta $), we can take larger steps. }{figure.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces  {\bf  Top.} Left: An undirected \gls {graph} $\mathcal  {G}$ with four nodes $i=1,2,3,4$, each representing a \gls {datapoint}. Right: Laplacian matrix ${\bf  L}^{(\mathcal  {G})} \in \mathbb  {R}^{4 \times 4}$ and its \gls {evd}. {\bf  Bottom.} Left: \Gls {scatterplot} of \gls {datapoint}s using the \gls {featurevec}s ${\bf  x}^{(i)} = \big  ( v^{(1)}_{i},v^{(2)}_{i} \big  )^{T}$. Right: Two \gls {eigenvector}s ${\bf  v}^{(1)},{\bf  v}^{(2)} \in \mathbb  {R}^{d}$ of the \gls {LapMat} ${\bf  L}^{(\mathcal  {G})}$ corresponding to the \gls {eigenvalue} $\lambda =0$. }}{79}{figure.17}\protected@file@percent }
\newlabel{fig_lap_mtx_specclustering}{{17}{79}{ {\protect \bf  Top.} Left: An undirected \gls {graph} $\protect \mathcal  {G}$ with four nodes $i=1,2,3,4$, each representing a \gls {datapoint}. Right: Laplacian matrix ${\protect \bf  L}^{(\protect \mathcal  {G})} \in \protect \mathbb  {R}^{4 \times 4}$ and its \gls {evd}. {\protect \bf  Bottom.} Left: \Gls {scatterplot} of \gls {datapoint}s using the \gls {featurevec}s ${\protect \bf  x}^{(i)} = \protect \big  ( v^{(1)}_{i},v^{(2)}_{i} \protect \big  )^{T}$. Right: Two \gls {eigenvector}s ${\protect \bf  v}^{(1)},{\protect \bf  v}^{(2)} \in \protect \mathbb  {R}^{d}$ of the \gls {LapMat} ${\protect \bf  L}^{(\protect \mathcal  {G})}$ corresponding to the \gls {eigenvalue} $\lambda =0$. }{figure.17}{}}
\citation{cohen1995time}
\citation{Li:2022aa}
\citation{TimeFrequencyAnalysisBoashash,MallatBook}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Left: A time signal consisting of two modulated Gaussian pulses. Right: Intensity plot of the spectrogram. }}{80}{figure.18}\protected@file@percent }
\newlabel{fig:spectrogram}{{18}{80}{Left: A time signal consisting of two modulated Gaussian pulses. Right: Intensity plot of the spectrogram. }{figure.18}{}}
\citation{AbbeSBM2018}
\citation{Bottou99}
\citation{nesterov04}
\citation{CvxAlgBertsekas}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Stochastic \gls {gd} for \gls {erm} approximates the \gls {gradient} $\DOTSB \sum@ \slimits@ _{r=1}^{m} \nabla _{{\bf  w}} L\left ({\bf  z}^{(r)},{\bf  w} \right )$ by replacing the sum over all \gls {datapoint}s in the \gls {trainset} (indexed by $r=1,\ldots  ,m$) with a sum over a randomly chosen subset $\mathcal  {B}\subseteq \{1,\ldots  ,m\}$.}}{82}{figure.19}\protected@file@percent }
\newlabel{fig_sgd_approx}{{19}{82}{Stochastic \gls {gd} for \gls {erm} approximates the \gls {gradient} $\DOTSB \sum@ \slimits@ _{r=1}^{m} \nabla _{{\protect \bf  w}} L\left ({\protect \bf  z}^{(r)},{\protect \bf  w} \right )$ by replacing the sum over all \gls {datapoint}s in the \gls {trainset} (indexed by $r=1,\protect \ldots  ,m$) with a sum over a randomly chosen subset $\protect \mathcal  {B}\subseteq \{1,\protect \ldots  ,m\}$}{figure.19}{}}
\citation{BertCvxAnalOpt,BertsekasNonLinProgr}
\citation{LampertNowKernel,Cristianini_Shawe-Taylor_2000,BishopBook}
\citation{MLBasics}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces The \gls {svm} learns a hypothesis (or classifier) $h^{({\bf  w})}$ with minimum average soft-margin \gls {hingeloss}. Minimizing this \gls {loss} is equivalent to maximizing the margin $\xi $ between the \gls {decisionboundary} of $h^{({\bf  w})}$ and each class of the \gls {trainset}.}}{84}{figure.20}\protected@file@percent }
\newlabel{fig_svm_gls}{{20}{84}{The \gls {svm} learns a hypothesis (or classifier) $h^{({\protect \bf  w})}$ with minimum average soft-margin \gls {hingeloss}. Minimizing this \gls {loss} is equivalent to maximizing the margin $\xi $ between the \gls {decisionboundary} of $h^{({\protect \bf  w})}$ and each class of the \gls {trainset}}{figure.20}{}}
\citation{HLEGTrustworhtyAI}
\citation{gallese2023ai,JunXML2020}
\citation{Shahriari2017,DatasheetData2021,10.1145/3287560.3287596}
\citation{pfau2024engineeringtrustworthyaideveloper}
\citation{ALTAIEU}
\citation{MLBasics}
\citation{ShalevMLBook}
\citation{VFLChapter}
\bibstyle{IEEEtran}
\bibdata{assets/Literature.bib}
\bibcite{RudinBook}{1}
\bibcite{RudinBookPrinciplesMatheAnalysis}{2}
\bibcite{GolubVanLoanBook}{3}
\bibcite{Golub1980}{4}
\bibcite{BertsekasProb}{5}
\bibcite{MLBasics}{6}
\bibcite{LC}{7}
\bibcite{GrayProbBook}{8}
\bibcite{SemiSupervisedBook}{9}
\bibcite{BoydConvexBook}{10}
\bibcite{JMLR:v22:18-694}{11}
\bibcite{Pelckmans2005}{12}
\bibcite{GDPR2016}{13}
\bibcite{EURegulation2018}{14}
\bibcite{coverthomas}{15}
\bibcite{Liu2021}{16}
\bibcite{PoisonGAN}{17}
\bibcite{silberschatz2019database}{18}
\bibcite{abiteboul1995foundations}{19}
\bibcite{hoberman2009data}{20}
\bibcite{ramakrishnan2002database}{21}
\bibcite{DatasheetData2021}{22}
\bibcite{Goodfellow-et-al-2016}{23}
\bibcite{kay}{24}
\bibcite{hastie01statisticallearning}{25}
\bibcite{HalmosMeasure}{26}
\bibcite{BillingsleyProbMeasure}{27}
\bibcite{BishopBook}{28}
\bibcite{GraphModExpFamVarInfWainJor}{29}
\bibcite{PredictionLearningGames}{30}
\bibcite{HazanOCO}{31}
\bibcite{Colin:2022aa}{32}
\bibcite{Zhang:2024aa}{33}
\bibcite{JunXML2020}{34}
\bibcite{Chen2018}{35}
\bibcite{rudin2019stop}{36}
\bibcite{Molnar2019}{37}
\bibcite{GradCamPaper}{38}
\bibcite{Gujarati2021}{39}
\bibcite{Dodge2003}{40}
\bibcite{Everitt2022}{41}
\bibcite{LearningKernelsBook}{42}
\bibcite{Ribeiro2016}{43}
\bibcite{pmlr-v54-mcmahan17a}{44}
\bibcite{FedProx2020}{45}
\bibcite{FlowSpecClustering2021}{46}
\bibcite{papoulis}{47}
\bibcite{Rasmussen2006Gaussian}{48}
\bibcite{ross2013first}{49}
\bibcite{ShalevMLBook}{50}
\bibcite{OnePixelAttack}{51}
\bibcite{ProximalMethods}{52}
\bibcite{RockNetworks}{53}
\bibcite{Luxburg2007}{54}
\bibcite{ClusteredFLTVMinTSP}{55}
\bibcite{Wain2019}{56}
\bibcite{BuhlGeerBook}{57}
\bibcite{LampertNowKernel}{58}
\bibcite{HFLChapter2020}{59}
\bibcite{Ng2001}{60}
\bibcite{vaswani2017attention}{61}
\bibcite{Caruana:1997wk}{62}
\bibcite{JungGaphLassoSPL}{63}
\bibcite{CSGraphSelJournal}{64}
\bibcite{Abayomi2008DiagnosticsFM}{65}
\bibcite{Lapidoth09}{66}
\bibcite{JungNetExp2020}{67}
\bibcite{nesterov04}{68}
\bibcite{HornMatAnalysis}{69}
\bibcite{GDOptimalRakhlin2012}{70}
\bibcite{Bubeck2012}{71}
\bibcite{Brockwell91}{72}
\bibcite{doi:10.1137/0222052}{73}
\bibcite{10.1214/20-AOS1961}{74}
\bibcite{PrivacyFunnel}{75}
\bibcite{InfThDiffPriv}{76}
\bibcite{KallenbergBook}{77}
\bibcite{Condat2013}{78}
\bibcite{Bauschke:2017}{79}
\bibcite{RenyiInfo95}{80}
\bibcite{CvxBubeck2015}{81}
\bibcite{CvxAlgBertsekas}{82}
\bibcite{cohen1995time}{83}
\bibcite{Li:2022aa}{84}
\bibcite{TimeFrequencyAnalysisBoashash}{85}
\bibcite{MallatBook}{86}
\bibcite{AbbeSBM2018}{87}
\bibcite{Bottou99}{88}
\bibcite{BertCvxAnalOpt}{89}
\bibcite{BertsekasNonLinProgr}{90}
\bibcite{Cristianini_Shawe-Taylor_2000}{91}
\bibcite{HLEGTrustworhtyAI}{92}
\bibcite{gallese2023ai}{93}
\bibcite{Shahriari2017}{94}
\bibcite{10.1145/3287560.3287596}{95}
\bibcite{pfau2024engineeringtrustworthyaideveloper}{96}
\bibcite{ALTAIEU}{97}
\bibcite{VFLChapter}{98}
\gdef \@abspage@last{106}

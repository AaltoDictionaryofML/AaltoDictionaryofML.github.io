% !TeX spellcheck = en_GB

\section*{Lists of Symbols}
\addcontentsline{toc}{section}{List of Symbols}
%\label{ch_list_of_symbols}



\vspace*{-2mm}
\subsection*{Sets and \Glspl{function}} 

\begin{align} 
	&a \in \mathcal{A} & \quad & \parbox{.85\textwidth}{The object $a$ is an element of the set $\mathcal{A}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&a \defeq b & \quad & \mbox{We define $a$ as $b$. }\nonumber \\[2mm] \hline \nonumber\\[-5mm] % Or 'a is defined as b'
	&|\mathcal{A}| & \quad & \mbox{The cardinality (i.e., number of elements) of a finite set $\mathcal{A}$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathcal{A} \subseteq \mathcal{B}& \quad & \mbox{$\mathcal{A}$ is a subset of $\mathcal{B}$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathcal{A} \subset \mathcal{B}& \quad & \mbox{$\mathcal{A}$ is a strict subset of $\mathcal{B}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathcal{A} \times \mathcal{B} & \quad & \mbox{The Cartesian product of the sets $\mathcal{A}$ and $\mathcal{B}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{N} & \quad & \mbox{The natural numbers $1, \,2, \,\dots$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}  &\quad &\mbox{The real numbers $x$ \cite{RudinBook}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}_{+}  &\quad &\mbox{The nonnegative real numbers $x\geq0$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}_{++}  &\quad &\mbox{The positive real numbers $x> 0$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\{0,1\}& \quad & \mbox{The set consisting of the two real numbers $0$ and $1$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&[0,1] &\quad &\mbox{The closed interval consisting of $x \in \mathbb{R}$ for which $0 \leq x \leq 1$. }\nonumber 
\end{align} 

\newpage
\begin{align}
    	&\argmin_{\weights \in \mathcal{C}} f(\weights) &\quad &\parbox{.70\textwidth}{The set of $\weights \in \mathcal{C}$ 
		minimizing the real-valued \gls{function} $f: \mathcal{C} \rightarrow \mathbb{R}$. 
    		\\ See also: \gls{function}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
    	&\sphere{\nrnodes} &\quad &\parbox{.70\textwidth}{The set of unit-\gls{norm} \glspl{vector} in $\mathbb{R}^{\nrnodes+1}$.
    		\\ See also: \gls{norm}, \gls{vector}. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\exp\,(a) &\quad &\parbox{.70\textwidth}{The exponential \gls{function} evaluated at the real number $a \in \mathbb{R}$.
		\\ See also: \gls{function}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm] % Why only real number? Exponential function is often used with complex numbers.
	&\log a &\quad &\mbox{The logarithm of the positive number $a \in \mathbb{R}_{++}$.  } \nonumber \\[2mm] \hline \nonumber\\[-5mm] % Again, isn't this usually just non-zero complex number (If we are fine having  a complex co-domain).
	&f(\cdot)\!:\!\mathcal{A}\!\rightarrow\!\mathcal{B} :  a \!\mapsto\!f(a) &\quad &\parbox{.70\textwidth}{
	 	A \gls{function} (or \gls{map}) from a set $\mathcal{A}$ to a set $\mathcal{B}$, which assigns to each input 
	 	$a \in \mathcal{A}$ a well-defined \gls{output} $f(a) \in \mathcal{B}$.
	 	The set $\mathcal{A}$ is the \gls{domain} of the \gls{function} $f$ and the set $\mathcal{B}$ is the 
	 	\gls{co-domain} of $f$. \Gls{ml} aims to learn a \gls{function} $\hypothesis$ that maps \glspl{feature} 
	 	$\featurevec$ of a \gls{datapoint} to a \gls{prediction} $\hypothesis(\featurevec)$ for its \gls{label} $\truelabel$.
		\\ See also: \gls{domain}, \gls{co-domain}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\operatorname{epi}(f)  & \quad & \parbox{.70\textwidth}{The \gls{epigraph} of a real-valued \gls{function} 
	 	$f: \mathbb{R}^{\featuredim}\rightarrow \mathbb{R}$.
		\\ See also: \gls{epigraph}, \gls{function}. } \nonumber \\[2mm]  \hline \nonumber\\[-5mm]
	&\big( a_{\sampleidx} \big)_{\sampleidx \in \mathbb{N}},
		\big( a^{(\sampleidx)} \big)_{\sampleidx \in \mathbb{N}},\big\{ a^{(\sampleidx)} \big\}_{\sampleidx \in \mathbb{N}} 
		& \quad & \parbox{.70\textwidth}{A \gls{sequence} of elements.
		\\ See also: \gls{sequence}. } \nonumber 
\end{align} 

\begin{align}
	&\indicatorfunc{\mathcal{A}}(x) & \quad & \parbox{.70\textwidth}{The \gls{indicatorfunc} of a set $\mathcal{A}$ 
		delivers $f(x)=1$ for any $x \in \mathcal{A}$ and $f(x)=0$ otherwise.
		\\ See also: \gls{indicatorfunc}. }  \nonumber \\[2mm]  \hline \nonumber\\[-5mm]
	&\frac{\partial f(\weight_{1}, \,\ldots, \,\weight_{\nrfeatures})}{\partial \weight_{\featureidx}} & \quad & \parbox{.70\textwidth}{The \gls{partialderivative} 
		(if it exists) of a real-valued \gls{function} $f: \mathbb{R}^{\featuredim}\rightarrow \mathbb{R}$ with respect 
		to\ $\weight_{\featureidx}$\cite[Ch. 9]{RudinBookPrinciplesMatheAnalysis}.
		\\ See also: \gls{partialderivative}, \gls{function}. } \nonumber \\[2mm]  \hline \nonumber\\[-5mm]
	 &\nabla f(\weights) & \quad & \parbox{.70\textwidth}{The \gls{gradient} of a \gls{differentiable} real-valued \gls{function} 
	 	$f: \mathbb{R}^{\featuredim}\rightarrow \mathbb{R}$ is the \gls{vector} 
	 	$\nabla f(\weights) = \big( {\partial f}/{\partial \weight_{1}}, \,\ldots, \,{\partial f}/{\partial \weight_{\featuredim}}  \big)\,^{T} \in \mathbb{R}^{\featuredim}$ 
		\cite[Ch. 9]{RudinBookPrinciplesMatheAnalysis}.
		\\ See also: \gls{gradient}, \gls{differentiable}, \gls{function}, \gls{vector}.}   \nonumber \\[2mm]  \hline \nonumber\\[-5mm]
	&\bd{\cluster} & \quad & \parbox{.70\textwidth}{The \gls{boundary} of a subset $\cluster$ of some \gls{metricspace}.
		\\ See also: \gls{boundary}, \gls{metricspace}. }   \nonumber \\[2mm]  \hline \nonumber\\[-5mm]
	&\identityop & \quad & \parbox{.70\textwidth}{The identity \gls{operator}.
		\\ See also: \gls{operator}.}   \nonumber
\end{align} 



\subsection*{\Glspl{vectorspace}} 

\begin{align} 
	 &\featurevec=\big(\feature_{1}, \,\ldots, \,\feature_{\featuredim})\,^{T} &\quad & \parbox{.75\textwidth}{A \gls{vector} of length $\featuredim$, with its 
		$\featureidx$th entry being $\feature_{\featureidx}$.
		\\ See also: \gls{vector}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}^{\featuredim} & \quad &  \parbox{.75\textwidth}{The set of all \glspl{vector} $\featurevec=\big(\feature_{1}, \,\ldots, \,\feature_{\featurelen}\big)\,^{T}$ 
		consisting of $\featuredim$ real-valued entries $\feature_{1}, \,\ldots, \,\feature_{\featurelen} \in \mathbb{R}$.
		\\ See also: \gls{vector}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{I}_{\modelidx \times \featuredim}  & \quad &  \parbox{.75\textwidth}{A generalized identity \gls{matrix} 
		with $\modelidx$ rows and $\featuredim$ columns. The entries of $\mathbf{I}_{\modelidx \times \featuredim} \in \mathbb{R}^{\modelidx \times \featuredim}$ 
		are equal to $1$ along the main diagonal and $0$ otherwise. 
		\\ See also: \gls{matrix}. }\nonumber \\[2mm] \hline \nonumber\\[-5mm] %For example, $\mathbf{I}_{1 \times 2} = \big(1, 0\big)$ and $\mathbf{I}_{2 \times 1}= \begin{pmatrix} 1 \\ 0 \end{pmatrix}$.} 
	&\mathbf{I}_{\dimlocalmodel}, \mathbf{I} & \quad &  \parbox{.75\textwidth}{A square identity 
		\gls{matrix} of size $\dimlocalmodel \times \dimlocalmodel$. If the size is clear from context, we drop the subscript.
		\\ See also: \gls{matrix}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\normgeneric{\featurevec}{2}  &\quad & \parbox{.75\textwidth}{The Euclidean (or $\ell_{2}$) \gls{norm} of the \gls{vector} 
		$\featurevec=\big(\feature_{1}, \,\ldots, \,\feature_{\featurelen}\big)\,^{T} \in \mathbb{R}^{\featuredim}$ defined as 
		$\| \featurevec \|_{2} \defeq \sqrt{\sum_{\featureidx=1}^{\featuredim} \feature_{\featureidx}^{2}}$.
		\\ See also: \gls{norm}, \gls{vector}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	&\normgeneric{\featurevec}{}  & \quad &  \parbox{.75\textwidth}{Some \gls{norm} of the \gls{vector} $\featurevec \in \mathbb{R}^{\featuredim}$ \cite{GolubVanLoanBook}. 
		Unless otherwise specified, we mean the \gls{euclidnorm} $\normgeneric{\featurevec}{2}$.
		\\ See also: \gls{norm}, \gls{vector}, \gls{euclidnorm}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\featurevec\,^{T} &\quad & \parbox{.75\textwidth}{The \gls{transpose} of a \gls{vector} $\featurevec \in \mathbb{R}^{\dimlocalmodel}$ 
		is a \gls{matrix} $\featurevec\,^{T} \in \mathbb{R}^{1 \times \dimlocalmodel}$ with the \gls{vector} as its single row.
		\\ See also: \gls{transpose}, \gls{vector}, \gls{matrix}. } \nonumber 
\end{align} 

\newpage
\begin{align} 
	&\mathbf{X}\,^{T} &\quad & \parbox{.75\textwidth}{The \gls{transpose} of a \gls{matrix} $\mathbf{X} \in \mathbb{R}^{\samplesize \times \featurelen}$. 
		A square real-valued \gls{matrix} $\mathbf{X} \in \mathbb{R}^{\samplesize \times \samplesize}$ 
		is called symmetric if $\mathbf{X} = \mathbf{X}\,^{T}$. 
		\\ See also: \gls{transpose}, \gls{matrix}. }  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{X}^{-1} &\quad & \parbox{.75\textwidth}{The \gls{inverse} of a \gls{matrix} $\mathbf{X} \in \mathbb{R}^{\featurelen \times \featurelen}$.
		\\ See also: \gls{inverse}, \gls{matrix}. }  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{0}= \big(0, \,\ldots, \,0\big)\,^{T}  & \quad &  \parbox{.75\textwidth}{The \gls{vector} in $\mathbb{R}^{\dimlocalmodel}$ with each entry equal to zero.
		\\ See also: \gls{vector}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{1}= \big(1, \,\ldots, \,1\big)\,^{T}  & \quad &  \parbox{.75\textwidth}{The \gls{vector} in $\mathbb{R}^{\dimlocalmodel}$ with each entry equal to one.
		\\ See also: \gls{vector}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\big(\vv\,^{T},\vw\,^{T} \big)\,^{T}  & \quad &  \parbox{.75\textwidth}{The \gls{vector} of length $\featurelen+\featurelen'$ 
		obtained by concatenating the entries of the \gls{vector} $\vv \in \mathbb{R}^{\featurelen}$ with the entries of $\vw \in \mathbb{R}^{\featurelen'}$.
		\\ See also: \gls{vector}. } \nonumber \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\linspan{\mathbf{B}}  & \quad &  \parbox{.75\textwidth}{The span of a \gls{matrix} $\mathbf{B} \in \mathbb{R}^{a \times b}$, 
		which is the \gls{subspace} of all linear combinations of the columns of $\mathbf{B}$ such that
		$\linspan{\mathbf{B}} = \big\{  \mathbf{B} \va : \va \in \mathbb{R}^{b} \big\} \subseteq \mathbb{R}^{a}$. 
		\\ See also: \gls{matrix}, \gls{subspace}. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\nullspace{\mA}& \quad &  \parbox{.75\textwidth}{The \gls{nullspace} of a \gls{matrix} $\mathbf{A} \in \mathbb{R}^{a \times b}$, 
		which is the \gls{subspace} of \glspl{vector} $\va \in \mathbb{R}^{b}$ such that $\mA \va=\mathbf{0}$. 
		\\ See also: \gls{nullspace}, \gls{matrix}, \gls{subspace}, \gls{vector}. }\nonumber 
\end{align} 

\newpage
\begin{align} 
	&\determinant{\mC} &\quad & \parbox{.85\textwidth}{The \gls{det} of the \gls{matrix} $\mC$.
		\\ See also: \gls{det}, \gls{matrix}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\tr{\mC} &\quad & \parbox{.85\textwidth}{The \gls{trace} of the \gls{matrix} $\mC$.
		\\ See also: \gls{trace}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{A} \otimes \mathbf{B} &\quad & \parbox{.85\textwidth}{The \gls{kroneckerproduct} 
		of the \glspl{matrix} $\mathbf{A}$ and $\mathbf{B}$ \cite{Golub1980}.
		\\ See also: \gls{kroneckerproduct}, \gls{matrix}. }  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\va \geq \vb &\quad & \parbox{.85\textwidth}{Entrywise inequality between 
		\glspl{vector} $\va,\vb\!\in\!\mathbb{R}^{\featuredim}$, i.e.,
		$$a_{\featureidx} \geq b_{\featureidx} \quad \mbox{for }\featureidx=1,\,\ldots,\,\featuredim.$$
		See also: \gls{vector}. }  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\closedball{\featurevec}{\varepsilon} &\quad & \parbox{.85\textwidth}{A closed ball in some \gls{metricspace} 
		that contains all points with a \gls{distance} from $\featurevec$ less or equal to $\varepsilon$.
		\\ See also: \gls{metricspace}, \gls{distance}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\normgeneric{\weights}{\hilbertspace} &\quad & \parbox{.85\textwidth}{The \gls{norm} of a 
	 	\gls{vector} $\weights \in \hilbertspace$ in a \gls{hilbertspace} $\hilbertspace$.
		\\ See also: \gls{norm}, \gls{hilbertspace}. } \nonumber
\end{align} 



\newpage
\subsection*{Probability Theory} 

\begin{align}
	&\featurevec \sim \probdist  &\quad & \parbox{.85\textwidth}{The \gls{rv} $\featurevec$ is distributed according to 
		the \gls{probdist} $\probdist$ \cite{klenke2020probability}, \cite{BillingsleyProbMeasure}.
		\\ See also: \gls{rv}, \gls{probdist}.}  \nonumber \\[2mm] \hline \nonumber\\[-5mm]  
	&\expect_{\probdist} \{ f(\datapoint) \}  &\quad & \parbox{.85\textwidth}{The \gls{expectation} of an \gls{rv} $f(\datapoint)$ that 
		is obtained by applying a deterministic \gls{function} $f$ to an \gls{rv}
		$\datapoint$ whose \gls{probdist} is $\probdist$. 
		If the \gls{probdist} is clear from context, we just write $\expect \{ f(\datapoint) \}$. 
		\\ See also: \gls{expectation}, \gls{rv}, \gls{function}, \gls{probdist}.}  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\cov{x}{y} &\quad & \parbox{.85\textwidth}{The \gls{covariance} between two real-valued \glspl{rv} defined 
		over a common \gls{probspace}. 
		\\ See also: \gls{covariance}, \gls{rv}, \gls{probdist}.}  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\probdist^{(\featurevec,\truelabel)} &\quad & \parbox{.85\textwidth}{A (joint) \gls{probdist} of an \gls{rv} 
		whose \glspl{realization} are \glspl{datapoint} with \glspl{feature} $\featurevec$ and \gls{label} $\truelabel$.
		\\ See also: \gls{probdist}, \gls{rv}, \gls{realization}, \gls{datapoint}, \gls{feature}, 
		\gls{label}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\probdist^{(\truelabel|\featurevec)} &\quad & \parbox{.85\textwidth}{A \gls{condprobdist} 
		of an \gls{rv} $\truelabel$ given (or conditioned on) the value of another \gls{rv} $\featurevec$ \cite[Sec.\ 3.5]{BertsekasProb}. 
		\\ See also: \gls{condprobdist}, \gls{rv}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\prob{\mathcal{A}} &\quad & \parbox{.85\textwidth}{The \gls{probability} of the \gls{measurable} \gls{event} $\mathcal{A}$. 
		\\ See also: \gls{probability}, \gls{measurable}, \gls{event}.} \nonumber 
\end{align} 

\newpage
\begin{align} 
	&\mgf{x}{t} &\quad & \parbox{.85\textwidth}{The \gls{mgf} of an \gls{rv} $x$.
		\\ See also: \gls{probdist}, \gls{pdf}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\probdist^{(\dataset)} &\quad & \parbox{.85\textwidth}{The \gls{empiricaldistribution} of a \gls{dataset} $\dataset$.
		\\ See also: \gls{empiricaldistribution}, \gls{dataset}, \gls{bootstrap}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\probdist^{(\featurevec;\weights)} &\quad & \parbox{.85\textwidth}{A parameterized 
		\gls{probdist} of an \gls{rv} $\featurevec$. The \gls{probdist} depends on 
		a \gls{parameter} \gls{vector} $\weights$. For example, $\probdist^{(\featurevec;\weights)}$ 
		could be a \gls{mvndist} with the \gls{parameter} \gls{vector} $\weights$ given 
		by the entries of the \gls{mean} \gls{vector} $\expect \{ \featurevec \}$ 
		and the \gls{covmtx} $\expect \bigg \{ \big( \featurevec - \expect \{ \featurevec \}\big) \big( \featurevec - \expect \{ \featurevec \}\big)\,^{T}  \bigg\}$.
		\\ See also: \gls{probdist}, \gls{parameter}, \gls{probmodel}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathcal{N}(\mu, \sigma^{2}) &\quad & \parbox{.85\textwidth}{The \gls{probdist} of a 
		\gls{gaussrv} $\feature \in \mathbb{R}$ with \gls{mean} (or \gls{expectation}) $\mu= \expect \{ \feature \}$ 
		and \gls{variance} $\sigma^{2} =   \expect \big\{  (  \feature - \mu )^2 \big\}$.
		\\ See also: \gls{probdist}, \gls{gaussrv}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathcal{N}(\meanvecgeneric, \mathbf{C}) &\quad & \parbox{.85\textwidth}{The \gls{mvndist} of a \gls{vector}-valued 
		\gls{gaussrv} $\featurevec \in \mathbb{R}^{\featuredim}$ with \gls{mean} (or \gls{expectation}) $\meanvecgeneric= \expect \{ \featurevec \}$ 
		and \gls{covmtx} $\mathbf{C} =  \expect \big\{ \big( \featurevec - \meanvecgeneric \big)\big( \featurevec - \meanvecgeneric \big)\,^{T} \big\}$.
		\\ See also: \gls{mvndist}, \gls{gaussrv}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]  
	&\simplex{\nrcluster} &\quad & \parbox{.85\textwidth}{The \gls{probabilitysimplex}, which consists of all \glspl{vector} 
		$\vp = \big( p_{1}, \,\ldots, \,p_{\nrcluster} \big)\,^{T} \in \mathbb{R}^{\nrcluster}$ 
	        with nonnegative entries that sum to one, i.e., $p_{\clusteridx} \geq 0$ for $\clusteridx=1, \,\ldots, \,\nrcluster$ and
		$\sum_{\clusteridx=1}^{\nrcluster} p_{\clusteridx} = 1$.
		\\ See also: \gls{pmf}.}  \nonumber
\end{align} 

\newpage
\begin{align} 
	&\entropy{x} &\quad & \parbox{.85\textwidth}{The \gls{entropy} of a \gls{discreteRV} $x$. 
		\\ See also: \gls{entropy}, \gls{discreteRV}.}  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\samplespace &\quad & \parbox{.85\textwidth}{A \gls{samplespace} of all possible \glspl{outcome} of a \gls{randomexperiment}. 
		\\ See also: \gls{event}.}  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\sigmaalgebra &\quad & \parbox{.85\textwidth}{A collection of \gls{measurable} subsets of a \gls{samplespace} $\samplespace$. 
		\\ See also: \gls{samplespace}, \gls{event}.}  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathcal{P} &\quad & \parbox{.85\textwidth}{A \gls{probspace} that consists of a \gls{samplespace} $\samplespace$, a 
		\gls{sigmaalgebra} $\sigmaalgebra$ of \gls{measurable} subsets of $\samplespace$, and a \gls{probdist} $\prob{ \cdot}$.
		\\ See also: \gls{samplespace}, \gls{measurable}, \gls{probdist}.} \nonumber                                       
\end{align}



\newpage
\subsection*{\Gls{ml}}

\begin{align}
%	\datapoint \quad\quad & \parbox{.75\textwidth}{A \gls{datapoint} which is characterized by several properties that we 
%		divide into low-level properties (= \gls{feature}s) and high-level properties (= \gls{label}s) \cite[Ch. 2]{MLBasics}.}    \nonumber   \\[4mm] 
	&\sampleidx &\quad & \parbox{.90\textwidth}{An index $\sampleidx=1, \,2, \,\ldots$ that 
		enumerates \glspl{datapoint}.
		\\ See also: \gls{datapoint}. }   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\samplesize &\quad &\parbox{.90\textwidth}{The number of \glspl{datapoint} in (i.e., the size of) a \gls{dataset}.
		\\ See also: \gls{datapoint}, \gls{dataset}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	&\dataset &\quad & \parbox{.90\textwidth}{A \gls{dataset} $\dataset = \{ \datapoint^{(1)}, \,\ldots, \,\datapoint^{(\samplesize)} \}$ 
		is a list of individual \glspl{datapoint} $\datapoint^{(\sampleidx)}$, for $\sampleidx=1, \,\ldots, \,\samplesize$.
		\\ See also: \gls{dataset}, \gls{datapoint}.}   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\featurelen &\quad &\parbox{.90\textwidth}{The number of \glspl{feature} that characterize a \gls{datapoint}.
		\\ See also: \gls{feature}, \gls{datapoint}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\feature_{\featureidx} &\quad &\parbox{.90\textwidth}{The $\featureidx$th \gls{feature} of a \gls{datapoint}. The first \gls{feature} 
		is denoted by $\feature_{1}$, the second \gls{feature} $\feature_{2}$, and so on.
		\\ See also: \gls{datapoint}, \gls{feature}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	&\featurevec &\quad &\parbox{.90\textwidth}{The \gls{featurevec} $\featurevec=\big(\feature_{1}, \,\ldots, \,\feature_{\featuredim}\big)\,^{T}$ of 
		a \gls{datapoint}. The \gls{vector}'s entries are the individual \glspl{feature} of a \gls{datapoint}.
		\\ See also: \gls{featurevec}, \gls{datapoint}, \gls{vector}, \gls{feature}. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\featurespace &\quad & \parbox{.90\textwidth}{The \gls{featurespace} $\featurespace$ is 
		the set of all possible values that the \glspl{feature} $\featurevec$ of a \gls{datapoint} can take on.
		\\ See also: \gls{featurespace}, \gls{feature}, \gls{datapoint}.} \nonumber 
\end{align}        

\begin{align}
	&\rawfeaturevec &\quad &\parbox{.85\textwidth}{Instead of the symbol $\featurevec$, we 
		sometimes use $\rawfeaturevec$ as another symbol to denote a \gls{vector} whose entries 
		are the individual \glspl{feature} of a \gls{datapoint}. We need two 
		different symbols to distinguish between raw and learned \glspl{feature} \cite[Ch. 9]{MLBasics}.
		\\ See also: \gls{vector}, \gls{feature}, \gls{datapoint}. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\featurevec^{(\sampleidx)} &\quad &\parbox{.85\textwidth}{The \gls{featurevec} of the $\sampleidx$th \gls{datapoint} within a \gls{dataset}.
		\\ See also: \gls{featurevec}, \gls{datapoint}, \gls{dataset}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\feature_{\featureidx}^{(\sampleidx)} &\quad &\parbox{.85\textwidth}{The $\featureidx$th \gls{feature} of the $\sampleidx$th 
		\gls{datapoint} within a \gls{dataset}.
		\\ See also: \gls{feature}, \gls{datapoint}, \gls{dataset}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\batch &\quad &\parbox{.85\textwidth}{A mini-\gls{batch} (or subset) of randomly chosen \glspl{datapoint}.
		\\ See also: \gls{batch}, \gls{datapoint}. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\batchsize &\quad &\parbox{.85\textwidth}{The size of (i.e., the number of \glspl{datapoint} in) a mini-\gls{batch}.
		\\ See also: \gls{datapoint}, \gls{batch}. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\truelabel &\quad &\parbox{.85\textwidth}{The \gls{label} (or quantity of interest) of a \gls{datapoint}.
		\\ See also: \gls{label}, \gls{datapoint}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\truelabel^{(\sampleidx)} &\quad &\parbox{.85\textwidth}{The \gls{label} of the $\sampleidx$th \gls{datapoint}.
		\\ See also: \gls{label}, \gls{datapoint}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\big(\featurevec^{(\sampleidx)},\truelabel^{(\sampleidx)}\big) &\quad &\parbox{.85\textwidth}{The \glspl{feature} and \gls{label} of the $\sampleidx$th \gls{datapoint}.
		\\ See also: \gls{feature}, \gls{label}, \gls{datapoint}. }\nonumber 
\end{align}                  

\begin{align}
	&\labelspace  &\quad & \parbox{.90\textwidth}{The \gls{labelspace} $\labelspace$ of 
		an \gls{ml} method consists of all potential \gls{label} values that a \gls{datapoint} can 
		carry. The nominal \gls{labelspace} might be larger than the set of different \gls{label} 
		values arising in a given \gls{dataset} (e.g., a \gls{trainset}). \Gls{ml} problems 
		(or methods) using a numeric \gls{labelspace}, such as $\labelspace=\mathbb{R}$ 
		or $\labelspace=\mathbb{R}^{3}$, are referred to as \gls{regression} problems (or methods). \Gls{ml} 
		problems (or methods) that use a discrete \gls{labelspace}, such as $\labelspace=\{0,1\}$ or $\labelspace=\{\mbox{\emph{cat}},\mbox{\emph{dog}},\mbox{\emph{mouse}}\}$, 
		are referred to as \gls{classification} problems (or methods).
		\\ See also: \gls{labelspace}, \gls{ml}, \gls{label}, \gls{datapoint},  \gls{dataset}, \gls{trainset}, 
		\gls{regression}, \gls{classification}.}  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\lrate  &\quad & \parbox{.90\textwidth}{\Gls{learnrate} (or \gls{stepsize}) used by \glspl{gdmethod}.
		\\ See also: \gls{learnrate}, \gls{stepsize}, \gls{gdmethod}. }  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\hypothesis(\cdot)  &\quad &\parbox{.90\textwidth}{A \gls{hypothesis} \gls{map} that maps the \glspl{feature} of a \gls{datapoint} 
		to a \gls{prediction} $\hat{\truelabel}=\hypothesis(\featurevec)$ for its \gls{label} $\truelabel$.
		\\ See also: \gls{hypothesis}, \gls{map}, \gls{feature}, \gls{datapoint}, \gls{prediction}, \gls{label}. }  	 \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	 &\labelspace^{\featurespace} &\quad & \parbox{.90\textwidth}{Given two sets $\featurespace$ and $\labelspace$, we denote by $\labelspace^{\featurespace}$ 
	 	the set of all possible \gls{hypothesis} \glspl{map} $\hypothesis: \featurespace \rightarrow \labelspace$.
		\\ See also: \gls{hypothesis}, \gls{map}. } 	 \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\hypospace  &\quad & \parbox{.90\textwidth}{A \gls{hypospace} or \gls{model} used by an \gls{ml} method. 
		The \gls{hypospace} consists of different \gls{hypothesis} \glspl{map} $\hypothesis: \featurespace \rightarrow \labelspace$ between which 
		the \gls{ml} method must choose.
		\\ See also: \gls{hypospace}, \gls{model}, \gls{ml}, \gls{hypothesis}, \gls{map}. }   \nonumber 
\end{align}     

\begin{align}
	&\effdim{\hypospace}  &\quad & \parbox{.80\textwidth}{The \gls{effdim} of a \gls{hypospace} $\hypospace$.
		\\ See also: \gls{effdim}, \gls{hypospace}. }   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\biasterm^2 &\quad &\parbox{.80\textwidth}{
		The squared \gls{bias} of a learned \gls{hypothesis} $\learnthypothesis$, or its \glspl{parameter}. Note that $\learnthypothesis$ 
		becomes an \gls{rv} if it is learned from \glspl{datapoint} being \glspl{rv} themselves.
		\\ See also: \gls{bias}, \gls{hypothesis}, \gls{parameter}, \gls{rv}, \gls{datapoint}. } \nonumber  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\varianceterm &\quad &\parbox{.80\textwidth}{The \gls{variance} of a learned 
	  	\gls{hypothesis} $\learnthypothesis$, or its \glspl{parameter}. Note that $\learnthypothesis$ 
	  	becomes an \gls{rv} if it is learned from \glspl{datapoint} being \glspl{rv} themselves.
		\\ See also: \gls{variance}, \gls{hypothesis}, \gls{parameter}, \gls{rv}, \gls{datapoint}. } \nonumber \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\lossfunc{(\featurevec,\truelabel)}{\hypothesis}  &\quad & \parbox{.80\textwidth}{The \gls{loss} incurred by predicting the 
		\gls{label} $\truelabel$ of a \gls{datapoint} using the \gls{prediction} $\hat{\truelabel}=h(\featurevec)$. The 
		\gls{prediction} $\hat{\truelabel}$ is obtained by evaluating the \gls{hypothesis} $\hypothesis \in \hypospace$ for 
		the \gls{featurevec} $\featurevec$ of the \gls{datapoint}.
		\\ See also: \gls{loss}, \gls{label}, \gls{datapoint}, \gls{prediction}, \gls{hypothesis}, 
		\gls{featurevec}. }    \nonumber  \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	&\valerror &\quad &\parbox{.80\textwidth}{The \gls{valerr} of a \gls{hypothesis} $\hypothesis$, which is its 
		average \gls{loss} incurred over a \gls{valset}.
		\\ See also: \gls{valerr}, \gls{hypothesis}, \gls{loss}, \gls{valset}. }  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\emperror\big(\hypothesis| \dataset \big) &\quad &\parbox{.80\textwidth}{The \gls{emprisk}, or average \gls{loss}, 
		incurred by the \gls{hypothesis} $\hypothesis$ on a \gls{dataset} $\dataset$.
		\\ See also: \gls{emprisk}, \gls{loss}, \gls{hypothesis}, \gls{dataset}. } \nonumber 
\end{align}     

\begin{align}                          
	&\trainerror &\quad &\parbox{.85\textwidth}{The \gls{trainerr} of a \gls{hypothesis} $\hypothesis$, which is its 
		average \gls{loss} incurred over a \gls{trainset}.
		\\ See also: \gls{trainerr}, \gls{hypothesis}, \gls{loss}, \gls{trainset}. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\timeidx &\quad &\parbox{.85\textwidth}{A discrete-time index $\timeidx=0, \,1, \,\ldots$ used to 
		enumerate sequential \glspl{event} (or time instants). 
		\\ See also: \gls{event}. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\taskidx &\quad &\parbox{.85\textwidth}{An index that enumerates
		\glspl{learningtask} within a \gls{multitask learning} problem.
		\\ See also: \gls{learningtask}, \gls{multitask learning}. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\regparam &\quad &\parbox{.85\textwidth}{A \gls{regularization} \gls{parameter} that controls 
		the amount of \gls{regularization}.
		\\ See also: \gls{regularization}, \gls{parameter}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\eigval{\featureidx}\big( \mathbf{Q} \big) &\quad &\parbox{.85\textwidth}{The $\featureidx$th 
		\gls{eigenvalue} (sorted in either ascending or descending order) of a \gls{psd} \gls{matrix} $\mathbf{Q}$. We also 
		use the shorthand $\eigval{\featureidx}$ if the corresponding \gls{matrix} is clear from context.
		\\ See also: \gls{eigenvalue}, \gls{psd}, \gls{matrix}. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\actfun(\cdot) &\quad &\parbox{.85\textwidth}{The \gls{actfun} used by an artificial neuron within an \gls{ann}.
		\\ See also: \gls{actfun}, \gls{ann}. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\decreg{\hat{\truelabel}} &\quad &\parbox{.85\textwidth}{A \gls{decisionregion} within a \gls{featurespace}.
		\\ See also: \gls{decisionregion}, \gls{featurespace}. }\nonumber
\end{align}     

\begin{align} 
	&\weights  &\quad & \parbox{.85\textwidth}{A \gls{parameter} \gls{vector} $\weights = \big(\weight_{1}, \,\ldots, \,\weight_{\featuredim}\big)\,^{T}$ 
		of a \gls{model}, e.g., the \glspl{weight} of a \gls{linmodel} or an \gls{ann}.
		\\ See also: \gls{parameter}, \gls{vector}, \gls{model}, \gls{weight}, \gls{linmodel}, \gls{ann}. }     \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\hypothesis^{(\weights)}(\cdot)  &\quad &\parbox{.85\textwidth}{A \gls{hypothesis} \gls{map} that involves tunable \glspl{modelparam} 
		$\weight_{1}, \,\ldots, \,\weight_{\featuredim}$ stacked into the \gls{vector} $\weights=\big(\weight_{1}, \,\ldots, \,\weight_{\featuredim} \big)\,^{T}$.
		\\ See also: \gls{hypothesis}, \gls{map}, \gls{modelparam}, \gls{vector}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\featuremap(\cdot)  &\quad & \parbox{.85\textwidth}{A \gls{featuremap} 
		$\featuremap: \featurespace \rightarrow \featurespace' : \featurevec \mapsto \featuremap\big( \featurevec \big)$ that 
		transforms the \gls{featurevec} $\featurevec$ of a \gls{datapoint} into a new \gls{featurevec} $\featurevec'= \featuremap\big( \featurevec \big) \in \featurespace'$.
		\\ See also: \gls{featuremap}. }   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\kernelmap{\cdot}{\cdot} &\quad & \parbox{.85\textwidth}{Given some \gls{featurespace} $\featurespace$, 
		a \gls{kernel} is a \gls{map} $\kernel: \featurespace \times \featurespace \rightarrow \mathbb{C}$ that is \gls{psd}.
		\\ See also: \gls{featurespace}, \gls{kernel}, \gls{map}, \gls{psd}. }    \nonumber \\[2mm] \hline \nonumber\\[-5mm]  
	&\vcdim{\hypospace} &\quad & \parbox{.85\textwidth}{The \gls{vcdim} of the \gls{hypospace} $\hypospace$. 
		\\ See also: \gls{vcdim}, \gls{hypospace}. }    \nonumber                                                                                                                                               
\end{align}              



\newpage
\subsection*{\Gls{fl}}

\begin{align}
 	&\graph = \pair{\nodes}{\edges} & \quad & \parbox{.80\textwidth}{An \gls{undirectedgraph} whose nodes $\nodeidx \in \nodes$ represent 
		\glspl{device} within a \gls{flnetwork}. The undirected weighted edges $\edges$ represent connectivity between 
		\glspl{device} and statistical similarities between their \glspl{dataset} and \glspl{learningtask}.
		\\ See also: \gls{undirectedgraph}, \gls{device}, \gls{flnetwork}, \gls{dataset}, \gls{learningtask}. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\nodeidx \in \nodes& \quad & \parbox{.80\textwidth}{A node that represents some 
		\gls{device} within an \gls{flnetwork}. The \gls{device} can access a \gls{localdataset} and train a \gls{localmodel}.
		\\ See also: \gls{device}, \gls{flnetwork}, \gls{localdataset}, \gls{localmodel}. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\indsubgraph{\graph}{\cluster}& \quad & \parbox{.80\textwidth}{The induced subgraph of $\graph$ using the nodes in $\cluster \subseteq \nodes$. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\LapMat{\graph}   & \quad & \parbox{.80\textwidth}{The \gls{LapMat} of a \gls{graph} $\graph$.
		\\ See also: \gls{LapMat}, \gls{graph}. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\LapMat{\cluster}   & \quad & \parbox{.80\textwidth}{The \gls{LapMat} of the induced \gls{graph} $\indsubgraph{\graph}{\cluster}$.
		\\ See also: \gls{LapMat}, \gls{graph}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	 &\neighbourhood{\nodeidx}  & \quad & \parbox{.80\textwidth}{The \gls{neighborhood} of node $\nodeidx$ in a \gls{graph} $\graph$.
	 	\\ See also: \gls{neighborhood}, \gls{graph}. }   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\nodedegree{\nodeidx} & \quad & \parbox{.80\textwidth}{The weighted \gls{nodedegree} 
		$\nodedegree{\nodeidx}\!\defeq\!\sum_{\nodeidx' \in \neighbourhood{\nodeidx}}\hspace*{-1mm} \edgeweight_{\nodeidx,\nodeidx'}$ of node $\nodeidx$. 
		\\ See also: \gls{nodedegree}. }  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\maxnodedegree^{(\graph)} & \quad & \parbox{.80\textwidth}{The \gls{maximum} weighted \gls{nodedegree} of a \gls{graph} $\graph$.
		\\ See also: \gls{maximum}, \gls{nodedegree}, \gls{graph}. } \nonumber 
\end{align} 

\begin{align} 
	&\localdataset{\nodeidx} & \quad & \parbox{.70\textwidth}{The \gls{localdataset} $\localdataset{\nodeidx}$ carried by 
		node $\nodeidx\in \nodes$ of an \gls{flnetwork}.
		\\ See also: \gls{localdataset}, \gls{flnetwork}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	&\localsamplesize{\nodeidx} & \quad & \parbox{.70\textwidth}{The number of \glspl{datapoint} (i.e., the \gls{samplesize}) contained in the 
		\gls{localdataset} $\localdataset{\nodeidx}$ at node $\nodeidx\in \nodes$.
		\\ See also: \gls{datapoint}, \gls{samplesize}, \gls{localdataset}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\featurevec^{(\nodeidx,\sampleidx)} & \quad & \parbox{.70\textwidth}{The \glspl{feature} of the $\sampleidx$th \gls{datapoint} in 
		the \gls{localdataset} $\localdataset{\nodeidx}$.
		\\ See also: \gls{feature}, \gls{datapoint}, \gls{localdataset}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\truelabel^{(\nodeidx,\sampleidx)} & \quad & \parbox{.70\textwidth}{The \gls{label} of the $\sampleidx$th \gls{datapoint} in 
		the \gls{localdataset} $\localdataset{\nodeidx}$.
		\\ See also: \gls{label}, \gls{datapoint}, \gls{localdataset}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\localparams{\nodeidx} & \quad & \parbox{.70\textwidth}{The local \glspl{modelparam} of \gls{device} $\nodeidx$ within an \gls{flnetwork}.
		\\ See also: \gls{modelparam}, \gls{device}, \gls{flnetwork}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\locallossfunc{\nodeidx}{\weights} & \quad & \parbox{.70\textwidth}{The local \gls{lossfunc} used by \gls{device} $\nodeidx$ 
		to measure the usefulness of some choice $\weights$ for the local \glspl{modelparam}.
		\\ See also: \gls{lossfunc}, \gls{device}, \gls{modelparam}. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	& \gtvloss{\featurevec}{\hypothesis\big(\featurevec\big)}{\hypothesis'\big(\featurevec\big)}& \quad & \parbox{.70\textwidth}{The \gls{loss} 
		incurred by a \gls{hypothesis} $\hypothesis'$ on a \gls{datapoint} with \glspl{feature} $\featurevec$ and \gls{label} 
		$\hypothesis\big( \featurevec\big)$ that is obtained from another \gls{hypothesis}.
		\\ See also: \gls{loss}, \gls{hypothesis}, \gls{datapoint}, \gls{feature}, \gls{label}. }\nonumber 
\end{align} 

\begin{align} 
	& {\rm stack} \big\{ \weights^{(\nodeidx)} \big\}_{\nodeidx=1}^{\nrnodes} & \quad & \parbox{.70\textwidth}{The \gls{vector} 
		$\bigg( \big(\weights^{(1)}  \big)\,^{T}, \,\ldots, \,\big(\weights^{(\nrnodes)}  \big)\,^{T} \bigg)\,^{T} \in \mathbb{R}^{\dimlocalmodel\nrnodes}$ that 
		is obtained by vertically \gls{stacking} the local \glspl{modelparam} $\weights^{(\nodeidx)} \in \mathbb{R}^{\dimlocalmodel}$ for $\nodeidx=1,\,\ldots,\,\nrnodes$.
		\\ See also: \gls{vector}, \gls{modelparam}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	& \localhypothesis{\nodeidx} & \quad & \parbox{.70\textwidth}{A \gls{hypothesis} 
		$\localhypothesis{\nodeidx}\in \localmodel{\nodeidx}$ at some node $\nodeidx$ within an \gls{flnetwork}.
		\\ See also: \gls{hypothesis}, \gls{flnetwork}. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	& \learntlocalhypothesis{\nodeidx} & \quad & \parbox{.70\textwidth}{A learned 
		\gls{hypothesis} $\learntlocalhypothesis{\nodeidx}\in \localmodel{\nodeidx}$, obtained by some \gls{fl} method,
		at some node $\nodeidx$ within an \gls{flnetwork}.
		\\ See also: \gls{hypothesis}, \gls{flnetwork}. }\nonumber
\end{align}        



% !TeX spellcheck = en_GB

\section*{Lists of Symbols}
%\label{ch_list_of_symbols}

\vspace*{-2mm}
\section*{Sets and Functions} 

\begin{align} 
	&a \in \mathcal{A} & \quad & \parbox{.85\textwidth}{The object $a$ is an element of the set $\mathcal{A}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&a \defeq b & \quad & \mbox{We use $a$ as a shorthand for $b$. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&|\mathcal{A}| & \quad & \mbox{The cardinality (i.e., number of elements) of a finite set $\mathcal{A}$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathcal{A} \subseteq \mathcal{B}& \quad & \mbox{$\mathcal{A}$ is a subset of $\mathcal{B}$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathcal{A} \subset \mathcal{B}& \quad & \mbox{$\mathcal{A}$ is a strict subset of $\mathcal{B}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{N} & \quad & \mbox{The natural numbers $1,2,\ldots$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}  &\quad &\mbox{The real numbers $x$ \cite{RudinBook}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}_{+}  &\quad &\mbox{The non-negative real numbers $x\geq0$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}_{++}  &\quad &\mbox{The positive real numbers $x> 0$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\{0,1\}& \quad & \mbox{The set consisting of the two real numbers $0$ and $1$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&[0,1] &\quad &\mbox{The closed interval of real numbers $x$ with $0 \leq x \leq 1$. }\nonumber 
\end{align} 

\newpage
\begin{align}
    &\argmin_{\weights} f(\weights) &\quad &\parbox{.70\textwidth}{The set of minimizers for a real-valued \gls{function} $f(\weights)$. 
    	\\ See also: \gls{function}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
    &\sphere{\nrnodes} &\quad &\parbox{.70\textwidth}{The set of unit-\gls{norm} vectors in $\mathbb{R}^{\nrnodes+1}$.
    	\\ See also: \gls{norm}. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
     & \exp(a) & \quad & \mbox{The exponential function evaluated at the real number $a \in \mathbb{R}$.} \nonumber \\[2mm] \hline \nonumber \\[-5mm]
	 &\log(a) &\quad &\mbox{The logarithm of the positive number $a \in \mathbb{R}_{++}$.  } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	 &f(\cdot)\!:\!\mathcal{A}\!\rightarrow\!\mathcal{B} :  a \!\mapsto\!f(a) &\quad &\parbox{.70\textwidth}{
	 	A \gls{function} (or \gls{map}) from a set $\mathcal{A}$ to a set $\mathcal{B}$, assigning to each input 
	 	$a \in \mathcal{A}$ a well-defined output $f(a) \in \mathcal{B}$.
	 	The set $\mathcal{A}$ is the domain of the \gls{function} $f$ and the set $\mathcal{B}$ is the 
	 	codomain of $f$. \Gls{ml} aims to learn a \gls{function} $\hypothesis$ that maps \glspl{feature} 
	 	$\featurevec$ of a \gls{datapoint} to a \gls{prediction} $\hypothesis(\featurevec)$ for its \gls{label} $\truelabel$.
		\\ See also: \gls{function}, \gls{map}, \gls{ml}, \gls{hypothesis}, \gls{feature}, \gls{datapoint}, \gls{prediction}, \gls{label}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	 &\operatorname{epi}(f)  & \quad & \parbox{.70\textwidth}{The \gls{epigraph} of a real-valued \gls{function} 
	 	$f: \mathbb{R}^{\featuredim}\rightarrow \mathbb{R}$.
		\\ See also: \gls{epigraph}, \gls{function}. } \nonumber \\[2mm]  \hline \nonumber\\[-5mm]
	 &  \frac{\partial f(\weight_{1},\ldots,\weight_{\nrfeatures})}{\partial \weight_{\featureidx}} & \quad & \parbox{.70\textwidth}{The partial derivative (if it exists) of 
	 		a real-valued \gls{function} $f: \mathbb{R}^{\featuredim}\rightarrow \mathbb{R}$ with respect to\ $\weight_{\featureidx}$\cite[Ch. 9]{RudinBookPrinciplesMatheAnalysis}.
			\\ See also: \gls{function}.
			} \nonumber 
\end{align} 

\begin{align}
	 &\nabla f(\weights) & \quad & \parbox{.70\textwidth}{The \gls{gradient} of a \gls{differentiable} real-valued \gls{function} 
	 	$f: \mathbb{R}^{\featuredim}\rightarrow \mathbb{R}$ is the vector 
	 	$\nabla f(\weights) = \big( \frac{\partial f}{\partial \weight_{1}},\ldots,\frac{\partial f}{\partial \weight_{\featuredim}}  \big)^{T} \in \mathbb{R}^{\featuredim}$ \cite[Ch. 9]{RudinBookPrinciplesMatheAnalysis}.
		\\ See also: \gls{gradient}, \gls{differentiable}, \gls{function}.}   \nonumber
\end{align} 
\section*{Matrices and Vectors} 

\begin{align} 
	 &\featurevec=\big(\feature_{1},\ldots,\feature_{\featuredim})^{T} &\quad & \parbox{.75\textwidth}{A vector of length $\featuredim$, with its 
		$\featureidx$-th entry being $\feature_{\featureidx}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}^{\featuredim} & \quad &  \parbox{.75\textwidth}{The set of vectors $\featurevec=\big(\feature_{1},\ldots,\feature_{\featurelen}\big)^{T}$ consisting of 
		$\featuredim$ real-valued entries $\feature_{1},\ldots,\feature_{\featurelen} \in \mathbb{R}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{I}_{\modelidx \times \featuredim}  & \quad &  \parbox{.75\textwidth}{A generalized identity matrix 
		with $\modelidx$ rows and $\featuredim$ columns. The entries of $\mathbf{I}_{\modelidx \times \featuredim} \in \mathbb{R}^{\modelidx \times \featuredim}$ 
		are equal to $1$ along the main diagonal and equal to $0$ otherwise. }\nonumber \\[2mm] \hline \nonumber\\[-5mm] %For example, $\mathbf{I}_{1 \times 2} = \big(1, 0\big)$ and $\mathbf{I}_{2 \times 1}= \begin{pmatrix} 1 \\ 0 \end{pmatrix}$.} 
	&\mathbf{I}_{\dimlocalmodel}, \mathbf{I} & \quad &  \parbox{.75\textwidth}{A square identity 
		matrix of size $\dimlocalmodel \times \dimlocalmodel$. If the size is clear from 
		context, we drop the subscript.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\normgeneric{\featurevec}{2}  &\quad & \parbox{.75\textwidth}{The Euclidean (or $\ell_{2}$) \gls{norm} of the vector 
		$\featurevec=\big(\feature_{1},\ldots,\feature_{\featurelen}\big)^{T} \in \mathbb{R}^{\featuredim}$ defined as 
		$ \| \featurevec \|_{2} \defeq \sqrt{\sum_{\featureidx=1}^{\featuredim} \feature_{\featureidx}^{2}}$.
		\\ See also: \gls{norm}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	&\normgeneric{\featurevec}{}  & \quad &  \parbox{.75\textwidth}{Some \gls{norm} of the vector $\featurevec \in \mathbb{R}^{\featuredim}$ \cite{GolubVanLoanBook}. 
		Unless specified otherwise, we mean the Euclidean \gls{norm} $\normgeneric{\featurevec}{2}$.
		\\ See also: \gls{norm}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\featurevec^{T} &\quad & \parbox{.75\textwidth}{The transpose of a matrix that has the vector 
		$\featurevec \in \mathbb{R}^{\dimlocalmodel}$ as its single column.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{X}^{T} &\quad & \parbox{.75\textwidth}{The transpose of a matrix $\mathbf{X} \in \mathbb{R}^{\samplesize \times \featurelen}$. 
		A square real-valued matrix $\mathbf{X} \in \mathbb{R}^{\samplesize \times \samplesize}$ 
		is called symmetric if $\mathbf{X} = \mathbf{X}^{T}$. }  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{X}^{-1} &\quad & \parbox{.75\textwidth}{The \gls{inverse} of a matrix $\mathbf{X} \in \mathbb{R}^{\featurelen \times \featurelen}$.
		\\ See also: \gls{inverse}. }  \nonumber 
\end{align} 
\newpage
\begin{align} 
	&\mathbf{0}= \big(0,\ldots,0\big)^{T}  & \quad &  \parbox{.75\textwidth}{The vector in $\mathbb{R}^{\dimlocalmodel}$ with each entry equal to zero.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{1}= \big(1,\ldots,1\big)^{T}  & \quad &  \parbox{.75\textwidth}{The vector in $\mathbb{R}^{\dimlocalmodel}$ with each entry equal to one.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\big(\vv^{T},\vw^{T} \big)^{T}  & \quad &  \parbox{.85\textwidth}{The vector of length $\featurelen+\featurelen'$ 
		obtained by concatenating the entries of vector $\vv \in \mathbb{R}^{\featurelen}$ with the entries of $\vw \in \mathbb{R}^{\featurelen'}$.} \nonumber \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&	{\rm span}\{ \mathbf{B} \}  & \quad &  \parbox{.85\textwidth}{The span of a matrix $\mathbf{B} \in \mathbb{R}^{a \times b}$, 
		which is the subspace of all linear combinations of the columns of $\mathbf{B}$, such that
		${\rm span}\{ \mathbf{B} \} = \big\{  \mathbf{B} \va : \va \in \mathbb{R}^{b} \big\} \subseteq \mathbb{R}^{a}$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\determinant{\mC} &\quad & \parbox{.85\textwidth}{The \gls{det} of the matrix $\mC$.
		\\ See also: \gls{det}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{A} \otimes \mathbf{B} &\quad & \parbox{.85\textwidth}{The Kronecker product of $\mathbf{A}$ and $\mathbf{B}$ \cite{Golub1980}. }  \nonumber
\end{align} 

\newpage
\section*{Probability Theory} 
\begin{align}
	&\featurevec \sim p(\vz)  &\quad & \parbox{.85\textwidth}{The \gls{rv} $\featurevec$ is distributed according to 
		the \gls{probdist} $p(\vz)$ \cite{klenke2020probability}, \cite{BillingsleyProbMeasure}.
		\\ See also: \gls{rv}, \gls{probdist}.}  \nonumber \\[2mm] \hline \nonumber\\[-5mm]  
	&\expect_{p} \{ f(\datapoint) \}  &\quad & \parbox{.85\textwidth}{The \gls{expectation} of a \gls{rv} $f(\datapoint)$ that 
		is obtained by applying a deterministic \gls{function} $f$ to an \gls{rv}
		$\datapoint$ whose \gls{probdist} is $\prob{\datapoint}$. If the \gls{probdist} is clear from context, 
		we just write $\expect \{ f(\datapoint) \}$. 
		\\ See also: \gls{expectation}, \gls{rv}, \gls{function}, \gls{probdist}.}  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
		&\cov{x}{y} &\quad & \parbox{.85\textwidth}{The \gls{covariance} between two real-valued \glspl{rv} defined 
			over a common \gls{probspace}. 
		\\ See also: \gls{covariance}, \gls{rv}, \gls{probdist}.}  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\prob{\featurevec,\truelabel} &\quad & \parbox{.85\textwidth}{A (joint) \gls{probdist} of an \gls{rv} 
		whose \glspl{realization} are \glspl{datapoint} with \glspl{feature} $\featurevec$ and \gls{label} $\truelabel$.
		\\ See also: \gls{probdist}, \gls{rv}, \gls{realization}, \gls{datapoint}, \gls{feature}, 
		\gls{label}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\prob{\featurevec|\truelabel} &\quad & \parbox{.85\textwidth}{A conditional \gls{probdist} of an \gls{rv} 
		$\featurevec$ given the value of another \gls{rv} $\truelabel$ \cite[Sec.\ 3.5]{BertsekasProb}. 
		\\ See also: \gls{probdist}, \gls{rv}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\prob{\featurevec;\weights} &\quad & \parbox{.85\textwidth}{A parametrized \gls{probdist} of an \gls{rv} $\featurevec$. 
		The \gls{probdist} depends on a \gls{parameter} vector $\weights$. For example, $\prob{\featurevec;\weights}$ could be a 
		\gls{mvndist} with the \gls{parameter} vector $\weights$ given by the entries of the \gls{mean} vector $\expect \{ \featurevec \}$ 
		and the \gls{covmtx} $\expect \bigg \{ \big( \featurevec - \expect \{ \featurevec \}\big) \big( \featurevec - \expect \{ \featurevec \}\big)^{T}  \bigg\}$.
		\\ See also: \gls{probdist}, \gls{rv}, \gls{parameter}, \gls{mvndist}, \gls{mean}, \gls{covmtx}.} \nonumber  
\end{align} 
\newpage
\begin{align} 
	&\mathcal{N}(\mu, \sigma^{2}) &\quad & \parbox{.85\textwidth}{The \gls{probdist} of a 
		\gls{gaussrv} $\feature \in \mathbb{R}$ with \gls{mean} (or \gls{expectation}) $\mu= \expect \{ \feature \}$ 
		and \gls{variance} $\sigma^{2} =   \expect \big\{  (  \feature - \mu )^2 \big\}$.
		\\ See also: \gls{probdist}, \gls{gaussrv}, \gls{mean}, \gls{expectation}, \gls{variance}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathcal{N}(\meanvecgeneric, \mathbf{C}) &\quad & \parbox{.85\textwidth}{The \gls{mvndist} of a vector-valued 
		\gls{gaussrv} $\featurevec \in \mathbb{R}^{\featuredim}$ with \gls{mean} (or \gls{expectation}) $\clustermean= \expect \{ \featurevec \}$ 
		and \gls{covmtx} $\mathbf{C} =  \expect \big\{ \big( \featurevec - \clustermean \big)\big( \featurevec - \clustermean \big)^{T} \big\}$.
		\\ See also: \gls{mvndist}, \gls{gaussrv}, \gls{mean}, \gls{expectation}, \gls{covmtx}.} \nonumber                                             
\end{align}





\newpage
\section*{Machine Learning}

\begin{align}
%	\datapoint \quad\quad & \parbox{.75\textwidth}{A \gls{datapoint} which is characterized by several properties that we 
%		divide into low-level properties (= \gls{feature}s) and high-level properties (= \gls{label}s) \cite[Ch. 2]{MLBasics}.}    \nonumber   \\[4mm] 
	&\sampleidx &\quad & \parbox{.90\textwidth}{An index $\sampleidx=1,2,\ldots$ that 
		enumerates \glspl{datapoint}.
		\\ See also: \gls{datapoint}. }   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\samplesize &\quad &\parbox{.90\textwidth}{The number of \glspl{datapoint} in (i.e., the size of) a \gls{dataset}.
		\\ See also: \gls{datapoint}, \gls{dataset}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	&\dataset &\quad & \parbox{.90\textwidth}{A \gls{dataset} $\dataset = \{ \datapoint^{(1)},\ldots, \datapoint^{(\samplesize)} \}$ 
		is a list of individual \glspl{datapoint} $\datapoint^{(\sampleidx)}$, for $\sampleidx=1,\ldots,\samplesize$.
		\\ See also: \gls{dataset}, \gls{datapoint}.}   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\featurelen &\quad &\parbox{.90\textwidth}{The number of \glspl{feature} that characterize a \gls{datapoint}.
		\\ See also: \gls{feature}, \gls{datapoint}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\feature_{\featureidx} &\quad &\parbox{.90\textwidth}{The $\featureidx$-th \gls{feature} of a \gls{datapoint}. The first \gls{feature} 
		is denoted $\feature_{1}$, the second \gls{feature} $\feature_{2}$, and so on.
		\\ See also: \gls{datapoint}, \gls{feature}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	&\featurevec &\quad &\parbox{.90\textwidth}{The \gls{featurevec} $\featurevec=\big(\feature_{1},\ldots,\feature_{\featuredim}\big)^{T}$ of a \gls{datapoint}. The vector's entries 
		are the individual \glspl{feature} of a \gls{datapoint}.
		\\ See also: \gls{featurevec}, \gls{datapoint}, \gls{feature}. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\featurespace &\quad & \parbox{.90\textwidth}{The \gls{featurespace} $\featurespace$ is 
		the set of all possible values that the \glspl{feature} $\featurevec$ of a \gls{datapoint} can take on.
		\\ See also: \gls{featurespace}, \gls{feature}, \gls{datapoint}.} \nonumber 
\end{align}        


\begin{align}
	&\rawfeaturevec &\quad &\parbox{.85\textwidth}{Instead of the symbol $\featurevec$, we 
		sometimes use $\rawfeaturevec$ as another symbol to denote a vector whose entries 
		are the individual \glspl{feature} of a \gls{datapoint}. We need two 
		different symbols to distinguish between raw and learned \glspl{feature} \cite[Ch. 9]{MLBasics}.
		\\ See also: \gls{feature}, \gls{datapoint}. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\featurevec^{(\sampleidx)} &\quad &\parbox{.85\textwidth}{The \gls{featurevec} of the $\sampleidx$-th \gls{datapoint} within a \gls{dataset}.
		\\ See also: \gls{feature}, \gls{datapoint}, \gls{dataset}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\feature_{\featureidx}^{(\sampleidx)} &\quad &\parbox{.85\textwidth}{The $\featureidx$-th \gls{feature} of the $\sampleidx$-th 
		\gls{datapoint} within a \gls{dataset}.
		\\ See also: \gls{feature}, \gls{datapoint}, \gls{dataset}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\batch &\quad &\parbox{.85\textwidth}{A mini-\gls{batch} (or subset) of randomly chosen \glspl{datapoint}.
		\\ See also: \gls{batch}, \gls{datapoint}. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\batchsize &\quad &\parbox{.85\textwidth}{The size of (i.e., the number of \glspl{datapoint} in) a mini-\gls{batch}.
		\\ See also: \gls{datapoint}, \gls{batch}. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\truelabel &\quad &\parbox{.85\textwidth}{The \gls{label} (or quantity of interest) of a \gls{datapoint}.
		\\ See also: \gls{label}, \gls{datapoint}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\truelabel^{(\sampleidx)} &\quad &\parbox{.85\textwidth}{The \gls{label} of the $\sampleidx$-th \gls{datapoint}.
		\\ See also: \gls{label}, \gls{datapoint}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\big(\featurevec^{(\sampleidx)},\truelabel^{(\sampleidx)}\big) &\quad &\parbox{.85\textwidth}{The \glspl{feature} and \gls{label} of the $\sampleidx$-th \gls{datapoint}.
		\\ See also: \gls{feature}, \gls{label}, \gls{datapoint}. }\nonumber 
\end{align}                  


\begin{align}
	&\labelspace  &\quad & \parbox{.90\textwidth}{The \gls{labelspace} $\labelspace$ of 
		an \gls{ml} method consists of all potential \gls{label} values that a \gls{datapoint} can 
		carry. The nominal \gls{labelspace} might be larger than the set of different \gls{label} 
		values arising in a given \gls{dataset} (e.g., a \gls{trainset}). \Gls{ml} problems 
		(or methods) using a numeric \gls{labelspace}, such as $\labelspace=\mathbb{R}$ 
		or $\labelspace=\mathbb{R}^{3}$, are referred to as \gls{regression} problems (or methods). \Gls{ml} 
		problems (or methods) that use a discrete \gls{labelspace}, such as $\labelspace=\{0,1\}$ or $\labelspace=\{\mbox{\emph{cat}},\mbox{\emph{dog}},\mbox{\emph{mouse}}\}$, 
		are referred to as \gls{classification} problems (or methods).
		\\ See also: \gls{labelspace}, \gls{ml}, \gls{label}, \gls{datapoint},  \gls{dataset}, \gls{trainset}, 
		\gls{regression}, \gls{classification}.}  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\lrate  &\quad & \parbox{.90\textwidth}{\Gls{learnrate} (or \gls{stepsize}) used by \gls{gdmethods}.
		\\ See also: \gls{learnrate}, \gls{stepsize}, \gls{gdmethods}. }  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\hypothesis(\cdot)  &\quad &\parbox{.90\textwidth}{A \gls{hypothesis} \gls{map} that maps the \glspl{feature} 
		of a \gls{datapoint} to a \gls{prediction} $\hat{\truelabel}=\hypothesis(\featurevec)$ for its \gls{label} $\truelabel$.
		\\ See also: \gls{hypothesis}, \gls{map}, \gls{feature}, \gls{datapoint}, \gls{prediction}, \gls{label}. }  	 \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	 &\labelspace^{\featurespace} &\quad & \parbox{.90\textwidth}{Given two sets $\featurespace$ and $\labelspace$, we denote by $ \labelspace^{\featurespace}$ 
	 	the set of all possible \gls{hypothesis} \glspl{map} $\hypothesis: \featurespace \rightarrow \labelspace$.
		\\ See also: \gls{hypothesis}, \gls{map}. } 	 \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\hypospace  &\quad & \parbox{.90\textwidth}{A \gls{hypospace} or \gls{model} used by an \gls{ml} method. 
		The \gls{hypospace} consists of different \gls{hypothesis} \glspl{map} $\hypothesis: \featurespace \rightarrow \labelspace$, between which 
		the \gls{ml} method must choose.
		\\ See also: \gls{hypospace}, \gls{model}, \gls{ml}, \gls{hypothesis}, \gls{map}. }   \nonumber 
\end{align}     

\begin{align}
	&\effdim{\hypospace}  &\quad & \parbox{.80\textwidth}{The \gls{effdim} of a \gls{hypospace} $\hypospace$.
		\\ See also: \gls{effdim}, \gls{hypospace}. }   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\biasterm^2 &\quad &\parbox{.80\textwidth}{
		The squared \gls{bias} of a learned \gls{hypothesis} $\learnthypothesis$, or its \glspl{parameter}. Note that $\learnthypothesis$ 
		becomes a \gls{rv} if it is learned from \glspl{datapoint} being \glspl{rv}.
		\\ See also: \gls{bias}, \gls{hypothesis}, \gls{parameter}, \gls{rv}, \gls{datapoint}. } \nonumber  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\varianceterm &\quad &\parbox{.80\textwidth}{The \gls{variance} of a learned 
	  	\gls{hypothesis} $\learnthypothesis$, or its \glspl{parameter}. Note that $\learnthypothesis$ 
	  	becomes a \gls{rv} if it is learned from \glspl{datapoint} being \glspl{rv}.
		\\ See also: \gls{variance}, \gls{hypothesis}, \gls{parameter}, \gls{rv}, \gls{datapoint}. } \nonumber \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\lossfunc{(\featurevec,\truelabel)}{\hypothesis}  &\quad & \parbox{.80\textwidth}{The \gls{loss} incurred by predicting the 
		\gls{label} $\truelabel$ of a \gls{datapoint} using the \gls{prediction} $\hat{\truelabel}=h(\featurevec)$. The 
		\gls{prediction} $\hat{\truelabel}$ is obtained by evaluating the \gls{hypothesis} $\hypothesis \in \hypospace$ for 
		the \gls{featurevec} $\featurevec$ of the \gls{datapoint}.
		\\ See also: \gls{loss}, \gls{label}, \gls{datapoint}, \gls{prediction}, \gls{hypothesis}, 
		\gls{featurevec}. }    \nonumber  \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	&\valerror &\quad &\parbox{.80\textwidth}{The \gls{valerr} of a \gls{hypothesis} $\hypothesis$, which is its 
		average \gls{loss} incurred over a \gls{valset}.
		\\ See also: \gls{valerr}, \gls{hypothesis}, \gls{loss}, \gls{valset}. }  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\emperror\big(\hypothesis| \dataset \big) &\quad &\parbox{.80\textwidth}{The \gls{emprisk} or average \gls{loss} 
		incurred by the \gls{hypothesis} $\hypothesis$ on a \gls{dataset} $\dataset$.
		\\ See also: \gls{emprisk}, \gls{loss}, \gls{hypothesis}, \gls{dataset}. } \nonumber 
\end{align}     

\begin{align}                          
	&\trainerror &\quad &\parbox{.85\textwidth}{The \gls{trainerr} of a \gls{hypothesis} $\hypothesis$, which is its 
		average \gls{loss} incurred over a \gls{trainset}.
		\\ See also: \gls{trainerr}, \gls{hypothesis}, \gls{loss}, \gls{trainset}. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\timeidx &\quad &\parbox{.85\textwidth}{A discrete-time index $\timeidx=0,1,\ldots$ used to 
		enumerate sequential events (or time instants). }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\taskidx &\quad &\parbox{.85\textwidth}{An index that enumerates
		\glspl{learningtask} within a \gls{multitask learning} problem.
		\\ See also: \gls{learningtask}, \gls{multitask learning}. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\regparam &\quad &\parbox{.85\textwidth}{A \gls{regularization} \gls{parameter} that controls 
		the amount of \gls{regularization}.
		\\ See also: \gls{regularization}, \gls{parameter}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\eigval{\featureidx}\big( \mathbf{Q} \big) &\quad &\parbox{.85\textwidth}{The $\featureidx$-th 
		\gls{eigenvalue} (sorted in either ascending or descending order) of a \gls{psd} matrix $\mathbf{Q}$. We also 
		use the shorthand $\eigval{\featureidx}$ if the corresponding matrix is clear from context.
		\\ See also: \gls{eigenvalue}, \gls{psd}. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\actfun(\cdot) &\quad &\parbox{.85\textwidth}{The \gls{actfun} used by an artificial neuron within an \gls{ann}.
		\\ See also: \gls{actfun}, \gls{ann}. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\decreg{\hat{\truelabel}} &\quad &\parbox{.85\textwidth}{A \gls{decisionregion} within a \gls{featurespace}.
		\\ See also: \gls{decisionregion}, \gls{featurespace}. }\nonumber
\end{align}     

\begin{align} 
	&\weights  &\quad & \parbox{.85\textwidth}{A \gls{parameter} vector $\weights = \big(\weight_{1},\ldots,\weight_{\featuredim}\big)^{T}$ 
		of a \gls{model}, e.g., the \gls{weights} of a \gls{linmodel} or in an \gls{ann}.
		\\ See also: \gls{parameter}, \gls{model}, \gls{weights}, \gls{linmodel}, \gls{ann}. }     \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\hypothesis^{(\weights)}(\cdot)  &\quad &\parbox{.85\textwidth}{A \gls{hypothesis} \gls{map} that involves tunable \gls{modelparams} 
		$\weight_{1},\ldots,\weight_{\featuredim}$ stacked into the vector $\weights=\big(\weight_{1},\ldots,\weight_{\featuredim} \big)^{T}$.
		\\ See also: \gls{hypothesis}, \gls{map}, \gls{modelparams}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\featuremap(\cdot)  &\quad & \parbox{.85\textwidth}{A \gls{featuremap} 
		$\featuremap: \featurespace \rightarrow \featurespace' : \featurevec \mapsto \featurevec' \defeq \featuremap\big( \featurevec \big) \in \featurespace'$.
		\\ See also: \gls{featuremap}. }   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\kernelmap{\cdot}{\cdot} &\quad & \parbox{.85\textwidth}{Given some \gls{featurespace} $\featurespace$, 
		a \gls{kernel} is a \gls{map} $\kernel: \featurespace \times \featurespace \rightarrow \mathbb{C}$ that is \gls{psd}.
		\\ See also: \gls{featurespace}, \gls{kernel}, \gls{map}, \gls{psd}. }    \nonumber                                                                                                                                                     
\end{align}              






\newpage
\section*{Federated Learning}

\begin{align}
 	&\graph = \pair{\nodes}{\edges} & \quad & \parbox{.80\textwidth}{An undirected \gls{graph} whose nodes $\nodeidx \in \nodes$ represent 
		\glspl{device} within a \gls{empgraph}. The undirected weighted edges $\edges$ represent connectivity between 
		\glspl{device} and statistical similarities between their \glspl{dataset} and \glspl{learningtask}.
		\\ See also: \gls{graph}, \gls{device}, \gls{empgraph}, \gls{dataset}, \gls{learningtask}. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\nodeidx \in \nodes& \quad & \parbox{.80\textwidth}{A node that represents some 
		\gls{device} within an \gls{empgraph}. The \gls{device} can access a \gls{localdataset} and train a \gls{localmodel}.
		\\ See also: \gls{device}, \gls{empgraph}, \gls{localdataset}, \gls{localmodel}. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\indsubgraph{\graph}{\cluster}& \quad & \parbox{.80\textwidth}{The induced subgraph of $\graph$ using the nodes in $\cluster \subseteq \nodes$. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\LapMat{\graph}   & \quad & \parbox{.80\textwidth}{The \gls{LapMat} of a \gls{graph} $\graph$.
		\\ See also: \gls{LapMat}, \gls{graph}. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\LapMat{\cluster}   & \quad & \parbox{.80\textwidth}{The \gls{LapMat} of the induced \gls{graph} $\indsubgraph{\graph}{\cluster}$.
		\\ See also: \gls{LapMat}, \gls{graph}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	 &		\neighbourhood{\nodeidx}  & \quad & \parbox{.80\textwidth}{The \gls{neighborhood} of a node $\nodeidx$ in a \gls{graph} $\graph$.
	 	\\ See also: \gls{neighborhood}, \gls{graph}. }   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\nodedegree{\nodeidx} & \quad & \parbox{.80\textwidth}{The weighted degree $\nodedegree{\nodeidx}\defeq \sum_{\nodeidx' \in \neighbourhood{\nodeidx}} \edgeweight_{\nodeidx,\nodeidx'}$ 
		of a node $\nodeidx$ in a \gls{graph} $\graph$.
		\\ See also: \gls{graph}. }  \nonumber 
\end{align} 

\begin{align} 
	&\maxnodedegree^{(\graph)} & \quad & \parbox{.85\textwidth}{The \gls{maximum} weighted \gls{nodedegree} of a \gls{graph} $\graph$.
		\\ See also: \gls{maximum}, \gls{nodedegree}, \gls{graph}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	&\localdataset{\nodeidx} & \quad & \parbox{.85\textwidth}{The \gls{localdataset} $\localdataset{\nodeidx}$ carried by 
		node $\nodeidx\in \nodes$ of an \gls{empgraph}.
		\\ See also: \gls{localdataset}, \gls{empgraph}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\localsamplesize{\nodeidx} & \quad & \parbox{.85\textwidth}{The number of \glspl{datapoint} (i.e., \gls{samplesize}) contained in the 
		\gls{localdataset} $\localdataset{\nodeidx}$ at node $\nodeidx\in \nodes$.
		\\ See also: \gls{datapoint}, \gls{samplesize}, \gls{localdataset}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\featurevec^{(\nodeidx,\sampleidx)} & \quad & \parbox{.85\textwidth}{The \glspl{feature} of the $\sampleidx$-th \gls{datapoint} in 
		the \gls{localdataset} $\localdataset{\nodeidx}$.
		\\ See also: \gls{feature}, \gls{datapoint}, \gls{localdataset}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\truelabel^{(\nodeidx,\sampleidx)} & \quad & \parbox{.85\textwidth}{The \gls{label} of the $\sampleidx$-th \gls{datapoint} in 
		the \gls{localdataset} $\localdataset{\nodeidx}$.
		\\ See also: \gls{label}, \gls{datapoint}, \gls{localdataset}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\localparams{\nodeidx} & \quad & \parbox{.85\textwidth}{The local \gls{modelparams} of \gls{device} $\nodeidx$ within an \gls{empgraph}.
		\\ See also: \gls{modelparams}, \gls{device}, \gls{empgraph}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\locallossfunc{\nodeidx}{\weights} & \quad & \parbox{.85\textwidth}{The local \gls{lossfunc} used by \gls{device} $\nodeidx$ 
		to measure the usefulness of some choice $\weights$ for the local \gls{modelparams}.
		\\ See also: \gls{lossfunc}, \gls{device}, \gls{modelparams}. }\nonumber 
\end{align} 

\begin{align} 
	& \gtvloss{\featurevec}{\hypothesis\big(\featurevec\big)}{\hypothesis'\big(\featurevec\big)}& \quad & \parbox{.70\textwidth}{The \gls{loss} 
		incurred by a \gls{hypothesis} $\hypothesis'$ on a \gls{datapoint} with \glspl{feature} $\featurevec$ and \gls{label} 
		$\hypothesis\big( \featurevec\big)$ that is obtained from another \gls{hypothesis}.
		\\ See also: \gls{loss}, \gls{hypothesis}, \gls{datapoint}, \gls{feature}, \gls{label}. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	& 	{\rm stack} \big\{ \weights^{(\nodeidx)} \big\}_{\nodeidx=1}^{\nrnodes} & \quad & \parbox{.70\textwidth}{The vector 
		$\bigg( \big(\weights^{(1)}  \big)^{T}, \ldots, \big(\weights^{(\nrnodes)}  \big)^{T} \bigg)^{T} \in \mathbb{R}^{\dimlocalmodel\nrnodes}$ that 
		is obtained by vertically stacking the local \gls{modelparams} $\weights^{(\nodeidx)} \in \mathbb{R}^{\dimlocalmodel}$.
		\\ See also: \gls{modelparams}. } \nonumber  
\end{align}        



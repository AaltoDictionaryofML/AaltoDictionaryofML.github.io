% !TeX spellcheck = en_GB

\section*{Lists of Symbols}
%\label{ch_list_of_symbols}

\vspace*{-2mm}
\section*{Sets and Functions} 

\begin{align} 
	&a \in \mathcal{A} & \quad & \parbox{.75\textwidth}{The object $a$ is an element of the set $\mathcal{A}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&a \defeq b & \quad & \mbox{We use $a$ as a shorthand for $b$. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&|\mathcal{A}| & \quad & \mbox{The cardinality (i.e., number of elements) of a finite set $\mathcal{A}$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathcal{A} \subseteq \mathcal{B}& \quad & \mbox{$\mathcal{A}$ is a subset of $\mathcal{B}$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathcal{A} \subset \mathcal{B}& \quad & \mbox{$\mathcal{A}$ is a strict subset of $\mathcal{B}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{N} & \quad & \mbox{The natural numbers $1,2,\ldots$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}  &\quad &\mbox{The real numbers $x$ \cite{RudinBook}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}_{+}  &\quad &\mbox{The non-negative real numbers $x\geq0$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}_{++}  &\quad &\mbox{The positive real numbers $x> 0$.} \nonumber
\end{align} 

\newpage
\begin{align}
		&\{0,1\}& \quad & \mbox{The set consisting of the two real numbers $0$ and $1$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&[0,1] &\quad &\mbox{The closed interval of real numbers $x$ with $0 \leq x \leq 1$. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
    &\argmin_{\weights} f(\weights) &\quad &\mbox{The set of minimizers for a real-valued function $f(\weights)$.  } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
    &\sphere{\nrnodes} &\quad &\mbox{The set of unit-\gls{norm} vectors in $\mathbb{R}^{\nrnodes+1}$.  }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	 &\log a &\quad &\mbox{The logarithm of the positive number $a \in \mathbb{R}_{++}$.  } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	 &\hypothesis(\cdot)\!:\!\mathcal{A}\!\rightarrow\!\mathcal{B} :  a \!\mapsto\!h(a) &\quad &\parbox{.75\textwidth}{
	 	A function (map) that accepts any element $a \in \mathcal{A}$ from a set $\mathcal{A}$ 
	 	as input and delivers a well-defined element $h(a) \in \mathcal{B}$ of a set $\mathcal{B}$. 
	 	The set $\mathcal{A}$ is the domain of the function $h$ and the set $\mathcal{B}$ is the 
	 	codomain of $\hypothesis$. \Gls{ml} aims at finding (or learning) a function $\hypothesis$ (i.e., a \gls{hypothesis}) 
	 	that reads in the \gls{feature}s $\featurevec$ of a \gls{datapoint} and delivers a \gls{prediction} $\hypothesis(\featurevec)$
	 	for its \gls{label} $\truelabel$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	 	&\nabla f(\weights) & \quad & \parbox{.75\textwidth}{The \gls{gradient} of a \gls{differentiable} real-valued function 
	 	$f: \mathbb{R}^{\featuredim}\rightarrow \mathbb{R}$ is the vector 
	 	$\nabla f(\weights) = \big( \frac{\partial f}{\partial \weight_{1}},\ldots,\frac{\partial f}{\partial \weight_{\featuredim}}  \big)^{T} \in \mathbb{R}^{\featuredim}$ \cite[Ch. 9]{RudinBookPrinciplesMatheAnalysis}.}   \nonumber
\end{align} 
\section*{Matrices and Vectors} 

\begin{align} 
	 &\featurevec=\big(\feature_{1},\ldots,\feature_{\featuredim})^{T} &\quad & \parbox{.75\textwidth}{A vector of length $\featuredim$, with its 
		$\featureidx$-th entry being $\feature_{\featureidx}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}^{\featuredim} & \quad &  \parbox{.75\textwidth}{The set of vectors $\featurevec=\big(\feature_{1},\ldots,\feature_{\featurelen}\big)^{T}$ consisting of $\featuredim$ real-valued entries $\feature_{1},\ldots,\feature_{\featurelen} \in \mathbb{R}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{I}_{\modelidx \times \featuredim}  & \quad &  \parbox{.75\textwidth}{A generalized identity matrix 
		with $\modelidx$ rows and $\featuredim$ columns. The entries of $\mathbf{I}_{\modelidx \times \featuredim} \in \mathbb{R}^{\modelidx \times \featuredim}$ 
		are equal to $1$ along the main diagonal and equal to $0$ otherwise. }\nonumber \\[2mm] \hline \nonumber\\[-5mm] %For example, $\mathbf{I}_{1 \times 2} = \big(1, 0\big)$ and $\mathbf{I}_{2 \times 1}= \begin{pmatrix} 1 \\ 0 \end{pmatrix}$.} 
	&\mathbf{I}_{\dimlocalmodel}, \mathbf{I} & \quad &  \parbox{.75\textwidth}{A square identity 
		matrix of size $\dimlocalmodel \times \dimlocalmodel$. If the size is clear from 
		context, we drop the subscript.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\normgeneric{\featurevec}{2}  &\quad & \parbox{.75\textwidth}{The Euclidean (or $\ell_{2}$) \gls{norm} of the vector 
		$\featurevec=\big(\feature_{1},\ldots,\feature_{\featurelen}\big)^{T} \in \mathbb{R}^{\featuredim}$ defined as $ \| \featurevec \|_{2} \defeq \sqrt{\sum_{\featureidx=1}^{\featuredim} \feature_{\featureidx}^{2}}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	&\normgeneric{\featurevec}{}  & \quad &  \parbox{.75\textwidth}{Some \gls{norm} of the vector $\featurevec \in \mathbb{R}^{\featuredim}$ \cite{GolubVanLoanBook}. Unless specified otherwise, we mean the Euclidean \gls{norm} $\normgeneric{\featurevec}{2}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\featurevec^{T} &\quad & \parbox{.75\textwidth}{The transpose of a matrix that has the vector 
		$\featurevec \in \mathbb{R}^{\dimlocalmodel}$ as its single column.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{X}^{T} &\quad & \parbox{.75\textwidth}{The transpose of a matrix $\mathbf{X} \in \mathbb{R}^{\samplesize \times \featurelen}$. 
		A square real-valued matrix $\mathbf{X} \in \mathbb{R}^{\samplesize \times \samplesize}$ 
		is called symmetric if $\mathbf{X} = \mathbf{X}^{T}$. }  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{0}= \big(0,\ldots,0\big)^{T}  & \quad &  \parbox{.75\textwidth}{The vector in $\mathbb{R}^{\dimlocalmodel}$ with each entry equal to zero.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{1}= \big(1,\ldots,1\big)^{T}  & \quad &  \parbox{.75\textwidth}{The vector in $\mathbb{R}^{\dimlocalmodel}$ with each entry equal to one.} \nonumber
\end{align} 
\newpage
\begin{align} 
	&\big(\vv^{T},\vw^{T} \big)^{T}  & \quad &  \parbox{.75\textwidth}{The vector of length $\featurelen+\featurelen'$ 
		obtained by concatenating the entries of vector $\vv \in \mathbb{R}^{\featurelen}$ with the entries of $\vw \in \mathbb{R}^{\featurelen'}$.} \nonumber \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&	{\rm span}\{ \mathbf{B} \}  & \quad &  \parbox{.75\textwidth}{The span of a matrix $\mathbf{B} \in \mathbb{R}^{a \times b}$, 
		which is the subspace of all linear combinations of the columns of $\mathbf{B}$, 
		${\rm span}\{ \mathbf{B} \} = \big\{  \mathbf{B} \va : \va \in \mathbb{R}^{b} \big\} \subseteq \mathbb{R}^{a}$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\determinant{\mC} &\quad & \parbox{.75\textwidth}{The determinant of the matrix $\mC$. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{A} \otimes \mathbf{B} &\quad & \parbox{.75\textwidth}{The Kronecker product of $\mathbf{A}$ and $\mathbf{B}$ \cite{Golub1980}. }  \nonumber
\end{align} 

\newpage
\section*{Probability Theory} 
\begin{align}
	\expect_{p} \{ f(\datapoint) \}  \quad\quad & \parbox{.75\textwidth}{The \gls{expectation} of a function $f(\datapoint)$ of a \gls{rv} 
		$\datapoint$ whose \gls{probdist} is $\prob{\datapoint}$. If the \gls{probdist} is clear from context, 
		we just write $\expect \{ f(\datapoint) \}$. }  \nonumber \\[2mm] \hline \nonumber\\[-5mm]    
	\prob{\featurevec,\truelabel} \quad\quad & \parbox{.75\textwidth}{A (joint) \gls{probdist} of an \gls{rv} 
		whose \gls{realization}s are \gls{datapoint}s with \gls{feature}s $\featurevec$ and \gls{label} $\truelabel$.} \nonumber        \nonumber \\[2mm] \hline \nonumber\\[-5mm]        
	\prob{\featurevec|\truelabel} \quad\quad & \parbox{.75\textwidth}{A conditional \gls{probdist} of an \gls{rv} 
		$\featurevec$ given the value of another \gls{rv} $\truelabel$ \cite[Sec.\ 3.5]{BertsekasProb}. } \nonumber       \nonumber \\[2mm] \hline \nonumber\\[-5mm]           
	\prob{\featurevec;\weights} \quad\quad & \parbox{.75\textwidth}{A parametrized \gls{probdist} of an \gls{rv} $\featurevec$. 
		The \gls{probdist} depends on a parameter vector $\weights$. For example, $\prob{\featurevec;\weights}$ could be a 
		\gls{mvndist} with the parameter vector $\weights$ given by the entries of the \gls{mean} vector $\expect \{ \featurevec \}$ 
		and the \gls{covmtx} $\expect \bigg \{ \big( \featurevec - \expect \{ \featurevec \}\big) \big( \featurevec - \expect \{ \featurevec \}\big)^{T}  \bigg\}$.} \nonumber           \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\mathcal{N}(\mu, \sigma^{2}) \quad\quad & \parbox{.75\textwidth}{The \gls{probdist} of a 
		\gls{gaussrv} $\feature \in \mathbb{R}$ with \gls{mean} (or \gls{expectation}) $\mu= \expect \{ \feature \}$ 
		and \gls{variance} $\sigma^{2} =   \expect \big\{  (  \feature - \mu )^2 \big\}$.} \nonumber    \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\mathcal{N}(\clustermean, \mathbf{C}) \quad\quad & \parbox{.75\textwidth}{The \gls{mvndist} of a vector-valued 
		\gls{gaussrv} $\featurevec \in \mathbb{R}^{\featuredim}$ with \gls{mean} (or \gls{expectation}) $\clustermean= \expect \{ \featurevec \}$ 
		and \gls{covmtx} $\mathbf{C} =  \expect \big\{ \big( \featurevec - \clustermean \big)\big( \featurevec - \clustermean \big)^{T} \big\}$.} \nonumber                                             
\end{align}





\newpage
\section*{Machine Learning}

\begin{align}
%	\datapoint \quad\quad & \parbox{.75\textwidth}{A \gls{datapoint} which is characterized by several properties that we 
%		divide into low-level properties (= \gls{feature}s) and high-level properties (= \gls{label}s) \cite[Ch. 2]{MLBasics}.}    \nonumber   \\[4mm] 
	\sampleidx \quad\quad & \parbox{.75\textwidth}{An index $\sampleidx=1,2,\ldots$ that 
		enumerates \gls{datapoint}s.}   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\samplesize \quad\quad &\parbox{.75\textwidth}{The number of \gls{datapoint}s in (i.e., the size of) a \gls{dataset}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	\dataset \quad\quad & \parbox{.75\textwidth}{A \gls{dataset} $\dataset = \{ \datapoint^{(1)},\ldots, \datapoint^{(\samplesize)} \}$ 
		is a list of individual \gls{datapoint}s $\datapoint^{(\sampleidx)}$, for $\sampleidx=1,\ldots,\samplesize$.}   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\featurelen \quad\quad &\parbox{.75\textwidth}{The number of \gls{feature}s that characterize a \gls{datapoint}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\feature_{\featureidx} \quad\quad &\parbox{.75\textwidth}{The $\featureidx$-th feature of a \gls{datapoint}. The first \gls{feature} 
		is denoted $\feature_{1}$, the second \gls{feature} $\feature_{2}$, and so on. } \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	\featurevec \quad\quad &\parbox{.75\textwidth}{The \gls{featurevec} $\featurevec=\big(\feature_{1},\ldots,\feature_{\featuredim}\big)^{T}$ of a \gls{datapoint} whose entries 
		are the individual \gls{feature}s of a \gls{datapoint}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\featurespace \quad\quad & \parbox{.75\textwidth}{The \gls{featurespace} $\featurespace$ is 
		the set of all possible values that the \gls{feature}s $\featurevec$ of a \gls{datapoint} can take on.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\rawfeaturevec \quad\quad &\parbox{.75\textwidth}{Instead of the symbol $\featurevec$, we 
		sometimes use $\rawfeaturevec$ as another symbol to denote a vector whose entries 
		are the individual \gls{feature}s of a \gls{datapoint}. We need two 
		different symbols to distinguish between raw and learned \gls{feature}s \cite[Ch. 9]{MLBasics}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\featurevec^{(\sampleidx)} \quad\quad &\parbox{.75\textwidth}{The \gls{feature} vector of the $\sampleidx$-th \gls{datapoint} within a \gls{dataset}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\feature_{\featureidx}^{(\sampleidx)}\quad\quad &\parbox{.75\textwidth}{The $\featureidx$-th \gls{feature} of the $\sampleidx$-th 
		\gls{datapoint} within a \gls{dataset}.} \nonumber
\end{align}        


\begin{align}
	\batch \quad\quad &\parbox{.75\textwidth}{A mini-\gls{batch} (or subset) of randomly chosen \gls{datapoint}s.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\batchsize \quad\quad &\parbox{.75\textwidth}{The size of (i.e., the number of \gls{datapoint}s in) a mini-\gls{batch}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\truelabel \quad\quad &\parbox{.75\textwidth}{The \gls{label} (or quantity of interest) of a \gls{datapoint}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\truelabel^{(\sampleidx)} \quad\quad &\parbox{.75\textwidth}{The \gls{label} of the $\sampleidx$-th \gls{datapoint}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\big(\featurevec^{(\sampleidx)},\truelabel^{(\sampleidx)}\big)  \quad\quad &\parbox{.75\textwidth}{The \gls{feature}s and \gls{label} of the $\sampleidx$-th \gls{datapoint}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\labelspace  \quad\quad & \parbox{.75\textwidth}{The \gls{labelspace} $\labelspace$ of 
		an \gls{ml} method consists of all potential \gls{label} values that a \gls{datapoint} can 
		carry. The nominal \gls{labelspace} might be larger than the set of different \gls{label} 
		values arising in a given \gls{dataset} (e.g., a \gls{trainset}). \Gls{ml} problems 
		(or methods) using a numeric \gls{labelspace}, such as $\labelspace=\mathbb{R}$ 
		or $\labelspace=\mathbb{R}^{3}$, are referred to as \gls{regression} problems (or methods). \Gls{ml} 
		problems (or methods) that use a discrete \gls{labelspace}, such as $\labelspace=\{0,1\}$ or $\labelspace=\{\mbox{\emph{cat}},\mbox{\emph{dog}},\mbox{\emph{mouse}}\}$, 
		are referred to as \gls{classification} problems (or methods).}  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\lrate  \quad\quad & \parbox{.75\textwidth}{\Gls{learnrate} (or \gls{stepsize}) used by \gls{gdmethods}.}  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\hypothesis(\cdot)  \quad\quad &\parbox{.75\textwidth}{A \gls{hypothesis} map that reads in \gls{feature}s $\featurevec$ of a \gls{datapoint} 
		and delivers a \gls{prediction} $\hat{\truelabel}=\hypothesis(\featurevec)$ for its \gls{label} $\truelabel$.}  	 \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	 \labelspace^{\featurespace} \quad\quad & \parbox{.75\textwidth}{Given two sets $\featurespace$ and $\labelspace$, we denote by $ \labelspace^{\featurespace}$ the set of all possible \gls{hypothesis} maps $\hypothesis: \featurespace \rightarrow \labelspace$.} 	 \nonumber 
\end{align}                  


\begin{align}
	\hypospace  \quad\quad & \parbox{.75\textwidth}{A \gls{hypospace} or \gls{model} used by an \gls{ml} method. 
		The \gls{hypospace} consists of different \gls{hypothesis} maps $\hypothesis: \featurespace \rightarrow \labelspace$, between which 
		the \gls{ml} method must choose.}   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\effdim{\hypospace}  \quad\quad & \parbox{.75\textwidth}{The \gls{effdim} of a \gls{hypospace} $\hypospace$.}   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\biasterm^2 \quad\quad &\parbox{.75\textwidth}{
	The squared \gls{bias} of a learned 
	\gls{hypothesis} $\learnthypothesis$, or its parameters. Note that $\learnthypothesis$ 
	becomes a \gls{rv} if it is learned from \gls{datapoint}s being \gls{rv}s.} \nonumber  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\varianceterm \quad\quad &\parbox{.75\textwidth}{The \gls{variance} of a learned 
	  \gls{hypothesis} $\learnthypothesis$, or its parameters. Note that $\learnthypothesis$ 
	  becomes a \gls{rv} if it is learned from \gls{datapoint}s being \gls{rv}s.} \nonumber \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\lossfunc{(\featurevec,\truelabel)}{\hypothesis}  \quad\quad & \parbox{.75\textwidth}{The \gls{loss} incurred by predicting the 
		\gls{label} $\truelabel$ of a \gls{datapoint} using the \gls{prediction} $\hat{\truelabel}=h(\featurevec)$. The 
		\gls{prediction} $\hat{\truelabel}$ is obtained by evaluating the \gls{hypothesis} $\hypothesis \in \hypospace$ for 
		the \gls{featurevec} $\featurevec$ of the \gls{datapoint}.}    \nonumber  \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	\valerror \quad\quad &\parbox{.75\textwidth}{The \gls{valerr} of a \gls{hypothesis} $\hypothesis$, which is its 
		average \gls{loss} incurred over a \gls{valset}.}  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\emperror\big(\hypothesis| \dataset \big) \quad\quad &\parbox{.75\textwidth}{The \gls{emprisk} or average \gls{loss} 
		incurred by the \gls{hypothesis} $\hypothesis$ on a \gls{dataset} $\dataset$.} \nonumber                           
\end{align}     

\begin{align}
	\trainerror \quad\quad &\parbox{.75\textwidth}{The \gls{trainerr} of a \gls{hypothesis} $\hypothesis$, which is its 
		average \gls{loss} incurred over a \gls{trainset}. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\timeidx \quad\quad &\parbox{.75\textwidth}{A discrete-time index $\timeidx=0,1,\ldots$ used to 
		enumerate sequential events (or time instants). }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\taskidx \quad\quad &\parbox{.75\textwidth}{An index that enumerates
		\gls{learningtask}s within a \gls{multitask learning} problem.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\regparam \quad\quad &\parbox{.75\textwidth}{A \gls{regularization} parameter that controls 
		the amount of \gls{regularization}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\eigval{\featureidx}\big( \mathbf{Q} \big) \quad\quad &\parbox{.75\textwidth}{The $\featureidx$-th 
		\gls{eigenvalue} (sorted in either ascending or descending order) of a \gls{psd} matrix $\mathbf{Q}$. We also 
		use the shorthand $\eigval{\featureidx}$ if the corresponding matrix is clear from context. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\actfun(\cdot) \quad\quad &\parbox{.75\textwidth}{The \gls{actfun} used by an artificial neuron within an \gls{ann}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\decreg{\hat{\truelabel}} \quad\quad &\parbox{.75\textwidth}{A \gls{decisionregion} within a \gls{featurespace}.  }\nonumber \\[2mm] \hline \nonumber\\[-5mm]  
	\weights  \quad\quad & \parbox{.75\textwidth}{A parameter vector $\weights = \big(\weight_{1},\ldots,\weight_{\featuredim}\big)^{T}$ 
		of a \gls{model}, e.g., the \gls{weights} of a \gls{linmodel} or in an \gls{ann}.}     \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\hypothesis^{(\weights)}(\cdot)  \quad\quad &\parbox{.75\textwidth}{A \gls{hypothesis} map that involves tunable \gls{modelparams} $\weight_{1},\ldots,\weight_{\featuredim}$ stacked into the vector $\weights=\big(\weight_{1},\ldots,\weight_{\featuredim} \big)^{T}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\featuremap(\cdot)  \quad\quad & \parbox{.75\textwidth}{A \gls{featuremap} $\featuremap: \featurespace \rightarrow \featurespace' : \featurevec \mapsto \featurevec' \defeq \featuremap\big( \featurevec \big) \in \featurespace'$.}   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\kernelmap{\cdot}{\cdot} \quad\quad & \parbox{.75\textwidth}{Given some \gls{featurespace} $\featurespace$, 
		a \gls{kernel} is a map $\kernel: \featurespace \times \featurespace \rightarrow \mathbb{C}$ that is \gls{psd}.}    \nonumber                                                                                                                                                     
\end{align}              






\newpage
\section*{Federated Learning}

\begin{align}
 	&\graph = \pair{\nodes}{\edges} & \quad & \parbox{.75\textwidth}{An undirected \gls{graph} whose nodes $\nodeidx \in \nodes$ represent 
	\gls{device}s within a \gls{empgraph}. The undirected weighted edges $\edges$ represent connectivity between 
	\gls{device}s and statistical similarities between their \gls{dataset}s and \gls{learningtask}s.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
&\nodeidx \in \nodes& \quad & \parbox{.75\textwidth}{A node that represents some 
	\gls{device} within an \gls{empgraph}. The device can access a \gls{localdataset} and train a \gls{localmodel}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\indsubgraph{\graph}{\cluster}& \quad & \parbox{.75\textwidth}{The induced subgraph of $\graph$ using the nodes in $\cluster \subseteq \nodes$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\LapMat{\graph}   & \quad & \parbox{.75\textwidth}{The \gls{LapMat} of a \gls{graph} $\graph$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
		&\LapMat{\cluster}   & \quad & \parbox{.75\textwidth}{The \gls{LapMat} of the induced \gls{graph} $\indsubgraph{\graph}{\cluster}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	 &		\neighbourhood{\nodeidx}  & \quad & \parbox{.75\textwidth}{The \gls{neighborhood} of a node $\nodeidx$ in a \gls{graph} $\graph$.}   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\nodedegree{\nodeidx} & \quad & \parbox{.75\textwidth}{The weighted degree $\nodedegree{\nodeidx}\defeq \sum_{\nodeidx' \in \neighbourhood{\nodeidx}} \edgeweight_{\nodeidx,\nodeidx'}$ of a node $\nodeidx$ in a \gls{graph} $\graph$.}  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\maxnodedegree^{(\graph)} & \quad & \parbox{.75\textwidth}{The maximum weighted node degree of a \gls{graph} $\graph$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
&\localdataset{\nodeidx} & \quad & \parbox{.75\textwidth}{The \gls{localdataset} $\localdataset{\nodeidx}$ carried by 
			node $\nodeidx\in \nodes$ of an \gls{empgraph}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
&\localsamplesize{\nodeidx} & \quad & \parbox{.75\textwidth}{The number of \gls{datapoint}s (i.e., \gls{samplesize}) contained in the 
			\gls{localdataset} $\localdataset{\nodeidx}$ at node $\nodeidx\in \nodes$.} \nonumber 
\end{align} 
\begin{align} 
		&\featurevec^{(\nodeidx,\sampleidx)} & \quad & \parbox{.75\textwidth}{The \gls{feature}s of the $\sampleidx$-th \gls{datapoint} in 
		the \gls{localdataset} $\localdataset{\nodeidx}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\truelabel^{(\nodeidx,\sampleidx)} & \quad & \parbox{.75\textwidth}{The \gls{label} of the $\sampleidx$-th \gls{datapoint} in 
		the \gls{localdataset} $\localdataset{\nodeidx}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
		&\localparams{\nodeidx} & \quad & \parbox{.75\textwidth}{The local \gls{modelparams} of \gls{device} $\nodeidx$ within an \gls{empgraph}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
		&\locallossfunc{\nodeidx}{\weights} & \quad & \parbox{.75\textwidth}{The local \gls{lossfunc} used by \gls{device} $\nodeidx$ 
		to measure the usefulness of some choice $\weights$ for the local \gls{modelparams}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	& \gtvloss{\featurevec}{\hypothesis\big(\featurevec\big)}{\hypothesis'\big(\featurevec\big)}& \quad & \parbox{.75\textwidth}{The \gls{loss} 
		incurred by a \gls{hypothesis} $\hypothesis'$ on a \gls{datapoint} with \gls{feature}s $\featurevec$ and \gls{label} 
		$\hypothesis\big( \featurevec\big)$ that is obtained from another \gls{hypothesis}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
		& 	{\rm stack} \big\{ \weights^{(\nodeidx)} \big\}_{\nodeidx=1}^{\nrnodes} & \quad & \parbox{.75\textwidth}{The vector $\bigg( \big(\weights^{(1)}  \big)^{T}, \ldots, \big(\weights^{(\nrnodes)}  \big)^{T} \bigg)^{T} \in \mathbb{R}^{\dimlocalmodel\nrnodes}$ that 
			is obtained by vertically stacking the local \gls{modelparams} $\weights^{(\nodeidx)} \in \mathbb{R}^{\dimlocalmodel}$.} \nonumber  
\end{align}        



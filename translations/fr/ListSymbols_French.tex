% !TeX spellcheck = fr_FR

\section*{Notations et symboles}
%\label{ch_list_of_symbols}

\vspace*{-2mm}
\subsection*{Ensembles et fonctions} 

\begin{align} 
	&a \in \mathcal{A} & \quad & \parbox{.75\textwidth}{L'objet $a$ est un élément de l'ensemble $\mathcal{A}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&a \defeq b & \quad & \parbox{.75\textwidth}{On note $a$ comme abréviation de $b$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&|\mathcal{A}| & \quad & \parbox{.75\textwidth}{Le cardinal (i.e., le nombre d'éléments) d'un ensemble fini $\mathcal{A}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathcal{A} \subseteq \mathcal{B}& \quad & \parbox{.75\textwidth}{$\mathcal{A}$ est un sous-ensemble de $\mathcal{B}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathcal{A} \subset \mathcal{B}& \quad & \parbox{.75\textwidth}{$\mathcal{A}$ est un sous-ensemble strict de $\mathcal{B}$ (i.e., non égal à $\mathcal{B}$).} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{N} & \quad & \parbox{.75\textwidth}{Les entiers naturels $1,2,\ldots$} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}  &\quad &\parbox{.75\textwidth}{Les nombres réels $x$ \cite{RudinBook}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}_{+}  &\quad &\parbox{.75\textwidth}{Les réels positifs ou nuls $x\geq0$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}_{++}  &\quad &\parbox{.75\textwidth}{Les réels strictement positifs $x> 0$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\{0,1\}& \quad & \parbox{.75\textwidth}{L'ensemble composé des deux réels $0$ et $1$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&[0,1] &\quad &\parbox{.75\textwidth}{L'intervalle fermé des nombres réels $x$ tels que $0 \leq x \leq 1$.} \nonumber
\end{align} 

\newpage
\begin{align}
	&\argmin_{\weights} f(\weights) &\quad &\parbox{.70\textwidth}{L'ensemble des points qui minimisent la fonction à valeurs réelles \gls{function} $f(\weights)$. 
		\\ Voir aussi: \gls{function}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\sphere{\nrnodes} &\quad &\parbox{.70\textwidth}{L'ensemble des \glspl{vector} de \gls{norm} unitaire dans $\mathbb{R}^{\nrnodes+1}$.
		\\ Voir aussi: \gls{norm}, \gls{vector}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	& \exp(a) & \quad & \parbox{.70\textwidth}{La \gls{function} exponentielle évaluée en un réel $a \in \mathbb{R}$. 
		\\ Voir aussi: \gls{function}.} \nonumber \\[2mm] \hline \nonumber \\[-5mm]
	&\log(a) &\quad &\parbox{.70\textwidth}{Le logarithme d'un réel strictement positif $a \in \mathbb{R}_{++}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&f(\cdot)\!:\!\mathcal{A}\!\rightarrow\!\mathcal{B} :  a \!\mapsto\!f(a) &\quad &\parbox{.70\textwidth}{
		Une \gls{function} (ou \gls{map}) d'un ensemble $\mathcal{A}$ dans un ensemble $\mathcal{B}$, qui associe à chaque entrée
		$a \in \mathcal{A}$ une image bien définie $f(a) \in \mathcal{B}$.
		L'ensemble $\mathcal{A}$ est le domaine de définition de la \gls{function} $f$ et l'ensemble $\mathcal{B}$ est l'ensemble d'arrivée de $f$. L'\gls{ml} vise à apprendre une \gls{function} $\hypothesis$ qui prend en entrée les \glspl{feature} 
		$\featurevec$ d'un \gls{datapoint} et renvoie une \gls{prediction} $\hypothesis(\featurevec)$ pour son étiquette \gls{label} $\truelabel$.
		\\ Voir aussi: \gls{function}, \gls{map}, \gls{ml}, \gls{hypothesis}, \gls{feature}, \gls{datapoint}, \gls{prediction}, \gls{label}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\operatorname{epi}(f)  & \quad & \parbox{.70\textwidth}{L' \gls{epigraph} d'une \gls{function} à valeurs réelles
		$f: \mathbb{R}^{\featuredim}\rightarrow \mathbb{R}$.
		\\ Voir aussi: \gls{epigraph}, \gls{function}.} \nonumber
\end{align} 

\newpage
\begin{align}
	&  \frac{\partial f(\weight_{1},\ldots,\weight_{\nrfeatures})}{\partial \weight_{\featureidx}} & \quad & \parbox{.70\textwidth}{La dérivée partielle (si elle existe)
		d'une \gls{function} à valeurs réelles $f: \mathbb{R}^{\featuredim}\rightarrow \mathbb{R}$ par rapport à\ $\weight_{\featureidx}$\cite[Ch. 9]{RudinBookPrinciplesMatheAnalysis}.
		\\ Voir aussi: \gls{function}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\nabla f(\weights) & \quad & \parbox{.70\textwidth}{Le \gls{gradient} d'une \gls{function} à valeurs réelles \gls{differentiable} 
		$f: \mathbb{R}^{\featuredim}\rightarrow \mathbb{R}$ est le \gls{vector}
		$\nabla f(\weights) = \big( \frac{\partial f}{\partial \weight_{1}},\ldots,\frac{\partial f}{\partial \weight_{\featuredim}}  \big)^{T} \in \mathbb{R}^{\featuredim}$ \cite[Ch. 9]{RudinBookPrinciplesMatheAnalysis}.
		\\ Voir aussi: \gls{gradient}, \gls{differentiable}, \gls{function}, \gls{vector}.} \nonumber
\end{align} 


\subsection*{Matrices et Vecteurs} 

\begin{align} 
	&\featurevec=\big(\feature_{1},\ldots,\feature_{\featuredim}\big)^{T} &\quad & \parbox{.75\textwidth}{Un \gls{vector} de taille $\featuredim$, dont la $\featureidx$-ième composante est $\feature_{\featureidx}$.
	\\ Voir aussi: \gls{vector}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}^{\featuredim} & \quad &  \parbox{.75\textwidth}{L’ensemble des \glspl{vector} $\featurevec=\big(\feature_{1},\ldots,\feature_{\featuredim}\big)^{T}$ constitués de $\featuredim$ composantes réelles $\feature_{1},\ldots,\feature_{\featuredim} \in \mathbb{R}$.
		\\ Voir aussi: \gls{vector}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{I}_{\modelidx \times \featuredim}  & \quad &  \parbox{.75\textwidth}{Une \gls{matrix} identité généralisée de $\modelidx$ lignes et $\featuredim$ colonnes. Les composantes de $\mathbf{I}_{\modelidx \times \featuredim} \in \mathbb{R}^{\modelidx \times \featuredim}$ valent $1$ sur la diagonale principale et $0$ ailleurs.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{I}_{\dimlocalmodel}, \mathbf{I} & \quad &  \parbox{.75\textwidth}{Une \gls{matrix} identité carrée de taille $\dimlocalmodel \times \dimlocalmodel$. Si la dimension est claire dans le contexte, on peut omettre l'indice.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\normgeneric{\featurevec}{2}  &\quad & \parbox{.75\textwidth}{La \gls{norm} euclidienne (ou $\ell_{2}$) du \gls{vector} $\featurevec=\big(\feature_{1},\ldots,\feature_{\featuredim}\big)^{T} \in \mathbb{R}^{\featuredim}$ définie par $ \| \featurevec \|_{2} \defeq \sqrt{\sum_{\featureidx=1}^{\featuredim} \feature_{\featureidx}^{2}}$.
		\\ Voir aussi: \gls{norm}, \gls{vector}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	&\normgeneric{\featurevec}{}  & \quad &  \parbox{.75\textwidth}{Une certaine \gls{norm} du \gls{vector} $\featurevec \in \mathbb{R}^{\featuredim}$ \cite{GolubVanLoanBook}. Sauf indication contraire, on entend par là la \gls{norm} euclidienne $\normgeneric{\featurevec}{2}$.
		\\ Voir aussi: \gls{norm}, \gls{vector}} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\featurevec^{T} &\quad & \parbox{.75\textwidth}{La transposée d’une \gls{matrix} ayant pour unique colonne le \gls{vector} $\featurevec \in \mathbb{R}^{\dimlocalmodel}$.
		\\ Voir aussi: \gls{vector}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{X}^{T} &\quad & \parbox{.75\textwidth}{La transposée d’une \gls{matrix} $\mathbf{X} \in \mathbb{R}^{\samplesize \times \featuredim}$. Une \gls{matrix} carrée à valeurs réelles $\mathbf{X} \in \mathbb{R}^{\samplesize \times \samplesize}$ est dite symétrique si $\mathbf{X} = \mathbf{X}^{T}$.}  \nonumber 
\end{align}

\newpage
\begin{align} 
	&\mathbf{X}^{-1} &\quad & \parbox{.75\textwidth}{La \gls{inverse} d'une \gls{matrix} $\mathbf{X} \in \mathbb{R}^{\featuredim \times \featuredim}$.
		\\ Voir aussi: \gls{inverse}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{0}= \big(0,\ldots,0\big)^{T}  & \quad &  \parbox{.75\textwidth}{Le \gls{vector} de $\mathbb{R}^{\dimlocalmodel}$ dont toutes les composantes valent 0.
		\\ Voir aussi: \gls{vector}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{1}= \big(1,\ldots,1\big)^{T}  & \quad &  \parbox{.75\textwidth}{Le \gls{vector} de $\mathbb{R}^{\dimlocalmodel}$ dont toutes les composantes valent 1.
		\\ Voir aussi: \gls{vector}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\big(\vv^{T},\vw^{T} \big)^{T}  & \quad &  \parbox{.75\textwidth}{Le \gls{vector} de longueur $\featuredim+\featuredim'$ obtenu en concaténant les $\vv \in \mathbb{R}^{\featuredim}$ avec celles de $\vw \in \mathbb{R}^{\featuredim'}$.
		\\ Voir aussi: \gls{vector}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&	{\rm span}\{ \mathbf{B} \}  & \quad &  \parbox{.75\textwidth}{Le sous-espace engendré par une \gls{matrix} $\mathbf{B} \in \mathbb{R}^{a \times b}$, c’est-à-dire l’ensemble de toutes les combinaisons linéaires des colonnes de $\mathbf{B}$ : ${\rm span}\{ \mathbf{B} \} = \big\{  \mathbf{B} \va : \va \in \mathbb{R}^{b} \big\} \subseteq \mathbb{R}^{a}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&{\rm null}\{ \mA\}& \quad &  \parbox{.85\textwidth}{Le \gls{nullspace} d'une \gls{matrix} $\mathbf{A} \in \mathbb{R}^{a \times b}$, 
	qui est le sous-espace des \glspl{vector} $\va \in \mathbb{R}^{b}$ tels que $\mA \va = \mathbf{0}$.
		\\ Voir aussi : \gls{matrix}. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\determinant{\mC} &\quad & \parbox{.75\textwidth}{Le \gls{det} de la \gls{matrix} $\mC$. \\ Voir aussi: \gls{det}} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{A} \otimes \mathbf{B} &\quad & \parbox{.75\textwidth}{Le produit de Kronecker des \glspl{matrix} $\mathbf{A}$ et $\mathbf{B}$ \cite{Golub1980}. \\ Voir aussi: \gls{kroneckerproduct}} \nonumber
\end{align}

\newpage
\subsection*{Théorie des probabilités} 

\begin{align}
		&\prob{\mathcal{A}} &\quad & \parbox{.85\textwidth}{La probabilité de l'événement mesurable $\mathcal{A}$. 
		\\ Voir aussi: \gls{probability}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\featurevec \sim p(\vz) & \quad & \parbox{.75\textwidth}{
		La \gls{rv} $\featurevec$ suit la \gls{probdist} $p(\vz)$ \cite{klenke2020probability,BillingsleyProbMeasure}. \\
		Voir aussi : \gls{rv}, \gls{probdist}
	} \nonumber \\[2mm] \hline \nonumber\\[-5mm]    
	&\expect_{p} \{ f(\datapoint) \} & \quad & \parbox{.75\textwidth}{
		L'\gls{expectation} d'une \gls{rv} $f(\datapoint)$ obtenue en appliquant une \gls{function} déterministe $f$ à une \gls{rv} 
		$\datapoint$ dont la \gls{probdist} est $\prob{\datapoint}$. Si la \gls{probdist} est claire dans le contexte, 
		on écrit simplement $\expect \{ f(\datapoint) \}$. \\
		Voir aussi : \gls{expectation}, \gls{rv}, \gls{function}, \gls{probdist}
	} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\cov{x}{y} & \quad & \parbox{.85\textwidth}{
		La \gls{covariance} entre deux \gls{rv} à valeurs réelles définies 
		sur un même \gls{probspace}. \\
		Voir aussi : \gls{covariance}, \gls{rv}, \gls{probspace}
	} \nonumber \\[2mm] \hline \nonumber\\[-5mm]     
	&\prob{\featurevec,\truelabel} & \quad & \parbox{.75\textwidth}{
		Une \gls{probdist} (conjointe) d'une \gls{rv} 
		dont les \glspl{realization} sont des \glspl{datapoint} avec des \glspl{feature} $\featurevec$ et une \gls{label} $\truelabel$. \\
		Voir aussi : \gls{probdist}, \gls{rv}, \gls{realization}, \gls{datapoint}, \gls{feature}, 
		\gls{label}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]        
	&\prob{\featurevec|\truelabel} & \quad & \parbox{.75\textwidth}{
		Une \gls{probdist} conditionnelle d'une \gls{rv} 
		$\featurevec$ étant donnée la valeur d'une autre \gls{rv} $\truelabel$ \cite[Sec.\ 3.5]{BertsekasProb}. \\
		Voir aussi : \gls{probdist}, \gls{rv}.
	} \nonumber                                             
\end{align}

\begin{align}
	&\prob{\featurevec;\weights} & \quad & \parbox{.75\textwidth}{
		Une \gls{probdist} paramétrée d'une \gls{rv} $\featurevec$. 
		La \gls{probdist} dépend d'un \gls{vector} de \glspl{parameter} $\weights$. Par exemple, $\prob{\featurevec;\weights}$ pourrait être une 
		\gls{mvndist} avec un \gls{vector} de \glspl{parameter} $\weights$ donné par les composantes du \gls{vector} de \gls{mean} $\expect \{ \featurevec \}$ 
		et la \gls{covmtx} $\expect \bigg \{ \big( \featurevec - \expect \{ \featurevec \}\big) \big( \featurevec - \expect \{ \featurevec \}\big)^{T}  \bigg\}$. \\
		Voir aussi : \gls{probdist}, \gls{rv}, \gls{parameter}, \gls{mvndist}, \gls{mean}, \gls{covmtx}, \gls{vector}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathcal{N}(\mu, \sigma^{2}) & \quad & \parbox{.75\textwidth}{
		La \gls{probdist} d'une 
		\gls{gaussrv} $\feature \in \mathbb{R}$ ayant comme \gls{mean} (ou \gls{expectation}) $\mu= \expect \{ \feature \}$ 
		et comme \gls{variance} $\sigma^{2} =   \expect \big\{  (  \feature - \mu )^2 \big\}$. \\
		Voir aussi : \gls{gaussrv}, \gls{mean}, \gls{expectation}, \gls{variance}, \gls{probdist}
	} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathcal{N}(\clustermean, \mathbf{C}) & \quad & \parbox{.75\textwidth}{
		La \gls{mvndist} d'une \gls{gaussrv} vectorielle 
		$\featurevec \in \mathbb{R}^{\featuredim}$ ayant comme \gls{mean} (ou \gls{expectation}) $\clustermean= \expect \{ \featurevec \}$ 
		et comme \gls{covmtx} $\mathbf{C} =  \expect \big\{ \big( \featurevec - \clustermean \big)\big( \featurevec - \clustermean \big)^{T} \big\}$. \\
		Voir aussi : \gls{mvndist}, \gls{gaussrv}, \gls{mean}, \gls{covmtx}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\Omega &\quad & \parbox{.85\textwidth}{Un \gls{samplespace}, c’est-à-dire l’ensemble de tous les résultats possibles d’une expérience aléatoire. \\ Voir aussi : \gls{event}.}  \nonumber
\end{align}

\begin{align}
	&\mathcal{F} &\quad & \parbox{.85\textwidth}{Une collection de sous-ensembles \glspl{measurable} d’un \gls{samplespace} $\Omega$. \\ Voir aussi : \gls{samplespace}, \gls{event}.}  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathcal{P} &\quad & \parbox{.85\textwidth}{Un \gls{probspace} constitué d’un \gls{samplespace} $\Omega$, d’une tribu $\mathcal{F}$ de sous-ensembles \glspl{measurable} de $\Omega$, et d’une \gls{probdist} $\prob{\cdot}$. \\ Voir aussi : \gls{probdist}, \gls{samplespace}, \gls{measurable}.} \nonumber
\end{align}


\newpage
\subsection*{Apprentissage automatique}

\begin{align}
	& \sampleidx & \quad & \parbox{.75\textwidth}{Un indice $\sampleidx=1,2,\ldots$ qui énumère les \glspl{datapoint}. \\ Voir aussi : \glspl{datapoint}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	& \samplesize & \quad & \parbox{.75\textwidth}{Le nombre de \glspl{datapoint} dans un \gls{dataset} (c’est-à-dire la taille du \gls{dataset}). \\ Voir aussi : \glspl{datapoint}, \gls{dataset}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	& \dataset & \quad & \parbox{.75\textwidth}{Un \gls{dataset} $\dataset = \{ \datapoint^{(1)},\ldots, \datapoint^{(\samplesize)} \}$ est une liste de \glspl{datapoint} individuels $\datapoint^{(\sampleidx)}$, pour $\sampleidx=1,\ldots,\samplesize$. \\ Voir aussi : \glspl{datapoint}, \gls{dataset}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	& \featurelen & \quad & \parbox{.75\textwidth}{Le nombre de \glspl{feature} qui constituent un \gls{datapoint}. \\ Voir aussi : \glspl{feature}, \gls{datapoint}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	& \feature_{\featureidx} & \quad & \parbox{.75\textwidth}{La $\featureidx$-ième \gls{feature} d’un \gls{datapoint}. La première \gls{feature} est notée $\feature_{1}$, la deuxième $\feature_{2}$, et ainsi de suite. \\ Voir aussi : \glspl{feature}, \gls{datapoint}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	& \featurevec & \quad & \parbox{.75\textwidth}{Le \gls{featurevec} $\featurevec=\big(\feature_{1},\ldots,\feature_{\featuredim}\big)^{T}$ d’un \gls{datapoint}, dont les composantes sont les différentes \glspl{feature} du \gls{datapoint}. \\ Voir aussi : \gls{featurevec}, \glspl{feature}, \gls{datapoint}.} \nonumber
\end{align}


\begin{align}	
	& \featurespace & \quad & \parbox{.75\textwidth}{L'\gls{featurespace} $\featurespace$ est l’ensemble de toutes les valeurs possibles que les \glspl{feature} $\featurevec$ d’un \gls{datapoint} peuvent prendre. \\ Voir aussi : \gls{featurespace}, \glspl{feature}, \gls{datapoint}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	& \rawfeaturevec & \quad & \parbox{.75\textwidth}{Au lieu du symbole $\featurevec$, on utilise parfois $\rawfeaturevec$ comme un autre symbole pour désigner un \gls{vector} dont les composantes sont les différentes \glspl{feature} d’un \gls{datapoint}. On a besoin de deux symboles différents pour distinguer les \glspl{feature} brutes des \glspl{feature} apprises \cite[Ch. 9]{MLBasics}. \\ Voir aussi : \glspl{feature}, \gls{datapoint}, \gls{vector}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	& \featurevec^{(\sampleidx)} & \quad & \parbox{.75\textwidth}{Le \gls{featurevec} du $\sampleidx$-ième \gls{datapoint} dans un \gls{dataset}. \\ Voir aussi : \gls{featurevec}, \gls{datapoint}, \gls{dataset}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	& \feature_{\featureidx}^{(\sampleidx)} & \quad & \parbox{.75\textwidth}{La $\featureidx$-ième \gls{feature} du $\sampleidx$-ième \gls{datapoint} dans un \gls{dataset}. \\ Voir aussi : \glspl{feature}, \gls{datapoint}, \gls{dataset}.} \nonumber  \\[2mm] \hline \nonumber\\[-5mm]
	& \truelabel & \quad & \parbox{.75\textwidth}{L'\gls{label} (ou quantité d’intérêt) d’un \gls{datapoint}. \\ Voir aussi : \gls{label}, \gls{datapoint}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	& \truelabel^{(\sampleidx)} & \quad & \parbox{.75\textwidth}{L'\gls{label} du $\sampleidx$-ième \gls{datapoint}. \\ Voir aussi : \gls{label}, \gls{datapoint}.} \nonumber
\end{align}

\begin{align}
	& \big(\featurevec^{(\sampleidx)},\truelabel^{(\sampleidx)}\big) & \quad & \parbox{.75\textwidth}{Les \glspl{feature} et l'\gls{label} du $\sampleidx$-ième \gls{datapoint}. \\ Voir aussi : \glspl{feature}, \gls{label}, \gls{datapoint}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	& \labelspace & \quad & \parbox{.75\textwidth}{
	L’\gls{labelspace} $\labelspace$ d’une méthode d’\gls{ml} comprend toutes les valeurs d'\gls{label} qu’un \gls{datapoint} peut porter. L’\gls{labelspace} nominal peut être plus grand que l’ensemble des différentes valeurs d'\gls{label} présentes dans un \gls{dataset} donné (par exemple, un \gls{trainset}). \\
	Les problèmes (ou méthodes) d'\gls{ml} utilisant un \gls{labelspace} numérique, comme $\labelspace=\mathbb{R}$ ou $\labelspace=\mathbb{R}^{3}$, sont appelés problèmes (ou méthodes) de \gls{regression}. \\
	Les problèmes (ou méthodes) d'\gls{ml} utilisant un \gls{labelspace} discret, comme $\labelspace=\{0,1\}$ ou $\labelspace=\{\mbox{\emph{chat}},\mbox{\emph{chien}},\mbox{\emph{souris}}\}$, sont appelés problèmes (ou méthodes) de \gls{classification}. \\ 
	Voir aussi : \gls{labelspace}, \gls{ml}, \gls{label}, \gls{datapoint},  \gls{dataset}, \gls{trainset}, 
	\gls{regression}, \gls{classification}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	& \batch & \quad & \parbox{.75\textwidth}{Un mini-\gls{batch} (ou sous-ensemble) de \glspl{datapoint} choisis aléatoirement. \\ Voir aussi : \gls{batch}, \glspl{datapoint}.} \nonumber
\end{align}


\begin{align}
	& \batchsize & \quad & \parbox{.75\textwidth}{La taille (c’est-à-dire le nombre de \glspl{datapoint}) d’un mini-\gls{batch}. \\ Voir aussi : \gls{batch}, \glspl{datapoint}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	& \hypothesis(\cdot) & \quad & \parbox{.75\textwidth}{Une fonction \gls{hypothesis} qui lit les \glspl{feature} $\featurevec$ d’un \gls{datapoint} et produit une \gls{prediction} $\hat{\truelabel}=\hypothesis(\featurevec)$ pour son \gls{label} $\truelabel$. \\ Voir aussi : \gls{hypothesis}, \gls{map}, \gls{feature}, \gls{datapoint}, \gls{prediction}, \gls{label}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	& \labelspace^{\featurespace} & \quad & \parbox{.75\textwidth}{Étant donnés deux ensembles $\featurespace$ et $\labelspace$, on note $\labelspace^{\featurespace}$ l’ensemble de toutes les fonctions \glspl{hypothesis} possibles $\hypothesis: \featurespace \rightarrow \labelspace$. \\ Voir aussi : \gls{hypothesis}, \gls{map}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	& \hypospace & \quad & \parbox{.75\textwidth}{Un \gls{hypospace} ou \gls{model} utilisé par une méthode d’\gls{ml}. L'\gls{hypospace} est constitué des différentes \glspl{hypothesis} $\hypothesis: \featurespace \rightarrow \labelspace$, parmi lesquelles la méthode d’\gls{ml} doit choisir. \\ Voir aussi : \gls{hypospace}, \gls{model}, \gls{ml}, \gls{hypothesis}, \gls{map}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	& \effdim{\hypospace} & \quad & \parbox{.75\textwidth}{La \gls{effdim} d’un \gls{hypospace} $\hypospace$. \\ Voir aussi : \gls{effdim}, \gls{hypospace}.} \nonumber
\end{align}

\begin{align}
	& \biasterm^2 & \quad & \parbox{.75\textwidth}{Le \gls{bias} au carré d’une \gls{hypothesis} apprise $\learnthypothesis$, ou de ses \glspl{parameter}. 
		Notons que $\learnthypothesis$ devient une \gls{rv} lorsqu’elle est apprise à partir de \glspl{datapoint} 
		eux-mêmes considérés comme des \glspl{rv}.
		\\ Voir aussi : \gls{bias}, \gls{hypothesis}, \gls{parameter}, \gls{rv}, \gls{datapoint}.
	} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	& \varianceterm & \quad & \parbox{.75\textwidth}{La \gls{variance} d’une \gls{hypothesis} apprise $\learnthypothesis$, ou de ses \glspl{parameter}. 
		Notons que $\learnthypothesis$ devient une \gls{rv} lorsqu’elle est apprise à partir de \glspl{datapoint} 
		eux-mêmes considérés comme des \glspl{rv}.
		\\ Voir aussi : \gls{variance}, \gls{hypothesis}, \gls{parameter}, \gls{rv}, \gls{datapoint}.
	} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	& \lossfunc{(\featurevec,\truelabel)}{\hypothesis} & \quad & \parbox{.75\textwidth}{La \gls{loss} encourue en prédisant l'\gls{label} $\truelabel$ d’un \gls{datapoint} à l’aide de la \gls{prediction} $\hat{\truelabel}=h(\featurevec)$. La \gls{prediction} $\hat{\truelabel}$ est obtenue en évaluant la fonction \gls{hypothesis} $\hypothesis \in \hypospace$ en $\featurevec$, le \gls{featurevec} du \gls{datapoint}. \\ Voir aussi : \gls{loss}, \gls{label}, \gls{prediction}, \gls{hypothesis}, \gls{featurevec}, \gls{datapoint}.} \nonumber
\end{align}

\begin{align}
	& \valerror & \quad & \parbox{.75\textwidth}{L'\gls{valerr} d’une \gls{hypothesis} $\hypothesis$, c’est-à-dire sa \gls{loss} moyenne sur un \gls{valset}. \\ Voir aussi : \gls{valerr}, \gls{loss}, \gls{hypothesis}, \gls{valset}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	& \emperror\big(\hypothesis| \dataset \big) & \quad & \parbox{.75\textwidth}{Le \gls{emprisk}, ou \gls{loss} moyenne, encouru par l'\gls{hypothesis} $\hypothesis$ sur un \gls{dataset} $\dataset$. \\ Voir aussi : \gls{emprisk}, \gls{loss}, \gls{hypothesis}, \gls{dataset}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	& \trainerror & \quad & \parbox{.75\textwidth}{L’\gls{trainerr} d’une \gls{hypothesis} $\hypothesis$, c’est-à-dire sa \gls{loss} moyenne sur un \gls{trainset}. \\ Voir aussi : \gls{trainerr}, \gls{loss}, \gls{hypothesis}, \gls{trainset}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	& \timeidx & \quad & \parbox{.75\textwidth}{Un indice de temps discret $\timeidx=0,1,\ldots$ utilisé pour énumérer des événements séquentiels (ou des instants temporels).} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	& \regparam & \quad & \parbox{.75\textwidth}{
	Un \gls{parameter} de \gls{regularization} qui contrôle la quantité de \gls{regularization}. \\
	[2mm] Voir aussi : \gls{parameter}, \gls{regularization}.
	} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	& \taskidx & \quad & \parbox{.75\textwidth}{Un indice qui énumère les \glspl{learningtask} dans un problème d'\gls{multitask learning}. \\ Voir aussi : \glspl{learningtask}, \gls{multitask learning}.} \nonumber
\end{align}


\begin{align}
	& \lrate & \quad & \parbox{.75\textwidth}{
		Le \gls{learnrate} (ou \gls{stepsize}) utilisé par les \gls{gdmethods}. \\
		[2mm] Voir aussi : \gls{learnrate}, \gls{stepsize}, \gls{gdmethods}
	} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	& \eigval{\featureidx}\big( \mathbf{Q} \big) & \quad & \parbox{.75\textwidth}{
		La $\featureidx$-ième \gls{eigenvalue} (triée par ordre croissant ou décroissant) d’une \gls{matrix} \gls{psd} $\mathbf{Q}$. Si la \gls{matrix} est claire dans le contexte, on écrit simplement $\eigval{\featureidx}$. \\
		[2mm] Voir aussi : \gls{eigenvalue}, \gls{psd}
	} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	& \actfun(\cdot) & \quad & \parbox{.75\textwidth}{
		La \gls{actfun} utilisée par un neurone artificiel dans un \gls{ann}. \\
		[2mm] Voir aussi : \gls{actfun}, \gls{ann}
	} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	& \decreg{\hat{\truelabel}} & \quad & \parbox{.75\textwidth}{
		Une \gls{decisionregion} dans un \gls{featurespace}. \\
		[2mm] Voir aussi : \gls{decisionregion}, \gls{featurespace}
	} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	& \weights & \quad & \parbox{.75\textwidth}{
		Un \gls{vector} de paramètres $\weights = \big(\weight_{1},\ldots,\weight_{\featuredim}\big)^{T}$ d’un \gls{model}, par exemple les \gls{weights} d’un \gls{linmodel} ou dans un \gls{ann}. \\
		[2mm] Voir aussi : \gls{weights}, \gls{model}, \gls{linmodel}, \gls{ann}, \gls{vector}
	} \nonumber
\end{align}

\begin{align}
	& \hypothesis^{(\weights)}(\cdot) & \quad & \parbox{.75\textwidth}{
	Une fonction \gls{hypothesis} qui dépend de \gls{modelparams} $\weight_{1},\ldots,\weight_{\featuredim}$ regroupés dans le \gls{vector} $\weights=\big(\weight_{1},\ldots,\weight_{\featuredim} \big)^{T}$ et qui peuvent être ajustés. \\
	[2mm] Voir aussi : \gls{hypothesis}, \gls{modelparams}, \gls{map}, \gls{vector}
	} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	& \featuremap(\cdot) & \quad & \parbox{.75\textwidth}{
		Une \gls{featuremap} $\featuremap: \featurespace \rightarrow \featurespace' : \featurevec \mapsto \featurevec' \defeq \featuremap\big( \featurevec \big) \in \featurespace'$. \\
		[2mm] Voir aussi : \gls{featuremap}, \gls{featurespace}
	} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	& \kernelmap{\cdot}{\cdot} & \quad & \parbox{.75\textwidth}{
		Étant donné un \gls{featurespace} $\featurespace$, un \gls{kernel} est une \gls{map} $\kernel: \featurespace \times \featurespace \rightarrow \mathbb{C}$ qui est \gls{psd}. \\
		[2mm] Voir aussi : \gls{kernel}, \gls{featurespace}, \gls{psd}, \gls{map}
	} \nonumber
\end{align}
            

\newpage
\subsection*{Apprentissage fédéré}

\begin{align}
	&\graph = \pair{\nodes}{\edges} & \quad & \parbox{.75\textwidth}{
		Un \gls{graph} non orienté dont les sommets $\nodeidx \in \nodes$ représentent des \glspl{device} au sein d’un \gls{empgraph}. Les arêtes pondérées non orientées $\edges$ représentent la connectivité entre les \glspl{device} et les similarités statistiques entre leurs \glspl{dataset} et \glspl{learningtask}. \\
		[2mm] Voir aussi : \gls{graph}, \gls{device}, \gls{empgraph}, \gls{dataset}, \gls{learningtask}
	} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\nodeidx \in \nodes & \quad & \parbox{.75\textwidth}{
		Un sommet représentant un \gls{device} dans un \gls{empgraph}. L'\gls{device} peut accéder à un \gls{localdataset} et entraîner un \gls{localmodel}. \\
		[2mm] Voir aussi : \gls{device}, \gls{empgraph}, \gls{localdataset}, \gls{localmodel}
	} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\indsubgraph{\graph}{\cluster} & \quad & \parbox{.75\textwidth}{
		Le sous-\gls{graph} induit de $\graph$ utilisant les sommets de $\cluster \subseteq \nodes$. \\
		[2mm] Voir aussi : \gls{graph}
	} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\LapMat{\graph} & \quad & \parbox{.75\textwidth}{
		La \gls{LapMat} d’un \gls{graph} $\graph$. \\
		[2mm] Voir aussi : \gls{LapMat}, \gls{graph}
	} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\LapMat{\cluster} & \quad & \parbox{.75\textwidth}{
		La \gls{LapMat} du \gls{graph} induit $\indsubgraph{\graph}{\cluster}$. \\
		[2mm] Voir aussi : \gls{LapMat}, \gls{graph}
	} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\neighbourhood{\nodeidx} & \quad & \parbox{.75\textwidth}{
		Le \gls{neighborhood} du sommet $\nodeidx$ dans un \gls{graph} $\graph$. \\
		[2mm] Voir aussi : \gls{neighborhood}, \gls{graph}
	} \nonumber
\end{align}

\begin{align}
	&\nodedegree{\nodeidx} & \quad & \parbox{.75\textwidth}{
	Le degré pondéré $\nodedegree{\nodeidx}\defeq \sum_{\nodeidx' \in \neighbourhood{\nodeidx}} \edgeweight_{\nodeidx,\nodeidx'}$ d’un sommet $\nodeidx$ dans un \gls{graph} $\graph$. \\
	[2mm] Voir aussi : \gls{graph}.
	} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\maxnodedegree^{(\graph)} & \quad & \parbox{.85\textwidth}{Le \gls{nodedegree} pondéré maximal d’un \gls{graph} $\graph$. 
		\\ Voir aussi : \gls{maximum}, \gls{nodedegree}, \gls{graph}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\localdataset{\nodeidx} & \quad & \parbox{.75\textwidth}{
		Le \gls{localdataset} $\localdataset{\nodeidx}$ détenu par le sommet $\nodeidx\in \nodes$ d’un \gls{empgraph}. \\
		[2mm] Voir aussi : \gls{localdataset}, \gls{empgraph}
	} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\localsamplesize{\nodeidx} & \quad & \parbox{.75\textwidth}{
		Le nombre de \glspl{datapoint} (i.e., la \gls{samplesize}) contenus dans le \gls{localdataset} $\localdataset{\nodeidx}$ au sommet $\nodeidx \in \nodes$. \\
		[2mm] Voir aussi : \gls{datapoint}, \gls{samplesize}, \gls{localdataset}
	} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\featurevec^{(\nodeidx,\sampleidx)} & \quad & \parbox{.75\textwidth}{
		Les \glspl{feature} du $\sampleidx$-ième \gls{datapoint} dans le \gls{localdataset} $\localdataset{\nodeidx}$. \\
		[2mm] Voir aussi : \gls{feature}, \gls{datapoint}, \gls{localdataset}
	} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\truelabel^{(\nodeidx,\sampleidx)} & \quad & \parbox{.75\textwidth}{
		L'\gls{label} du $\sampleidx$-ième \gls{datapoint} dans le \gls{localdataset} $\localdataset{\nodeidx}$. \\
		[2mm] Voir aussi : \gls{label}, \gls{datapoint}, \gls{localdataset}
	} \nonumber
\end{align}

\begin{align}
	&\localparams{\nodeidx} & \quad & \parbox{.75\textwidth}{
		Les \gls{modelparams} locaux de l'\gls{device} $\nodeidx$ au sein d’un \gls{empgraph}. \\
		[2mm] Voir aussi : \gls{modelparams}, \gls{device}, \gls{empgraph}
	} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\locallossfunc{\nodeidx}{\weights} & \quad & \parbox{.75\textwidth}{
		La \gls{lossfunc} locale utilisée par l'\gls{device} $\nodeidx$ pour évaluer l’utilité d’un certain choix $\weights$ pour les \gls{modelparams} locaux. \\
		[2mm] Voir aussi : \gls{lossfunc}, \gls{device}, \gls{modelparams}
	} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\gtvloss{\featurevec}{\hypothesis\big(\featurevec\big)}{\hypothesis'\big(\featurevec\big)} & \quad & \parbox{.75\textwidth}{
		La \gls{loss} encourue par une \gls{hypothesis} $\hypothesis'$ sur un \gls{datapoint} de \glspl{feature} $\featurevec$ et d'\gls{label} $\hypothesis\big( \featurevec\big)$ obtenue à partir d’une autre \gls{hypothesis}. \\
		[2mm] Voir aussi : \gls{loss}, \gls{hypothesis}, \gls{datapoint}, \gls{feature}, \gls{label}
	} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&{\rm stack} \big\{ \weights^{(\nodeidx)} \big\}_{\nodeidx=1}^{\nrnodes} & \quad & \parbox{.75\textwidth}{
		Le \gls{vector} $\bigg( \big(\weights^{(1)}  \big)^{T}, \ldots, \big(\weights^{(\nrnodes)}  \big)^{T} \bigg)^{T} \in \mathbb{R}^{\dimlocalmodel\nrnodes}$ obtenu en empilant verticalement les \gls{modelparams} locaux $\weights^{(\nodeidx)} \in \mathbb{R}^{\dimlocalmodel}$. \\
		[2mm] Voir aussi : \gls{modelparams}, \gls{vector}.
	} \nonumber
\end{align}



% !TeX spellcheck = fr_FR

\section*{Notations et symboles}
%\label{ch_list_of_symbols}

\vspace*{-2mm}
\section*{Ensembles et fonctions} 

\begin{align} 
	&a \in \mathcal{A} & \quad & \parbox{.75\textwidth}{L'objet $a$ est un élément de l'ensemble $\mathcal{A}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&a \defeq b & \quad & \mbox{On note $a$ comme abréviation de $b$. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&|\mathcal{A}| & \quad & \mbox{Le cardinal (i.e., le nombre d'éléments) d'un ensemble fini $\mathcal{A}$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathcal{A} \subseteq \mathcal{B}& \quad & \mbox{$\mathcal{A}$ est un sous-ensemble de $\mathcal{B}$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathcal{A} \subset \mathcal{B}& \quad & \mbox{$\mathcal{A}$ est un sous-ensemble strict (i.e., non égal) de $\mathcal{B}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{N} & \quad & \mbox{Les entiers naturels $1,2,\ldots$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}  &\quad &\mbox{Les nombres réels $x$ \cite{RudinBook}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}_{+}  &\quad &\mbox{Les réels positifs ou nuls $x\geq0$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}_{++}  &\quad &\mbox{Les réels strictement positifs $x> 0$.} \nonumber
\end{align} 

\newpage
\begin{align}
		&\{0,1\}& \quad & \mbox{L'ensemble composé des deux réels $0$ et $1$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&[0,1] &\quad &\mbox{L'intervalle fermé des nombres réels $x$ tels que $0 \leq x \leq 1$. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
    &\argmin_{\weights} f(\weights) &\quad &\mbox{L'ensemble des point qui minimisent la fonction à valeurs réelles $f(\weights)$.  } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
    &\sphere{\nrnodes} &\quad &\mbox{L'ensemble des vecteurs de \gls{norm} unitaire dans $\mathbb{R}^{\nrnodes+1}$.  }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	 &\log a &\quad &\mbox{Le logarithme d'un réel strictement positif $a \in \mathbb{R}_{++}$.  } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	 &\hypothesis(\cdot)\!:\!\mathcal{A}\!\rightarrow\!\mathcal{B} :  a \!\mapsto\!h(a) &\quad &\parbox{.75\textwidth}{
	 	Une fonction (ou application) qui accepte tout élément \( a \in \mathcal{A} \) d'un ensemble \( \mathcal{A} \) en entrée et fournit un élément bien défini \( h(a) \in \mathcal{B} \) d'un ensemble \( \mathcal{B} \). L'ensemble \( \mathcal{A} \) est le domaine de définition de la fonction \( h \) et l'ensemble \( \mathcal{B} \) est l'ensemble d'arrivée de \( \hypothesis \). L'\gls{ml} (ou machine learning) vise à trouver (ou apprendre en la construisant) une fonction \( \hypothesis \) (c'est-à-dire une \gls{hypothesis}) qui prend en entrée les \gls{feature}s \( \featurevec \) d'un \gls{datapoint} et renvoie une \gls{prediction} \( \hypothesis(\featurevec) \) pour son \gls{label} \( \truelabel \).
 	} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	 	&\nabla f(\weights) & \quad & \parbox{.75\textwidth}{Le  \gls{gradient} d'une fonction à valeurs réelles \gls{differentiable}
	 	$f: \mathbb{R}^{\featuredim}\rightarrow \mathbb{R}$ est le vecteur
	 	$\nabla f(\weights) = \big( \frac{\partial f}{\partial \weight_{1}},\ldots,\frac{\partial f}{\partial \weight_{\featuredim}}  \big)^{T} \in \mathbb{R}^{\featuredim}$ \cite[Ch. 9]{RudinBookPrinciplesMatheAnalysis}.}   \nonumber
\end{align} 
\section*{Matrices et Vecteurs} 

\begin{align} 
	&\featurevec=\big(\feature_{1},\ldots,\feature_{\featuredim})^{T} &\quad & \parbox{.75\textwidth}{Un vecteur de taille $\featuredim$, dont la $\featureidx$-ième composante est $\feature_{\featureidx}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}^{\featuredim} & \quad &  \parbox{.75\textwidth}{L’ensemble des vecteurs $\featurevec=\big(\feature_{1},\ldots,\feature_{\featurelen}\big)^{T}$ composés de $\featuredim$ composantes réelles $\feature_{1},\ldots,\feature_{\featurelen} \in \mathbb{R}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{I}_{\modelidx \times \featuredim}  & \quad &  \parbox{.75\textwidth}{Une matrice identité généralisée de $\modelidx$ lignes et $\featuredim$ colonnes. Les composantes de $\mathbf{I}_{\modelidx \times \featuredim} \in \mathbb{R}^{\modelidx \times \featuredim}$ valent $1$ sur la diagonale principale et $0$ ailleurs.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{I}_{\dimlocalmodel}, \mathbf{I} & \quad &  \parbox{.75\textwidth}{Une matrice identité carrée de taille $\dimlocalmodel \times \dimlocalmodel$. Si la dimension est claire dans le contexte, on peut omettre l'indice.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\normgeneric{\featurevec}{2}  &\quad & \parbox{.75\textwidth}{La \gls{norm} euclidienne (ou $\ell_{2}$) du vecteur $\featurevec=\big(\feature_{1},\ldots,\feature_{\featurelen}\big)^{T} \in \mathbb{R}^{\featuredim}$ définie par $ \| \featurevec \|_{2} \defeq \sqrt{\sum_{\featureidx=1}^{\featuredim} \feature_{\featureidx}^{2}}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	&\normgeneric{\featurevec}{}  & \quad &  \parbox{.75\textwidth}{Une certaine \gls{norm} du vecteur $\featurevec \in \mathbb{R}^{\featuredim}$ \cite{GolubVanLoanBook}. Sauf indication contraire, on entend par là la \gls{norm} euclidienne $\normgeneric{\featurevec}{2}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\featurevec^{T} &\quad & \parbox{.75\textwidth}{La transposée d’une matrice ayant pour unique colonne le vecteur $\featurevec \in \mathbb{R}^{\dimlocalmodel}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{X}^{T} &\quad & \parbox{.75\textwidth}{La transposée d’une matrice $\mathbf{X} \in \mathbb{R}^{\samplesize \times \featurelen}$. Une matrice carrée à valeurs réelles $\mathbf{X} \in \mathbb{R}^{\samplesize \times \samplesize}$ est dite symétrique si $\mathbf{X} = \mathbf{X}^{T}$.}  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{0}= \big(0,\ldots,0\big)^{T}  & \quad &  \parbox{.75\textwidth}{Le vecteur de $\mathbb{R}^{\dimlocalmodel}$ dont toutes les composantes valent 0.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{1}= \big(1,\ldots,1\big)^{T}  & \quad &  \parbox{.75\textwidth}{Le vecteur de $\mathbb{R}^{\dimlocalmodel}$ dont toutes les composantes valent 1.} \nonumber
\end{align}
\newpage
\begin{align} 
	&\big(\vv^{T},\vw^{T} \big)^{T}  & \quad &  \parbox{.75\textwidth}{Le vecteur de longueur $\featurelen+\featurelen'$ obtenu en concaténant les composantes du vecteur $\vv \in \mathbb{R}^{\featurelen}$ avec celles de $\vw \in \mathbb{R}^{\featurelen'}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&	{\rm span}\{ \mathbf{B} \}  & \quad &  \parbox{.75\textwidth}{Le sous-espace engendré par une matrice $\mathbf{B} \in \mathbb{R}^{a \times b}$, c’est-à-dire l’ensemble de toutes les combinaisons linéaires des colonnes de $\mathbf{B}$ : ${\rm span}\{ \mathbf{B} \} = \big\{  \mathbf{B} \va : \va \in \mathbb{R}^{b} \big\} \subseteq \mathbb{R}^{a}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\determinant{\mC} &\quad & \parbox{.75\textwidth}{Le déterminant de la matrice $\mC$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{A} \otimes \mathbf{B} &\quad & \parbox{.75\textwidth}{Le produit de Kronecker des matrices $\mathbf{A}$ et $\mathbf{B}$ \cite{Golub1980}.} \nonumber
\end{align} 

\newpage
\section*{Théorie des probabilités} 
\begin{align}
	\expect_{p} \{ f(\datapoint) \}  \quad\quad & \parbox{.75\textwidth}{L'\gls{expectation} d'une fonction $f(\datapoint)$ d'une \gls{rv} 
		$\datapoint$ dont la \gls{probdist} est $\prob{\datapoint}$. Si la \gls{probdist} est claire dans le contexte, 
		on écrit simplement $\expect \{ f(\datapoint) \}$. }  \nonumber \\[2mm] \hline \nonumber\\[-5mm]    
	\prob{\featurevec,\truelabel} \quad\quad & \parbox{.75\textwidth}{Une \gls{probdist} (conjointe) d'une \gls{rv} 
		dont les \gls{realization}s sont des \gls{datapoint}s avec des \gls{feature}s $\featurevec$ et un \gls{label} $\truelabel$.} \nonumber        \nonumber \\[2mm] \hline \nonumber\\[-5mm]        
	\prob{\featurevec|\truelabel} \quad\quad & \parbox{.75\textwidth}{Une \gls{probdist} conditionnelle d'une \gls{rv} 
		$\featurevec$ étant donné la valeur d'une autre \gls{rv} $\truelabel$ \cite[Sec.\ 3.5]{BertsekasProb}. } \nonumber       \nonumber \\[2mm] \hline \nonumber\\[-5mm]           
	\prob{\featurevec;\weights} \quad\quad & \parbox{.75\textwidth}{Une \gls{probdist} paramétrée d'une \gls{rv} $\featurevec$. 
		La \gls{probdist} dépend d'un vecteur de paramètres $\weights$. Par exemple, $\prob{\featurevec;\weights}$ pourrait être une 
		\gls{mvndist} avec un vecteur de paramètres $\weights$ donné par les composantes du vecteur \gls{mean} $\expect \{ \featurevec \}$ 
		et la \gls{covmtx} $\expect \bigg \{ \big( \featurevec - \expect \{ \featurevec \}\big) \big( \featurevec - \expect \{ \featurevec \}\big)^{T}  \bigg\}$.} \nonumber           \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\mathcal{N}(\mu, \sigma^{2}) \quad\quad & \parbox{.75\textwidth}{La \gls{probdist} d'une 
		\gls{gaussrv} $\feature \in \mathbb{R}$ avec \gls{mean} (ou \gls{expectation}) $\mu= \expect \{ \feature \}$ 
		et \gls{variance} $\sigma^{2} =   \expect \big\{  (  \feature - \mu )^2 \big\}$.} \nonumber    \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\mathcal{N}(\clustermean, \mathbf{C}) \quad\quad & \parbox{.75\textwidth}{La \gls{mvndist} d'une \gls{gaussrv} vectorielle 
		$\featurevec \in \mathbb{R}^{\featuredim}$ avec \gls{mean} (ou \gls{expectation}) $\clustermean= \expect \{ \featurevec \}$ 
		et \gls{covmtx} $\mathbf{C} =  \expect \big\{ \big( \featurevec - \clustermean \big)\big( \featurevec - \clustermean \big)^{T} \big\}$.} \nonumber                                             
\end{align}






\newpage
\section*{Machine Learning}

\begin{align}
%	\datapoint \quad\quad & \parbox{.75\textwidth}{A \gls{datapoint} which is characterized by several properties that we 
%		divide into low-level properties (= \gls{feature}s) and high-level properties (= \gls{label}s) \cite[Ch. 2]{MLBasics}.}    \nonumber   \\[4mm] 
	\sampleidx \quad\quad & \parbox{.75\textwidth}{An index $\sampleidx=1,2,\ldots$ that 
		enumerates \gls{datapoint}s.}   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\samplesize \quad\quad &\parbox{.75\textwidth}{The number of \gls{datapoint}s in (i.e., the size of) a \gls{dataset}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	\dataset \quad\quad & \parbox{.75\textwidth}{A \gls{dataset} $\dataset = \{ \datapoint^{(1)},\ldots, \datapoint^{(\samplesize)} \}$ 
		is a list of individual \gls{datapoint}s $\datapoint^{(\sampleidx)}$, for $\sampleidx=1,\ldots,\samplesize$.}   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\featurelen \quad\quad &\parbox{.75\textwidth}{The number of \gls{feature}s that characterize a \gls{datapoint}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\feature_{\featureidx} \quad\quad &\parbox{.75\textwidth}{The $\featureidx$-th feature of a \gls{datapoint}. The first \gls{feature} 
		is denoted $\feature_{1}$, the second \gls{feature} $\feature_{2}$, and so on. } \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	\featurevec \quad\quad &\parbox{.75\textwidth}{The \gls{featurevec} $\featurevec=\big(\feature_{1},\ldots,\feature_{\featuredim}\big)^{T}$ of a \gls{datapoint} whose entries 
		are the individual \gls{feature}s of a \gls{datapoint}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\featurespace \quad\quad & \parbox{.75\textwidth}{The \gls{featurespace} $\featurespace$ is 
		the set of all possible values that the \gls{feature}s $\featurevec$ of a \gls{datapoint} can take on.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\rawfeaturevec \quad\quad &\parbox{.75\textwidth}{Instead of the symbol $\featurevec$, we 
		sometimes use $\rawfeaturevec$ as another symbol to denote a vector whose entries 
		are the individual \gls{feature}s of a \gls{datapoint}. We need two 
		different symbols to distinguish between raw and learned \gls{feature}s \cite[Ch. 9]{MLBasics}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\featurevec^{(\sampleidx)} \quad\quad &\parbox{.75\textwidth}{The \gls{feature} vector of the $\sampleidx$-th \gls{datapoint} within a \gls{dataset}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\feature_{\featureidx}^{(\sampleidx)}\quad\quad &\parbox{.75\textwidth}{The $\featureidx$-th \gls{feature} of the $\sampleidx$-th 
		\gls{datapoint} within a \gls{dataset}.} \nonumber
\end{align}        


\begin{align}
	\batch \quad\quad &\parbox{.75\textwidth}{A mini-\gls{batch} (or subset) of randomly chosen \gls{datapoint}s.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\batchsize \quad\quad &\parbox{.75\textwidth}{The size of (i.e., the number of \gls{datapoint}s in) a mini-\gls{batch}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\truelabel \quad\quad &\parbox{.75\textwidth}{The \gls{label} (or quantity of interest) of a \gls{datapoint}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\truelabel^{(\sampleidx)} \quad\quad &\parbox{.75\textwidth}{The \gls{label} of the $\sampleidx$-th \gls{datapoint}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\big(\featurevec^{(\sampleidx)},\truelabel^{(\sampleidx)}\big)  \quad\quad &\parbox{.75\textwidth}{The \gls{feature}s and \gls{label} of the $\sampleidx$-th \gls{datapoint}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\labelspace  \quad\quad & \parbox{.75\textwidth}{The \gls{labelspace} $\labelspace$ of 
		an \gls{ml} method consists of all potential \gls{label} values that a \gls{datapoint} can 
		carry. The nominal \gls{labelspace} might be larger than the set of different \gls{label} 
		values arising in a given \gls{dataset} (e.g., a \gls{trainset}). \Gls{ml} problems 
		(or methods) using a numeric \gls{labelspace}, such as $\labelspace=\mathbb{R}$ 
		or $\labelspace=\mathbb{R}^{3}$, are referred to as \gls{regression} problems (or methods). \Gls{ml} 
		problems (or methods) that use a discrete \gls{labelspace}, such as $\labelspace=\{0,1\}$ or $\labelspace=\{\mbox{\emph{cat}},\mbox{\emph{dog}},\mbox{\emph{mouse}}\}$, 
		are referred to as \gls{classification} problems (or methods).}  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\lrate  \quad\quad & \parbox{.75\textwidth}{\Gls{learnrate} (or \gls{stepsize}) used by \gls{gdmethods}.}  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\hypothesis(\cdot)  \quad\quad &\parbox{.75\textwidth}{A \gls{hypothesis} map that reads in \gls{feature}s $\featurevec$ of a \gls{datapoint} 
		and delivers a \gls{prediction} $\hat{\truelabel}=\hypothesis(\featurevec)$ for its \gls{label} $\truelabel$.}  	 \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	 \labelspace^{\featurespace} \quad\quad & \parbox{.75\textwidth}{Given two sets $\featurespace$ and $\labelspace$, we denote by $ \labelspace^{\featurespace}$ the set of all possible \gls{hypothesis} maps $\hypothesis: \featurespace \rightarrow \labelspace$.} 	 \nonumber 
\end{align}                  


\begin{align}
	\hypospace  \quad\quad & \parbox{.75\textwidth}{A \gls{hypospace} or \gls{model} used by an \gls{ml} method. 
		The \gls{hypospace} consists of different \gls{hypothesis} maps $\hypothesis: \featurespace \rightarrow \labelspace$, between which 
		the \gls{ml} method must choose.}   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\effdim{\hypospace}  \quad\quad & \parbox{.75\textwidth}{The \gls{effdim} of a \gls{hypospace} $\hypospace$.}   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\biasterm^2 \quad\quad &\parbox{.75\textwidth}{
		The squared \gls{bias} of a learned \gls{hypothesis} $\learnthypothesis$ 
		produced by an \gls{ml} method. The method is trained on \gls{datapoint}s 
		that are modeled as the \gls{realization}s of \gls{rv}s. Since the \gls{data} is a \gls{realization} 
		of \gls{rv}s, the learned \gls{hypothesis} $\learnthypothesis$ is also the \gls{realization} 
		of an \gls{rv}.} \nonumber  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\varianceterm \quad\quad &\parbox{.75\textwidth}{The \gls{variance} of the learned 
		(\gls{parameters} of the) \gls{hypothesis} produced by an \gls{ml} method. 
		The method is trained on \gls{datapoint}s that are modeled as the \gls{realization}s 
		of \gls{rv}s. Since the \gls{data} is a \gls{realization} of \gls{rv}s, the learned \gls{hypothesis} $\learnthypothesis$ is also the \gls{realization} 
		of an \gls{rv}.} \nonumber \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\lossfunc{(\featurevec,\truelabel)}{\hypothesis}  \quad\quad & \parbox{.75\textwidth}{The \gls{loss} incurred by predicting the 
		\gls{label} $\truelabel$ of a \gls{datapoint} using the \gls{prediction} $\hat{\truelabel}=h(\featurevec)$. The 
		\gls{prediction} $\hat{\truelabel}$ is obtained by evaluating the \gls{hypothesis} $\hypothesis \in \hypospace$ for 
		the \gls{featurevec} $\featurevec$ of the \gls{datapoint}.}    \nonumber  \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	\valerror \quad\quad &\parbox{.75\textwidth}{The \gls{valerr} of a \gls{hypothesis} $\hypothesis$, which is its 
		average \gls{loss} incurred over a \gls{valset}.}  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\emperror\big(\hypothesis| \dataset \big) \quad\quad &\parbox{.75\textwidth}{The \gls{emprisk} or average \gls{loss} 
		incurred by the \gls{hypothesis} $\hypothesis$ on a \gls{dataset} $\dataset$.} \nonumber                           
\end{align}     

\begin{align}
	\trainerror \quad\quad &\parbox{.75\textwidth}{The \gls{trainerr} of a \gls{hypothesis} $\hypothesis$, which is its 
		average \gls{loss} incurred over a \gls{trainset}. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\timeidx \quad\quad &\parbox{.75\textwidth}{A discrete-time index $\timeidx=0,1,\ldots$ used to 
		enumerate sequential events (or time instants). }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\taskidx \quad\quad &\parbox{.75\textwidth}{An index that enumerates
		\gls{learningtask}s within a \gls{multitask learning} problem.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\regparam \quad\quad &\parbox{.75\textwidth}{A \gls{regularization} parameter that controls 
		the amount of \gls{regularization}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\eigval{\featureidx}\big( \mathbf{Q} \big) \quad\quad &\parbox{.75\textwidth}{The $\featureidx$-th 
		\gls{eigenvalue} (sorted in either ascending or descending order) of a \gls{psd} matrix $\mathbf{Q}$. We also 
		use the shorthand $\eigval{\featureidx}$ if the corresponding matrix is clear from context. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\actfun(\cdot) \quad\quad &\parbox{.75\textwidth}{The \gls{actfun} used by an artificial neuron within an \gls{ann}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\decreg{\hat{\truelabel}} \quad\quad &\parbox{.75\textwidth}{A \gls{decisionregion} within a \gls{featurespace}.  }\nonumber \\[2mm] \hline \nonumber\\[-5mm]  
	\weights  \quad\quad & \parbox{.75\textwidth}{A parameter vector $\weights = \big(\weight_{1},\ldots,\weight_{\featuredim}\big)^{T}$ 
		of a \gls{model}, e.g., the \gls{weights} of a \gls{linmodel} or in an \gls{ann}.}     \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\hypothesis^{(\weights)}(\cdot)  \quad\quad &\parbox{.75\textwidth}{A \gls{hypothesis} map that involves tunable \gls{modelparams} $\weight_{1},\ldots,\weight_{\featuredim}$ stacked into the vector $\weights=\big(\weight_{1},\ldots,\weight_{\featuredim} \big)^{T}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\featuremap(\cdot)  \quad\quad & \parbox{.75\textwidth}{A \gls{featuremap} $\featuremap: \featurespace \rightarrow \featurespace' : \featurevec \mapsto \featurevec' \defeq \featuremap\big( \featurevec \big) \in \featurespace'$.}   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\kernelmap{\cdot}{\cdot} \quad\quad & \parbox{.75\textwidth}{Given some \gls{featurespace} $\featurespace$, 
		a \gls{kernel} is a map $\kernel: \featurespace \times \featurespace \rightarrow \mathbb{C}$ that is \gls{psd}.}    \nonumber                                                                                                                                                     
\end{align}              






\newpage
\section*{Federated Learning}

\begin{align}
 	&\graph = \pair{\nodes}{\edges} & \quad & \parbox{.75\textwidth}{An undirected \gls{graph} whose nodes $\nodeidx \in \nodes$ represent 
	\gls{device}s within a \gls{empgraph}. The undirected weighted edges $\edges$ represent connectivity between 
	\gls{device}s and statistical similarities between their \gls{dataset}s and \gls{learningtask}s.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
&\nodeidx \in \nodes& \quad & \parbox{.75\textwidth}{A node that represents some 
	\gls{device} within an \gls{empgraph}. The device can access a \gls{localdataset} and train a \gls{localmodel}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\indsubgraph{\graph}{\cluster}& \quad & \parbox{.75\textwidth}{The induced subgraph of $\graph$ using the nodes in $\cluster \subseteq \nodes$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\LapMat{\graph}   & \quad & \parbox{.75\textwidth}{The \gls{LapMat} of a \gls{graph} $\graph$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
		&\LapMat{\cluster}   & \quad & \parbox{.75\textwidth}{The \gls{LapMat} of the induced \gls{graph} $\indsubgraph{\graph}{\cluster}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	 &		\neighbourhood{\nodeidx}  & \quad & \parbox{.75\textwidth}{The \gls{neighborhood} of a node $\nodeidx$ in a \gls{graph} $\graph$.}   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\nodedegree{\nodeidx} & \quad & \parbox{.75\textwidth}{The weighted degree $\nodedegree{\nodeidx}\defeq \sum_{\nodeidx' \in \neighbourhood{\nodeidx}} \edgeweight_{\nodeidx,\nodeidx'}$ of a node $\nodeidx$ in a \gls{graph} $\graph$.}  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\maxnodedegree^{(\graph)} & \quad & \parbox{.75\textwidth}{The maximum weighted node degree of a \gls{graph} $\graph$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
&\localdataset{\nodeidx} & \quad & \parbox{.75\textwidth}{The \gls{localdataset} $\localdataset{\nodeidx}$ carried by 
			node $\nodeidx\in \nodes$ of an \gls{empgraph}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
&\localsamplesize{\nodeidx} & \quad & \parbox{.75\textwidth}{The number of \gls{datapoint}s (i.e., \gls{samplesize}) contained in the 
			\gls{localdataset} $\localdataset{\nodeidx}$ at node $\nodeidx\in \nodes$.} \nonumber 
\end{align} 
\begin{align} 
		&\featurevec^{(\nodeidx,\sampleidx)} & \quad & \parbox{.75\textwidth}{The \gls{feature}s of the $\sampleidx$-th \gls{datapoint} in 
		the \gls{localdataset} $\localdataset{\nodeidx}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\truelabel^{(\nodeidx,\sampleidx)} & \quad & \parbox{.75\textwidth}{The \gls{label} of the $\sampleidx$-th \gls{datapoint} in 
		the \gls{localdataset} $\localdataset{\nodeidx}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
		&\localparams{\nodeidx} & \quad & \parbox{.75\textwidth}{The local \gls{modelparams} of \gls{device} $\nodeidx$ within an \gls{empgraph}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
		&\locallossfunc{\nodeidx}{\weights} & \quad & \parbox{.75\textwidth}{The local \gls{lossfunc} used by \gls{device} $\nodeidx$ 
		to measure the usefulness of some choice $\weights$ for the local \gls{modelparams}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	& \gtvloss{\featurevec}{\hypothesis\big(\featurevec\big)}{\hypothesis'\big(\featurevec\big)}& \quad & \parbox{.75\textwidth}{The \gls{loss} 
		incurred by a \gls{hypothesis} $\hypothesis'$ on a \gls{datapoint} with \gls{feature}s $\featurevec$ and \gls{label} 
		$\hypothesis\big( \featurevec\big)$ that is obtained from another \gls{hypothesis}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
		& 	{\rm stack} \big\{ \weights^{(\nodeidx)} \big\}_{\nodeidx=1}^{\nrnodes} & \quad & \parbox{.75\textwidth}{The vector $\bigg( \big(\weights^{(1)}  \big)^{T}, \ldots, \big(\weights^{(\nrnodes)}  \big)^{T} \bigg)^{T} \in \mathbb{R}^{\dimlocalmodel\nrnodes}$ that 
			is obtained by vertically stacking the local \gls{modelparams} $\weights^{(\nodeidx)} \in \mathbb{R}^{\dimlocalmodel}$.} \nonumber  
\end{align}        



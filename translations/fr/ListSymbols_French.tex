% !TeX spellcheck = fr_FR

\section*{Notations et symboles}
%\label{ch_list_of_symbols}

\vspace*{-2mm}
\section*{Ensembles et fonctions} 

\begin{align} 
	&a \in \mathcal{A} & \quad & \parbox{.75\textwidth}{L'objet $a$ est un élément de l'ensemble $\mathcal{A}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&a \defeq b & \quad & \mbox{On note $a$ comme abréviation de $b$. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&|\mathcal{A}| & \quad & \mbox{Le cardinal (i.e., le nombre d'éléments) d'un ensemble fini $\mathcal{A}$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathcal{A} \subseteq \mathcal{B}& \quad & \mbox{$\mathcal{A}$ est un sous-ensemble de $\mathcal{B}$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathcal{A} \subset \mathcal{B}& \quad & \mbox{$\mathcal{A}$ est un sous-ensemble strict de $\mathcal{B}$ (i.e., non égal à $\mathcal{B}$).} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{N} & \quad & \mbox{Les entiers naturels $1,2,\ldots$}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}  &\quad &\mbox{Les nombres réels $x$ \cite{RudinBook}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}_{+}  &\quad &\mbox{Les réels positifs ou nuls $x\geq0$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}_{++}  &\quad &\mbox{Les réels strictement positifs $x> 0$.} \nonumber
\end{align} 

\newpage
\begin{align}
		&\{0,1\}& \quad & \mbox{L'ensemble composé des deux réels $0$ et $1$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&[0,1] &\quad &\mbox{L'intervalle fermé des nombres réels $x$ tels que $0 \leq x \leq 1$. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
    &\argmin_{\weights} f(\weights) &\quad &\mbox{L'ensemble des point qui minimisent la fonction à valeurs réelles $f(\weights)$.  } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
    &\sphere{\nrnodes} &\quad &\mbox{L'ensemble des vecteurs de \gls{norm} unitaire dans $\mathbb{R}^{\nrnodes+1}$.  }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	 &\log a &\quad &\mbox{Le logarithme d'un réel strictement positif $a \in \mathbb{R}_{++}$.  } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	 &\hypothesis(\cdot)\!:\!\mathcal{A}\!\rightarrow\!\mathcal{B} :  a \!\mapsto\!h(a) &\quad &\parbox{.75\textwidth}{
	 	Une fonction (ou application) qui accepte tout élément \( a \in \mathcal{A} \) d'un ensemble \( \mathcal{A} \) en entrée et fournit un élément bien défini \( h(a) \in \mathcal{B} \) d'un ensemble \( \mathcal{B} \). L'ensemble \( \mathcal{A} \) est le domaine de définition de la fonction \( h \) et l'ensemble \( \mathcal{B} \) est l'ensemble d'arrivée de \( \hypothesis \). L'\gls{ml} vise à trouver (ou apprendre en la construisant) une fonction \( \hypothesis \) (c'est-à-dire une \gls{hypothesis}) qui prend en entrée les \gls{feature}s \( \featurevec \) d'un \gls{datapoint} et renvoie une \gls{prediction} \( \hypothesis(\featurevec) \) pour son \gls{label} \( \truelabel \).
 	} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	 	&\nabla f(\weights) & \quad & \parbox{.75\textwidth}{Le  \gls{gradient} d'une fonction \gls{differentiable} à valeurs réelles
	 	$f: \mathbb{R}^{\featuredim}\rightarrow \mathbb{R}$ est le vecteur
	 	$\nabla f(\weights) = \big( \frac{\partial f}{\partial \weight_{1}},\ldots,\frac{\partial f}{\partial \weight_{\featuredim}}  \big)^{T} \in \mathbb{R}^{\featuredim}$ \cite[Ch. 9]{RudinBookPrinciplesMatheAnalysis}.}   \nonumber
\end{align} 
\section*{Matrices et Vecteurs} 

\begin{align} 
	&\featurevec=\big(\feature_{1},\ldots,\feature_{\featuredim})^{T} &\quad & \parbox{.75\textwidth}{Un vecteur de taille $\featuredim$, dont la $\featureidx$-ième composante est $\feature_{\featureidx}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}^{\featuredim} & \quad &  \parbox{.75\textwidth}{L’ensemble des vecteurs $\featurevec=\big(\feature_{1},\ldots,\feature_{\featurelen}\big)^{T}$ constitués de $\featuredim$ composantes réelles $\feature_{1},\ldots,\feature_{\featurelen} \in \mathbb{R}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{I}_{\modelidx \times \featuredim}  & \quad &  \parbox{.75\textwidth}{Une matrice identité généralisée de $\modelidx$ lignes et $\featuredim$ colonnes. Les composantes de $\mathbf{I}_{\modelidx \times \featuredim} \in \mathbb{R}^{\modelidx \times \featuredim}$ valent $1$ sur la diagonale principale et $0$ ailleurs.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{I}_{\dimlocalmodel}, \mathbf{I} & \quad &  \parbox{.75\textwidth}{Une matrice identité carrée de taille $\dimlocalmodel \times \dimlocalmodel$. Si la dimension est claire dans le contexte, on peut omettre l'indice.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\normgeneric{\featurevec}{2}  &\quad & \parbox{.75\textwidth}{La \gls{norm} euclidienne (ou $\ell_{2}$) du vecteur $\featurevec=\big(\feature_{1},\ldots,\feature_{\featurelen}\big)^{T} \in \mathbb{R}^{\featuredim}$ définie par $ \| \featurevec \|_{2} \defeq \sqrt{\sum_{\featureidx=1}^{\featuredim} \feature_{\featureidx}^{2}}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	&\normgeneric{\featurevec}{}  & \quad &  \parbox{.75\textwidth}{Une certaine \gls{norm} du vecteur $\featurevec \in \mathbb{R}^{\featuredim}$ \cite{GolubVanLoanBook}. Sauf indication contraire, on entend par là la \gls{norm} euclidienne $\normgeneric{\featurevec}{2}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\featurevec^{T} &\quad & \parbox{.75\textwidth}{La transposée d’une matrice ayant pour unique colonne le vecteur $\featurevec \in \mathbb{R}^{\dimlocalmodel}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{X}^{T} &\quad & \parbox{.75\textwidth}{La transposée d’une matrice $\mathbf{X} \in \mathbb{R}^{\samplesize \times \featurelen}$. Une matrice carrée à valeurs réelles $\mathbf{X} \in \mathbb{R}^{\samplesize \times \samplesize}$ est dite symétrique si $\mathbf{X} = \mathbf{X}^{T}$.}  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{0}= \big(0,\ldots,0\big)^{T}  & \quad &  \parbox{.75\textwidth}{Le vecteur de $\mathbb{R}^{\dimlocalmodel}$ dont toutes les composantes valent 0.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{1}= \big(1,\ldots,1\big)^{T}  & \quad &  \parbox{.75\textwidth}{Le vecteur de $\mathbb{R}^{\dimlocalmodel}$ dont toutes les composantes valent 1.} \nonumber
\end{align}
\newpage
\begin{align} 
	&\big(\vv^{T},\vw^{T} \big)^{T}  & \quad &  \parbox{.75\textwidth}{Le vecteur de longueur $\featurelen+\featurelen'$ obtenu en concaténant les composantes du vecteur $\vv \in \mathbb{R}^{\featurelen}$ avec celles de $\vw \in \mathbb{R}^{\featurelen'}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&	{\rm span}\{ \mathbf{B} \}  & \quad &  \parbox{.75\textwidth}{Le sous-espace engendré par une matrice $\mathbf{B} \in \mathbb{R}^{a \times b}$, c’est-à-dire l’ensemble de toutes les combinaisons linéaires des colonnes de $\mathbf{B}$ : ${\rm span}\{ \mathbf{B} \} = \big\{  \mathbf{B} \va : \va \in \mathbb{R}^{b} \big\} \subseteq \mathbb{R}^{a}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\determinant{\mC} &\quad & \parbox{.75\textwidth}{Le déterminant de la matrice $\mC$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{A} \otimes \mathbf{B} &\quad & \parbox{.75\textwidth}{Le produit de Kronecker des matrices $\mathbf{A}$ et $\mathbf{B}$ \cite{Golub1980}.} \nonumber
\end{align} 

\newpage
\section*{Théorie des probabilités} 
\begin{align}
	\expect_{p} \{ f(\datapoint) \}  \quad\quad & \parbox{.75\textwidth}{L'\gls{expectation} d'une fonction $f(\datapoint)$ d'une \gls{rv} 
		$\datapoint$ dont la \gls{probdist} est $\prob{\datapoint}$. Si la \gls{probdist} est claire dans le contexte, 
		on écrit simplement $\expect \{ f(\datapoint) \}$. }  \nonumber \\[2mm] \hline \nonumber\\[-5mm]    
	\prob{\featurevec,\truelabel} \quad\quad & \parbox{.75\textwidth}{Une \gls{probdist} (conjointe) d'une \gls{rv} 
		dont les \gls{realization}s sont des \gls{datapoint}s de \gls{feature}s $\featurevec$ et une \gls{label} $\truelabel$.} \nonumber        \nonumber \\[2mm] \hline \nonumber\\[-5mm]        
	\prob{\featurevec|\truelabel} \quad\quad & \parbox{.75\textwidth}{Une \gls{probdist} conditionnelle d'une \gls{rv} 
		$\featurevec$ étant donnée la valeur d'une autre \gls{rv} $\truelabel$ \cite[Sec.\ 3.5]{BertsekasProb}. } \nonumber       \nonumber \\[2mm] \hline \nonumber\\[-5mm]           
	\prob{\featurevec;\weights} \quad\quad & \parbox{.75\textwidth}{Une \gls{probdist} paramétrée d'une \gls{rv} $\featurevec$. 
		La \gls{probdist} dépend d'un vecteur de paramètres $\weights$. Par exemple, $\prob{\featurevec;\weights}$ pourrait être une 
		\gls{mvndist} avec un vecteur de paramètres $\weights$ donné par les composantes du vecteur \gls{mean} $\expect \{ \featurevec \}$ 
		et la \gls{covmtx} $\expect \bigg \{ \big( \featurevec - \expect \{ \featurevec \}\big) \big( \featurevec - \expect \{ \featurevec \}\big)^{T}  \bigg\}$.} \nonumber           \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\mathcal{N}(\mu, \sigma^{2}) \quad\quad & \parbox{.75\textwidth}{La \gls{probdist} d'une 
		\gls{gaussrv} $\feature \in \mathbb{R}$ ayant comme \gls{mean} (ou \gls{expectation}) $\mu= \expect \{ \feature \}$ 
		et comme \gls{variance} $\sigma^{2} =   \expect \big\{  (  \feature - \mu )^2 \big\}$.} \nonumber    \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\mathcal{N}(\clustermean, \mathbf{C}) \quad\quad & \parbox{.75\textwidth}{La \gls{mvndist} d'une \gls{gaussrv} vectorielle 
		$\featurevec \in \mathbb{R}^{\featuredim}$ ayant comme \gls{mean} (ou \gls{expectation}) $\clustermean= \expect \{ \featurevec \}$ 
		et comme \gls{covmtx} $\mathbf{C} =  \expect \big\{ \big( \featurevec - \clustermean \big)\big( \featurevec - \clustermean \big)^{T} \big\}$.} \nonumber                                             
\end{align}






\newpage
\section*{Apprentissage automatique}

\begin{align}
	\sampleidx \quad\quad & \parbox{.75\textwidth}{Un indice $\sampleidx=1,2,\ldots$ qui énumère les \gls{datapoint}s.}   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\samplesize \quad\quad &\parbox{.75\textwidth}{Le nombre de \gls{datapoint}s dans un \gls{dataset} (c’est-à-dire la taille du \gls{dataset}).} \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	\dataset \quad\quad & \parbox{.75\textwidth}{Un \gls{dataset} $\dataset = \{ \datapoint^{(1)},\ldots, \datapoint^{(\samplesize)} \}$ 
		est une liste de \gls{datapoint}s individuels $\datapoint^{(\sampleidx)}$, pour $\sampleidx=1,\ldots,\samplesize$.}   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\featurelen \quad\quad &\parbox{.75\textwidth}{Le nombre de \gls{feature}s qui constituent une \gls{datapoint}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\feature_{\featureidx} \quad\quad &\parbox{.75\textwidth}{La $\featureidx$-ième \gls{feature} d’un \gls{datapoint}. La première \gls{feature} 
		est noté $\feature_{1}$, le deuxième $\feature_{2}$, et ainsi de suite. } \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	\featurevec \quad\quad &\parbox{.75\textwidth}{Le \gls{featurevec} $\featurevec=\big(\feature_{1},\ldots,\feature_{\featuredim}\big)^{T}$ d’un \gls{datapoint}, dont les composantes 
		sont les différentes \gls{feature}s du \gls{datapoint}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\featurespace \quad\quad & \parbox{.75\textwidth}{L'\gls{featurespace} $\featurespace$ est 
		l’ensemble de toutes les valeurs possibles que les \gls{feature}s $\featurevec$ d’un \gls{datapoint} peuvent prendre.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\rawfeaturevec \quad\quad &\parbox{.75\textwidth}{Au lieu du symbole $\featurevec$, on utilise parfois $\rawfeaturevec$ comme un autre symbole pour désigner un vecteur dont les composantes 
		sont les différentes \gls{feature}s d’un \gls{datapoint}. On a besoin de deux symboles différents pour distinguer les \gls{feature}s brutes des \gls{feature}s apprises \cite[Ch. 9]{MLBasics}.}\nonumber
\end{align}

\begin{align}
	\featurevec^{(\sampleidx)} \quad\quad &\parbox{.75\textwidth}{Le vecteur de \gls{feature}s du $\sampleidx$-ième \gls{datapoint} dans un \gls{dataset}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\feature_{\featureidx}^{(\sampleidx)}\quad\quad &\parbox{.75\textwidth}{La $\featureidx$-ième \gls{feature} du $\sampleidx$-ième 
		\gls{datapoint} dans un \gls{dataset}.} \nonumber  \\[2mm] \hline \nonumber\\[-5mm]
	\truelabel \quad\quad &\parbox{.75\textwidth}{L'\gls{label} (ou quantité d’intérêt) d’un \gls{datapoint}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\truelabel^{(\sampleidx)} \quad\quad &\parbox{.75\textwidth}{L'\gls{label} du $\sampleidx$-ième \gls{datapoint}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\big(\featurevec^{(\sampleidx)},\truelabel^{(\sampleidx)}\big)  \quad\quad &\parbox{.75\textwidth}{Les \glspl{feature} et l'\gls{label} du $\sampleidx$-ième \gls{datapoint}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\labelspace  \quad\quad & \parbox{.75\textwidth}{L’\gls{labelspace} $\labelspace$ d’une méthode d’\gls{ml} comprend toutes les valeurs d'\gls{label} qu’un \gls{datapoint} peut porter. L’\gls{labelspace} nominal peut être plus grand que l’ensemble des différentes valeurs de \gls{label} présentes dans un \gls{dataset} donné (par exemple, un \gls{trainset}). Les problèmes (ou méthodes) de \gls{ml} utilisant une \gls{labelspace} numérique, comme $\labelspace=\mathbb{R}$ ou $\labelspace=\mathbb{R}^{3}$, sont appelés problèmes (ou méthodes) de \gls{regression}. Les problèmes (ou méthodes) de \gls{ml} utilisant une \gls{labelspace} discrète, comme $\labelspace=\{0,1\}$ ou $\labelspace=\{\mbox{\emph{chat}},\mbox{\emph{chien}},\mbox{\emph{souris}}\}$, sont appelés problèmes (ou méthodes) de \gls{classification}.}  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\lrate  \quad\quad & \parbox{.75\textwidth}{Le \gls{learnrate} (ou \gls{stepsize}) utilisé par les \glspl{gdmethods}.}  \nonumber
\end{align}
                  


\begin{align}
	\batch \quad\quad &\parbox{.75\textwidth}{Un mini-\gls{batch} (ou sous-ensemble) de \glspl{datapoint} choisis aléatoirement.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\batchsize \quad\quad &\parbox{.75\textwidth}{La taille (c’est-à-dire le nombre de \glspl{datapoint}) d’un mini-\gls{batch}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\hypothesis(\cdot)  \quad\quad &\parbox{.75\textwidth}{Une fonction \gls{hypothesis} qui lit les \glspl{feature} $\featurevec$ d’un \gls{datapoint} et produit une \gls{prediction} $\hat{\truelabel}=\hypothesis(\featurevec)$ pour son \gls{label} $\truelabel$.}  	 \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\labelspace^{\featurespace} \quad\quad & \parbox{.75\textwidth}{Étant donnés deux ensembles $\featurespace$ et $\labelspace$, on note $\labelspace^{\featurespace}$ l’ensemble de toutes les fonctions \gls{hypothesis} possibles $\hypothesis: \featurespace \rightarrow \labelspace$.}  	 \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\hypospace  \quad\quad & \parbox{.75\textwidth}{Un \gls{hypospace} ou \gls{model} utilisé par une méthode d’\gls{ml}. Le \gls{hypospace} est constitué de différentes fonctions \gls{hypothesis} $\hypothesis: \featurespace \rightarrow \labelspace$, parmi lesquelles la méthode d’\gls{ml} doit choisir.}   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\effdim{\hypospace}  \quad\quad & \parbox{.75\textwidth}{La \gls{effdim} d’un \gls{hypospace} $\hypospace$.}   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\biasterm^2 \quad\quad &\parbox{.75\textwidth}{
		Le \gls{bias} au carré d’une fonction \gls{hypothesis} apprise $\learnthypothesis$ produite par une méthode d’\gls{ml}. La méthode est entraînée sur des \glspl{datapoint} modélisés comme des \glspl{realization} de \glspl{rv}. Puisque les \glspl{data} sont des \glspl{realization} de \glspl{rv}, la fonction apprise $\learnthypothesis$ est également une \gls{realization} d’une \gls{rv}.} \nonumber                             
\end{align}

\begin{align}
	\varianceterm \quad\quad &\parbox{.75\textwidth}{La \gls{variance} de la fonction \gls{hypothesis} apprise (ou de ses \glspl{parameters}) par une méthode d’\gls{ml}. La méthode est entraînée sur des \glspl{datapoint} modélisés comme des \glspl{realization} de \glspl{rv}. Puisque les \glspl{data} sont des \glspl{realization} de \glspl{rv}, la fonction apprise $\learnthypothesis$ est également une \gls{realization} d’une \gls{rv}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\lossfunc{(\featurevec,\truelabel)}{\hypothesis}  \quad\quad & \parbox{.75\textwidth}{La \gls{loss} encourue lors de la prédiction du \gls{label} $\truelabel$ d’un \gls{datapoint} à l’aide de la \gls{prediction} $\hat{\truelabel}=h(\featurevec)$. La \gls{prediction} $\hat{\truelabel}$ est obtenue en évaluant la fonction \gls{hypothesis} $\hypothesis \in \hypospace$ sur le \gls{featurevec} $\featurevec$ du \gls{datapoint}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	\valerror \quad\quad &\parbox{.75\textwidth}{La \gls{valerr} d’une fonction \gls{hypothesis} $\hypothesis$, c’est-à-dire sa \gls{loss} moyenne sur un \gls{valset}.}  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\emperror\big(\hypothesis| \dataset \big) \quad\quad &\parbox{.75\textwidth}{L’\gls{emprisk}, ou \gls{loss} moyenne, encourue par la fonction \gls{hypothesis} $\hypothesis$ sur un \gls{dataset} $\dataset$.} \nonumber  \\[2mm] \hline \nonumber\\[-5mm]
	\trainerror \quad\quad &\parbox{.75\textwidth}{L’\gls{trainerr} d’une fonction \gls{hypothesis} $\hypothesis$, c’est-à-dire sa \gls{loss} moyenne sur un \gls{trainset}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\timeidx \quad\quad &\parbox{.75\textwidth}{Un indice temporel discret $\timeidx=0,1,\ldots$ utilisé pour énumérer des événements séquentiels (ou des instants de temps).} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\taskidx \quad\quad &\parbox{.75\textwidth}{Un indice qui énumère les \glspl{learningtask} dans un problème d’\gls{multitask learning}.} \nonumber
\end{align}

\begin{align}
	\regparam \quad\quad &\parbox{.75\textwidth}{Un paramètre de \gls{regularization} qui contrôle la quantité de \gls{regularization}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\eigval{\featureidx}\big( \mathbf{Q} \big) \quad\quad &\parbox{.75\textwidth}{La $\featureidx$-ième \gls{eigenvalue} (triée par ordre croissant ou décroissant) d’une matrice \gls{psd} $\mathbf{Q}$. On utilise aussi l’abréviation $\eigval{\featureidx}$ si la matrice est claire par le contexte.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\actfun(\cdot) \quad\quad &\parbox{.75\textwidth}{La \gls{actfun} utilisée par un neurone artificiel dans un \gls{ann}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\decreg{\hat{\truelabel}} \quad\quad &\parbox{.75\textwidth}{Une \gls{decisionregion} dans un \gls{featurespace}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]  
	\weights  \quad\quad & \parbox{.75\textwidth}{Un vecteur de paramètres $\weights = \big(\weight_{1},\ldots,\weight_{\featuredim}\big)^{T}$ d’un \gls{model}, par exemple les \gls{weights} d’un \gls{linmodel} ou dans un \gls{ann}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\hypothesis^{(\weights)}(\cdot)  \quad\quad &\parbox{.75\textwidth}{Une fonction \gls{hypothesis} qui dépend de \glspl{modelparams} ajustables $\weight_{1},\ldots,\weight_{\featuredim}$ regroupés dans le vecteur $\weights=\big(\weight_{1},\ldots,\weight_{\featuredim} \big)^{T}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\featuremap(\cdot)  \quad\quad & \parbox{.75\textwidth}{Une \gls{featuremap} $\featuremap: \featurespace \rightarrow \featurespace' : \featurevec \mapsto \featurevec' \defeq \featuremap\big( \featurevec \big) \in \featurespace'$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\kernelmap{\cdot}{\cdot} \quad\quad & \parbox{.75\textwidth}{Étant donné un \gls{featurespace} $\featurespace$, un \gls{kernel} est une application $\kernel: \featurespace \times \featurespace \rightarrow \mathbb{C}$ qui est \gls{psd}.} \nonumber                                                                                                                                               
\end{align}
            






\newpage
\section*{Apprentissage fedéré}

\begin{align}
	&\graph = \pair{\nodes}{\edges} & \quad & \parbox{.75\textwidth}{Un \gls{graph} non orienté dont les nœuds $\nodeidx \in \nodes$ représentent des \gls{device}s au sein d’un \gls{empgraph}. Les arêtes pondérées non orientées $\edges$ représentent la connectivité entre les \gls{device}s et les similarités statistiques entre leurs \gls{dataset}s et \gls{learningtask}s.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\nodeidx \in \nodes& \quad & \parbox{.75\textwidth}{Un nœud représentant un \gls{device} dans un \gls{empgraph}. Le \gls{device} peut accéder à un \gls{localdataset} et entraîner un \gls{localmodel}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\indsubgraph{\graph}{\cluster}& \quad & \parbox{.75\textwidth}{Le sous-graphe induit de $\graph$ utilisant les nœuds de $\cluster \subseteq \nodes$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\LapMat{\graph} & \quad & \parbox{.75\textwidth}{La \gls{LapMat} d’un \gls{graph} $\graph$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\LapMat{\cluster} & \quad & \parbox{.75\textwidth}{La \gls{LapMat} du \gls{graph} induit $\indsubgraph{\graph}{\cluster}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\neighbourhood{\nodeidx} & \quad & \parbox{.75\textwidth}{Le \gls{neighborhood} d’un nœud $\nodeidx$ dans un \gls{graph} $\graph$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\nodedegree{\nodeidx} & \quad & \parbox{.75\textwidth}{Le degré pondéré $\nodedegree{\nodeidx}\defeq \sum_{\nodeidx' \in \neighbourhood{\nodeidx}} \edgeweight_{\nodeidx,\nodeidx'}$ d’un nœud $\nodeidx$ dans un \gls{graph} $\graph$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\maxnodedegree^{(\graph)} & \quad & \parbox{.75\textwidth}{Le degré de nœud pondéré maximal d’un \gls{graph} $\graph$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\localdataset{\nodeidx} & \quad & \parbox{.75\textwidth}{Le \gls{localdataset} $\localdataset{\nodeidx}$ détenu par le nœud $\nodeidx\in \nodes$ d’un \gls{empgraph}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\localsamplesize{\nodeidx} & \quad & \parbox{.75\textwidth}{Le nombre de \gls{datapoint}s (i.e., la \gls{samplesize}) contenus dans le \gls{localdataset} $\localdataset{\nodeidx}$ au nœud $\nodeidx \in \nodes$.} \nonumber 
\end{align}

\begin{align}
	&\featurevec^{(\nodeidx,\sampleidx)} & \quad & \parbox{.75\textwidth}{Les \gls{feature}s du $\sampleidx$-ième \gls{datapoint} dans le \gls{localdataset} $\localdataset{\nodeidx}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\truelabel^{(\nodeidx,\sampleidx)} & \quad & \parbox{.75\textwidth}{Le \gls{label} du $\sampleidx$-ième \gls{datapoint} dans le \gls{localdataset} $\localdataset{\nodeidx}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\localparams{\nodeidx} & \quad & \parbox{.75\textwidth}{Les \gls{modelparams} locaux du \gls{device} $\nodeidx$ au sein d’un \gls{empgraph}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\locallossfunc{\nodeidx}{\weights} & \quad & \parbox{.75\textwidth}{La \gls{lossfunc} locale utilisée par le \gls{device} $\nodeidx$ pour évaluer l’utilité d’un certain choix $\weights$ pour les \gls{modelparams} locaux.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\gtvloss{\featurevec}{\hypothesis\big(\featurevec\big)}{\hypothesis'\big(\featurevec\big)} & \quad & \parbox{.75\textwidth}{La \gls{loss} encourue par une \gls{hypothesis} $\hypothesis'$ sur un \gls{datapoint} avec les \gls{feature}s $\featurevec$ et le \gls{label} $\hypothesis\big( \featurevec\big)$ obtenu à partir d’une autre \gls{hypothesis}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&{\rm stack} \big\{ \weights^{(\nodeidx)} \big\}_{\nodeidx=1}^{\nrnodes} & \quad & \parbox{.75\textwidth}{Le vecteur $\bigg( \big(\weights^{(1)}  \big)^{T}, \ldots, \big(\weights^{(\nrnodes)}  \big)^{T} \bigg)^{T} \in \mathbb{R}^{\dimlocalmodel\nrnodes}$ obtenu en empilant verticalement les \gls{modelparams} locaux $\weights^{(\nodeidx)} \in \mathbb{R}^{\dimlocalmodel}$.} \nonumber
\end{align}
        



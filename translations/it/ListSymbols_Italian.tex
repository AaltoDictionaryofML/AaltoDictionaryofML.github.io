% !TeX spellcheck = en_GB

\section*{Lists of Symbols}
%\label{ch_list_of_symbols}

\vspace*{-2mm}
\section*{Insiemi e Funzioni} 

\begin{align} 
	&a \in \mathcal{A} & \quad & \parbox{.75\textwidth}{L'oggetto $a$  è un elemento dell'insieme $\mathcal{A}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&a \defeq b & \quad & \mbox{Usiamo $a$ come abbreviazione di  $b$. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&|\mathcal{A}| & \quad & \mbox{La cardinalità (cioè, il numero di elementi) di un insieme finito $\mathcal{A}$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathcal{A} \subseteq \mathcal{B}& \quad & \mbox{$\mathcal{A}$ è un sottoinsieme di $\mathcal{B}$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathcal{A} \subset \mathcal{B}& \quad & \mbox{$\mathcal{A}$ è un sottoinsieme proprio di $\mathcal{B}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{N} & \quad & \mbox{I numeri naturali $1,2,\ldots$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}  &\quad &\mbox{I numeri reali $x$ \cite{RudinBook}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}_{+}  &\quad &\mbox{I numer reali non negativi $x\geq0$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}_{++}  &\quad &\mbox{I numeri reali strettamente positivi $x> 0$.} \nonumber
\end{align} 

\newpage
\begin{align}
		&\{0,1\}& \quad & \mbox{L'insieme costituito dai due numeri reali $0$ e $1$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&[0,1] &\quad &\mbox{L'intervallo chiuso dei numeri reali $x$ tali che $0 \leq x \leq 1$. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
    &\argmin_{\weights} f(\weights) &\quad &\mbox{L'insieme dei minimizzatori di una funzione reale $f(\weights)$.  } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
    &\sphere{\nrnodes} &\quad &\mbox{L'insieme dei vettori a \gls{norm} unitaria in $\mathbb{R}^{\nrnodes+1}$.  }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	 &\log a &\quad &\mbox{I logaritmo del numero reale strettamente positivo $a \in \mathbb{R}_{++}$.  } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	 &\hypothesis(\cdot)\!:\!\mathcal{A}\!\rightarrow\!\mathcal{B} :  a \!\mapsto\!h(a) &\quad &\parbox{.75\textwidth}{
	 	Una funzione (mappa) che accetta come input un qualsiasi elemento $a \in \mathcal{A}$, appartenente ad un insieme $\mathcal{A}$, e restituisce un elemento ben definito $h(a) \in \mathcal{B}$, appartenente ad un insieme $\mathcal{B}$. 
	 	L'insieme $\mathcal{A}$ è detto dominio della funzione $h$, mentre l'insieme $\mathcal{B}$ è il codominio di $\hypothesis$. \Gls{ml} punta ad individuare (o apprendere) una funzione $\hypothesis$ (cioè, un \gls{hypothesis}) che, a partire dalle \glspl{feature} $\featurevec$ di un \gls{datapoint}, fornisca una \gls{prediction} $\hypothesis(\featurevec)$
	 	del suo \gls{label} $\truelabel$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	 	&\nabla f(\weights) & \quad & \parbox{.75\textwidth}{Il \gls{gradient} di una funzione reale \gls{differentiable}
	 	$f: \mathbb{R}^{\featuredim}\rightarrow \mathbb{R}$ è il vettore
	 	$\nabla f(\weights) = \big( \frac{\partial f}{\partial \weight_{1}},\ldots,\frac{\partial f}{\partial \weight_{\featuredim}}  \big)^{T} \in \mathbb{R}^{\featuredim}$ \cite[Ch. 9]{RudinBookPrinciplesMatheAnalysis}.}   \nonumber
\end{align} 
\section*{Matrici e Vettori} 

\begin{align} 
	 &\featurevec=\big(\feature_{1},\ldots,\feature_{\featuredim})^{T} &\quad & \parbox{.75\textwidth}{Un vettore di lunghezza $\featuredim$, il cui
		$\featureidx$-th elemento è $\feature_{\featureidx}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}^{\featuredim} & \quad &  \parbox{.75\textwidth}{L'insieme di vettori $\featurevec=\big(\feature_{1},\ldots,\feature_{\featurelen}\big)^{T}$ composto da $\featuredim$ elementi a valori reali $\feature_{1},\ldots,\feature_{\featurelen} \in \mathbb{R}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{I}_{\modelidx \times \featuredim}  & \quad &  \parbox{.75\textwidth}{Una matrice identità generalizzata con $\modelidx$ righe e $\featuredim$ colonne. Gli elementi di $\mathbf{I}_{\modelidx \times \featuredim} \in \mathbb{R}^{\modelidx \times \featuredim}$ 
		sono pari a $1$ lungo la diagonale principale e pari a $0$ altrove. }\nonumber \\[2mm] \hline \nonumber\\[-5mm] %For example, $\mathbf{I}_{1 \times 2} = \big(1, 0\big)$ and $\mathbf{I}_{2 \times 1}= \begin{pmatrix} 1 \\ 0 \end{pmatrix}$.} 
	&\mathbf{I}_{\dimlocalmodel}, \mathbf{I} & \quad &  \parbox{.75\textwidth}{Una matrice identità quadrata di dimensione $\dimlocalmodel \times \dimlocalmodel$. Se la dimensione è chiara dal contesto, si omette l'indice.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\normgeneric{\featurevec}{2}  &\quad & \parbox{.75\textwidth}{La \gls{norm} Euclidea (or $\ell_{2}$) del vettore 
		$\featurevec=\big(\feature_{1},\ldots,\feature_{\featurelen}\big)^{T} \in \mathbb{R}^{\featuredim}$ definita come $ \| \featurevec \|_{2} \defeq \sqrt{\sum_{\featureidx=1}^{\featuredim} \feature_{\featureidx}^{2}}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	&\normgeneric{\featurevec}{}  & \quad &  \parbox{.75\textwidth}{Una qualche \gls{norm} del vettore $\featurevec \in \mathbb{R}^{\featuredim}$ \cite{GolubVanLoanBook}. Salvo diversa specificazione, intendiamo la \gls{norm} Euclidea $\normgeneric{\featurevec}{2}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\featurevec^{T} &\quad & \parbox{.75\textwidth}{La trasposizione di una matrice che ha il vettore
		$\featurevec \in \mathbb{R}^{\dimlocalmodel}$ come unica colonna.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{X}^{T} &\quad & \parbox{.75\textwidth}{La trasposizione di una matrice $\mathbf{X} \in \mathbb{R}^{\samplesize \times \featurelen}$. 
		Una matrice quadrata a valori reali $\mathbf{X} \in \mathbb{R}^{\samplesize \times \samplesize}$ 
		è detta simmetrica se $\mathbf{X} = \mathbf{X}^{T}$. }  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{0}= \big(0,\ldots,0\big)^{T}  & \quad &  \parbox{.75\textwidth}{Il vettore in $\mathbb{R}^{\dimlocalmodel}$ con ciascuna componente uguale a zero.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{1}= \big(1,\ldots,1\big)^{T}  & \quad &  \parbox{.75\textwidth}{Il vettore in $\mathbb{R}^{\dimlocalmodel}$ con ciascuna componente uguale a uno.} \nonumber
\end{align} 
\newpage
\begin{align} 
	&\big(\vv^{T},\vw^{T} \big)^{T}  & \quad &  \parbox{.75\textwidth}{Il vettore di lunghezza $\featurelen+\featurelen'$ 
		ottenuto concatenando le componenti del vettore $\vv \in \mathbb{R}^{\featurelen}$ con le componenti di $\vw \in \mathbb{R}^{\featurelen'}$.} \nonumber \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&	{\rm span}\{ \mathbf{B} \}  & \quad &  \parbox{.75\textwidth}{Lo span di una matrice $\mathbf{B} \in \mathbb{R}^{a \times b}$, 
		che rappresenta il sottospazio generato da tutte le combinazioni lineari delle colonne di $\mathbf{B}$, 
		${\rm span}\{ \mathbf{B} \} = \big\{  \mathbf{B} \va : \va \in \mathbb{R}^{b} \big\} \subseteq \mathbb{R}^{a}$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\determinant{\mC} &\quad & \parbox{.75\textwidth}{Il determinante della matrice $\mC$. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{A} \otimes \mathbf{B} &\quad & \parbox{.75\textwidth}{Il prodotto di Kronecker di $\mathbf{A}$ e $\mathbf{B}$ \cite{Golub1980}. }  \nonumber
\end{align} 

\newpage
\section*{Teoria della Probabilità} 
\begin{align}
	\expect_{p} \{ f(\datapoint) \}  \quad\quad & \parbox{.75\textwidth}{L' \gls{expectation} di una funzione $f(\datapoint)$ di una \gls{rv} 
		$\datapoint$ la cui \gls{probdist} è $\prob{\datapoint}$. Se la \gls{probdist} è chiara dal contesto, scriviamo semplicemente $\expect \{ f(\datapoint) \}$. }  \nonumber \\[2mm] \hline \nonumber\\[-5mm]    
	\prob{\featurevec,\truelabel} \quad\quad & \parbox{.75\textwidth}{Una \gls{probdist} (congiunta) di una \gls{rv} 
		le cui \gls{realization}ni  sono \gls{datapoint} con \glspl{feature} $\featurevec$ e \gls{label} $\truelabel$.} \nonumber        \nonumber \\[2mm] \hline \nonumber\\[-5mm]        
	\prob{\featurevec|\truelabel} \quad\quad & \parbox{.75\textwidth}{Una \gls{probdist} condizionata di una \gls{rv} 
		$\featurevec$, dato il valore di un'altra \gls{rv} $\truelabel$ \cite[Sec.\ 3.5]{BertsekasProb}. } \nonumber       \nonumber \\[2mm] \hline \nonumber\\[-5mm]           
	\prob{\featurevec;\weights} \quad\quad & \parbox{.75\textwidth}{Una \gls{probdist} parametrizzata di una \gls{rv} $\featurevec$. 
		La \gls{probdist} dipende da un vettore di parametri $\weights$. Ad esempio, $\prob{\featurevec;\weights}$ potrebbe essere una
		\gls{mvndist} con il vettore di parametri $\weights$ costituito dagli elementi del vettore di \gls{mean} $\expect \{ \featurevec \}$ 
		e della \gls{covmtx} $\expect \bigg \{ \big( \featurevec - \expect \{ \featurevec \}\big) \big( \featurevec - \expect \{ \featurevec \}\big)^{T}  \bigg\}$.} \nonumber           \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\mathcal{N}(\mu, \sigma^{2}) \quad\quad & \parbox{.75\textwidth}{La \gls{probdist} di una 
		\gls{gaussrv} $\feature \in \mathbb{R}$ con \gls{mean} (o \gls{expectation}) $\mu= \expect \{ \feature \}$ 
		e \gls{variance} $\sigma^{2} =   \expect \big\{  (  \feature - \mu )^2 \big\}$.} \nonumber    \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\mathcal{N}(\clustermean, \mathbf{C}) \quad\quad & \parbox{.75\textwidth}{La \gls{mvndist} di una		\gls{gaussrv} vettoriale $\featurevec \in \mathbb{R}^{\featuredim}$ con \gls{mean} (o \gls{expectation}) $\clustermean= \expect \{ \featurevec \}$ 
		e \gls{covmtx} $\mathbf{C} =  \expect \big\{ \big( \featurevec - \clustermean \big)\big( \featurevec - \clustermean \big)^{T} \big\}$.} \nonumber                                             
\end{align}





\newpage
\section*{Machine Learning}

\begin{align}
%	\datapoint \quad\quad & \parbox{.75\textwidth}{A \gls{datapoint} which is characterized by several properties that we 
%		divide into low-level properties (= \glspl{feature}) and high-level properties (= \gls{label}s) \cite[Ch. 2]{MLBasics}.}    \nonumber   \\[4mm] 
	\sampleidx \quad\quad & \parbox{.75\textwidth}{Un indice $\sampleidx=1,2,\ldots$ che enumera \gls{datapoint}s.}   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\samplesize \quad\quad &\parbox{.75\textwidth}{Il numero di  \gls{datapoint}s presenti in (cioè, la dimensione di) un \gls{dataset}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	\dataset \quad\quad & \parbox{.75\textwidth}{Un \gls{dataset} $\dataset = \{ \datapoint^{(1)},\ldots, \datapoint^{(\samplesize)} \}$ 
		è un elenco di singoli \gls{datapoint}s $\datapoint^{(\sampleidx)}$, con $\sampleidx=1,\ldots,\samplesize$.}   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\featurelen \quad\quad &\parbox{.75\textwidth}{Il numero di \glspl{feature} che caratterizza un \gls{datapoint}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\feature_{\featureidx} \quad\quad &\parbox{.75\textwidth}{La $\featureidx$-esima feature di un \gls{datapoint}. La prima \gls{feature} 
		è indicata con $\feature_{1}$, la seconda \gls{feature} $\feature_{2}$, e così via. } \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	\featurevec \quad\quad &\parbox{.75\textwidth}{Il \gls{featurevec} $\featurevec=\big(\feature_{1},\ldots,\feature_{\featuredim}\big)^{T}$ di un \gls{datapoint} le cui componenti sono le singole \glspl{feature} di un \gls{datapoint}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\featurespace \quad\quad & \parbox{.75\textwidth}{Lo \gls{featurespace} $\featurespace$ è l'insieme di tutti i valori possibili che le \glspl{feature} $\featurevec$ di un \gls{datapoint} possono assumere.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\rawfeaturevec \quad\quad &\parbox{.75\textwidth}{Invece del simbolo $\featurevec$, talvolta utilizziamo $\rawfeaturevec$ come simbolo alternativo per denotare un vettore i cui elementi sono le singole \glspl{feature} di un \gls{datapoint}. È necessario ricorrere a due simboli distinti per differenziare tra \glspl{feature} grezze e quelle apprese \cite[Ch. 9]{MLBasics}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\featurevec^{(\sampleidx)} \quad\quad &\parbox{.75\textwidth}{Il vettore di \gls{feature} relativo al $\sampleidx$-esimo \gls{datapoint} all'interno di un \gls{dataset}. } \nonumber %\\[2mm] \hline \nonumber\\[-5mm]
\end{align}        


\begin{align}
	\feature_{\featureidx}^{(\sampleidx)}\quad\quad &\parbox{.75\textwidth}{La $\featureidx$-esima \gls{feature} relativa all' $\sampleidx$-esimo 
		\gls{datapoint} all'interno di un \gls{dataset}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\batch \quad\quad &\parbox{.75\textwidth}{Un mini-\gls{batch} (o sottoinsieme) di \gls{datapoint}s scelti casualmente.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\batchsize \quad\quad &\parbox{.75\textwidth}{La dimensione di un mini-\gls{batch}, ovvero il numero di \gls{datapoint}s che esso contiene.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\truelabel \quad\quad &\parbox{.75\textwidth}{L' \gls{label} (o quantità di interesse) di un \gls{datapoint}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\truelabel^{(\sampleidx)} \quad\quad &\parbox{.75\textwidth}{L' \gls{label} del $\sampleidx$-esimo \gls{datapoint}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\big(\featurevec^{(\sampleidx)},\truelabel^{(\sampleidx)}\big)  \quad\quad &\parbox{.75\textwidth}{Le \glspl{feature} e le \gls{label} del $\sampleidx$-esimo \gls{datapoint}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\labelspace  \quad\quad & \parbox{.75\textwidth}{Lo \gls{labelspace} $\labelspace$ di un algoritmo di \gls{ml} consiste nell’insieme di tutti i possibili valori di \gls{label} che un \gls{datapoint} può assumere. Lo \gls{labelspace} nominale può essere più ampio rispetto all’insieme dei diversi valori di \gls{label} effettivamente presenti in un dato \gls{dataset} (ad esempio, un \gls{trainset}). I problemi (o metodi) di \gls{ml} che impiegano uno \gls{labelspace} numerico, come $\labelspace=\mathbb{R}$ 
		or $\labelspace=\mathbb{R}^{3}$, sono detti problemi (o metodi) di \gls{regression}. I problemi (o metodi) di \gls{ml} che utilizzano uno \gls{labelspace} discreto, come $\labelspace=\{0,1\}$ or $\labelspace=\{\mbox{\emph{cat}},\mbox{\emph{dog}},\mbox{\emph{mouse}}\}$, 
		sono detti problemi (o metodi) di \gls{classification}.}  \nonumber % \\[2mm] \hline \nonumber\\[-5mm]
\end{align}                  


\begin{align}
\lrate  \quad\quad & \parbox{.75\textwidth}{\Gls{learnrate} (o \gls{stepsize}) usato dai \gls{gdmethods}.}  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\hypothesis(\cdot)  \quad\quad &\parbox{.75\textwidth}{Una funzione \gls{hypothesis} che, dato in input un vettore di \glspl{feature} $\featurevec$ relativo ad un \gls{datapoint}, restituisce una \gls{prediction} $\hat{\truelabel}=\hypothesis(\featurevec)$ della sua \gls{label} $\truelabel$.}  	 \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	 \labelspace^{\featurespace} \quad\quad & \parbox{.75\textwidth}{Dati due insiemi $\featurespace$ e $\labelspace$, indichiamo con $ \labelspace^{\featurespace}$ l'insieme di tutte le possibili funzioni \gls{hypothesis} $\hypothesis: \featurespace \rightarrow \labelspace$.} 	 \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\hypospace  \quad\quad & \parbox{.75\textwidth}{Uno \gls{hypospace} o \gls{model} utilizzato da un algoritmo di \gls{ml}. 
		Lo \gls{hypospace} consiste in diverse funzioni \gls{hypothesis} $\hypothesis: \featurespace \rightarrow \labelspace$, tra le quali il metodo di \gls{ml} deve scegliere.}   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\effdim{\hypospace}  \quad\quad & \parbox{.75\textwidth}{L' \gls{effdim} di uno \gls{hypospace} $\hypospace$.}   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\biasterm^2 \quad\quad &\parbox{.75\textwidth}{
		Il \gls{bias} al quadrato di un' \gls{hypothesis} $\learnthypothesis$ 
		appresa, generata da un metodo di \gls{ml}. Il metodo è addestrato su \gls{datapoint}s 
		modellati come \gls{realization}s di \gls{rv}. Poiché i \gls{data} costituiscono una \gls{realization} 
		di \gls{rv}, anche l'\gls{hypothesis} appresa $\learnthypothesis$ rappresenta una \gls{realization} 
		di una \gls{rv}.} \nonumber  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\varianceterm \quad\quad &\parbox{.75\textwidth}{La \gls{variance} dell' 
		\gls{hypothesis} appresa (o dei suoi \gls{parameters}) generata da un metodo di \gls{ml}. 
		Il metodo è addestrato su \gls{datapoint}s 
		modellati come \gls{realization}s di \gls{rv}. Poiché i \gls{data} costituiscono una \gls{realization} 
		di \gls{rv}, anche l'\gls{hypothesis} appresa $\learnthypothesis$ rappresenta una \gls{realization} 
		di una \gls{rv}.} \nonumber \nonumber %\\[2mm] \hline \nonumber\\[-5mm]                           
\end{align}     

\begin{align}
\lossfunc{(\featurevec,\truelabel)}{\hypothesis}  \quad\quad & \parbox{.75\textwidth}{La \gls{loss} alla stima della
		\gls{label} $\truelabel$ di \gls{datapoint} mediante l'utilizzo della \gls{prediction} $\hat{\truelabel}=h(\featurevec)$. La 
		\gls{prediction} $\hat{\truelabel}$ è ottenuta applicando l'\gls{hypothesis} $\hypothesis \in \hypospace$ al \gls{featurevec} $\featurevec$ del \gls{datapoint} in questione.}    \nonumber  \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	\valerror \quad\quad &\parbox{.75\textwidth}{L'\gls{valerr} di un'\gls{hypothesis} $\hypothesis$, ovvero la media delle \gls{loss} da essa sostenute sull'\gls{valset}.}  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\emperror\big(\hypothesis| \dataset \big) \quad\quad &\parbox{.75\textwidth}{Il \gls{emprisk}, o \gls{loss} media, 
		associata all'\gls{hypothesis} $\hypothesis$ su un \gls{dataset} $\dataset$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\trainerror \quad\quad &\parbox{.75\textwidth}{L'\gls{trainerr} di un'\gls{hypothesis} $\hypothesis$, ovvero la media delle \gls{loss} da essa sostenute sull'\gls{trainset}. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\timeidx \quad\quad &\parbox{.75\textwidth}{Un indice temporale discreto $\timeidx=0,1,\ldots$ utilizzato per enumerare eventi sequenziali (o istanti temporali). }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\taskidx \quad\quad &\parbox{.75\textwidth}{Un indice che elenca i
		\gls{learningtask} all'interno di un problema di \gls{multitask learning}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\regparam \quad\quad &\parbox{.75\textwidth}{Un parametro di \gls{regularization} che controlla il grado di \gls{regularization}. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\eigval{\featureidx}\big( \mathbf{Q} \big) \quad\quad &\parbox{.75\textwidth}{Il $\featureidx$-esimo 
		\gls{eigenvalue} (ordinato in modo crescente o decrescente) di una matrice \gls{psd} $\mathbf{Q}$. Utilizziamo inoltre la notazione abbreviata $\eigval{\featureidx}$ qualora la matrice corrispondente sia chiara dal contesto. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\actfun(\cdot) \quad\quad &\parbox{.75\textwidth}{La \gls{actfun} utilizzata da un neurone artificiale all'interno di una \gls{ann}.}\nonumber %\\[2mm] \hline \nonumber\\[-5mm]
\end{align}              


\begin{align}
\decreg{\hat{\truelabel}} \quad\quad &\parbox{.75\textwidth}{Una \gls{decisionregion} all'interno di uno \gls{featurespace}.  }\nonumber \\[2mm] \hline \nonumber\\[-5mm]  
	\weights  \quad\quad & \parbox{.75\textwidth}{Un vettore di parametri $\weights = \big(\weight_{1},\ldots,\weight_{\featuredim}\big)^{T}$ 
		di un \gls{model}, e.g., i \gls{weights} di un\gls{linmodel} o di una \gls{ann}.}     \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\hypothesis^{(\weights)}(\cdot)  \quad\quad &\parbox{.75\textwidth}{Una funzione di \gls{hypothesis} espressa in termini di \gls{modelparams} regolabili $\weight_{1},\ldots,\weight_{\featuredim}$ concatenati nel vettore $\weights=\big(\weight_{1},\ldots,\weight_{\featuredim} \big)^{T}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\featuremap(\cdot)  \quad\quad & \parbox{.75\textwidth}{Una \gls{featuremap} $\featuremap: \featurespace \rightarrow \featurespace' : \featurevec \mapsto \featurevec' \defeq \featuremap\big( \featurevec \big) \in \featurespace'$.}   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	\kernelmap{\cdot}{\cdot} \quad\quad & \parbox{.75\textwidth}{Dato uno \gls{featurespace} $\featurespace$, 
		un \gls{kernel} è una mappa $\kernel: \featurespace \times \featurespace \rightarrow \mathbb{C}$ \gls{psd}.}   \nonumber                                                                                                                                                     
\end{align}




\newpage
\section*{Federated Learning}

\begin{align}
 	&\graph = \pair{\nodes}{\edges} & \quad & \parbox{.75\textwidth}{Un \gls{graph} non orientato i cui nodi $\nodeidx \in \nodes$ rappresentano i 
	\gls{device}s all'interno di un \gls{empgraph}. Gli spigoli pesati $\edges$ rappresentano la connettività tra i 
	\gls{device} e le similitudini statistiche tra i rispettivi \gls{dataset} e \gls{learningtask}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
&\nodeidx \in \nodes& \quad & \parbox{.75\textwidth}{Un nodo che rappresenta un 
	\gls{device} all'interno di un \gls{empgraph}. Il device ha accesso ad un \gls{localdataset} e può addestrare un \gls{localmodel}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\indsubgraph{\graph}{\cluster}& \quad & \parbox{.75\textwidth}{Il sottografo di $\graph$ indotto dai nodi $\cluster \subseteq \nodes$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\LapMat{\graph}   & \quad & \parbox{.75\textwidth}{La \gls{LapMat} di un \gls{graph} $\graph$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
		&\LapMat{\cluster}   & \quad & \parbox{.75\textwidth}{La \gls{LapMat} del \gls{graph} indotto $\indsubgraph{\graph}{\cluster}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	 &		\neighbourhood{\nodeidx}  & \quad & \parbox{.75\textwidth}{L'\gls{neighborhood} di un nodo $\nodeidx$ in un \gls{graph} $\graph$.}   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\nodedegree{\nodeidx} & \quad & \parbox{.75\textwidth}{Il grado pesato $\nodedegree{\nodeidx}\defeq \sum_{\nodeidx' \in \neighbourhood{\nodeidx}} \edgeweight_{\nodeidx,\nodeidx'}$ di un nodo $\nodeidx$ in un \gls{graph} $\graph$.}  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\maxnodedegree^{(\graph)} & \quad & \parbox{.75\textwidth}{Il massimo grado pesato dei nodi di un $\graph$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
&\localdataset{\nodeidx} & \quad & \parbox{.75\textwidth}{Il \gls{localdataset} $\localdataset{\nodeidx}$ associato al nodo $\nodeidx\in \nodes$ of an \gls{empgraph}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
&\localsamplesize{\nodeidx} & \quad & \parbox{.75\textwidth}{Il numero di \gls{datapoint}s (ossia, la \gls{samplesize}) contenuti nel 
			\gls{localdataset} $\localdataset{\nodeidx}$ relativo al nodo $\nodeidx\in \nodes$.} \nonumber 
\end{align} 
\begin{align} 
		&\featurevec^{(\nodeidx,\sampleidx)} & \quad & \parbox{.75\textwidth}{Le \gls{feature} dell'$\sampleidx$-esimo \gls{datapoint} nel \gls{localdataset} $\localdataset{\nodeidx}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\truelabel^{(\nodeidx,\sampleidx)} & \quad & \parbox{.75\textwidth}{La \gls{label} dell'$\sampleidx$-esimo \gls{datapoint} nel \gls{localdataset} $\localdataset{\nodeidx}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
		&\localparams{\nodeidx} & \quad & \parbox{.75\textwidth}{I \gls{modelparams} locali del \gls{device} $\nodeidx$ all'interno di un \gls{empgraph}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
		&\locallossfunc{\nodeidx}{\weights} & \quad & \parbox{.75\textwidth}{La \gls{lossfunc} locale utilizzata dal \gls{device} $\nodeidx$ 
		per valutare l'efficacia di una certa scelta $\weights$ come \gls{modelparams} locali.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	& \gtvloss{\featurevec}{\hypothesis\big(\featurevec\big)}{\hypothesis'\big(\featurevec\big)}& \quad & \parbox{.75\textwidth}{La \gls{loss} 
		associata ad un' \gls{hypothesis} $\hypothesis'$ su un \gls{datapoint} con \glspl{feature} $\featurevec$ e \gls{label} 
		$\hypothesis\big( \featurevec\big)$, ottenuta da un'altra \gls{hypothesis}.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
		& 	{\rm stack} \big\{ \weights^{(\nodeidx)} \big\}_{\nodeidx=1}^{\nrnodes} & \quad & \parbox{.75\textwidth}{Il vettore $\bigg( \big(\weights^{(1)}  \big)^{T}, \ldots, \big(\weights^{(\nrnodes)}  \big)^{T} \bigg)^{T} \in \mathbb{R}^{\dimlocalmodel\nrnodes}$ ottenuto concatenando verticalmente i \gls{modelparams} locali $\weights^{(\nodeidx)} \in \mathbb{R}^{\dimlocalmodel}$.} \nonumber  
\end{align}        



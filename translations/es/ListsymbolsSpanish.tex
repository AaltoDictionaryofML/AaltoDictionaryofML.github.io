% !TeX spellcheck = en_GB

\section*{Lists of Symbols}
%\label{ch_list_of_symbols}

\vspace*{-2mm}
\section*{Conjuntos y Funciones} 

\begin{align} 
	&a \in \mathcal{A} & \quad & \parbox{.85\textwidth}{El objeto $a$ es un elemento del conjunto $\mathcal{A}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&a \defeq b & \quad & \mbox{Usamos $a$ como una abreviatura para $b$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&|\mathcal{A}| & \quad & \mbox{La cardinalidad (es decir, el número de elementos) de un conjunto finito $\mathcal{A}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathcal{A} \subseteq \mathcal{B}& \quad & \mbox{$\mathcal{A}$ es un subconjunto de $\mathcal{B}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathcal{A} \subset \mathcal{B}& \quad & \mbox{$\mathcal{A}$ es un subconjunto estricto de $\mathcal{B}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{N} & \quad & \mbox{Los números naturales $1, 2, \dots$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}  &\quad &\mbox{Los números reales $x$ \cite{RudinBook}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}_{+}  &\quad &\mbox{Los números reales no negativos $x \geq 0$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}_{++}  &\quad &\mbox{Los números reales positivos $x > 0$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\{0,1\}& \quad & \mbox{El conjunto que consta de los dos números reales $0$ y $1$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&[0,1] &\quad &\mbox{El intervalo cerrado de números reales $x$ con $0 \leq x \leq 1$.}\nonumber 
\end{align} 

\newpage
\begin{align}
    	&\aargmin_{\weights} f(\weights) &\quad &\parbox{.70\textwidth}{El conjunto de minimizadores para una \gls{function} con valores reales $f(\weights)$. 
    		\\ Véa también: \gls{function}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
    	&\sphere{\nrnodes} &\quad &\parbox{.70\textwidth}{El conjunto de vectores con \gls{norm} unitaria en $\mathbb{R}^{\nrnodes+1}$.
    		\\ Véa también: \gls{norm}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\exp\,(a) &\quad &\parbox{.70\textwidth}{La \gls{function} exponencial evaluada en el número real $a \in \mathbb{R}$.
		\\ Véa también: \gls{function}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\log a &\quad &\mbox{El logaritmo del número positivo $a \in \mathbb{R}_{++}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\hypothesis(\cdot)\!:\!\mathcal{A}\!\rightarrow\!\mathcal{B} :  a \!\mapsto\!h(a) &\quad &\parbox{.70\textwidth}{
	 	Una \gls{function} (o \gls{map}) de un conjunto $\mathcal{A}$ a un conjunto $\mathcal{B}$, que asigna a cada entrada 
	 	$a \in \mathcal{A}$ una salida bien definida $f(a) \in \mathcal{B}$.
	 	El conjunto $\mathcal{A}$ es el dominio de la \gls{function} $f$ y el conjunto $\mathcal{B}$ es el 
	 	codominio de $f$. \Gls{ml} tiene como objetivo aprender una \gls{function} $\hypothesis$ que mapea \glspl{feature} 
	 	$\featurevec$ de un \gls{datapoint} a una \gls{prediction} $\hypothesis(\featurevec)$ para su \gls{label} $\truelabel$.
		\\ Véa también: \gls{function}, \gls{map}, \gls{ml}, \gls{feature}, \gls{datapoint}, \gls{prediction}, \gls{label}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\operatorname{epi}(f)  & \quad & \parbox{.70\textwidth}{El \gls{epigraph} de una \gls{function} con valores reales 
	 	$f: \mathbb{R}^{\featuredim}\rightarrow \mathbb{R}$.
		\\ Véa también: \gls{epigraph}, \gls{function}.} \nonumber \\[2mm]  \hline \nonumber\\[-5mm]
	&  \frac{\partial f(\weight_{1}, \ldots, \weight_{\nrfeatures})}{\partial \weight_{\featureidx}} & \quad & \parbox{.70\textwidth}{La derivada parcial (si existe) de 
	 	una \gls{function} con valores reales $f: \mathbb{R}^{\featuredim}\rightarrow \mathbb{R}$ con respecto a $\weight_{\featureidx}$ \cite[Ch. 9]{RudinBookPrinciplesMatheAnalysis}.
		\\ Véa también: \gls{function}.} \nonumber 
\end{align} 

\begin{align}
	&\nabla f(\weights) & \quad & \parbox{.70\textwidth}{El \gls{gradient} of a \gls{differentiable} real-valued \gls{function} 
		$f: \mathbb{R}^{\featuredim}\rightarrow \mathbb{R}$ is the vector 
		$\nabla f(\weights) = \big( {\partial f}/{\partial \weight_{1}}, \ldots, {\partial f}/{\partial \weight_{\featuredim}}  \big)^{T} \in \mathbb{R}^{\featuredim}$ \cite[Ch. 9]{RudinBookPrinciplesMatheAnalysis}.
	   \\ See also: \gls{gradient}, \gls{differentiable}, \gls{function}.}   \nonumber
\end{align} 
\section*{Matrices y Vectores}

\begin{align} 
	&\featurevec=\big(\feature_{1}, \ldots, \feature_{\featuredim})^{T} &\quad & \parbox{.75\textwidth}{Un vector de longitud $\featuredim$, con su 
		entrada $\featureidx$-ésima siendo $\feature_{\featureidx}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}^{\featuredim} & \quad &  \parbox{.75\textwidth}{El conjunto de vectores $\featurevec=\big(\feature_{1}, \ldots, \feature_{\featurelen}\big)^{T}$ que consta de 
		$\featuredim$ entradas con valores reales $\feature_{1}, \ldots, \feature_{\featurelen} \in \mathbb{R}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{I}_{\modelidx \times \featuredim}  & \quad &  \parbox{.75\textwidth}{Una matriz identidad generalizada 
		con $\modelidx$ filas y $\featuredim$ columnas. Las entradas de $\mathbf{I}_{\modelidx \times \featuredim} \in \mathbb{R}^{\modelidx \times \featuredim}$ 
		son iguales a $1$ a lo largo de la diagonal principal y de lo contrario iguales a $0$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{I}_{\dimlocalmodel}, \mathbf{I} & \quad &  \parbox{.75\textwidth}{Una matriz identidad 
		cuadrada de tamaño $\dimlocalmodel \times \dimlocalmodel$. Si el tamaño es claro por 
		el contexto, omitimos el subíndice.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\normgeneric{\featurevec}{2}  &\quad & \parbox{.75\textwidth}{La \gls{norm} euclidiana (o $\ell_{2}$) del vector 
		$\featurevec=\big(\feature_{1}, \ldots, \feature_{\featurelen}\big)^{T} \in \mathbb{R}^{\featuredim}$ definida como 
		$\| \featurevec \|_{2} \defeq \sqrt{\sum_{\featureidx=1}^{\featuredim} \feature_{\featureidx}^{2}}$.
		\\ Véa también: \gls{norm}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	&\normgeneric{\featurevec}{}  & \quad &  \parbox{.75\textwidth}{Alguna \gls{norm} del vector $\featurevec \in \mathbb{R}^{\featuredim}$ \cite{GolubVanLoanBook}. 
		A menos que se especifique lo contrario, nos referimos a la \gls{norm} euclidiana $\normgeneric{\featurevec}{2}$.
		\\ Véa también: \gls{norm}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\featurevec^{T} &\quad & \parbox{.75\textwidth}{La transpuesta de una matriz que tiene al vector 
		$\featurevec \in \mathbb{R}^{\dimlocalmodel}$ como su única columna.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{X}^{T} &\quad & \parbox{.75\textwidth}{La transpuesta de una matriz $\mathbf{X} \in \mathbb{R}^{\samplesize \times \featurelen}$. 
		Una matriz cuadrada con valores reales $\mathbf{X} \in \mathbb{R}^{\samplesize \times \samplesize}$ 
		se llama simétrica si $\mathbf{X} = \mathbf{X}^{T}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{X}^{-1} &\quad & \parbox{.75\textwidth}{La \gls{inverse} de una matriz $\mathbf{X} \in \mathbb{R}^{\featurelen \times \featurelen}$.
		\\ Véa también: \gls{inverse}.} \nonumber 
\end{align} 

%\newpage
\begin{align} 
	&\mathbf{0}= \big(0, \ldots, 0\big)^{T}  & \quad &  \parbox{.75\textwidth}{El vector en $\mathbb{R}^{\dimlocalmodel}$ con cada entrada igual a cero.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{1}= \big(1, \ldots, 1\big)^{T}  & \quad &  \parbox{.75\textwidth}{El vector en $\mathbb{R}^{\dimlocalmodel}$ con cada entrada igual a uno.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\big(\vv^{T},\vw^{T} \big)^{T}  & \quad &  \parbox{.85\textwidth}{El vector de longitud $\featurelen+\featurelen'$ 
		obtenido al concatenar las entradas del vector $\vv \in \mathbb{R}^{\featurelen}$ con las entradas de $\vw \in \mathbb{R}^{\featurelen'}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&{\rm span}\{ \mathbf{B} \}  & \quad &  \parbox{.85\textwidth}{El espacio generado por una matriz $\mathbf{B} \in \mathbb{R}^{a \times b}$, 
		que es el subespacio de todas las combinaciones lineales de las columnas de $\mathbf{B}$, de modo que
		${\rm span}\{ \mathbf{B} \} = \big\{ \mathbf{B} \va : \va \in \mathbb{R}^{b} \big\} \subseteq \mathbb{R}^{a}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\determinant{\mC} &\quad & \parbox{.85\textwidth}{El \gls{det} de la matriz $\mC$.
		\\ Véa también: \gls{det}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{A} \otimes \mathbf{B} &\quad & \parbox{.85\textwidth}{El \gls{kroneckerproduct} de $\mathbf{A}$ y $\mathbf{B}$ \cite{Golub1980}.
		\\ Véa también: \gls{kroneckerproduct}.} \nonumber
\end{align}

\newpage
\section*{Teoría de la Probabilidad} 
\begin{align}
	&\featurevec \sim p(\vz)  &\quad & \parbox{.85\textwidth}{La \gls{rv} $\featurevec$ está distribuida según 
		la \gls{probdist} $p(\vz)$ \cite{klenke2020probability}, \cite{BillingsleyProbMeasure}.
		\\ Véa también: \gls{rv}, \gls{probdist}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]  
	&\expect_{p} \{ f(\datapoint) \}  &\quad & \parbox{.85\textwidth}{La \gls{expectation} de una \gls{rv} $f(\datapoint)$ que 
		se obtiene al aplicar una \gls{function} determinista $f$ a una \gls{rv}
		$\datapoint$ cuya \gls{probdist} es $\prob{\datapoint}$. Si la \gls{probdist} es clara por el contexto, 
		solo escribimos $\expect \{ f(\datapoint) \}$. 
		\\ Véa también: \gls{expectation}, \gls{rv}, \gls{function}, \gls{probdist}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\cov{x}{y} &\quad & \parbox{.85\textwidth}{La \gls{covariance} entre dos \glspl{rv} con valores reales definidas 
		sobre un mismo \gls{probspace}. 
		\\ Véa también: \gls{covariance}, \gls{rv}, \gls{probdist}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\prob{\featurevec,\truelabel} &\quad & \parbox{.85\textwidth}{Una \gls{probdist} (conjunta) de una \gls{rv} 
		cuyas \glspl{realization} son \glspl{datapoint} con \glspl{feature} $\featurevec$ y \gls{label} $\truelabel$.
		\\ Véa también: \gls{probdist}, \gls{rv}, \gls{realization}, \gls{datapoint}, \gls{feature}, 
		\gls{label}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\prob{\featurevec|\truelabel} &\quad & \parbox{.85\textwidth}{Una \gls{probdist} condicional de una \gls{rv} 
		$\featurevec$ dado el valor de otra \gls{rv} $\truelabel$ \cite[Sec.\ 3.5]{BertsekasProb}. 
		\\ Véa también: \gls{probdist}, \gls{rv}.} \nonumber 
\end{align} 






\newpage
\begin{align} 
	&\prob{\featurevec;\weights} &\quad & \parbox{.85\textwidth}{Una \gls{probdist} parametrizada de una \gls{rv} $\featurevec$. 
		La \gls{probdist} depende de un vector de \gls{parameter} $\weights$. Por ejemplo, $\prob{\featurevec;\weights}$ podría ser una 
		\gls{mvndist} con el vector de \gls{parameter} $\weights$ dado por las entradas del vector de \gls{mean} $\expect \{ \featurevec \}$ 
		y la \gls{covmtx} $\expect \bigg \{ \big( \featurevec - \expect \{ \featurevec \}\big) \big( \featurevec - \expect \{ \featurevec \}\big)^{T}  \bigg\}$.
		\\ Véa también: \gls{probdist}, \gls{rv}, \gls{parameter}, \gls{mvndist}, \gls{mean}, \gls{covmtx}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathcal{N}(\mu, \sigma^{2}) &\quad & \parbox{.85\textwidth}{La \gls{probdist} de una 
		\gls{gaussrv} $\feature \in \mathbb{R}$ con \gls{mean} (o \gls{expectation}) $\mu= \expect \{ \feature \}$ 
		y \gls{variance} $\sigma^{2} = \expect \big\{ ( \feature - \mu )^2 \big\}$.
		\\ Véa también: \gls{probdist}, \gls{gaussrv}, \gls{mean}, \gls{expectation}, \gls{variance}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathcal{N}(\meanvecgeneric, \mathbf{C}) &\quad & \parbox{.85\textwidth}{La \gls{mvndist} de una \gls{gaussrv} con valores vectoriales 
		$\featurevec \in \mathbb{R}^{\featuredim}$ con \gls{mean} (o \gls{expectation}) $\meanvecgeneric= \expect \{ \featurevec \}$ 
		y \gls{covmtx} $\mathbf{C} = \expect \big\{ \big( \featurevec - \meanvecgeneric \big)\big( \featurevec - \meanvecgeneric \big)^{T} \big\}$.
		\\ Véa también: \gls{mvndist}, \gls{gaussrv}, \gls{mean}, \gls{expectation}, \gls{covmtx}.} \nonumber                                             
\end{align}





\newpage
\section*{Aprendizaje Automático}
\begin{align}
	&\sampleidx &\quad & \parbox{.90\textwidth}{Un índice $\sampleidx=1, 2, \ldots$ que 
		enumera \glspl{datapoint}.
		\\ Véa también: \gls{datapoint}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\samplesize &\quad &\parbox{.90\textwidth}{El número de \glspl{datapoint} en (es decir, el tamaño de) un \gls{dataset}.
		\\ Véa también: \gls{datapoint}, \gls{dataset}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	&\dataset &\quad & \parbox{.90\textwidth}{Un \gls{dataset} $\dataset = \{ \datapoint^{(1)}, \ldots, \datapoint^{(\samplesize)} \}$ 
		es una lista de \glspl{datapoint} individuales $\datapoint^{(\sampleidx)}$, para $\sampleidx=1, \ldots, \samplesize$.
		\\ Véa también: \gls{dataset}, \gls{datapoint}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\featurelen &\quad &\parbox{.90\textwidth}{El número de \glspl{feature} que caracterizan un \gls{datapoint}.
		\\ Véa también: \gls{feature}, \gls{datapoint}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\feature_{\featureidx} &\quad &\parbox{.90\textwidth}{La \gls{feature} $\featureidx$-ésima de un \gls{datapoint}. La primera \gls{feature} 
		se denota $\feature_{1}$, la segunda \gls{feature} $\feature_{2}$, y así sucesivamente.
		\\ Véa también: \gls{datapoint}, \gls{feature}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	&\featurevec &\quad &\parbox{.90\textwidth}{El \gls{featurevec} $\featurevec=\big(\feature_{1}, \ldots, \feature_{\featuredim}\big)^{T}$ de un \gls{datapoint}. Las entradas del vector 
		son las \glspl{feature} individuales de un \gls{datapoint}.
		\\ Véa también: \gls{featurevec}, \gls{datapoint}, \gls{feature}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\featurespace &\quad & \parbox{.90\textwidth}{El \gls{featurespace} $\featurespace$ es 
		el conjunto de todos los valores posibles que las \glspl{feature} $\featurevec$ de un \gls{datapoint} pueden tomar.
		\\ Véa también: \gls{featurespace}, \gls{feature}, \gls{datapoint}.} \nonumber 
\end{align}        

\begin{align}
	&\rawfeaturevec &\quad &\parbox{.85\textwidth}{En lugar del símbolo $\featurevec$, a veces 
		usamos $\rawfeaturevec$ como otro símbolo para denotar un vector cuyas entradas 
		son las \glspl{feature} individuales de un \gls{datapoint}. Necesitamos dos 
		símbolos diferentes para distinguir entre \glspl{feature} crudas y aprendidas \cite[Ch. 9]{MLBasics}.
		\\ Véa también: \gls{feature}, \gls{datapoint}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\featurevec^{(\sampleidx)} &\quad &\parbox{.85\textwidth}{El \gls{featurevec} del \gls{datapoint} $\sampleidx$-ésimo dentro de un \gls{dataset}.
		\\ Véa también: \gls{feature}, \gls{datapoint}, \gls{dataset}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\feature_{\featureidx}^{(\sampleidx)} &\quad &\parbox{.85\textwidth}{La \gls{feature} $\featureidx$-ésima del \gls{datapoint} $\sampleidx$-ésimo 
		dentro de un \gls{dataset}.
		\\ Véa también: \gls{feature}, \gls{datapoint}, \gls{dataset}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\batch &\quad &\parbox{.85\textwidth}{Un mini-\gls{batch} (o subconjunto) de \glspl{datapoint} elegidos aleatoriamente.
		\\ Véa también: \gls{batch}, \gls{datapoint}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\batchsize &\quad &\parbox{.85\textwidth}{El tamaño de (es decir, el número de \glspl{datapoint} en) un mini-\gls{batch}.
		\\ Véa también: \gls{datapoint}, \gls{batch}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\truelabel &\quad &\parbox{.85\textwidth}{La \gls{label} (o cantidad de interés) de un \gls{datapoint}.
		\\ Véa también: \gls{label}, \gls{datapoint}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\truelabel^{(\sampleidx)} &\quad &\parbox{.85\textwidth}{La \gls{label} del \gls{datapoint} $\sampleidx$-ésimo.
		\\ Véa también: \gls{label}, \gls{datapoint}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\big(\featurevec^{(\sampleidx)},\truelabel^{(\sampleidx)}\big) &\quad &\parbox{.85\textwidth}{Las \glspl{feature} y la \gls{label} del \gls{datapoint} $\sampleidx$-ésimo.
		\\ Véa también: \gls{feature}, \gls{label}, \gls{datapoint}.} \nonumber 
\end{align}                  

\begin{align}
	&\labelspace  &\quad & \parbox{.90\textwidth}{El \gls{labelspace} $\labelspace$ de 
		un método de \gls{ml} consiste en todos los valores posibles de \gls{label} que un \gls{datapoint} puede 
		tener. El \gls{labelspace} nominal puede ser más grande que el conjunto de diferentes valores de \gls{label} 
		que aparecen en un \gls{dataset} dado (por ejemplo, un \gls{trainset}). Los problemas 
		(o métodos) de \gls{ml} que usan un \gls{labelspace} numérico, como $\labelspace=\mathbb{R}$ 
		o $\labelspace=\mathbb{R}^{3}$, se denominan problemas (o métodos) de \gls{regression}. Los problemas 
		(o métodos) de \gls{ml} que usan un \gls{labelspace} discreto, como $\labelspace=\{0,1\}$ o $\labelspace=\{\mbox{\emph{gato}},\mbox{\emph{perro}},\mbox{\emph{ratón}}\}$, 
		se denominan problemas (o métodos) de \gls{classification}.
		\\ Véa también: \gls{labelspace}, \gls{ml}, \gls{label}, \gls{datapoint}, \gls{dataset}, \gls{trainset}, 
		\gls{regression}, \gls{classification}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\lrate  &\quad & \parbox{.90\textwidth}{La \gls{learnrate} (o \gls{stepsize}) utilizada por los \gls{gdmethods}.
		\\ Véa también: \gls{learnrate}, \gls{stepsize}, \gls{gdmethods}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\hypothesis(\cdot)  &\quad &\parbox{.90\textwidth}{Un \gls{hypothesis} \gls{map} que mapea las \glspl{feature} de un \gls{datapoint} 
		a una \gls{prediction} $\hat{\truelabel}=\hypothesis(\featurevec)$ para su \gls{label} $\truelabel$.
		\\ Véa también: \gls{hypothesis}, \gls{map}, \gls{feature}, \gls{datapoint}, \gls{prediction}, \gls{label}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\labelspace^{\featurespace} &\quad & \parbox{.90\textwidth}{Dados dos conjuntos $\featurespace$ y $\labelspace$, denotamos por $\labelspace^{\featurespace}$ 
		el conjunto de todos los \gls{hypothesis} \glspl{map} posibles $\hypothesis: \featurespace \rightarrow \labelspace$.
		\\ Véa también: \gls{hypothesis}, \gls{map}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\hypospace  &\quad & \parbox{.90\textwidth}{Un \gls{hypospace} o \gls{model} utilizado por un método de \gls{ml}. 
		El \gls{hypospace} consiste en diferentes \gls{hypothesis} \glspl{map} $\hypothesis: \featurespace \rightarrow \labelspace$, entre los cuales 
		el método de \gls{ml} debe elegir.
		\\ Véa también: \gls{hypospace}, \gls{model}, \gls{ml}, \gls{hypothesis}, \gls{map}.} \nonumber 
\end{align}     

\begin{align}
	&\effdim{\hypospace}  &\quad & \parbox{.80\textwidth}{La \gls{effdim} de un \gls{hypospace} $\hypospace$.
		\\ Véa también: \gls{effdim}, \gls{hypospace}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\biasterm^2 &\quad &\parbox{.80\textwidth}{
		El \gls{bias} al cuadrado de un \gls{hypothesis} aprendido $\learnthypothesis$, o sus \glspl{parameter}. Nótese que $\learnthypothesis$ 
		se convierte en una \gls{rv} si se aprende de \glspl{datapoint} que son \glspl{rv} en sí mismos.
		\\ Véa también: \gls{bias}, \gls{hypothesis}, \gls{parameter}, \gls{rv}, \gls{datapoint}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\varianceterm &\quad &\parbox{.80\textwidth}{La \gls{variance} de un \gls{hypothesis} aprendido 
		$\learnthypothesis$, o sus \glspl{parameter}. Nótese que $\learnthypothesis$ 
		se convierte en una \gls{rv} si se aprende de \glspl{datapoint} que son \glspl{rv} en sí mismos.
		\\ Véa también: \gls{variance}, \gls{hypothesis}, \gls{parameter}, \gls{rv}, \gls{datapoint}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\lossfunc{(\featurevec,\truelabel)}{\hypothesis}  &\quad & \parbox{.80\textwidth}{La \gls{loss} incurrida al predecir la 
		\gls{label} $\truelabel$ de un \gls{datapoint} usando la \gls{prediction} $\hat{\truelabel}=h(\featurevec)$. La 
		\gls{prediction} $\hat{\truelabel}$ se obtiene al evaluar el \gls{hypothesis} $\hypothesis \in \hypospace$ para 
		el \gls{featurevec} $\featurevec$ del \gls{datapoint}.
		\\ Véa también: \gls{loss}, \gls{label}, \gls{datapoint}, \gls{prediction}, \gls{hypothesis}, 
		\gls{featurevec}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	&\valerror &\quad &\parbox{.80\textwidth}{El \gls{valerr} de un \gls{hypothesis} $\hypothesis$, que es su 
		\gls{loss} promedio incurrido sobre un \gls{valset}.
		\\ Véa también: \gls{valerr}, \gls{hypothesis}, \gls{loss}, \gls{valset}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\emperror\big(\hypothesis| \dataset \big) &\quad &\parbox{.80\textwidth}{El \gls{emprisk}, o \gls{loss} promedio, 
		incurrido por el \gls{hypothesis} $\hypothesis$ en un \gls{dataset} $\dataset$.
		\\ Véa también: \gls{emprisk}, \gls{loss}, \gls{hypothesis}, \gls{dataset}.} \nonumber 
\end{align}     

\begin{align}                          
	&\trainerror &\quad &\parbox{.85\textwidth}{El \gls{trainerr} de un \gls{hypothesis} $\hypothesis$, que es su 
		\gls{loss} promedio incurrido sobre un \gls{trainset}.
		\\ Véa también: \gls{trainerr}, \gls{hypothesis}, \gls{loss}, \gls{trainset}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\timeidx &\quad &\parbox{.85\textwidth}{Un índice de tiempo discreto $\timeidx=0, 1, \ldots$ utilizado para 
		enumerar eventos secuenciales (o instantes de tiempo).} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\taskidx &\quad &\parbox{.85\textwidth}{Un índice que enumera
		\glspl{learningtask} dentro de un problema de \gls{multitask learning}.
		\\ Véa también: \gls{learningtask}, \gls{multitask learning}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\regparam &\quad &\parbox{.85\textwidth}{Un \gls{regularization} \gls{parameter} que controla 
		el grado de \gls{regularization}.
		\\ Véa también: \gls{regularization}, \gls{parameter}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\eigval{\featureidx}\big( \mathbf{Q} \big) &\quad &\parbox{.85\textwidth}{El \gls{eigenvalue} $\featureidx$-ésimo 
		(ordenado en orden ascendente o descendente) de una matriz \gls{psd} $\mathbf{Q}$. También 
		usamos la abreviatura $\eigval{\featureidx}$ si la matriz correspondiente es clara por el contexto.
		\\ Véa también: \gls{eigenvalue}, \gls{psd}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\actfun(\cdot) &\quad &\parbox{.85\textwidth}{La \gls{actfun} utilizada por una neurona artificial dentro de una \gls{ann}.
		\\ Véa también: \gls{actfun}, \gls{ann}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\decreg{\hat{\truelabel}} &\quad &\parbox{.85\textwidth}{Una \gls{decisionregion} dentro de un \gls{featurespace}.
		\\ Véa también: \gls{decisionregion}, \gls{featurespace}.} \nonumber 
\end{align}     

\begin{align} 
	&\weights  &\quad & \parbox{.85\textwidth}{Un vector de \gls{parameter} $\weights = \big(\weight_{1}, \ldots, \weight_{\featuredim}\big)^{T}$ 
		de un \gls{model}, por ejemplo, los \gls{weights} de un \gls{linmodel} o una \gls{ann}.
		\\ Véa también: \gls{parameter}, \gls{model}, \gls{weights}, \gls{linmodel}, \gls{ann}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\hypothesis^{(\weights)}(\cdot)  &\quad &\parbox{.85\textwidth}{Un \gls{hypothesis} \gls{map} que involucra \gls{modelparams} ajustables 
		$\weight_{1}, \ldots, \weight_{\featuredim}$ apilados en el vector $\weights=\big(\weight_{1}, \ldots, \weight_{\featuredim} \big)^{T}$.
		\\ Véa también: \gls{hypothesis}, \gls{map}, \gls{modelparams}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\featuremap(\cdot)  &\quad & \parbox{.85\textwidth}{Una \gls{featuremap} 
		$\featuremap: \featurespace \rightarrow \featurespace' : \featurevec \mapsto \featurevec' \defeq \featuremap\big( \featurevec \big) \in \featurespace'$.
		\\ Véa también: \gls{featuremap}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\kernelmap{\cdot}{\cdot} &\quad & \parbox{.85\textwidth}{Dado un \gls{featurespace} $\featurespace$, 
		un \gls{kernel} es una \gls{map} $\kernel: \featurespace \times \featurespace \rightarrow \mathbb{C}$ que es \gls{psd}.
		\\ Véa también: \gls{featurespace}, \gls{kernel}, \gls{map}, \gls{psd}.} \nonumber 
\end{align}





\newpage
\section*{Aprendizaje Federado}

\begin{align}
	&\graph = \pair{\nodes}{\edges} & \quad & \parbox{.80\textwidth}{Un \gls{graph} no dirigido cuyos nodos $\nodeidx \in \nodes$ representan 
	   \glspl{device} dentro de un \gls{empgraph}. Los bordes no dirigidos ponderados $\edges$ representan la conectividad entre 
	   \glspl{device} y las similitudes estadísticas entre sus \glspl{dataset} y \glspl{learningtask}.
	   \\ Véa también: \gls{graph}, \gls{device}, \gls{empgraph}, \gls{dataset}, \gls{learningtask}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
   &\nodeidx \in \nodes& \quad & \parbox{.80\textwidth}{Un nodo que representa algún 
	   \gls{device} dentro de un \gls{empgraph}. El \gls{device} puede acceder a un \gls{localdataset} y entrenar un \gls{localmodel}.
	   \\ Véa también: \gls{device}, \gls{empgraph}, \gls{localdataset}, \gls{localmodel}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
   &\indsubgraph{\graph}{\cluster}& \quad & \parbox{.80\textwidth}{El subgrafo inducido de $\graph$ usando los nodos en $\cluster \subseteq \nodes$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
   &\LapMat{\graph}   & \quad & \parbox{.80\textwidth}{La \gls{LapMat} de un \gls{graph} $\graph$.
	   \\ Véa también: \gls{LapMat}, \gls{graph}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
   &\LapMat{\cluster}   & \quad & \parbox{.80\textwidth}{La \gls{LapMat} del \gls{graph} inducido $\indsubgraph{\graph}{\cluster}$.
	   \\ Véa también: \gls{LapMat}, \gls{graph}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
   &\neighbourhood{\nodeidx}  & \quad & \parbox{.80\textwidth}{El \gls{neighborhood} de un nodo $\nodeidx$ en un \gls{graph} $\graph$.
	   \\ Véa también: \gls{neighborhood}, \gls{graph}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
   &\nodedegree{\nodeidx} & \quad & \parbox{.80\textwidth}{El grado ponderado $\nodedegree{\nodeidx}\defeq \sum_{\nodeidx' \in \neighbourhood{\nodeidx}} \edgeweight_{\nodeidx,\nodeidx'}$ 
	   de un nodo $\nodeidx$ en un \gls{graph} $\graph$.
	   \\ Véa también: \gls{graph}.} \nonumber 
\end{align} 

\begin{align} 
   &\maxnodedegree^{(\graph)} & \quad & \parbox{.85\textwidth}{El \gls{maximum} \gls{nodedegree} ponderado de un \gls{graph} $\graph$.
	   \\ Véa también: \gls{maximum}, \gls{nodedegree}, \gls{graph}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
   &\localdataset{\nodeidx} & \quad & \parbox{.85\textwidth}{El \gls{localdataset} $\localdataset{\nodeidx}$ llevado por 
	   el nodo $\nodeidx\in \nodes$ de un \gls{empgraph}.
	   \\ Véa también: \gls{localdataset}, \gls{empgraph}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
   &\localsamplesize{\nodeidx} & \quad & \parbox{.85\textwidth}{El número de \glspl{datapoint} (es decir, \gls{samplesize}) contenido en el 
	   \gls{localdataset} $\localdataset{\nodeidx}$ en el nodo $\nodeidx\in \nodes$.
	   \\ Véa también: \gls{datapoint}, \gls{samplesize}, \gls{localdataset}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
   &\featurevec^{(\nodeidx,\sampleidx)} & \quad & \parbox{.85\textwidth}{Las \glspl{feature} del \gls{datapoint} $\sampleidx$-ésimo en 
	   el \gls{localdataset} $\localdataset{\nodeidx}$.
	   \\ Véa también: \gls{feature}, \gls{datapoint}, \gls{localdataset}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
   &\truelabel^{(\nodeidx,\sampleidx)} & \quad & \parbox{.85\textwidth}{La \gls{label} del \gls{datapoint} $\sampleidx$-ésimo en 
	   el \gls{localdataset} $\localdataset{\nodeidx}$.
	   \\ Véa también: \gls{label}, \gls{datapoint}, \gls{localdataset}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
   &\localparams{\nodeidx} & \quad & \parbox{.85\textwidth}{Los \gls{modelparams} locales del \gls{device} $\nodeidx$ dentro de un \gls{empgraph}.
	   \\ Véa también: \gls{modelparams}, \gls{device}, \gls{empgraph}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
   &\locallossfunc{\nodeidx}{\weights} & \quad & \parbox{.85\textwidth}{La \gls{lossfunc} local utilizada por el \gls{device} $\nodeidx$ 
	   para medir la utilidad de alguna elección $\weights$ para los \gls{modelparams} locales.
	   \\ Véa también: \gls{lossfunc}, \gls{device}, \gls{modelparams}.} \nonumber 
\end{align} 

\begin{align} 
   &\gtvloss{\featurevec}{\hypothesis\big(\featurevec\big)}{\hypothesis'\big(\featurevec\big)}& \quad & \parbox{.70\textwidth}{La \gls{loss} 
	   incurrida por un \gls{hypothesis} $\hypothesis'$ en un \gls{datapoint} con \glspl{feature} $\featurevec$ y \gls{label} 
	   $\hypothesis\big( \featurevec\big)$ que se obtiene de otro \gls{hypothesis}.
	   \\ Véa también: \gls{loss}, \gls{hypothesis}, \gls{datapoint}, \gls{feature}, \gls{label}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
   &{\rm stack} \big\{ \weights^{(\nodeidx)} \big\}_{\nodeidx=1}^{\nrnodes} & \quad & \parbox{.70\textwidth}{El vector 
	   $\bigg( \big(\weights^{(1)}  \big)^{T}, \ldots, \big(\weights^{(\nrnodes)}  \big)^{T} \bigg)^{T} \in \mathbb{R}^{\dimlocalmodel\nrnodes}$ que 
	   se obtiene al apilar verticalmente los \gls{modelparams} locales $\weights^{(\nodeidx)} \in \mathbb{R}^{\dimlocalmodel}$.
	   \\ Véa también: \gls{modelparams}.} \nonumber 
\end{align}
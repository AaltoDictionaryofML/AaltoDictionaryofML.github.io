\relax 
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/D>>}
\ifx\qr@savematrix\@undefined\def\qr@savematrix{\begingroup\let\do\@makeother\dospecials\catcode`\{=1\catcode`\}=2\relax \qr@savematrix@int}\def\qr@savematrix@int#1#2#3#4{\endgroup}\fi
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\providecommand\@glsxtr@savepreloctag[2]{}
\providecommand\@glsorder[1]{}
\providecommand\@istfilename[1]{}
\@istfilename{ADictML_Greek.ist}
\@glsorder{word}
\babel@aux{english}{}
\qr@savematrix{https://aaltodictionaryofml.github.io/ADictML.pdf}{4}{0}{111111100011101100010111001111111100000100010100101001101101000001101110100101001101000010001011101101110100001110101010000001011101101110100111000010010000001011101100000101000011001001110101000001111111101010101010101010101111111000000000000101001011111100000000100101101000011110010100110100000001001010001101010101000111100011111011101101100001011110111110101001111010110001011000100101011001110101110101000100010100111011011111010000110010110000001100011011111101110100010001101011111101010000100000100001101110111101011010101001100111110001011110001010010001110001000000000000101110101000111010111001010010100010111000111011000000111100101111111100100000101010111100000111110100010000001000001001101100110111100111100111110101101110101101001100011110001011001001000100010100111001011001100100100111011111011100111110001000000001000110000011101100011001111111100001111000001000101011110100000101101000110010100100010010101110100010000110001110111111001101110101101011100011101001010100101110100110011001001000100011101100000100010001111011111101000000111111101110111010000100110011010}
\citation{RudinBook}
\citation{RudinBook}
\citation{RudinBookPrinciplesMatheAnalysis}
\citation{RudinBookPrinciplesMatheAnalysis}
\citation{GolubVanLoanBook}
\citation{GolubVanLoanBook}
\citation{Golub1980}
\citation{Golub1980}
\citation{BertsekasProb}
\citation{BertsekasProb}
\citation{MLBasics}
\citation{MLBasics}
\citation{LC}
\citation{LC}
\citation{LC,GrayProbBook}
\@writefile{toc}{\contentsline {section}{Glossary}{14}{section*.7}\protected@file@percent }
\citation{LC}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces If \gls {feature}s and \gls {label} of a \gls {datapoint} are drawn from a \gls {mvndist}, we can achieve minimum \gls {risk} (under \gls {sqerrloss}) by using the \gls {bayesestimator} $\mu _{y|{\bf  x}}$ to predict the \gls {label} $y$ of a \gls {datapoint} with \gls {feature}s ${\bf  x}$. The corresponding minimum \gls {risk} is given by the posterior \gls {variance} $\sigma ^{2}_{y|{\bf  x}}$. We can use this quantity as a baseline for the average \gls {loss} of a trained \gls {model} $\hat  {h}$. }}{16}{figure.1}\protected@file@percent }
\newlabel{fig_post_baseline}{{1}{16}{If \gls {feature}s and \gls {label} of a \gls {datapoint} are drawn from a \gls {mvndist}, we can achieve minimum \gls {risk} (under \gls {sqerrloss}) by using the \gls {bayesestimator} $\mu _{y|{\protect \bf  x}}$ to predict the \gls {label} $y$ of a \gls {datapoint} with \gls {feature}s ${\protect \bf  x}$. The corresponding minimum \gls {risk} is given by the posterior \gls {variance} $\sigma ^{2}_{y|{\protect \bf  x}}$. We can use this quantity as a baseline for the average \gls {loss} of a trained \gls {model} $\protect \hat  {h}$. }{figure.1}{}}
\citation{LC}
\newlabel{labelspace}{{}{18}{\glossarytitle }{figure.1}{}}
\newlabel{equ_def_threshold_bin_classifier}{{1}{18}{\glossarytitle }{equation.0.1}{}}
\citation{SemiSupervisedBook}
\citation{BoydConvexBook}
\citation{JMLR:v22:18-694,Pelckmans2005}
\citation{GolubVanLoanBook}
\citation{GDPR2016}
\citation{EURegulation2018}
\citation{coverthomas}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Data augmentation exploits intrinsic symmetries of \gls {datapoint}s in some \gls {featurespace} $\mathcal  {X}$. We can represent a symmetry by an operator $\mathcal  {T}^{(\eta )}: \mathcal  {X}\rightarrow \mathcal  {X}$, parametrized by some number $\eta \in \mathbb  {R}$. For example, $\mathcal  {T}^{(\eta )}$ might represent the effect of rotating a cat image by $\eta $ degrees. A \gls {datapoint} with \gls {featurevec} ${\bf  x}^{(2)} = \mathcal  {T}^{(\eta )} \big  ({\bf  x}^{(1)} \big  )$ must have the same \gls {label} $y^{(2)}=y^{(1)}$ as a \gls {datapoint} with \gls {featurevec} ${\bf  x}^{(1)}$.}}{23}{figure.2}\protected@file@percent }
\newlabel{fig_symmetry_dataaug}{{2}{23}{Data augmentation exploits intrinsic symmetries of \gls {datapoint}s in some \gls {featurespace} $\protect \mathcal  {X}$. We can represent a symmetry by an operator $\protect \mathcal  {T}^{(\eta )}: \protect \mathcal  {X}\rightarrow \protect \mathcal  {X}$, parametrized by some number $\eta \in \protect \mathbb  {R}$. For example, $\protect \mathcal  {T}^{(\eta )}$ might represent the effect of rotating a cat image by $\eta $ degrees. A \gls {datapoint} with \gls {featurevec} ${\protect \bf  x}^{(2)} = \protect \mathcal  {T}^{(\eta )} \protect \big  ({\protect \bf  x}^{(1)} \protect \big  )$ must have the same \gls {label} $y^{(2)}=y^{(1)}$ as a \gls {datapoint} with \gls {featurevec} ${\protect \bf  x}^{(1)}$}{figure.2}{}}
\citation{Liu2021,PoisonGAN}
\citation{silberschatz2019database,abiteboul1995foundations,hoberman2009data,ramakrishnan2002database}
\citation{silberschatz2019database}
\citation{DatasheetData2021}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Temperature observations at FMI station Kustavi Isokari.}}{25}{table.1}\protected@file@percent }
\newlabel{tab:temperature}{{1}{25}{Temperature observations at FMI station Kustavi Isokari}{table.1}{}}
\citation{Goodfellow-et-al-2016}
\citation{MLBasics}
\citation{RudinBookPrinciplesMatheAnalysis}
\citation{MLBasics}
\citation{kay,hastie01statisticallearning}
\citation{RudinBookPrinciplesMatheAnalysis}
\citation{HalmosMeasure,BillingsleyProbMeasure,RudinBookPrinciplesMatheAnalysis}
\citation{BishopBook,hastie01statisticallearning,GraphModExpFamVarInfWainJor}
\citation{PredictionLearningGames}
\citation{PredictionLearningGames,HazanOCO}
\citation{Colin:2022aa}
\citation{Zhang:2024aa,Colin:2022aa}
\citation{JunXML2020,Chen2018}
\citation{Zhang:2024aa}
\citation{rudin2019stop}
\citation{Molnar2019}
\citation{GradCamPaper}
\citation{Gujarati2021,Dodge2003,Everitt2022}
\citation{LearningKernelsBook}
\citation{Ribeiro2016}
\citation{pmlr-v54-mcmahan17a}
\citation{FedProx2020}
\citation{FlowSpecClustering2021}
\citation{papoulis,BertsekasProb,GrayProbBook}
\citation{Rasmussen2006Gaussian}
\citation{ross2013first}
\citation{GDPR2016}
\citation{ShalevMLBook}
\citation{OnePixelAttack}
\citation{RudinBookPrinciplesMatheAnalysis}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Two \gls {datapoint}s ${\bf  z}^{(1)},{\bf  z}^{(2)}$ that are used as a \gls {trainset} to learn a \gls {hypothesis} $\hat  {h}$ via \gls {erm}. We can evaluate $\hat  {h}$ \emph  {outside} $\mathcal  {D}^{(\rm  train)}$ either by an \gls {iidasspt} with underlying \gls {probdist} $p({\bf  z})$ or by perturbing the \gls {datapoint}s.}}{39}{figure.3}\protected@file@percent }
\newlabel{fig:polynomial_fit}{{3}{39}{Two \gls {datapoint}s ${\protect \bf  z}^{(1)},{\protect \bf  z}^{(2)}$ that are used as a \gls {trainset} to learn a \gls {hypothesis} $\protect \hat  {h}$ via \gls {erm}. We can evaluate $\protect \hat  {h}$ \protect \emph  {outside} $\protect \mathcal  {D}^{(\protect \rm  train)}$ either by an \gls {iidasspt} with underlying \gls {probdist} $p({\protect \bf  z})$ or by perturbing the \gls {datapoint}s}{figure.3}{}}
\newlabel{equ_def_GD_step}{{2}{40}{\glossarytitle }{equation.0.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces A single gradient step \eqref  {equ_def_GD_step} towards the minimizer $\overline  {{\bf  w}}$ of $f({\bf  w})$.}}{40}{figure.4}\protected@file@percent }
\newlabel{fig_basic_GD_step}{{4}{40}{A single gradient step \protect \eqref  {equ_def_GD_step} towards the minimizer $\protect \overline  {{\protect \bf  w}}$ of $f({\protect \bf  w})$}{figure.4}{}}
\newlabel{equ_def_gd_basic}{{3}{40}{\glossarytitle }{equation.0.3}{}}
\citation{ProximalMethods}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The basic gradient step \eqref  {equ_def_gd_basic} maps a given vector ${\bf  w}$ to the updated vector ${\bf  w}'$. It defines an operator $\mathcal  {T}^{(f,\eta )}(\cdot ): \mathbb  {R}^{d} \rightarrow \mathbb  {R}^{d}: {\bf  w}\DOTSB \mapstochar \rightarrow \setbox \z@ \hbox {\mathsurround \z@ $\textstyle {\bf  w}$}\mathaccent "0362{{\bf  w}}$.}}{41}{figure.5}\protected@file@percent }
\newlabel{fig_basic_GD_step_single}{{5}{41}{The basic gradient step \protect \eqref  {equ_def_gd_basic} maps a given vector ${\protect \bf  w}$ to the updated vector ${\protect \bf  w}'$. It defines an operator $\protect \mathcal  {T}^{(f,\eta )}(\cdot ): \protect \mathbb  {R}^{d} \rightarrow \protect \mathbb  {R}^{d}: {\protect \bf  w}\DOTSB \mapstochar \rightarrow \setbox \z@ \hbox {\mathsurround \z@ $\textstyle {\protect \bf  w}$}\mathaccent "0362{{\protect \bf  w}}$}{figure.5}{}}
\newlabel{equ_approx_gd_step}{{4}{41}{\glossarytitle }{equation.0.4}{}}
\citation{RockNetworks}
\citation{Luxburg2007,FlowSpecClustering2021}
\citation{ClusteredFLTVMinTSP}
\citation{Wain2019,BuhlGeerBook}
\newlabel{equ_hinge_loss_gls}{{5}{43}{\glossarytitle }{equation.0.5}{}}
\citation{LampertNowKernel}
\citation{HFLChapter2020}
\citation{JunXML2020}
\citation{LampertNowKernel,LearningKernelsBook}
\citation{LampertNowKernel,LearningKernelsBook}
\citation{coverthomas}
\citation{Gujarati2021,Dodge2003,Everitt2022}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  Five \gls {datapoint}s characterized by \gls {featurevec}s ${\bf  x}^{(r)}$ and \gls {label}s $y^{(r)} \in \{ \circ , \square \}$, for $r=1,\ldots  ,5$. With these \gls {featurevec}s, there is no way to separate the two classes by a straight line (representing the \gls {decisionboundary} of a \gls {linclass}). In contrast, the transformed \gls {featurevec}s ${\bf  z}^{(r)} = K\big  ({\bf  x}^{(r)},\cdot \big  )$ allow to separate the \gls {datapoint}s using a \gls {linclass}. }}{47}{figure.6}\protected@file@percent }
\newlabel{fig_linsep_kernel}{{6}{47}{ Five \gls {datapoint}s characterized by \gls {featurevec}s ${\protect \bf  x}^{(r)}$ and \gls {label}s $y^{(r)} \in \{ \circ , \square \}$, for $r=1,\protect \ldots  ,5$. With these \gls {featurevec}s, there is no way to separate the two classes by a straight line (representing the \gls {decisionboundary} of a \gls {linclass}). In contrast, the transformed \gls {featurevec}s ${\protect \bf  z}^{(r)} = K\protect \big  ({\protect \bf  x}^{(r)},\cdot \protect \big  )$ allow to separate the \gls {datapoint}s using a \gls {linclass}. }{figure.6}{}}
\citation{Luxburg2007,Ng2001}
\citation{vaswani2017attention}
\citation{papoulis}
\citation{MLBasics}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  Left: Some undirected graph $\mathcal  {G}$ with three nodes $i=1,2,3$. Right: Laplacian matrix ${\bf  L}^{(\mathcal  {G})} \in \mathbb  {R}^{3 \times 3}$ of $\mathcal  {G}$.}}{49}{figure.7}\protected@file@percent }
\newlabel{fig_lap_mtx}{{7}{49}{ Left: Some undirected graph $\protect \mathcal  {G}$ with three nodes $i=1,2,3$. Right: Laplacian matrix ${\protect \bf  L}^{(\protect \mathcal  {G})} \in \protect \mathbb  {R}^{3 \times 3}$ of $\protect \mathcal  {G}$}{figure.7}{}}
\citation{Caruana:1997wk,JungGaphLassoSPL,CSGraphSelJournal}
\citation{MLBasics}
\citation{Ribeiro2016,rudin2019stop}
\newlabel{equ_def_lin_model_hypspace}{{7}{51}{\glossarytitle }{equation.0.7}{}}
\newlabel{equ_log_loss_gls}{{8}{52}{\glossarytitle }{equation.0.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Some \gls {lossfunc} $L\left (({\bf  x},y),h \right )$ for a fixed \gls {datapoint}, with \gls {feature} vector ${\bf  x}$ and \gls {label} $y$, and varying \gls {hypothesis} $h$. ML methods try to find (learn) a \gls {hypothesis} that incurs minimal \gls {loss}.}}{53}{figure.8}\protected@file@percent }
\newlabel{fig_loss_function_gls}{{8}{53}{Some \gls {lossfunc} $L\left (({\protect \bf  x},y),h \right )$ for a fixed \gls {datapoint}, with \gls {feature} vector ${\protect \bf  x}$ and \gls {label} $y$, and varying \gls {hypothesis} $h$. ML methods try to find (learn) a \gls {hypothesis} that incurs minimal \gls {loss}}{figure.8}{}}
\citation{LC,kay}
\citation{Abayomi2008DiagnosticsFM}
\citation{ShalevMLBook,MLBasics}
\citation{MLBasics}
\citation{BertsekasProb,GrayProbBook,Lapidoth09}
\citation{coverthomas}
\citation{JungNetExp2020}
\citation{nesterov04}
\citation{HornMatAnalysis}
\citation{HazanOCO,GDOptimalRakhlin2012}
\newlabel{equ_def_ogd}{{9}{58}{\glossarytitle }{equation.0.9}{}}
\citation{ShalevMLBook}
\citation{Bubeck2012}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces An instance of online GD that updates the \gls {modelparams} $w^{(t)}$ using the \gls {datapoint} ${\bf  z}^{(t)} = x^{(t)}$ arriving at time $t$. This instance uses the \gls {sqerrloss} $L\left ({\bf  z}^{(t)},w \right ) = (x^{(t)} - w)^{2}$. }}{59}{figure.9}\protected@file@percent }
\citation{Brockwell91}
\citation{doi:10.1137/0222052,10.1214/20-AOS1961}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces ML methods learn \gls {modelparams} ${\bf  w}$ by using some estimate $f({\bf  w})$ for the ultimate performance criterion $\bar  {f}({\bf  w})$. Using a \gls {probmodel}, one can use $f({\bf  w})$ to construct confidence intervals $\big  [ l^{({\bf  w})}, u^{({\bf  w})} \big  ]$ which contain $\bar  {f}({\bf  w})$ with high probability. The best plausible performance measure for a specific choice ${\bf  w}$ of \gls {modelparams} is $\tilde  {f}({\bf  w}) :=l^{({\bf  w})}$.}}{60}{figure.10}\protected@file@percent }
\citation{LearningKernelsBook}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces The parameter space $\mathcal  {W}$ of an ML \gls {model} $\mathcal  {H}$ consists of all feasible choices for the \gls {modelparams}. Each choice ${\bf  w}$ for the \gls {modelparams} selects a \gls {hypothesis} map $h^{({\bf  w})} \in  \mathcal  {H}$. }}{62}{figure.11}\protected@file@percent }
\newlabel{fig_param_space}{{11}{62}{The parameter space $\protect \mathcal  {W}$ of an ML \gls {model} $\protect \mathcal  {H}$ consists of all feasible choices for the \gls {modelparams}. Each choice ${\protect \bf  w}$ for the \gls {modelparams} selects a \gls {hypothesis} map $h^{({\protect \bf  w})} \in  \protect \mathcal  {H}$. }{figure.11}{}}
\citation{MLBasics}
\citation{PrivacyFunnel}
\citation{InfThDiffPriv}
\citation{LC}
\citation{KallenbergBook,BertsekasProb,BillingsleyProbMeasure,HalmosMeasure}
\citation{BertsekasProb}
\citation{BertsekasProb}
\citation{GrayProbBook,BillingsleyProbMeasure}
\citation{BoydConvexBook}
\citation{Condat2013}
\citation{ProximalMethods,Bauschke:2017}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces \Gls {projgd} augments a basic \gls {gradstep} with a projection back onto the constraint set $\mathcal  {W}$.}}{66}{figure.12}\protected@file@percent }
\newlabel{fig_projected_GD}{{12}{66}{\Gls {projgd} augments a basic \gls {gradstep} with a projection back onto the constraint set $\protect \mathcal  {W}$}{figure.12}{}}
\newlabel{equ_def_proj_generic}{{10}{66}{\glossarytitle }{equation.0.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces A generalized gradient step updates a vector ${\bf  w}$ by minimizing a penalized version of the function $f(\cdot )$. The penalty term is the squared Euclidean distance between the optimization variable ${\bf  w}'$ and the given vector ${\bf  w}$. }}{67}{figure.13}\protected@file@percent }
\newlabel{fig_proxoperator_opt}{{13}{67}{A generalized gradient step updates a vector ${\protect \bf  w}$ by minimizing a penalized version of the function $f(\cdot )$. The penalty term is the squared Euclidean distance between the optimization variable ${\protect \bf  w}'$ and the given vector ${\protect \bf  w}$. }{figure.13}{}}
\citation{RenyiInfo95}
\citation{BillingsleyProbMeasure}
\citation{GrayProbBook,BillingsleyProbMeasure}
\citation{BillingsleyProbMeasure,RudinBookPrinciplesMatheAnalysis,HalmosMeasure}
\citation{MLBasics}
\citation{PredictionLearningGames}
\citation{MLBasics}
\citation{MLBasics}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Three approaches to regularization: \gls {dataaug}, \gls {loss} penalization, and \gls {model} pruning (via constraints on \gls {modelparams}).  }}{71}{figure.14}\protected@file@percent }
\newlabel{fig_equiv_dataaug_penal}{{14}{71}{Three approaches to regularization: \gls {dataaug}, \gls {loss} penalization, and \gls {model} pruning (via constraints on \gls {modelparams}).  }{figure.14}{}}
\citation{SemiSupervisedBook}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces A scatterplot of some \gls {datapoint}s representing daily weather conditions in Finland. Each \gls {datapoint} is characterized by its minimum daytime temperature $x$ as the \gls {feature} and its maximum daytime temperature $y$ as the \gls {label}. The temperatures have been measured at the \gls {fmi} weather station \emph  {Helsinki Kaisaniemi} during 1.9.2024 - 28.10.2024.}}{74}{figure.15}\protected@file@percent }
\newlabel{fig_scatterplot_temp_FMI}{{15}{74}{A scatterplot of some \gls {datapoint}s representing daily weather conditions in Finland. Each \gls {datapoint} is characterized by its minimum daytime temperature $x$ as the \gls {feature} and its maximum daytime temperature $y$ as the \gls {label}. The temperatures have been measured at the \gls {fmi} weather station \protect \emph  {Helsinki Kaisaniemi} during 1.9.2024 - 28.10.2024}{figure.15}{}}
\citation{GolubVanLoanBook}
\citation{nesterov04,CvxBubeck2015}
\citation{nesterov04,CvxAlgBertsekas,CvxBubeck2015}
\citation{nesterov04,CvxAlgBertsekas,CvxBubeck2015}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Consider an \gls {objfunc} $f({\bf  w})$ that is $\beta $-smooth. Taking a \gls {gradstep}, with \gls {stepsize} $\eta = 1/\beta $ decreases the objective by at least $\frac  {1}{2\beta }\left \Vert {\nabla f({\bf  w}^{(k)})} \right \Vert _{2}^{2}$ \cite  {nesterov04,CvxAlgBertsekas,CvxBubeck2015}. Note that the \gls {stepsize} $\eta = 1/\beta $ becomes larger for smaller $\beta $. Thus, for smoother \gls {objfunc}s (those who are $\beta $-smooth with smaller $\beta $), we can take larger steps. }}{76}{figure.16}\protected@file@percent }
\newlabel{fig_gd_smooth}{{16}{76}{Consider an \gls {objfunc} $f({\protect \bf  w})$ that is $\beta $-smooth. Taking a \gls {gradstep}, with \gls {stepsize} $\eta = 1/\beta $ decreases the objective by at least $\protect \frac  {1}{2\beta }\left \Vert {\nabla f({\protect \bf  w}^{(k)})} \right \Vert _{2}^{2}$ \protect \cite  {nesterov04,CvxAlgBertsekas,CvxBubeck2015}. Note that the \gls {stepsize} $\eta = 1/\beta $ becomes larger for smaller $\beta $. Thus, for smoother \gls {objfunc}s (those who are $\beta $-smooth with smaller $\beta $), we can take larger steps. }{figure.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces  {\bf  Top.} Left: An undirected \gls {graph} $\mathcal  {G}$ with four nodes $i=1,2,3,4$, each representing a \gls {datapoint}. Right: Laplacian matrix ${\bf  L}^{(\mathcal  {G})} \in \mathbb  {R}^{4 \times 4}$ and its \gls {evd}. {\bf  Bottom.} Left: \Gls {scatterplot} of \gls {datapoint}s using the \gls {featurevec}s ${\bf  x}^{(i)} = \big  ( v^{(1)}_{i},v^{(2)}_{i} \big  )^{T}$. Right: Two \gls {eigenvector}s ${\bf  v}^{(1)},{\bf  v}^{(2)} \in \mathbb  {R}^{d}$ of the \gls {LapMat} ${\bf  L}^{(\mathcal  {G})}$ corresponding to the \gls {eigenvalue} $\lambda =0$. }}{78}{figure.17}\protected@file@percent }
\newlabel{fig_lap_mtx_specclustering}{{17}{78}{ {\protect \bf  Top.} Left: An undirected \gls {graph} $\protect \mathcal  {G}$ with four nodes $i=1,2,3,4$, each representing a \gls {datapoint}. Right: Laplacian matrix ${\protect \bf  L}^{(\protect \mathcal  {G})} \in \protect \mathbb  {R}^{4 \times 4}$ and its \gls {evd}. {\protect \bf  Bottom.} Left: \Gls {scatterplot} of \gls {datapoint}s using the \gls {featurevec}s ${\protect \bf  x}^{(i)} = \protect \big  ( v^{(1)}_{i},v^{(2)}_{i} \protect \big  )^{T}$. Right: Two \gls {eigenvector}s ${\protect \bf  v}^{(1)},{\protect \bf  v}^{(2)} \in \protect \mathbb  {R}^{d}$ of the \gls {LapMat} ${\protect \bf  L}^{(\protect \mathcal  {G})}$ corresponding to the \gls {eigenvalue} $\lambda =0$. }{figure.17}{}}
\citation{cohen1995time}
\citation{Li:2022aa}
\citation{TimeFrequencyAnalysisBoashash,MallatBook}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Left: A time signal consisting of two modulated Gaussian pulses. Right: Intensity plot of the spectrogram. }}{79}{figure.18}\protected@file@percent }
\newlabel{fig:spectrogram}{{18}{79}{Left: A time signal consisting of two modulated Gaussian pulses. Right: Intensity plot of the spectrogram. }{figure.18}{}}
\citation{AbbeSBM2018}
\citation{Bottou99}
\citation{nesterov04}
\citation{CvxAlgBertsekas}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Stochastic \gls {gd} for \gls {erm} approximates the \gls {gradient} $\DOTSB \sum@ \slimits@ _{r=1}^{m} \nabla _{{\bf  w}} L\left ({\bf  z}^{(r)},{\bf  w} \right )$ by replacing the sum over all \gls {datapoint}s in the \gls {trainset} (indexed by $r=1,\ldots  ,m$) with a sum over a randomly chosen subset $\mathcal  {B}\subseteq \{1,\ldots  ,m\}$.}}{81}{figure.19}\protected@file@percent }
\newlabel{fig_sgd_approx}{{19}{81}{Stochastic \gls {gd} for \gls {erm} approximates the \gls {gradient} $\DOTSB \sum@ \slimits@ _{r=1}^{m} \nabla _{{\protect \bf  w}} L\left ({\protect \bf  z}^{(r)},{\protect \bf  w} \right )$ by replacing the sum over all \gls {datapoint}s in the \gls {trainset} (indexed by $r=1,\protect \ldots  ,m$) with a sum over a randomly chosen subset $\protect \mathcal  {B}\subseteq \{1,\protect \ldots  ,m\}$}{figure.19}{}}
\citation{BertCvxAnalOpt,BertsekasNonLinProgr}
\citation{LampertNowKernel,Cristianini_Shawe-Taylor_2000,BishopBook}
\citation{MLBasics}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces The \gls {svm} learns a hypothesis (or classifier) $h^{({\bf  w})}$ with minimum average soft-margin \gls {hingeloss}. Minimizing this \gls {loss} is equivalent to maximizing the margin $\xi $ between the \gls {decisionboundary} of $h^{({\bf  w})}$ and each class of the \gls {trainset}.}}{83}{figure.20}\protected@file@percent }
\newlabel{fig_svm_gls}{{20}{83}{The \gls {svm} learns a hypothesis (or classifier) $h^{({\protect \bf  w})}$ with minimum average soft-margin \gls {hingeloss}. Minimizing this \gls {loss} is equivalent to maximizing the margin $\xi $ between the \gls {decisionboundary} of $h^{({\protect \bf  w})}$ and each class of the \gls {trainset}}{figure.20}{}}
\citation{HLEGTrustworhtyAI}
\citation{gallese2023ai,JunXML2020}
\citation{Shahriari2017,DatasheetData2021,10.1145/3287560.3287596}
\citation{pfau2024engineeringtrustworthyaideveloper}
\citation{ALTAIEU}
\citation{MLBasics}
\citation{ShalevMLBook}
\citation{VFLChapter}
\citation{MLBasics}
\HyPL@Entry{88<</P()>>}
\bibstyle{IEEEtran}
\bibdata{assets/Literature.bib}
\bibcite{RudinBook}{1}
\bibcite{RudinBookPrinciplesMatheAnalysis}{2}
\bibcite{GolubVanLoanBook}{3}
\bibcite{Golub1980}{4}
\bibcite{BertsekasProb}{5}
\bibcite{MLBasics}{6}
\gdef \@abspage@last{94}

%% ------------------------------------------------------------------
%% AUTO-GENERATED by FlattenGlossary.py
%% Source: /Users/junga1/AaltoDictionaryofML.github.io/ListSymbols_English.tex
%% Repo root: /Users/junga1/AaltoDictionaryofML.github.io
%% ------------------------------------------------------------------



\section*{Lists of Symbols}
\addcontentsline{toc}{section}{List of Symbols}




\vspace*{-2mm}
\subsection*{Sets and Functions} 

\begin{align} 
	&a \in \mathcal{A} & \quad & \parbox{.85\textwidth}{The object $a$ is an element of the set $\mathcal{A}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&a :=b & \quad & \mbox{We use $a$ as a shorthand for $b$. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&|\mathcal{A}| & \quad & \mbox{The cardinality (i.e., number of elements) of a finite set $\mathcal{A}$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathcal{A} \subseteq \mathcal{B}& \quad & \mbox{$\mathcal{A}$ is a subset of $\mathcal{B}$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathcal{A} \subset \mathcal{B}& \quad & \mbox{$\mathcal{A}$ is a strict subset of $\mathcal{B}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathcal{A} \times \mathcal{B} & \quad & \mbox{The Cartesian product of the sets $\mathcal{A}$ and $\mathcal{B}$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{N} & \quad & \mbox{The natural numbers $1, \,2, \,\dots$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}  &\quad &\mbox{The real numbers $x$ \cite{RudinBook}.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}_{+}  &\quad &\mbox{The nonnegative real numbers $x\geq0$.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}_{++}  &\quad &\mbox{The positive real numbers $x> 0$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\{0,1\}& \quad & \mbox{The set consisting of the two real numbers $0$ and $1$.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&[0,1] &\quad &\mbox{The closed interval of real numbers $x$ with $0 \leq x \leq 1$. }\nonumber 
\end{align} 

\newpage
\begin{align}
    	&\argmin_{{\bf w}\in \mathcal{C}} f({\bf w}) &\quad &\parbox{.70\textwidth}{The set of minimizers 
		for a real-valued function $f: \mathcal{C} \rightarrow \mathbb{R}$. 
    		\\ See also: function. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
    	&\mathbb{S}^{(n)} &\quad &\parbox{.70\textwidth}{The set of unit-norm vectors in $\mathbb{R}^{n+1}$.
    		\\ See also: norm, vector. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\exp\,(a) &\quad &\parbox{.70\textwidth}{The exponential function evaluated at the real number $a \in \mathbb{R}$.
		\\ See also: function. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\log a &\quad &\mbox{The logarithm of the positive number $a \in \mathbb{R}_{++}$.  } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&f(\cdot)\!:\!\mathcal{A}\!\rightarrow\!\mathcal{B} :  a \!\mapsto\!f(a) &\quad &\parbox{.70\textwidth}{
	 	A function (or map) from a set $\mathcal{A}$ to a set $\mathcal{B}$, which assigns to each input 
	 	$a \in \mathcal{A}$ a well-defined output $f(a) \in \mathcal{B}$.
	 	The set $\mathcal{A}$ is the domain of the function $f$ and the set $\mathcal{B}$ is the 
	 	co-domain of $f$. Machine learning (ML) aims to learn a function $h$ that maps features 
	 	${\bf x}$ of a data point to a prediction $h({\bf x})$ for its label $y$.
		\\ See also: function, map, output, domain, co-domain, machine learning (ML), feature, data point, 
		prediction, label.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\operatorname{epi}(f)  & \quad & \parbox{.70\textwidth}{The epigraph of a real-valued function 
	 	$f: \mathbb{R}^{d}\rightarrow \mathbb{R}$.
		\\ See also: epigraph, function. } \nonumber \\[2mm]  \hline \nonumber\\[-5mm]
	&\big( a_{r} \big)_{r\in \mathbb{N}},
		\big( a^{(r)} \big)_{r\in \mathbb{N}},\big\{ a^{(r)} \big\}_{r\in \mathbb{N}} 
		& \quad & \parbox{.70\textwidth}{A sequence of elements.
		\\ See also: sequence. } \nonumber 
\end{align} 

\begin{align}
	&\mathbb{I}_{\mathcal{A}}(x) & \quad & \parbox{.70\textwidth}{The indicator function of a set $\mathcal{A}$ 
		delivers $f(x)=1$ for any $x \in \mathcal{A}$ and $f(x)=0$ otherwise.
		\\ See also: function. }  \nonumber \\[2mm]  \hline \nonumber\\[-5mm]
	&\frac{\partial f(\weight_{1}, \,\ldots, \,\weight_{d})}{\partial \weight_{j}} & \quad & \parbox{.70\textwidth}{The partial derivative 
		(if it exists) of a real-valued function $f: \mathbb{R}^{d}\rightarrow \mathbb{R}$ with respect 
		to\ $\weight_{j}$\cite[Ch. 9]{RudinBookPrinciplesMatheAnalysis}.
		\\ See also: partial derivative, function. } \nonumber \\[2mm]  \hline \nonumber\\[-5mm]
	 &\nabla f({\bf w}) & \quad & \parbox{.70\textwidth}{The gradient of a differentiable real-valued function 
	 	$f: \mathbb{R}^{d}\rightarrow \mathbb{R}$ is the vector 
	 	$\nabla f({\bf w}) = \big( {\partial f}/{\partial \weight_{1}}, \,\ldots, \,{\partial f}/{\partial \weight_{d}}  \big)\,^{T} \in \mathbb{R}^{d}$ 
		\cite[Ch. 9]{RudinBookPrinciplesMatheAnalysis}.
		\\ See also: gradient, differentiable, function, vector.}   \nonumber
\end{align} 



\subsection*{Matrices and Vectors} 

\begin{align} 
	 &{\bf x}=\big(\feature_{1}, \,\ldots, \,\feature_{d})\,^{T} &\quad & \parbox{.75\textwidth}{A vector of length $d$, with its 
		$j$th entry being $\feature_{j}$.
		\\ See also: vector. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{R}^{d} & \quad &  \parbox{.75\textwidth}{The set of vectors ${\bf x}=\big(\feature_{1}, \,\ldots, \,\feature_{d}\big)\,^{T}$ 
		consisting of $d$ real-valued entries $\feature_{1}, \,\ldots, \,\feature_{d} \in \mathbb{R}$.
		\\ See also: vector. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{I}_{l\times d}  & \quad &  \parbox{.75\textwidth}{A generalized identity matrix 
		with $l$ rows and $d$ columns. The entries of $\mathbf{I}_{l\times d} \in \mathbb{R}^{l\times d}$ 
		are equal to $1$ along the main diagonal and otherwise equal to $0$. 
		\\ See also: matrix. }\nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	&\mathbf{I}_{d}, \mathbf{I} & \quad &  \parbox{.75\textwidth}{A square identity 
		matrix of size $d\times d$. If the size is clear from context, we drop the subscript.
		\\ See also: matrix. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mleft\lVert {\bf x}\mright\rVert_{2}  &\quad & \parbox{.75\textwidth}{The Euclidean (or $\ell_{2}$) norm of the vector 
		${\bf x}=\big(\feature_{1}, \,\ldots, \,\feature_{d}\big)\,^{T} \in \mathbb{R}^{d}$ defined as 
		$\| {\bf x}\|_{2} :=\sqrt{\sum_{j=1}^{d} \feature_{j}^{2}}$.
		\\ See also: norm, vector. } \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	&\mleft\lVert {\bf x}\mright\rVert_{}  & \quad &  \parbox{.75\textwidth}{Some norm of the vector ${\bf x}\in \mathbb{R}^{d}$ \cite{GolubVanLoanBook}. 
		Unless otherwise specified, we mean the Euclidean norm $\mleft\lVert {\bf x}\mright\rVert_{2}$.
		\\ See also: norm, vector, Euclidean norm. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&{\bf x}\,^{T} &\quad & \parbox{.75\textwidth}{The transpose of a matrix that has the vector 
		${\bf x}\in \mathbb{R}^{d}$ as its single column.
		\\ See also: matrix, vector. } \nonumber 
\end{align} 

\newpage
\begin{align} 
	&\mathbf{X}\,^{T} &\quad & \parbox{.75\textwidth}{The transpose of a matrix $\mathbf{X} \in \mathbb{R}^{m\times d}$. 
		A square real-valued matrix $\mathbf{X} \in \mathbb{R}^{m\times m}$ 
		is called symmetric if $\mathbf{X} = \mathbf{X}\,^{T}$. 
		\\ See also: matrix. }  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{X}^{-1} &\quad & \parbox{.75\textwidth}{The inverse matrix of a matrix $\mathbf{X} \in \mathbb{R}^{d\times d}$.
		\\ See also: inverse matrix, matrix. }  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{0}= \big(0, \,\ldots, \,0\big)\,^{T}  & \quad &  \parbox{.75\textwidth}{The vector in $\mathbb{R}^{d}$ with each entry equal to zero.
		\\ See also: vector. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{1}= \big(1, \,\ldots, \,1\big)\,^{T}  & \quad &  \parbox{.75\textwidth}{The vector in $\mathbb{R}^{d}$ with each entry equal to one.
		\\ See also: vector. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\big({\bf v}\,^{T},{\bf w}\,^{T} \big)\,^{T}  & \quad &  \parbox{.75\textwidth}{The vector of length $d+d'$ 
		obtained by concatenating the entries of vector ${\bf v}\in \mathbb{R}^{d}$ with the entries of ${\bf w}\in \mathbb{R}^{d'}$.
		\\ See also: vector. } \nonumber \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&{\rm span}\left(\mathbf{B} \right)  & \quad &  \parbox{.75\textwidth}{The span of a matrix $\mathbf{B} \in \mathbb{R}^{a \times b}$, 
		which is the subspace of all linear combinations of the columns of $\mathbf{B}$, such that
		${\rm span}\left(\mathbf{B} \right) = \big\{  \mathbf{B} {\bf a}: {\bf a}\in \mathbb{R}^{b} \big\} \subseteq \mathbb{R}^{a}$. 
		\\ See also: matrix, subspace. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&{\rm null}\left({\bf A}\right)& \quad &  \parbox{.75\textwidth}{The nullspace of a matrix $\mathbf{A} \in \mathbb{R}^{a \times b}$, 
		which is the subspace of vectors ${\bf a}\in \mathbb{R}^{b}$ such that ${\bf A}{\bf a}=\mathbf{0}$. 
		\\ See also: nullspace, matrix, subspace, vector. }\nonumber 
\end{align} 

\newpage
\begin{align} 
	&{\rm det}\left( {\bf C}\right) &\quad & \parbox{.85\textwidth}{The determinant of the matrix ${\bf C}$.
		\\ See also: determinant, matrix. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&{\rm tr} \left({\bf C}\right) &\quad & \parbox{.85\textwidth}{The trace of the matrix ${\bf C}$.
		\\ See also: trace. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
		&\mathbf{A} \otimes \mathbf{B} &\quad & \parbox{.85\textwidth}{The Kronecker product of $\mathbf{A}$ and $\mathbf{B}$ \cite{Golub1980}.
		\\ See also: Kronecker product. }  \nonumber
\end{align} 



\newpage
\subsection*{Probability Theory} 

\begin{align}
	&{\bf x}\sim \mathbb{P}^{({\bf z})}  &\quad & \parbox{.85\textwidth}{The random variable (RV) ${\bf x}$ is distributed according to 
		the probability distribution $\mathbb{P}^{({\bf z})}$ \cite{klenke2020probability}, \cite{BillingsleyProbMeasure}.
		\\ See also: random variable (RV), probability distribution.}  \nonumber \\[2mm] \hline \nonumber\\[-5mm]  
	&\expect_{p} \{ f({\bf z}) \}  &\quad & \parbox{.85\textwidth}{The expectation of an random variable (RV) $f({\bf z})$ that 
		is obtained by applying a deterministic function $f$ to an random variable (RV)
		${\bf z}$ whose probability distribution is $\mathbb{P}^{({\bf z})}$. 
		If the probability distribution is clear from context, we just write $\mathbb{E} \{ f({\bf z}) \}$. 
		\\ See also: expectation, random variable (RV), function, probability distribution.}  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&{\rm cov}\left(x,y\right) &\quad & \parbox{.85\textwidth}{The covariance between two real-valued random variables (RVs) defined 
		over a common probability space. 
		\\ See also: covariance, random variable (RV), probability distribution.}  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{P}^{({\bf x},y)} &\quad & \parbox{.85\textwidth}{A (joint) probability distribution of an random variable (RV) 
		whose realizations are data points with features ${\bf x}$ and label $y$.
		\\ See also: probability distribution, random variable (RV), realization, data point, feature, 
		label.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{P}^{(y|{\bf x})} &\quad & \parbox{.85\textwidth}{A conditional probability distribution 
		of an random variable (RV) $y$ given (or conditioned on) the value of another random variable (RV) ${\bf x}$ \cite[Sec.\ 3.5]{BertsekasProb}. 
		\\ See also: conditional probability distribution, random variable (RV). } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{P}\left(\mathcal{A}\right) &\quad & \parbox{.85\textwidth}{The probability of the measurable event $\mathcal{A}$. 
		\\ See also: probability, measurable, event.} \nonumber 
\end{align} 

\newpage
\begin{align} 
	&M_{x}\left(t\right) &\quad & \parbox{.85\textwidth}{The moment generating function (MGF) of an random variable (RV) $x$.
		\\ See also: probability distribution, probability density function (pdf).} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{P}^{(\mathcal{D})} &\quad & \parbox{.85\textwidth}{The empirical distribution of a dataset $\mathcal{D}$.
		\\ See also: empirical distribution, dataset, bootstrap.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbb{P}^{({\bf x};{\bf w})} &\quad & \parbox{.85\textwidth}{A parameterized 
		probability distribution of an random variable (RV) ${\bf x}$. The probability distribution depends on 
		a parameter vector ${\bf w}$. For example, $\mathbb{P}^{({\bf x};{\bf w})}$ 
		could be a multivariate normal distribution with the parameter vector ${\bf w}$ given 
		by the entries of the mean vector $\mathbb{E} \{ {\bf x}\}$ 
		and the covariance matrix $\mathbb{E} \bigg \{ \big( {\bf x}- \mathbb{E} \{ {\bf x}\}\big) \big( {\bf x}- \mathbb{E} \{ {\bf x}\}\big)\,^{T}  \bigg\}$.
		\\ See also: probability distribution, parameter, probabilistic model.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathcal{N}(\mu, \sigma^{2}) &\quad & \parbox{.85\textwidth}{The probability distribution of a 
		Gaussian random variable (Gaussian RV) $x\in \mathbb{R}$ with mean (or expectation) $\mu= \mathbb{E} \{ x\}$ 
		and variance $\sigma^{2} =   \mathbb{E} \big\{  (  x- \mu )^2 \big\}$.
		\\ See also: probability distribution, Gaussian random variable (Gaussian RV).} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathcal{N}({\bm \mu}, \mathbf{C}) &\quad & \parbox{.85\textwidth}{The multivariate normal distribution of a vector-valued 
		Gaussian random variable (Gaussian RV) ${\bf x}\in \mathbb{R}^{d}$ with mean (or expectation) ${\bm \mu}= \mathbb{E} \{ {\bf x}\}$ 
		and covariance matrix $\mathbf{C} =  \mathbb{E} \big\{ \big( {\bf x}- {\bm \mu}\big)\big( {\bf x}- {\bm \mu}\big)\,^{T} \big\}$.
		\\ See also: multivariate normal distribution, Gaussian random variable (Gaussian RV).} \nonumber \\[2mm] \hline \nonumber\\[-5mm]  
	&\Delta^{k} &\quad & \parbox{.85\textwidth}{The probability simplex, which consists of all vectors 
		${\bf p}= \big( p_{1}, \,\ldots, \,p_{k} \big)\,^{T} \in \mathbb{R}^{k}$ 
	        with nonnegative entries that sum to one, i.e., $p_{c} \geq 0$ for $c=1, \,\ldots, \,k$ and
		$\sum_{c=1}^{k} p_{c} = 1$.
		\\ See also: probability mass function (pmf).}  \nonumber
\end{align} 

\newpage
\begin{align} 
	&H\left(x\right) &\quad & \parbox{.85\textwidth}{The entropy of a discrete random variable (discrete RV) $x$. 
		\\ See also: entropy, discrete random variable (discrete RV).}  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\Omega&\quad & \parbox{.85\textwidth}{A sample space of all possible outcomes of a random experiment. 
		\\ See also: event.}  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\Sigma&\quad & \parbox{.85\textwidth}{A collection of measurable subsets of a sample space $\Omega$. 
		\\ See also: sample space, event.}  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathcal{P} &\quad & \parbox{.85\textwidth}{A probability space that consists of a sample space $\Omega$, a 
		$\sigma$-algebra $\Sigma$ of measurable subsets of $\Omega$, and a probability distribution $\mathbb{P}\left( \cdot\right)$.
		\\ See also: sample space, measurable, probability distribution.} \nonumber                                       
\end{align}



\newpage
\subsection*{Machine Learning}

\begin{align}


	&r&\quad & \parbox{.90\textwidth}{An index $r=1, \,2, \,\ldots$ that 
		enumerates data points.
		\\ See also: data point. }   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&m&\quad &\parbox{.90\textwidth}{The number of data points in (i.e., the size of) a dataset.
		\\ See also: data point, dataset.} \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	&\mathcal{D}&\quad & \parbox{.90\textwidth}{A dataset $\mathcal{D}= \{ {\bf z}^{(1)}, \,\ldots, \,{\bf z}^{(m)} \}$ 
		is a list of individual data points ${\bf z}^{(r)}$, for $r=1, \,\ldots, \,m$.
		\\ See also: dataset, data point.}   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&d&\quad &\parbox{.90\textwidth}{The number of features that characterize a data point.
		\\ See also: feature, data point.}\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\feature_{j} &\quad &\parbox{.90\textwidth}{The $j$th feature of a data point. The first feature 
		is denoted by $\feature_{1}$, the second feature $\feature_{2}$, and so on.
		\\ See also: data point, feature. } \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	&{\bf x}&\quad &\parbox{.90\textwidth}{The feature vector ${\bf x}=\big(\feature_{1}, \,\ldots, \,\feature_{d}\big)\,^{T}$ of 
		a data point. The vector's entries are the individual features of a data point.
		\\ See also: feature vector, data point, vector, feature. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathcal{X}&\quad & \parbox{.90\textwidth}{The feature space $\mathcal{X}$ is 
		the set of all possible values that the features ${\bf x}$ of a data point can take on.
		\\ See also: feature space, feature, data point.} \nonumber 
\end{align}        

\begin{align}
	&\mathbf{z}&\quad &\parbox{.85\textwidth}{Instead of the symbol ${\bf x}$, we 
		sometimes use $\mathbf{z}$ as another symbol to denote a vector whose entries 
		are the individual features of a data point. We need two 
		different symbols to distinguish between raw and learned features \cite[Ch. 9]{MLBasics}.
		\\ See also: vector, feature, data point. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&{\bf x}^{(r)} &\quad &\parbox{.85\textwidth}{The feature vector of the $r$th data point within a dataset.
		\\ See also: feature vector, data point, dataset. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\feature_{j}^{(r)} &\quad &\parbox{.85\textwidth}{The $j$th feature of the $r$th 
		data point within a dataset.
		\\ See also: feature, data point, dataset.} \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathcal{B}&\quad &\parbox{.85\textwidth}{A mini-batch (or subset) of randomly chosen data points.
		\\ See also: batch, data point. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&B&\quad &\parbox{.85\textwidth}{The size of (i.e., the number of data points in) a mini-batch.
		\\ See also: data point, batch. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&y&\quad &\parbox{.85\textwidth}{The label (or quantity of interest) of a data point.
		\\ See also: label, data point. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&y^{(r)} &\quad &\parbox{.85\textwidth}{The label of the $r$th data point.
		\\ See also: label, data point. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\big({\bf x}^{(r)},y^{(r)}\big) &\quad &\parbox{.85\textwidth}{The features and label of the $r$th data point.
		\\ See also: feature, label, data point. }\nonumber 
\end{align}                  

\begin{align}
	&\mathcal{Y}&\quad & \parbox{.90\textwidth}{The label space $\mathcal{Y}$ of 
		an machine learning (ML) method consists of all potential label values that a data point can 
		carry. The nominal label space might be larger than the set of different label 
		values arising in a given dataset (e.g., a training set). Machine learning (ML) problems 
		(or methods) using a numeric label space, such as $\mathcal{Y}=\mathbb{R}$ 
		or $\mathcal{Y}=\mathbb{R}^{3}$, are referred to as regression problems (or methods). Machine learning (ML) 
		problems (or methods) that use a discrete label space, such as $\mathcal{Y}=\{0,1\}$ or $\mathcal{Y}=\{\mbox{\emph{cat}},\mbox{\emph{dog}},\mbox{\emph{mouse}}\}$, 
		are referred to as classification problems (or methods).
		\\ See also: label space, machine learning (ML), label, data point,  dataset, training set, 
		regression, classification.}  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\eta&\quad & \parbox{.90\textwidth}{Learning rate (or step size) used by gradient-based methods.
		\\ See also: learning rate, step size, gradient-based method. }  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&h(\cdot)  &\quad &\parbox{.90\textwidth}{A hypothesis map that maps the features of a data point 
		to a prediction $\hat{y}=h({\bf x})$ for its label $y$.
		\\ See also: hypothesis, map, feature, data point, prediction, label. }  	 \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	 &\mathcal{Y}^{\mathcal{X}} &\quad & \parbox{.90\textwidth}{Given two sets $\mathcal{X}$ and $\mathcal{Y}$, we denote by $\mathcal{Y}^{\mathcal{X}}$ 
	 	the set of all possible hypothesis maps $h: \mathcal{X}\rightarrow \mathcal{Y}$.
		\\ See also: hypothesis, map. } 	 \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathcal{H}&\quad & \parbox{.90\textwidth}{A hypothesis space or model used by an machine learning (ML) method. 
		The hypothesis space consists of different hypothesis maps $h: \mathcal{X}\rightarrow \mathcal{Y}$, between which 
		the machine learning (ML) method must choose.
		\\ See also: hypothesis space, model, machine learning (ML), hypothesis, map. }   \nonumber 
\end{align}     

\begin{align}
	&d_{\rm eff} \left( \mathcal{H}\right)  &\quad & \parbox{.80\textwidth}{The effective dimension of a hypothesis space $\mathcal{H}$.
		\\ See also: effective dimension, hypothesis space. }   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&B^2 &\quad &\parbox{.80\textwidth}{
		The squared bias of a learned hypothesis $\hat{h}$, or its parameters. Note that $\hat{h}$ 
		becomes an random variable (RV) if it is learned from data points being random variables (RVs) themselves.
		\\ See also: bias, hypothesis, parameter, random variable (RV), data point. } \nonumber  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&V&\quad &\parbox{.80\textwidth}{The variance of a learned 
	  	hypothesis $\hat{h}$, or its parameters. Note that $\hat{h}$ 
	  	becomes an random variable (RV) if it is learned from data points being random variables (RVs) themselves.
		\\ See also: variance, hypothesis, parameter, random variable (RV), data point. } \nonumber \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&L\left(({\bf x},y),h\right)  &\quad & \parbox{.80\textwidth}{The loss incurred by predicting the 
		label $y$ of a data point using the prediction $\hat{y}=h({\bf x})$. The 
		prediction $\hat{y}$ is obtained by evaluating the hypothesis $h\in \mathcal{H}$ for 
		the feature vector ${\bf x}$ of the data point.
		\\ See also: loss, label, data point, prediction, hypothesis, 
		feature vector. }    \nonumber  \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	&E_{v}&\quad &\parbox{.80\textwidth}{The validation error of a hypothesis $h$, which is its 
		average loss incurred over a validation set.
		\\ See also: validation error, hypothesis, loss, validation set. }  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\widehat{L}\big(h| \mathcal{D}\big) &\quad &\parbox{.80\textwidth}{The empirical risk, or average loss, 
		incurred by the hypothesis $h$ on a dataset $\mathcal{D}$.
		\\ See also: empirical risk, loss, hypothesis, dataset. } \nonumber 
\end{align}     

\begin{align}                          
	&E_{t}&\quad &\parbox{.85\textwidth}{The training error of a hypothesis $h$, which is its 
		average loss incurred over a training set.
		\\ See also: training error, hypothesis, loss, training set. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&t&\quad &\parbox{.85\textwidth}{A discrete-time index $t=0, \,1, \,\ldots$ used to 
		enumerate sequential events (or time instants). 
		\\ See also: event. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&t&\quad &\parbox{.85\textwidth}{An index that enumerates
		learning tasks within a multitask learning problem.
		\\ See also: learning task, multitask learning. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\alpha&\quad &\parbox{.85\textwidth}{A regularization parameter that controls 
		the amount of regularization.
		\\ See also: regularization, parameter. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\lambda_{j}\big( \mathbf{Q} \big) &\quad &\parbox{.85\textwidth}{The $j$th 
		eigenvalue (sorted in either ascending or descending order) of a positive semi-definite (psd) matrix $\mathbf{Q}$. We also 
		use the shorthand $\lambda_{j}$ if the corresponding matrix is clear from context.
		\\ See also: eigenvalue, positive semi-definite (psd), matrix. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\sigma(\cdot) &\quad &\parbox{.85\textwidth}{The activation function used by an artificial neuron within an artificial neural network (ANN).
		\\ See also: activation function, artificial neural network (ANN). }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathcal{R}_{\hat{y}} &\quad &\parbox{.85\textwidth}{A decision region within a feature space.
		\\ See also: decision region, feature space. }\nonumber
\end{align}     

\begin{align} 
	&{\bf w}&\quad & \parbox{.85\textwidth}{A parameter vector ${\bf w}= \big(\weight_{1}, \,\ldots, \,\weight_{d}\big)\,^{T}$ 
		of a model, e.g., the weights of a linear model or an artificial neural network (ANN).
		\\ See also: parameter, vector, model, weights, linear model, artificial neural network (ANN). }     \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&h^{({\bf w})}(\cdot)  &\quad &\parbox{.85\textwidth}{A hypothesis map that involves tunable model parameters 
		$\weight_{1}, \,\ldots, \,\weight_{d}$ stacked into the vector ${\bf w}=\big(\weight_{1}, \,\ldots, \,\weight_{d} \big)\,^{T}$.
		\\ See also: hypothesis, map, model parameter, vector. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\phi(\cdot)  &\quad & \parbox{.85\textwidth}{A feature map 
		$\phi: \mathcal{X}\rightarrow \mathcal{X}' : {\bf x}\mapsto \phi\big( {\bf x}\big)$ that 
		transforms the feature vector ${\bf x}$ of a data point into a new feature vector ${\bf x}'= \phi\big( {\bf x}\big) \in \mathcal{X}'$.
		\\ See also: feature map. }   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&K\big(\cdot,\cdot\big) &\quad & \parbox{.85\textwidth}{Given some feature space $\mathcal{X}$, 
		a kernel is a map $K: \mathcal{X}\times \mathcal{X}\rightarrow \mathbb{C}$ that is positive semi-definite (psd).
		\\ See also: feature space, kernel, map, positive semi-definite (psd). }    \nonumber \\[2mm] \hline \nonumber\\[-5mm]  
	&\operatorname{VCdim}\left(\mathcal{H}\right) &\quad & \parbox{.85\textwidth}{The Vapnik–Chervonenkis dimension (VC dimension) of the hypothesis space $\mathcal{H}$. 
		\\ See also: Vapnik–Chervonenkis dimension (VC dimension), hypothesis space. }    \nonumber                                                                                                                                               
\end{align}              



\newpage
\subsection*{Federated Learning}

\begin{align}
 	&\mathcal{G}= \left( \mathcal{V},\mathcal{E}\right) & \quad & \parbox{.80\textwidth}{An undirected graph whose nodes $i\in \mathcal{V}$ represent 
		devices within a federated learning network (FL network). The undirected weighted edges $\mathcal{E}$ represent connectivity between 
		devices and statistical similarities between their datasets and learning tasks.
		\\ See also: undirected graph, device, federated learning network (FL network), dataset, learning task. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&i\in \mathcal{V}& \quad & \parbox{.80\textwidth}{A node that represents some 
		device within an federated learning network (FL network). The device can access a local dataset and train a local model.
		\\ See also: device, federated learning network (FL network), local dataset, local model. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathcal{G}^{(\mathcal{C})}& \quad & \parbox{.80\textwidth}{The induced subgraph of $\mathcal{G}$ using the nodes in $\mathcal{C}\subseteq \mathcal{V}$. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&{\bf L}^{(\mathcal{G})}   & \quad & \parbox{.80\textwidth}{The Laplacian matrix of a graph $\mathcal{G}$.
		\\ See also: Laplacian matrix, graph. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&{\bf L}^{(\mathcal{C})}   & \quad & \parbox{.80\textwidth}{The Laplacian matrix of the induced graph $\mathcal{G}^{(\mathcal{C})}$.
		\\ See also: Laplacian matrix, graph. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	 &\mathcal{N}^{(i)}  & \quad & \parbox{.80\textwidth}{The neighborhood of the node $i$ in a graph $\mathcal{G}$.
	 	\\ See also: neighborhood, graph. }   \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&d^{(i)} & \quad & \parbox{.80\textwidth}{The weighted node degree 
		$d^{(i)}\!:=\!\sum_{i' \in \mathcal{N}^{(i)}}\hspace*{-1mm} \edgeweight_{i,i'}$ of node $i$. 
		\\ See also: node degree. }  \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&d_{\rm max}^{(\mathcal{G})} & \quad & \parbox{.80\textwidth}{The maximum weighted node degree of a graph $\mathcal{G}$.
		\\ See also: maximum, node degree, graph. } \nonumber 
\end{align} 

\begin{align} 
	&\mathcal{D}^{(i)} & \quad & \parbox{.70\textwidth}{The local dataset $\mathcal{D}^{(i)}$ carried by 
		node $i\in \mathcal{V}$ of an federated learning network (FL network).
		\\ See also: local dataset, federated learning network (FL network). } \nonumber \\[2mm] \hline \nonumber\\[-5mm] 
	&m_{i} & \quad & \parbox{.70\textwidth}{The number of data points (i.e., sample size) contained in the 
		local dataset $\mathcal{D}^{(i)}$ at node $i\in \mathcal{V}$.
		\\ See also: data point, sample size, local dataset. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&{\bf x}^{(i,r)} & \quad & \parbox{.70\textwidth}{The features of the $r$th data point in 
		the local dataset $\mathcal{D}^{(i)}$.
		\\ See also: feature, data point, local dataset. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&y^{(i,r)} & \quad & \parbox{.70\textwidth}{The label of the $r$th data point in 
		the local dataset $\mathcal{D}^{(i)}$.
		\\ See also: label, data point, local dataset. } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&\mathbf{w}^{(i)} & \quad & \parbox{.70\textwidth}{The local model parameters of device $i$ within an federated learning network (FL network).
		\\ See also: model parameter, device, federated learning network (FL network). } \nonumber \\[2mm] \hline \nonumber\\[-5mm]
	&L_{i}\left({\bf w}\right) & \quad & \parbox{.70\textwidth}{The local loss function used by device $i$ 
		to measure the usefulness of some choice ${\bf w}$ for the local model parameters.
		\\ See also: loss function, device, model parameter. }\nonumber \\[2mm] \hline \nonumber\\[-5mm]
	& L^{(\rm d)} \left({{\bf x}},{h\big({\bf x}\big)},{h'\big({\bf x}\big)} \right)& \quad & \parbox{.70\textwidth}{The loss 
		incurred by a hypothesis $h'$ on a data point with features ${\bf x}$ and label 
		$h\big( {\bf x}\big)$ that is obtained from another hypothesis.
		\\ See also: loss, hypothesis, data point, feature, label. }\nonumber 
\end{align} 

\begin{align} 
	& {\rm stack} \big\{ {\bf w}^{(i)} \big\}_{i=1}^{n} & \quad & \parbox{.70\textwidth}{The vector 
		$\bigg( \big({\bf w}^{(1)}  \big)\,^{T}, \,\ldots, \,\big({\bf w}^{(n)}  \big)\,^{T} \bigg)\,^{T} \in \mathbb{R}^{dn}$ that 
		is obtained by vertically stacking the local model parameters ${\bf w}^{(i)} \in \mathbb{R}^{d}$, for $i=1,\,\ldots,\,n$.
		\\ See also: vector, stacking, model parameter. } \nonumber  
\end{align}        


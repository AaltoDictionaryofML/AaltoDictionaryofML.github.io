% !TeX spellcheck = en_GB

\newglossaryentry{minimum}
{name={minimum},
 description={Given a set of real numbers, the minimum is the smallest of those numbers.},
 firstplural={minima}, 
 plural={minima}, 
 first={minimum},
 text={minimum}
}

\newglossaryentry{epigraph}
{name={epigraph},
  description={The epigraph of a real-valued function $f : \mathbb{R}^n \to \mathbb{R} \cup \{+\infty\}$ 
  	is the set of points lying on or above its graph:
		\[
		\operatorname{epi}(f) = \left\{ (\mathbf{x}, t) \in \mathbb{R}^n \times \mathbb{R} \,\middle|\, f(\mathbf{x}) \leq t \right\}.
		\]
		A function is convex if and only if its epigraph is a convex set \cite{BoydConvexBook}, \cite{BertCvxAnalOpt}.
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[scale=1.0]
				\begin{axis}[
					axis lines = middle,
					xlabel = $x$,
					ylabel = {$$},
					xmin=-2, xmax=2,
					ymin=0, ymax=4.5,
					samples=100,
					domain=-1.5:1.5,
					thick,
					width=8cm,
					height=6cm,
					grid=none,
					axis on top,
					]
					% Function
					\addplot [blue, thick, domain=-1.5:1.5] {x^2} node [pos=0.85, anchor=south west, xshift=5pt] {$f(x)$};
					% Epigraph shading
					\addplot [
					name path=f,
					draw=none,
					ytick=\empty,
					domain=-1.5:1.5,
					] {x^2};
					\path[name path=top] (axis cs:-1.5,4) -- (axis cs:1.5,4);
					\addplot [
					blue!20,
					opacity=0.6,
					draw=none,
					] fill between [
					of=f and top,
					soft clip={domain=-1.5:1.5},
					];
					    \node[font=\small] at (axis cs:-1.0,2.3) {$\operatorname{epi} f$};
				%	\node[align=center, fill=white, draw=black, rounded corners, font=\small] at (axis cs:0.5,3.5) {Epigraph\\$\{(x,t) \mid f(x) \le t\}$};
				\end{axis}
			\end{tikzpicture}
			\caption{Epigraph of the function $f(x) = x^2$ (i.e., shaded area).}
		\end{figure}
		See also: convex.
	},
	first={epigraph},
	text={epigraph},
	plural={epigraphs}
}


\newglossaryentry{maximum}
{name={maximum},
  description={The maximum of a set $\mathcal{A} \subseteq \mathbb{R}$ 
     	of real numbers is the greatest element in that set, if such an element exists. A set $\mathcal{A}$ 
     	has a maximum if it is bounded above and attains its supremum \cite[Sec.~1.4]{RudinBookPrinciplesMatheAnalysis}.
				\\ 
		See also: supremum.},
 first={maximum},
 firstplural={maxima}, 
 plural={maxima},
 text={maximum}
}

\newglossaryentry{supremum}
{name={supremum (or least upper bound)},
  description={The supremum of a set of real numbers is 
   the smallest number that is greater than or equal to every element in the set. More formally, a 
   real number $a$ is the supremum of a set $\mathcal{A} \subseteq \mathbb{R}$ if: 1) $a$ 
   is an upper bound of $\mathcal{A}$; and 2) no number smaller than $a$ is an upper bound of $\mathcal{A}$. 
   Every non-empty set of real numbers that is bounded above has a supremum, even if it does 
  not contain its supremum as an element \cite[Sec.~1.4]{RudinBookPrinciplesMatheAnalysis}.},
  firstplural={suprema}, 
  plural={suprema}, 
   first={supremum (or least upper bound)},
   text={supremum}
}

\newglossaryentry{discrepancy}
{name={discrepancy},
	description={
		Consider an FL application with networked data 
		represented by an FL network. FL methods use a discrepancy measure 
		to compare hypothesis maps from local models at nodes $i,i'$ 
		connected by an edge in the FL network.
					\\ 
		See also: FL, networked data, FL network, hypothesis, local model.},
  first={discrepancy},
  firstplural={discrepancies}, 
  plural={discrepancies}, 
  text={discrepancy}
}

\newglossaryentry{FedRelax}
{name={FedRelax},
	description={An FL distributed algorithm. 
		\\ 
		See also: FL, distributed algorithm.},
	first={FedRelax},text={FedRelax}
} 

\newglossaryentry{fedavg}
{name={FedAvg},
	description={FedAvg refers to a family of iterative FL algorithms. 
	It uses a server-client setting and alternates between client-wise local models 
	re-training, followed by the aggregation of updated model parameters at the server 
	\cite{pmlr-v54-mcmahan17a}. The local update at client $i=1,\ldots,n$ 
	at time $k$ starts from the current model parameters ${\bf w}^{(k)}$ provided 
	by the server and typically amounts to executing few iterations of SGD. After completing the local updates, they are aggregated 
	by the server (e.g., by averaging them). Fig.\ \ref{fig_single_iteration_fedavg} illustrates the execution of a single 
	iteration of FedAvg. 
	\begin{figure} 
		\begin{center}
	\begin{tikzpicture}[>=Stealth, node distance=1cm and 1.5cm, every node/.style={font=\small}]
		% Styles
		\tikzstyle{server} = [circle, fill=black, minimum size=6pt, inner sep=0pt]
		\tikzstyle{client} = [circle, draw=black, minimum size=6pt, inner sep=0pt]
		% Time step labels
		\node (label1) at (0,3.5) {broadcast};
		\node[right=2.5cm of label1] (label2) {local update};
		\node[right=2.5cm of label2] (label3) {aggregate};
		% Time step k
		\node[server] (s1) at (label1 |- 0,2.5) {};
		\node[client] (c1l) at ($(s1) + (-1cm,-1cm)$) {};
		\node[client] (c1r) at ($(s1) + (1cm,-1cm)$) {};
		\node[] (dots1) at ($(s1) + (0cm,-1cm)$) {\ldots};
		\draw[->] (s1) -- (c1l) node[midway,left] {${\bf w}^{(k)}$};
		\draw[->] (s1) -- (c1r) node[midway,right] {${\bf w}^{(k)}$};
		\draw[->] (s1) -- (dots1);
		% Time step k+1 (local updates)
		\node[server] (s2) at (label2 |- 0,2.5) {};
		\node[client] (c2l) at ($(s2) + (-1cm,-1cm)$) {};
		\node[client] (c2r) at ($(s2) + (1cm,-1cm)$) {};
		\node[] (dots2) at ($(s2) + (0cm,-1cm)$) {\ldots};
		\node[below=0.2cm of c2l] {$\mathbf{w}^{(k,1)}$};
		\node[below=0.2cm of c2r] {$\mathbf{w}^{(k,n)}$};
		% Time step k+2 (aggregation)
		\node[server] (s3) at (label3 |- 0,2.5) {};
			\node[above=0.01cm of s3, yshift=-4pt] {${\bf w}^{(k+1)}$};
		\node[client] (c3l) at ($(s3) + (-1cm,-1cm)$) {};
		\node[client] (c3r) at ($(s3) + (1cm,-1cm)$) {};
		\node[] (dots3) at ($(s3) + (0cm,-1cm)$) {\ldots};
		\draw[->] (c3l) -- (s3) node[midway,left] {$\mathbf{w}^{(k,1)}$};
		\draw[->] (c3r) -- (s3)  node[midway,right] {$\mathbf{w}^{(k,n)}$};
		\draw[->] (dots3) -- (s3);
	\end{tikzpicture}
	\end{center}
		\caption{Illustration of a single iteration of FedAvg which consisting of broadcasting model parameters by the 
			server, local updates at clients and their aggregation by the server. \label{fig_single_iteration_fedavg}} 
	\end{figure} 
		\\ 
		See also: FL, algorithm, local model.},
	first={FedAvg},text={FedAvg}
} 

\newglossaryentry{FedGD}
{name={FedGD},
	description={An FL distributed algorithm that 
		can be implemented as message passing across an FL network. 
		\\ 
		See also: FL, distributed algorithm, FL network, gradient step, gradient-based methods.},
	first={FedGD},text={FedGD}
} 

\newglossaryentry{FedSGD}
{name={FedSGD},
	description={An FL distributed algorithm that 
		can be implemented as message passing across an FL network. 
		\\ 
		See also: FL, distributed algorithm, FL network, gradient step, gradient-based methods, SGD.},
	first={FedSGD},text={FedSGD}
} 

\newglossaryentry{hfl}
{name={horizontal federated learning (HFL)},description=
	{HFL uses local datasets constituted by different
	   data points but uses the same features to characterize them \cite{HFLChapter2020}.
		For example, weather forecasting uses a network of spatially distributed
		weather (observation) stations. Each weather station measures the
		same quantities, such as daily temperature, air pressure, and precipitation.
		However, different weather stations measure the characteristics or
		features of different spatiotemporal regions. Each spatiotemporal region 
		represents an individual data point, each characterized by the same features 
		(e.g., daily temperature or air pressure).\\
		See also: local dataset, data point, feature, FL, VFL, CFL.},
	first={HFL},text={HFL}
} 

\newglossaryentry{dimred}
{name={dimensionality reduction},
	description={Dimensionality reduction refers 
		to methods that learn a transformation 
		$h: \mathbb{R}^{d} \rightarrow \mathbb{R}^{d'}$ 
		of a (typically large) set of raw features $x_{1},\ldots,x_{d}$ 
		into a smaller set of informative features $z_{1},\ldots,z_{d'}$. 
		Using a smaller set of features is beneficial in several ways: 
		\begin{itemize} 
			\item {Statistical benefit:} It typically reduces the risk of overfitting, as 
			reducing the number of features often reduces the effective dimension of a model. 
			\item {Computational benefit:} Using fewer features means less computation 
			for the training of ML models. As a case in point, linear regression methods 
			need to invert a matrix whose size is determined by the number of features. 
			\item {\bf Visualization.} Dimensionality reduction is also instrumental for data visualization. 
			For example, we can learn a transformation that delivers two features $z_{1},z_{2}$ 
			which we can use, in turn, as the coordinates of a scatterplot. Fig.\ \ref{fig:dimred-scatter} 
			depicts the scatterplot of hand-written digits that are placed 
			according transformed features. Here, the data points are 
			naturally represented by a large number of greyscale values (one of for each pixel).
		\end{itemize} 
		 \begin{figure}[H]
		 \centering
		 \begin{tikzpicture}[scale=1]	
		% % Axes
		 	\draw[->] (-0.5,0) -- (5.5,0) node[right] {$z_1$};
		 	\draw[->] (0,-0.5) -- (0,4.5) node[above] {$z_2$};
		% % Example points with mini-images (can replace adjustbox with \includegraphics)
		 	\foreach \x/\y/\label in {
  		 		1.2/0.5/3,
  		 		0.8/2.0/8,
  		 		2.5/1.8/1,
  		 		3.8/3.5/6,
  		 		4.2/0.7/9,
  		 		2.8/3.0/7,
  		 		1.5/3.8/2
		 	}{
  		 		\node[draw, minimum size=0.6cm, inner sep=0pt] at (\x,\y)
    	 		{\label};
		 	}
		 	\end{tikzpicture}
		 	\caption{Example of dimensionality reduction: High-dimensional image data 
			(e.g., high-resolution images of hand-written digits) embedded into 2D using 
			learned features $(z_1, z_2)$ and visualized in a scatterplot.}
		 	\label{fig:dimred-scatter}
		 \end{figure}
		See also: feature, overfitting, effective dimension, model, ML, linear regression, data, scatterplot, data point.}, first={dimensionality reduction},text={dimensionality reduction}
} 



\newglossaryentry{ml}
{name={machine learning (ML)},
		 description={ML aims to predict 
	 a label from the features of a data point. ML methods achieve 
	 this by learning a hypothesis from a hypothesis space (or model) 
	 through the minimization of a loss function \cite{MLBasics}, \cite{HastieWainwrightBook}. 
	 One precise formulation of this principle is ERM. Different ML methods are 
	 obtained from different design choices for data points (i.e., their features and label), 
	 the model, and the loss function \cite[Ch. 3]{MLBasics}.
	 			\\ 
		See also: label, feature, data point, hypothesis, hypothesis space, model, loss function, ERM.},
	first={machine learning (ML)},text={ML}
} 


\newglossaryentry{featlearn}
{name={feature learning},
	description={Consider an ML application with data points characterized by 
		raw features ${\bf x} \in \mathcal{X}$. Feature learning 
		refers to the task of learning a map 
		$${\bf \Phi}: \mathcal{X} \rightarrow \mathcal{X}': {\bf x} \mapsto {\bf x}',$$ 
		that reads in raw features ${\bf x} \in \mathcal{X}$ of a data point and delivers new 
		features ${\bf x}' \in \mathcal{X}'$ from a new feature space $\mathcal{X}'$. 
		Different feature learning methods are obtained for different design 
		choices of $\mathcal{X},\mathcal{X}'$, for a hypothesis space $\mathcal{H}$ 
		of potential maps ${\bf \Phi}$, and for a quantitative measure of the usefulness of 
		a specific ${\bf \Phi} \in \mathcal{H}$. For example, PCA 
		uses $\mathcal{X} := \mathbb{R}^{d}$, $\mathcal{X}' := \mathbb{R}^{d'}$ 
		with $d' < d$, and a hypothesis space 
		$$\mathcal{H}:= \big\{ {\bf \Phi}: \mathbb{R}^{d}
		\!\rightarrow\! \mathbb{R}^{d'}\!:\!{\bf x}'\!:=\!\mathbf{F} {\bf x} \mbox{ with some } \mathbf{F} \!\in\! \mathbb{R}^{d' \times d} \big\}.$$ PCA measures the usefulness of a specific map ${\bf \Phi}({\bf x})= \mathbf{F} {\bf x}$ 
	by the minimum linear reconstruction error incurred on a dataset such that 
$$ \min_{\mathbf{G} \in \mathbb{R}^{d \times d'}} \sum_{r=1}^{m} \left\Vert  {\mathbf{G} \mathbf{F} {\bf x}^{(r)} - {\bf x}^{(r)}} \right\Vert_{2}^{2}.$$ 
			\\ 
		See also: ML, data point, feature, feature space, hypothesis space, PCA, minimum, dataset.}, 
	first={feature learning},text={feature learning}
} 

\newglossaryentry{autoencoder}
{name={autoencoder},
	description={An autoencoder is an ML method that simultaneously learns an encoder map 
		$h(\cdot) \in \mathcal{H}$ and a decoder map $h^{*}(\cdot) \in \mathcal{H}^{*}$. 
		It is an instance of ERM using a loss computed from the reconstruction error 
		${\bf x} - h^{*}\big(  h \big( {\bf x} \big) \big)$.
					\\ 
		See also: ML, ERM, loss.},
	first={autoencoder},text={autoencoder}
} 

\newglossaryentry{vfl}
{name={vertical federated learning (VFL)},
	description={
		VFL refers to FL applications where  
		devices have access to different features of the same set of data points \cite{VFLChapter}. 
		Formally, the underlying global dataset is
		\[
		\mathcal{D}^{(\mathrm{global})} := \left\{ \left({\bf x}^{(1)}, y^{(1)}\right), \ldots, \left({\bf x}^{(m)}, y^{(m)}\right) \right\}.
		\]
		We denote by ${\bf x}^{(r)} = \big( x^{(r)}_{1}, \ldots, x^{(r)}_{d'} \big)^{T}$, for $r=1,\ldots,m$, 
	     the complete feature vectors for the data points. Each device $i \in \mathcal{V}$ 
		observes only a subset $\mathcal{F}^{(i)} \subseteq \{1,\ldots,d'\}$ of features, resulting 
		in a local dataset $\mathcal{D}^{(i)}$ with feature vectors
		\[
		{\bf x}^{(i,r)} = \big( x^{(r)}_{j_{1}}, \ldots, x^{(r)}_{j_{d}} \big)^{T}.
		\]
		Some of the devices might also have access to the labels $y^{(r)}$, for $r=1,\ldots,m$, 
		of the global dataset. One potential application of VFL is to enable collaboration between 
		different healthcare providers. Each provider collects distinct types of measurements—such as blood 
		values, electrocardiography, and lung X-rays—for the same patients. Another application is a 
		national social insurance system, where health records, financial indicators, consumer behavior, 
		and mobility data are collected by different institutions. VFL enables joint learning across 
		these parties while allowing well-defined levels of privacy protection.
		\begin{figure}[H]
			\begin{center}
			\begin{tikzpicture}[every node/.style={anchor=base}]
				  % --- Coordinate definitions ---
				\def\colX{0}
				\def\colY{1.6}
				\def\colZ{3.2}
				\def\colD{4.8}
				\def\colLabel{6.4} 
				\def\rowOne{0}
				\def\rowTwo{-1.2}
				\def\rowThree{-2.4}
				\def\rowFour{-3.6}
				% Manually place matrix entries
				\foreach \i/\label in {1/1, 2/2, 4/m} {
					\pgfmathsetmacro{\y}{-1.2*(\i-1)}
					\node (x\i1) at (0,\y) {$x^{(\label)}_{1}$};
					\node (x\i2) at (1.6,\y) {$x^{(\label)}_{2}$};
					\node (dots\i) at (3.2,\y) {$\cdots$};
					\node (x\i3) at (4.8,\y) {$x^{(\label)}_{d}$};
					\node (y\i) at (6.4,\y) {$y^{(\label)}$};
				}
				  % Outer rectangle for the full dataset
				\draw[dashed, rounded corners, thick]
				(-0.6,0.6) rectangle (6.9,-4.2);
				\node at (3.1,0.9) {$\mathcal{D}^{(\mathrm{global})} $};
			% Rectangle for local dataset 1 (e.g., first two features)
			\draw[dashed, rounded corners, thick]
			(-0.9,0.9) rectangle (2.1,-4.0);
			\node at (0.25,1.0) {$\mathcal{D}^{(1)}$};
		  % --- Local dataset k (columns 2–3, rows 1–3) ---
		\draw[dashed, rounded corners, thick]
			($( \colZ + 1,,0.9 )$) rectangle
			($( \colLabel + 0.4, -4.5)$);
				\node at ($( \colZ + 0.9,-5 )$) {$\mathcal{D}^{(i)}$};
			\end{tikzpicture}
			\end{center}
			\caption{VFL uses local datasets that are derived from the data points of a common global dataset. 
				The local datasets differ in the choice of features used to characterize the data points.\label{fig_vertical_FL}}
		\end{figure}
		See also: FL, device, feature, data point, dataset, feature vector, local dataset, label, data, privacy protection.},
	first={vertical federated learning (VFL)},text={VFL}
} 

\newglossaryentry{interpretability}
{name={interpretability},description=
		{An ML method is interpretable for a specific user if 
			they can well anticipate the predictions delivered by the method. 
			The notion of interpretability can be made precise using quantitative 
			measures of the uncertainty about the predictions \cite{JunXML2020}.
						\\ 
		See also: ML, prediction, uncertainty.},
		first={interpretability},text={interpretability}
}

\newglossaryentry{multitask learning}
{name={multitask learning},description=
	{Multitask learning aims at leveraging relations between 
	 different learning tasks. Consider two learning tasks obtained from the 
	 same dataset of webcam snapshots. The first task is to predict the presence 
	 of a human, while the second task is to predict the presence of a car. It might be useful 
	 to use the same deep net structure for both tasks and only allow the weights of 
	 the final output layer to be different.
	 			\\ 
		See also: learning task, dataset, deep net, weights.},
	first={multitask learning},text={multitask learning}
}

\newglossaryentry{learningtask}
{name={learning task}, plural={learning tasks}, description=
	{Consider a dataset $\mathcal{D}$ constituted by several data points, each of them 
	 characterized by features ${\bf x}$. For example, the dataset $\mathcal{D}$ 
	 might be constituted by the images of a particular database. Sometimes it might be useful 
	 to represent a dataset $\mathcal{D}$, along with the choice of features, by a probability distribution $p({\bf x})$. 
	 A learning task associated with $\mathcal{D}$ consists of a specific 
	 choice for the label of a data point and the corresponding label space. 
	 Given a choice for the loss function and model, a learning task gives rise to an 
	 instance of ERM. Thus, we could define a learning task also via an instance of ERM, i.e., 
	 via an objective function. Note that, for the same dataset, we obtain different learning tasks by using 
	 different choices for the features and label of a data point. These learning 
	 tasks are related, as they are based on the same dataset, and solving them jointly 
	 (via multitask learning methods) is typically preferable over solving them separately \cite{Caruana:1997wk}, \cite{JungGaphLassoSPL}, \cite{CSGraphSelJournal}.
	 			\\ 
		See also: dataset, data point, feature, probability distribution, label, label space, loss function, model, ERM, objective function, multitask learning.},
	first={learning task},text={learning task}
}

\newglossaryentry{explainability}
{name={explainability},description=
		{We define the (subjective) explainability of an ML method 
			as the level of simulatability \cite{Colin:2022aa} of the predictions 
			delivered by an ML system to a human user. Quantitative measures for the 
			(subjective) explainability of a trained model can be constructed by 
			comparing its predictions with the predictions provided by a user 
			on a test set \cite{Colin:2022aa}, \cite{Zhang:2024aa}. Alternatively, we can use 
			probabilistic models for data and measure the explainability of a trained ML 
			model via the conditional (or differential) entropy of its predictions, given the user predictions \cite{JunXML2020}, \cite{Chen2018}.
						\\ 
		See also: ML, prediction, model, test set, probabilistic model, data.
		},
		first={explainability},text={explainability}
	}

\newglossaryentry{lime}
{name={local interpretable model-agnostic explanations (LIME)},description={
		Consider 
		a trained model (or learned hypothesis) $\widehat{h} \in \mathcal{H}$, 
		which maps the feature vector of a data point to the prediction $\widehat{y}= \widehat{h}$. 
		LIME is a technique for explaining 
		the behavior of $\widehat{h}$, locally around a data point with feature vector ${\bf x}^{(0)}$ \cite{Ribeiro2016}. 
		The explanation is given in the form of a local approximation $g \in \mathcal{H}'$ of $\widehat{h}$ (see Fig. \ref{fig_lime}). 
		This approximation can be obtained by an instance of ERM with a carefully designed 
		training set. In particular, the training set consists of data points with 
		feature vector ${\bf x}$ close to ${\bf x}^{(0)}$ and the (pseudo-)label $\widehat{h}({\bf x})$. 
		Note that we can use a different model $\mathcal{H}'$ for the approximation from 
		the original model $\mathcal{H}$. For example, we can use a decision tree 
		to approximate (locally) a deep net. Another widely-used choice for $\mathcal{H}'$ is 
		the linear model. 
		\begin{figure}[H]
		\begin{center}
		\begin{tikzpicture}
			\begin{axis}[
				axis lines=middle,
				xlabel={${\bf x}$},
				ylabel={$y$},
				xtick=\empty,
				ytick=\empty,
				xmin=0, xmax=6,
				ymin=0, ymax=6,
				domain=0:6,
				samples=100,
				width=10cm,
				height=6cm,
				clip=false
			]
			  % Non-linear model h(x)
  			\addplot[blue, thick, domain=0:6] {2 + sin(deg(x))} node[pos=0.85, above right,yshift=3pt] {$\widehat{h}({\bf x})$};
			 % Feature value x0
  			\addplot[dashed, gray] coordinates {(3,0) (3,6)};
			% Piecewise constant local approximation g(x)
  			\addplot[red, thick, domain=2.5:3.5] {2 + sin(deg(3))} node[pos=0.9, above] {$g({\bf x})$};
			% Optional: mark the point of approximation
  			\addplot[mark=*] coordinates {(3, {2 + sin(deg(3))})};
			\node at (axis cs:3,-0.3) {${\bf x}^{(0)}$};
			\end{axis}
		  \end{tikzpicture}
		\end{center}
		\caption{To explain a trained model $\widehat{h} \in \mathcal{H}$, around a 
		given feature vector ${\bf x}^{(0)}$, we can use a local approximation $g \in \mathcal{H}'$. }
		\label{fig_lime}
		\end{figure}
		See also: model, hypothesis, feature vector, data point, prediction, explanation, ERM, training set, label, decision tree, deep net, linear model.},
	first={LIME},text={LIME}
}



\newglossaryentry{linmodel}{name={linear model}, plural={linear models},
	description={Consider data points, each characterized by a numeric feature vector 
		${\bf x} \in \mathbb{R}^{d}$. A linear model is 
		a hypothesis space which consists of all linear maps such that 
	\begin{equation} 
		\label{equ_def_lin_model_hypspace_dict}
		\mathcal{H}^{(d)} := \left\{ h({\bf x})= {\bf w}^{T} {\bf x}: {\bf w} \in \mathbb{R}^{d} \right\}. 
	\end{equation} 
	Note that \eqref{equ_def_lin_model_hypspace_dict} defines an entire family of hypothesis spaces, which is 
	parametrized by the number $d$ of features that are linearly combined to form the 
	prediction $h({\bf x})$. The design choice of $d$ is guided by computational aspects 
	(e.g., reducing $d$ means less computation), statistical aspects (e.g., increasing $d$ might 
	reduce prediction error), and interpretability. A linear model using few carefully chosen 
	features tends to be considered more interpretable \cite{rudin2019stop}, \cite{Ribeiro2016}.
				\\ 
		See also: data point, feature vector, model, hypothesis space, feature, prediction, computational aspects, statistical aspects, interpretability.}, 
   first={linear model},text={linear model}}
	
	
\newglossaryentry{gradstep}{name={gradient step}, plural={gradient steps}, description={Given a differentiable 
		real-valued function $f(\cdot): \mathbb{R}^{d} \rightarrow \mathbb{R}$ 
		 and a vector ${\bf w} \in \mathbb{R}^{d}$, the gradient step 
		 updates ${\bf w}$ by adding the scaled negative gradient $\nabla f({\bf w})$ to obtain 
		 the new vector (see Fig. \ref{fig_basic_GD_step_single_dict})
		 \begin{equation}
		 \label{equ_def_gd_basic_dict} 
		\widehat{{\bf w}}  := {\bf w} - \eta \nabla f({\bf w}).
		\end{equation} 
		Mathematically, the gradient step is a (typically non-linear) operator $\mathcal{T}^{(f,\eta)}$ 
		that is parametrized by the function $f$ and the step size $\eta$. 
		\begin{figure}[H]
			\begin{center}
				\begin{tikzpicture}[scale=0.8]
					\draw[loosely dotted] (-4,0) grid (4,4);
					\draw[blue, ultra thick, domain=-4.1:4.1] plot (\x,  {(1/4)*\x*\x});
					\draw[red, thick, domain=2:4.7] plot (\x,  {2*\x - 4});
					\draw[<-] (4,4) -- node[right] {$\nabla f({\bf w}^{(k)})$} (4,2);
					\draw[->] (4,4) -- node[above] {$-\eta \nabla f({\bf w}^{(k)})$} (2,4);
					\draw[<-] (4,2) -- node[below] {$1$} (3,2) ;
					%\draw[->] (-4.25,0) -- (4.25,0) node[right] {$a$};
					\node[left] at (-4.1, 4.1) {$f(\cdot)$}; 
					\draw[shift={(0,0)}] (0pt,2pt) -- (0pt,-2pt) node[below] {$\overline{{\bf w}}$};
					\draw[shift={(4,0)}] (0pt,2pt) -- (0pt,-2pt) node[below] {${\bf w}$};
					\draw[shift={(2,0)}] (0pt,2pt) -- (0pt,-2pt) node[below] {$\mathcal{T}^{(f,\eta)}({\bf w})$};
				\end{tikzpicture}
			\end{center}
			\caption{The basic gradient step \eqref{equ_def_gd_basic_dict} maps a given vector ${\bf w}$ 
			to the updated vector ${\bf w}'$. It defines an operator 
			$\mathcal{T}^{(f,\eta)}(\cdot): \mathbb{R}^{d} \rightarrow \mathbb{R}^{d}:
			 {\bf w} \mapsto \widehat{{\bf w}}$.}
			\label{fig_basic_GD_step_single_dict}
		\end{figure}
		Note that the gradient step \eqref{equ_def_gd_basic_dict} optimizes locally - 
		in a neighborhood whose size is determined by the step size $\eta$ - a linear approximation 
		to the function $f(\cdot)$. A natural generalization of \eqref{equ_def_gd_basic_dict} is to locally 
		optimize the function itself - instead of its linear approximation - such that
		\begin{align} 
		\label{equ_approx_gd_step_dict}
		\widehat{{\bf w}} = \argmin_{{\bf w}' \in \mathbb{R}^{d}} f({\bf w}')\!+\!(1/\eta)\left\Vert  {{\bf w}-{\bf w}'} \right\Vert_{2}^2. 
		\end{align}
		We intentionally use the same symbol $\eta$ for the parameter in \eqref{equ_approx_gd_step_dict} 
		as we used for the step size in \eqref{equ_def_gd_basic_dict}. The larger the $\eta$ we choose in 
		\eqref{equ_approx_gd_step_dict}, the more progress the update will make towards reducing the 
		function value $f(\widehat{{\bf w}})$. Note that, much like the gradient step \eqref{equ_def_gd_basic_dict}, 
		also the update \eqref{equ_approx_gd_step_dict} defines a (typically non-linear) operator 
		that is parametrized by the function $f(\cdot)$ and the parameter $\eta$. For a convex function 
		$f(\cdot)$, this operator is known as the proximal operator of $f(\cdot)$ \cite{ProximalMethods}. 
					\\ 
		See also: differentiable, gradient, step size, neighborhood, generalization, convex, proximal operator.
		},first={gradient step},text={gradient step}}
	

\newglossaryentry{proxop}{name={proximal operator},description={Given a convex 
		function $f({\bf w}')$, we define its proximal operator as \cite{ProximalMethods}, \cite{Bauschke:2017} 
		$${\rm\bf prox}_{f(\cdot),\rho}({\bf w}):= \argmin_{{\bf w}' \in \mathbb{R}^{d}} \bigg[ f({\bf w}')\!+\!(\rho/2) \left\Vert  {{\bf w}- {\bf w}'} \right\Vert_{2}^{2}\bigg] \mbox{ with } \rho > 0. $$ 
		As illustrated in Fig. \ref{fig_proxoperator_opt_dict}, evaluating the proximal operator 
		amounts to minimizing a penalized variant of $f({\bf w}')$. The penalty term is the 
		scaled squared Euclidean distance to a given vector ${\bf w}$ (which is the input to the proximal operator). 
		%Convex functions for which the proximal operator can be computed efficiently 
		%is sometimes referred to as \emph{proximable} or \emph{simple} \cite{Condat2013}. 
		The proximal operator can be interpreted as a generalization of the gradient step, which is defined 
		for a smooth convex function $f({\bf w}')$. Indeed, taking a 
		gradient step with step size $\eta$ at the current vector ${\bf w}$ 
		is the same as applying the proximal operator of the function $\tilde{f}({\bf w}')= \big( \nabla f({\bf w})\big)^{T} ({\bf w}'-{\bf w})$ 
		and using $\rho=1/\eta$.
			\begin{figure}[H]
			\begin{center}
				\begin{tikzpicture}[scale=0.8]
					% Original quadratic function
					\draw[blue, ultra thick, domain=-4.1:4.1] plot (\x, {(1/4)*\x*\x}) node[above right] {$f({\bf w}')$};		
					% Quadratic function with larger curvature, centered at w = 2
					\draw[red, thick, domain=1:3] plot (\x, {2*(\x - 2)*(\x - 2)}) node[below right] {$(1/\eta)\left\Vert  {{\bf w}-{\bf w}'} \right\Vert_{2}^{2}$};
					% Axes
					% Minimum point of second curve
					\draw[shift={(2,0)}] (0pt,2pt) -- (0pt,-2pt) node[below] {${\bf w}$};
					%\node at (2,0.5) [anchor=north] {${\bf w}$};
				\end{tikzpicture}
			\end{center}
			\caption{A generalized gradient step updates a vector ${\bf w}$ by minimizing a penalized version 
				of the function $f(\cdot)$. The penalty term is the scaled squared Euclidean distance between the optimization 
				variable ${\bf w}'$ and the given vector ${\bf w}$.	\label{fig_proxoperator_opt_dict}}
		\end{figure}
		See also: convex, generalization, gradient step, smooth, step size.
		},first={proximal operator},text={proximal operator}}

\newglossaryentry{proximable}{name={proximable},description={A 
		convex function for which the proximal operator can be computed efficiently is 
		sometimes referred to as proximable or simple \cite{Condat2013}.
					\\ 
		See also: convex, proximal operator.},first={proximable},text={proximable}}


\newglossaryentry{connected}{name ={connected graph}, description={An 
		undirected graph $\mathcal{G}=\left( \mathcal{V},\mathcal{E} \right)$ is connected if every 
		non-empty subset $\mathcal{V}' \subset \mathcal{V}$ has at least one edge connecting it to $\mathcal{V} \setminus \mathcal{V}'$.
					\\ 
		See also: graph.}, 
		first={connected graph},text={connected graph}}
	
	

\newglossaryentry{mvndist}{name ={multivariate normal distribution}, 
	description={The multivariate normal distribution 
		$\mathcal{N}\left({\bf m},{\bf C}\right)$ is an important probabilistic model for numeric feature vectors. 
		It is a family of probability distributions for a vector-valued RV 
		${\bf x} \in \mathbb{R}^{d}$ \cite{BertsekasProb}, \cite{GrayProbBook}, \cite{Lapidoth09}. 
		Each family member (i.e., a specific probability distribution) is specified by its mean ${\bf m}$ and  
		covariance matrix ${\bf C}$. If the covariance matrix is invertible, the probability distribution of ${\bf x}$ can 
		be written as 
		$$p({\bf x}) \propto \exp\bigg(-(1/2) \big( {\bf x} - {\bf m} \big)^{T} {\bf C}^{-1} \big( {\bf x} - {\bf m} \big) \bigg).$$
					\\ 
		See also: probability distribution, feature vector, RV, mean, covariance matrix.}, first={multivariate normal distribution},text={multivariate normal distribution}}

\newglossaryentry{statasp}{name ={statistical aspects}, description={By statistical aspects 
		of an ML method, we refer to (properties of) the probability distribution of its output 
		under a probabilistic model for the data fed into the method.
					\\ 
		See also: ML, probability distribution, probabilistic model, data.},first={statistical aspects},text={statistical aspects}}

\newglossaryentry{compasp}{name ={computational aspects}, description={By computational 
		aspects of an ML method, we mainly refer to the computational 
		resources required for its implementation. For example, if an ML method uses iterative 
		optimization techniques to solve ERM, then its computational aspects include: 1) how 
		many arithmetic operations are needed to implement a single iteration (i.e., a gradient step); 
		and 2) how many iterations are needed to obtain useful model parameters. One important 
		example of an iterative optimization technique is GD.
					\\ 
		See also: ML, ERM, gradient step, model parameters, GD.}, first={computational aspects},text={computational aspects}}

\newglossaryentry{zerooneloss}{name={$\bf 0/1$ loss},
	description={The $0/1$ loss $L^{(0/1)}\left(\left( {\bf x},y \right),h \right)$ 
		measures the quality of a classifier $h({\bf x})$ that delivers a 
		prediction $\hat{y}$ (e.g., via thresholding \eqref{equ_def_threshold_bin_classifier_dict}) 
		for the label $y$ of a data point with features ${\bf x}$. It is equal to $0$ if 
		the prediction is correct, i.e., 
	$L^{(0/1)}\left(\left( {\bf x},y \right),h \right)=0$ when $\hat{y}=y$. It is 
	equal to $1$ if the prediction is wrong, i.e., $L^{(0/1)}\left(\left( {\bf x},y \right),h \right)=1$ 
	when $\hat{y}\neqy$.
				\\ 
		See also: loss, classifier, prediction, label, data point, feature.},
	sort=zerooneloss, 
    first={$0/1$ loss},text={$0/1$ loss}}

\newglossaryentry{probability}{name={probability},
	description={We assign a probability value, typically chosen in the 
		interval $[0,1]$, to each event that might occur in a random experiment \cite{BertsekasProb}, \cite{HalmosMeasure}, \cite{BillingsleyProbMeasure}, \cite{KallenbergBook}.},first={probability},text={probability}}
	
\newglossaryentry{underfitting}{name={underfitting},description={Consider 
		an ML method that uses ERM to learn a hypothesis with the minimum empirical risk 
		on a given training set. Such a method is underfitting the training set if it is 
		not able to learn a hypothesis with a sufficiently small empirical risk on the training set. 
		If a method is underfitting, it will typically also not be able to learn a hypothesis with 
		a small risk.
					\\ 
		See also: ML, ERM, hypothesis, minimum, empirical risk, training set, risk.},first={underfitting},text={underfitting}}

\newglossaryentry{overfitting}{name={overfitting},description={Consider an 
		ML method that uses ERM to learn a hypothesis with the minimum empirical risk on 
		a given training set. Such a method is overfitting the training set if it learns 
		a hypothesis with a small empirical risk on the training set but a significantly larger loss outside the training set.
					\\ 
		See also: ML, ERM, hypothesis, minimum, empirical risk, training set, loss.},first={overfitting},text={overfitting}}

\newglossaryentry{gdpr}{name={general data protection regulation (GDPR)},description={
			The GDPR
			was enacted by the European Union (EU), effective from May 25, 2018 \cite{GDPR2016}. 
			It safeguards the privacy and data rights of individuals in the EU. 
			The GDPR has significant implications for how data is collected, stored, and used in ML  
			applications. Key provisions include the following:
			\begin{itemize}
				\item Data minimization principle: ML systems should only use the necessary amount of personal 
				data for their purpose.
				\item Transparency and explainability: ML systems should enable their users to 
				understand how the systems make decisions that impact the users.
				\item Data subject rights: Users should get an opportunity to access, rectify, and delete their personal data, as well as to object to automated decision-making and profiling.
				\item Accountability: Organizations must ensure robust data security and demonstrate 
				compliance through documentation and regular audits.
			\end{itemize}
		See also: data, ML, data minimization principle, transparency, explainability.}, 
	first={general data protection regulation (GDPR)},text={GDPR}}
	
\newglossaryentry{gaussrv}{name={Gaussian random variable (Gaussian RV)}, plural={Gaussian RVs}, description={
		A  standard Gaussian RV is a 
		real-valued RV $x$ with pdf \cite{BertsekasProb}, \cite{GrayProbBook}, \cite{papoulis}
		\begin{equation}
			\nonumber
			p(x) = \frac{1}{\sqrt{2\pi}} \exp^{-x^2/2}. 
		\end{equation}
		Given a standard Gaussian RV $x$, we can construct a general Gaussian RV $x'$ with 
		mean $\mu$ and variance $\sigma^2$ via $x' := \sigma (x+\mu)$. The probability distribution of a 
		Gaussian RV is referred to as normal distribution, denoted $\mathcal{N}(\mu,\sigma)$.  \\ 
		A Gaussian random vector ${\bf x} \in \mathbb{R}^{d}$ with 
		covariance matrix $\mathbf{C}$ and mean ${\bm \mu}$ can be constructed via 
		${\bf x} := \mathbf{A} \big( {\bf z} + {\bm \mu} \big)$. Here, ${\bf A}$ 
		is any matrix that satisfies ${\bf A}{\bf A}^{T} = {\bf C}$ and ${\bf z} := \big( z_{1},\ldots,z_{d} \big)^{T}$
		is a vector whose entries are i.i.d. standard Gaussian RVs $z_{1},\ldots,z_{d}$. 
		Gaussian random vectors are a special case of GPs which are 
		linear transformations of infinite sequences of standard Gaussian RVs \cite{Rasmussen2006Gaussian}.
		Gaussian RVs are widely used probabilistic models for the statistical analysis of 
		ML methods. Their significance arises partly from the central limit theorem, 
		which states that the average of an increasing number of independent RVs (not necessarily Gaussian themselves) 
		converges to a Gaussian RV \cite{ross2013first}. 
					\\ 
		See also: RV, pdf, mean, variance, probability distribution, covariance matrix, i.i.d., probabilistic model, ML.
},first={Gaussian random variable (Gaussian RV)},text={Gaussian RV}}

\newglossaryentry{GaussProc}
{name={Gaussian process (GP)},
  description={A GP is a collection of RVs 
  	$\{f({\bf x})\}_{{\bf x} \in \mathcal{X}}$ indexed by input values ${\bf x}$ 
  	from some input space $\mathcal{X}$, such that for any finite subset 
  	${\bf x}^{(1)}, \ldots, {\bf x}^{(m)} \in \mathcal{X}$, 
  	the corresponding RVs $f({\bf x}^{(1)}, \ldots, {\bf x}^{(m)}$ have a joint 
  	multivariate Gaussian distribution:
  	\[
  	\left( f({\bf x}^{(1)}, \ldots, {\bf x}^{(m)} \right) \sim \mathcal{N}(\boldsymbol{\mu}, \mathbf{K}).
  	\]
  	For a fixed input space $\mathcal{X}$, a GP is fully specified (or parametrized) by 
  	\begin{itemize}
  		\item a mean function $\mu({\bf x}) = \mathbb{E} \{ f({\bf x})\}$
  		\item and a covariance function $K\big({\bf x},{\bf x}'\big)= \mathbb{E} \{ \big(f({\bf x})-\mu({\bf x})\big) \big(f({\bf x}')-\mu({\bf x}')\big) \big\}$.
  	\end{itemize}
  	\text{Example:} We can interpret the temperature distribution across Finland (at a specific 
  	point in time) as the realization of a GP $f({\bf x})$, where each input ${\bf x} = (\text{lat}, \text{lon})$ 
  	denotes a geographic location. Temperature observations from FMI weather stations provide 
  	samples of $f({\bf x})$ at specific locations (see Fig.\ \ref{fig_gp_FMI}). A GP allows us to 
  	predict the temperature nearby FMI weather stations and to quantify the uncertainty 
  	of these predictions. 
  	\begin{figure}[H]
  	\begin{center}
  \begin{tikzpicture}
\begin{axis}[
	axis equal,
	hide axis,
	scale=1.2,
	xmin=17, xmax=32,
	ymin=55, ymax=71,
%	width=15cm,
%	height=20cm,
	clip=true
	]
	% --- Finland border (polyline) ---
	\addplot[
	color=black,
	thick
	] table [x=lon, y=lat, col sep=comma] {assets/finland_border.csv};
	% --- FMI sample stations ---
	\addplot[
	only marks,
	mark=*,
	mark options={fill=blue},
	color=black
	] table [x=lon, y=lat, col sep=comma] {assets/fmi_stations_subset.csv};
	% Draw manual axes
	\draw[->, thick] (axis cs:19,59) -- (axis cs:25.5,59) node[anchor=west] {lon};
	\draw[->, thick] (axis cs:19,59) -- (axis cs:19,65.5) node[anchor=south] {lat};
\end{axis}
\end{tikzpicture}
\vspace*{-15mm}
\end{center}
\caption{We can interpret the temperature distribution over Finland as a realization 
	of a GP indexed by geographic coordinates and sampled at FMI weather stations (indicated by 
	blue dots). \label{fig_gp_FMI}}
\end{figure}
See also: RV, mean, realization, FMI, uncertainty.}, 
first = {GP}, 
text = {GP}
}

\newglossaryentry{trustAI}{name={trustworthy artificial intelligence (trustworthy AI)},description=
	{Besides the computational aspects and statistical aspects, a third main design aspect of 
	ML methods is their trustworthiness \cite{pfau2024engineeringtrustworthyaideveloper}. 
		The EU has put forward seven key requirements (KRs) for trustworthy 
		AI (that typically build on ML methods)
	\cite{ALTAIEU}: 
	\begin{enumerate}[label=\arabic*)]
		\item KR1 - Human agency and oversight;
		\item KR2 - Technical robustness and safety;
		\item KR3 - Privacy and data governance;
		\item KR4 - Transparency;
		\item KR5 - Diversity, non-discrimination and fairness; 
		\item KR6 - Societal and environmental well-being;
		\item KR7 - Accountability. 
	\end{enumerate}
		See also: computational aspects, statistical aspects, ML, AI.
	},first={trustworthy artificial intelligence (trustworthy AI)},
	text={trustworthy AI}
}

\newglossaryentry{sqerrloss}
{name={squared error loss},
description={The squared 
		error loss measures the prediction error of a 
		hypothesis $h$ when predicting a numeric label $y \in \mathbb{R}$ 
		from the features ${\bf x}$ of a data point. It is 
	defined as 
\begin{equation} 
	\nonumber
%	\label{equ_squared_loss_gls}
	L\left(({\bf x},y),h \right) := \big(y - \underbrace{h({\bf x})}_{=\hat{y}} \big)^{2}. 
\end{equation} 
			\\ 
		See also: loss, prediction, hypothesis, label, feature, data point.
},
first={squared error loss},
text={squared error loss}
}


 \newglossaryentry{projection}
 {name={projection}, 
       description={Consider a subset $\mathcal{W} \subseteq \mathbb{R}^{d}$ of 
	   the $d$-dimensional Euclidean space. We define the projection $P_{\mathcal{W}}\big( {\bf w}\big) $
	   of a vector ${\bf w} \in \mathbb{R}^{d}$ onto $\mathcal{W}$ as
		\begin{equation} 
   	    \label{equ_def_proj_generic_dict}
  	     P_{\mathcal{W}}\big( {\bf w}\big)  = \argmin_{{\bf w}' \in \mathcal{W}} \left\Vert  {{\bf w} - {\bf w}'} \right\Vert_{2}. 
         \end{equation}
		 In other words, $P_{\mathcal{W}}\big( {\bf w}\big) $ is the vector in $\mathcal{W}$ 
		 which is closest to ${\bf w}$. The projection is only well-defined for subsets $\mathcal{W}$ 
		 for which the above minimum exists \cite{BoydConvexBook}.
		 			\\ 
		See also: Euclidean space, minimum.},
		 first={projection},
		 text={projection}
}


\newglossaryentry{projgd}
{name={projected gradient descent (projected GD)},
description={Consider an ERM-based method that uses a parametrized model with  
parameter space $\mathcal{W} \subseteq \mathbb{R}^{d}$. Even if 
the objective function of ERM is smooth, we cannot use basic GD, as 
it does not take into account contraints on the optimization variable (i.e., the model parameters). 
Projected GD 
extends basic GD to handle constraints on the optimization variable (i.e., the model parameters). 
A single iteration of projected GD consists of first taking a gradient step 
and then projecting the result back onto the parameter space.
\begin{figure}[H]
	\begin{center}
		\begin{tikzpicture}[scale=0.9]
			\node [right] at (-5.1,1.7) {$f({\bf w})$} ;
			\draw[ultra thick, domain=-4.1:4.1] plot (\x,  {(1/8)*\x*\x});
		%	\draw[dashed, thick, domain=1:3.6] plot (\x,  {\x - 1}) node[right] {$ f\big({\bf w}^{(k)}\big)\!+\!\big({\bf w}\!-\!{\bf w}^{(k)}\big)^{T} \nabla f\big({\bf w}^{(k)}\big)$};
			\draw [fill] (2.83,1) circle [radius=0.1] node[right] {${\bf w}$};
			\draw[line width =0.5mm,dashed,->] (2.83,1) -- node[midway,above] {grad. step} (-1.5,1);
			\draw[line width =0.2mm,dashed] (-1.5,1) --(-1.5,-1.5)  node [below, left]{$\widehat{{\bf w}}={\bf w}\!-\!\eta \nabla f\big({\bf w}\big)$} ;
			\draw[line width =0.5mm,dashed,->] (-1.5,-1.5)  -- node[midway,above] {} (1,-1.5) ; 
			\draw [fill] (1,-1.5) circle [radius=0.1] node[below] {$P_{\mathcal{W}}\big( \widehat{{\bf w}}\big) $};
			\draw[line width=1mm] (1,-1.5) -- (3,-1.5) node[midway, above] {$\mathcal{W}$};
		\end{tikzpicture}
		\vspace*{-5mm}
	\end{center}
	\caption{Projected GD augments a basic gradient step with a projection back 
	onto the constraint set $\mathcal{W}$.}
	\label{fig_projected_GD_dict}
\end{figure}
		See also: ERM, model, parameter space, objective function, smooth, GD, model parameters, gradient step, projection.},
		first={projected gradient descent (projected GD)},
		text={projected GD}
}

\newglossaryentry{diffpriv}
{name={differential privacy (DP)},
  description={Consider some ML method $\mathcal{A}$ 
  	that reads in a dataset (e.g., the training set 
  	used for ERM) and delivers some output $\mathcal{A}(\mathcal{D})$. The output 
  	could be either the learned model parameters or the predictions for specific data points. 
  	DP is a precise measure of privacy leakage incurred by revealing the 
  	output. Roughly speaking, an ML method is differentially private if the probability distribution 
  	of the output $\mathcal{A}(\mathcal{D})$ does not change too much if the sensitive attribute 
  	of one data point in the training set is changed. Note that DP 
  	builds on a probabilistic model for an ML method, i.e., we interpret its output $\mathcal{A}(\mathcal{D})$ 
  	as the realization of an RV. The randomness in the output can be ensured 
  	by intentionally adding the realization of an auxiliary RV (i.e., adding noise) to 
  	the output of the ML method.
				\\ 
		See also: ML, dataset, training set, ERM, model parameters, prediction, data point, privacy leakage, probability distribution, sensitive attribute, probabilistic model, realization, RV.}, 
	first = {DP}, 
	text={DP} 
}

\newglossaryentry{stability}
{name={stability},
	description={
		Stability is a desirable property of an ML method $\mathcal{A}$ that maps a 
		dataset $\mathcal{D}$ (e.g., a training set) to an output $\mathcal{A}(\mathcal{D})$. The output 
		$\mathcal{A}(\mathcal{D})$ can be the learned model parameters or the prediction delivered 
		by the trained model for a specific data point. Intuitively, $\mathcal{A}$ is 
		stable if small changes in the input dataset $\mathcal{D}$ lead to small changes in the 
		output $\mathcal{A}(\mathcal{D})$. Several formal notions of stability exist that enable bounds 
		on the generalization error or risk of the method (see \cite[Ch.~13]{ShalevMLBook}).
		To build intuition, consider the three datasets depicted in Fig.~\ref{fig_three_data_stability}, each 
		of which is equally likely under the same data-generating probability distribution. Since the 
		optimal model parameters are determined by this underlying probability distribution, an accurate 
		ML method $\mathcal{A}$ should return the same (or very similar) output $\mathcal{A}(\mathcal{D})$ 
		for all three datasets. In other words, any useful $\mathcal{A}$ must be robust to 
		variability in sample realizations from the same probability distribution, i.e., it must be stable. 
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}
				\begin{axis}[
				%title={Stem Plots of 3 Datasets},
				    axis lines=none,
					xlabel={$r$},
					ylabel={},
					legend pos=north west,
					ymin=0, ymax=10,
					xtick={1,2,3,4,5},
				%	ymajorgrids=true,
					grid style=dashed,
					every axis plot/.append style={very thick}
					]
					% Dataset 1
					\addplot+[only marks,mark=*] coordinates {
						(1,2) (2,4) (3,3) (4,5) (5,7)
					};
				%	\addlegendentry{$\mathcal{D}^{(*)}$}
					% Dataset 2
					\addplot+[only marks,mark=square*] coordinates {
						(1,3) (2,2) (3,6) (4,4) (5,5)
					};
				%	\addlegendentry{$\mathcal{D}^{(\square)}$}
					% Dataset 3
					\addplot+[only marks,mark=triangle*] coordinates {
						(1,5) (2,7) (3,4) (4,6) (5,3)
					};
				%	\addlegendentry{$\mathcal{D}^{(\triangle)}$}
				\end{axis}
			\end{tikzpicture}
			\caption{Three datasets $\mathcal{D}^{(*)}$, $\mathcal{D}^{(\square)}$, and $\mathcal{D}^{(\triangle)}$, 
				each sampled independently from the same data-generating probability distribution. A stable ML 
				method should return similar outputs when trained on any of these datasets. \label{fig_three_data_stability}}
		\end{figure}
		See also: ML, dataset, training set, model parameters, prediction, model, data point, generalization, risk, data, probability distribution, sample, realization.
		}, 
	first = {stability}, text={stability} 
}

\newglossaryentry{privprot}
{name={privacy protection},
     description={Consider some ML method $\mathcal{A}$ that reads 
	 in a dataset $\mathcal{D}$ and delivers some output $\mathcal{A}(\mathcal{D})$. The output 
	 could be the learned model parameters $\widehat{{\bf w}}$ or the prediction 
	 $\hat{h}({\bf x})$ obtained for a specific data point with features 
	 ${\bf x}$. Many important ML applications involve data points 
		representing humans. Each data point is characterized by features ${\bf x}$, 
		potentially a label $y$, and a sensitive attribute $s$ (e.g., a recent medical diagnosis). 
		Roughly speaking, privacy protection means that it should be impossible to infer, from the output $\mathcal{A}(\mathcal{D})$, 
		any of the sensitive attributes of data points in $\mathcal{D}$. Mathematically, privacy protection requires non-invertibility 
		of the map $\mathcal{A}(\mathcal{D})$. In general, just making $\mathcal{A}(\mathcal{D})$ non-invertible 
		is typically insufficient for privacy protection. We need to make $\mathcal{A}(\mathcal{D})$ sufficiently non-invertible. 
					\\ 
		See also: ML, dataset, model parameters, prediction, data point, feature, label, sensitive attribute.
	}, 
	first = {privacy protection}, text={privacy protection} 
}

\newglossaryentry{privleakage}
{
	name={privacy leakage},
	description={Consider an ML application that processes a 
	dataset $\mathcal{D}$ and delivers some output, such as the predictions 
	obtained for new data points. Privacy leakage arises 
	if the output carries information about a private (or sensitive) feature of 
	a data point (which might be a human) of $\mathcal{D}$. Based on a probabilistic model 
	for the data generation, we can measure the privacy leakage via the MI 
	between the output and the senstive feature. Another quantitative measure of privacy leakage 
	is DP. The relations between different measures of privacy leakage have been 
	studied in the literature (see \cite{InfThDiffPriv}). 
				\\ 
		See also: ML, dataset, prediction, data point, feature, probabilistic model, data, MI, DP. 
	}, 
	first = {privacy leakage}, text={privacy leakage} 
}



\newglossaryentry{probmodel}
{
	name={probabilistic model}, plural={probabilistic models},
	description={A probabilistic model interprets data points 
		as realizations of RVs with a joint probability distribution. This joint probability distribution typically 
		involves parameters which have to be manually chosen or learned via statistical inference 
		methods such as maximum likelihood estimation \cite{LC}.
					\\ 
		See also: model, data point, realization, RV, probability distribution, parameters, maximum likelihood. }, 
	first = {probabilistic model}, text={probabilistic model} 
}



\newglossaryentry{mean}
{
	name={mean}, plural={means},
	description={The  mean of an RV ${\bf x}$, taking 
 values in an Euclidean space $\mathbb{R}^{d}$, is its 
 expectation $\mathbb{E} \{{\bf x}\}$. It is defined as the Lebesgue 
 integral of ${\bf x}$ with respect to the underlying probability distribution $P$ (e.g., see \cite{BillingsleyProbMeasure} or \cite{RudinBookPrinciplesMatheAnalysis}), i.e.,
\[
\mathbb{E} \{{\bf x}\} = \int_{\mathbb{R}^{d}} {\bf x} \, \mathrm{d}P({\bf x}).
\] 
We also use the term to refer to the average of a finite sequence 
${\bf x}^{(1)}, \ldots, {\bf x}^{(m)} \in \mathbb{R}^{d}$. However, 
these two definitions are essentially the same. Indeed, we can use the sequence 
${\bf x}^{(1)}, \ldots, {\bf x}^{(m)} \in \mathbb{R}^{d}$ to construct a 
discrete RV $\widetilde{{\bf x}}={\bf x}^{(I)}$, with the index $I$ being chosen uniformly 
at random from the set $\{1,\ldots,m\}$. The mean of $\widetilde{{\bf x}}$ is 
precisely the average $\frac{1}{m} \sum_{r=1}^{m} {\bf x}^{(r)}$.
			\\ 
		See also: RV, Euclidean space, expectation, probability distribution.}, 
		first = {mean}, text={mean} 
}

\newglossaryentry{variance}
{
	name={variance},
	description={The variance of a real-valued RV $x$ is defined as the expectation 
		$\mathbb{E} \big\{ \big( x - \mathbb{E} \{x \} \big)^{2} \big\}$ of the squared difference between $x$ 
		and its expectation $\mathbb{E} \{x \}$. We extend this definition to vector-valued RVs ${\bf x}$ 
		as $\mathbb{E} \big\{ \big\| {\bf x} - \mathbb{E} \{{\bf x} \} \big\|_{2}^{2} \big\}$.
					\\ 
		See also: RV, expectation.} ,first={variance},text={variance} 
}

\newglossaryentry{nn}
{
	name={nearest neighbor (NN)},
	description={NN methods learn a hypothesis 
		$h: \mathcal{X} \rightarrow \mathcal{Y}$ whose function value $h({\bf x})$ 
		is solely determined by the NNs within a given dataset. Different 
		methods use different metrics for determining the NNs. If data points 
		are characterized by numeric feature vectors, we can use their Euclidean distances as 
		the metric.
					\\ 
		See also: hypothesis, dataset, data point, feature vector, neighbors.},
	first={nearest neighbor (NN)},text={NN} 
}

\newglossaryentry{neighborhood}
{
	name={neighborhood},
	description={The neighborhood of a node $i \in \mathcal{V}$ is 
	the subset of nodes constituted by the neighbors of $i$.
				\\ 
		See also: neighbors.},
	first={neighborhood},text={neighborhood} 
}


\newglossaryentry{neighbors}
{
	name={neighbors},
	description={The neighbors of a node $i \in \mathcal{V}$ 
	within an FL network are those nodes $i' \in \mathcal{V} \setminus \{ i\}$ that are connected (via an edge) to node $i$.
				\\ 
		See also: FL network.},
	first={neighbors},text={neighbors} 
}

\newglossaryentry{bias}
{
	name={bias},
	description={Consider an ML method using a parametrized hypothesis space $\mathcal{H}$. 
		It learns the model parameters ${\bf w} \in \mathbb{R}^{d}$ using the dataset $$ \mathcal{D}=\big\{ \left( {\bf x}^{(r)},y^{(r)} \right) \big\}_{r=1}^{m}.$$ 
		To analyze the properties of the ML method, we typically interpret the data points as realizations 
		of i.i.d. RVs, $$ y^{(r)} = h^{(\overline{{\bf w}})}\big( {\bf x}^{(r)} \big) + \bm{\varepsilon}^{(r)}, r=1,\ldots,m.$$ 
		We can then interpret the ML method as an estimator $\widehat{{\bf w}}$ 
		computed from $\mathcal{D}$ (e.g., by solving ERM). The (squared) bias incurred by the estimate $\widehat{{\bf w}}$ 
		is then defined as $B^{2} := \big\| \mathbb{E}  \{ \widehat{{\bf w}}  \}- \overline{{\bf w}}\big\|_{2}^{2}$.
					\\ 
		See also: ML, hypothesis space, model parameters, dataset, data point, realization, i.i.d., RV, ERM.},
first={bias},text={bias} 
}

\newglossaryentry{classification}
{name={classification},
 description={Classification is the task of determining a 
 	discrete-valued label $y$ for a given data point, based solely on its 
 	features ${\bf x}$. The label $y$ belongs to a finite set, such as 
 	$y \in \{-1,1\}$ or $y \in \{1,\ldots,19\}$, and represents the 
 	category to which the corresponding data point belongs.
				\\ 
		See also: data point.},first={classification},text={classification} 
}



\newglossaryentry{privfunnel}
{name={privacy funnel},
 description={The privacy funnel is a method for learning privacy-friendly features 
	of data points \cite{PrivacyFunnel}.
				\\ 
		See also: feature, data point.},
 first={privacy funnel},text={privacy funnel} 
}




\newglossaryentry{condnr}
{
	name={condition number},
	description={The condition number $\kappa(\mathbf{Q}) \geq 1$ of a 
		positive definite 
		matrix $\mathbf{Q} \in \mathbb{R}^{d \times d}$ is the ratio 
		$\alpha /\beta  $ between the 
		largest $\alpha$ and the smallest $\beta$ eigenvalue of 
		$\mathbf{Q}$. The condition number is useful for the analysis of ML methods. 
		The computational complexity of gradient-based methods for linear regression crucially depends on the 
		condition number of the matrix $\mathbf{Q} = {\bf X} {\bf X}^{T}$, with the feature matrix ${\bf X}$ 
		of the training set. Thus, from a computational perspective, we prefer features of 
		data points such that $\mathbf{Q}$ has a condition number close to $1$.
					\\ 
		See also: eigenvalue, ML, gradient-based methods, linear regression, feature matrix, training set, feature, data point.},first={condition number},text={condition number} 
}

\newglossaryentry{classifier}
{
	name={classifier},
	description={A classifier is a hypothesis (i.e., a map) $h({\bf x})$ 
		used to predict a label taking values from a finite label space. We might use the 
		function value $h({\bf x})$ itself as a prediction $\hat{y}$ for 
		the label. However, it is customary to use a map $h(\cdot)$ that delivers 
		a numeric quantity. The prediction is then obtained by a simple thresholding step. 
		For example, in a binary classification problem with \label{labelspace} $\mathcal{Y} \in  \{ -1,1\}$, 
		we might use a real-valued hypothesis map $h({\bf x}) \in \mathbb{R}$ 
		as a classifier. A prediction $\hat{y}$ can then be obtained via thresholding,  
		 \begin{equation} 
		 	\label{equ_def_threshold_bin_classifier_dict}
		 	\hat{y} =1   \mbox{ for } h({\bf x})\!\geq\!0 \mbox{ and } 	\hat{y} =-1  \mbox{ otherwise.}
	 		\end{equation}
 		We can characterize a classifier by its decision regions $\mathcal{R}_{a}$, for 
 		every possible label value $a \in \mathcal{Y}$.
					\\ 
		See also: hypothesis, label, label space, prediction, classification, decision region. },first={classifier},text={classifier} 
}

\newglossaryentry{emprisk}
{name={empirical risk},
  description={The empirical risk $\widehat{L}\big(h|\mathcal{D}\big)$ 
  	of a hypothesis on a dataset $\mathcal{D}$ is the average loss incurred 
  	by $h$ when applied to the data points in $\mathcal{D}$.
				\\ 
		See also: risk, hypothesis, dataset, loss, data point.},
  first={empirical risk},text={empirical risk} 
}

\newglossaryentry{nodedegree}
{name={node degree},
	description={The degree $d^{(i)}$ of a node $i \in \mathcal{V}$ 
		in an undirected graph is the number of its neighbors, i.e., $d^{(i)} := \big|\mathcal{N}^{(i)}\big|$.
					\\ 
		See also: graph, neighbors.},first={node degree},text={node degree} 
}

\newglossaryentry{graph}
{name={graph},
	description={A graph $\mathcal{G} = \left( \mathcal{V},\mathcal{E} \right)$ is a pair that consists of 
		a node set $\mathcal{V}$ and an edge set $\mathcal{E}$. In its most general form, a graph is 
		specified by a map that assigns each edge $e \in \mathcal{E}$ a pair of nodes \cite{RockNetworks}. 
		One important family of graphs is simple undirected graphs. A simple undirected graph 
		is obtained by identifying each edge $e \in \mathcal{E}$ with two different nodes $\{i,i'\}$. 
		Weighted graphs also specify numeric weights $A_{e}$ for each 
		edge $e \in \mathcal{E}$.
					\\ 
		See also: weights.},first={graph},text={graph} 
}

\newglossaryentry{uncertainty}
{name={uncertainty},
	description={Uncertainty refers to the degree of confidence—or 
		lack thereof—associated with a quantity such as a model prediction, parameter estimate, or 
		observed data point. In ML, uncertainty arises from various sources, including 
		noisy data, limited training samples, or ambiguity in model assumptions. Probability theory 
		offers a principled framework for representing and quantifying such uncertainty.
					\\ 
		See also: model, prediction, data point, ML, data, sample, probability.},
	first={uncertainty},text={uncertainty}
}

\newglossaryentry{ucb}
{name={upper confidence bound (UCB)},
	description={Consider an ML 
		application that requires selecting, at each time step $k$, an action $a_{k}$ 
		from a finite set of alternatives $\actionset$. The utility of selecting action $a_{k}$ 
		is quantified by a numeric reward signal $r^{(a_{k})}$. 
		A widely used probabilistic model for this type of sequential decision-making problem 
		is the stochastic MAB setting \cite{Bubeck2012}. In this model, 
		the reward $r^{(a)}$ is viewed as the realization of an RV 
		with unknown mean $\mu^{(a)}$. Ideally, we would always choose the 
		action with the largest expected reward $\mu^{(a)}$, but these 
		means are unknown and must be estimated from observed data. Simply 
		choosing the action with the largest estimate $\widehat{\mu}^{(a)}$ can 
		lead to suboptimal outcomes due to estimation uncertainty. The UCB strategy 
		addresses this by selecting actions not only based on their estimated means but 
		also by incorporating a term that reflects the uncertainty in these estimates—favoring 
		actions with high potential reward and high uncertainty. Theoretical guarantees 
		for the performance of UCB strategies, including logarithmic regret bounds, are established in \cite{Bubeck2012}.
					\\ 
		See also: ML, reward, probabilistic model, MAB, model, realization, RV, mean, data, uncertainty, regret.},
	first={upper confidence bound (UCB)},text={UCB} 
}

\newglossaryentry{mab}
{name={multi-armed bandit (MAB)},
	description={A MAB problem models 
		a repeated decision-making scenario in which, at each time step $k$, a learner must 
		choose one out of several possible actions, often referred to as arms, from a finite 
		set $\actionset$. Each arm $a \in \actionset$ yields a stochastic reward $r^{(a)}$ 
		drawn from an unknown probability distribution with mean $\mu^{(a)}$. 
		The learner’s goal is to maximize the cumulative reward over time by 
		strategically balancing exploration (i.e., gathering information about 
		uncertain arms) and exploitation (i.e., selecting arms known to perform well). 
		This balance is quantified by the notion of regret, which measures the performance 
		gap between the learner's strategy and the optimal strategy that always selects the best arm. 
		MAB problems form a foundational model in online learning, reinforcement learning, 
		and sequential experimental design \cite{Bubeck2012}.
					\\ 
		See also: reward, probability distribution, mean, regret, model.},
	first={MAB},text={MAB}
}



\newglossaryentry{optimism in the face of uncertainty}
{name={optimism in the face of uncertainty},
	description={ML methods learn model parameters ${\bf w}$ 
		according to some performance criterion $\bar{f}({\bf w})$. However, they usually 
		cannot access $\bar{f}({\bf w})$ directly but rely on an estimate (or approximation) 
		$f({\bf w})$ of $\bar{f}({\bf w})$. As a case in point, ERM-based methods use 
		the average loss on a given dataset (i.e., the training set) as an estimate 
		for the risk of a hypothesis. Using a probabilistic model, one can construct 
		a confidence interval 
	$\big[ l^{({\bf w})},  u^{({\bf w})} \big]$ for each choice ${\bf w}$ for the model parameters.
		One simple construction is $l^{({\bf w})} := f({\bf w}) - \sigma/2$, $u^{({\bf w})} := f({\bf w})+ \sigma/2$, 
	    with $\sigma$ being a measure of the (expected) deviation of $f({\bf w})$ from $\bar{f}({\bf w})$.
	We can also use other constructions for this interval as long as they ensure that $\bar{f}({\bf w}) \in\big[ l^{({\bf w})},  u^{({\bf w})} \big]$ 
	with a sufficiently high probability. An optimist chooses the model parameters 
	according to the most favorable - yet still plausible - value $\tilde{f}({\bf w}) :=  l^{({\bf w})}$ 
	of the performance criterion. Two examples of ML methods that use such an optimistic 
	construction of an objective function are SRM \cite[Ch. 11]{ShalevMLBook} and UCB methods 
	for sequential decision making \cite[Sec. 2.2]{Bubeck2012}. 
		\begin{figure}[H]
				\begin{center}
\begin{tikzpicture}[x=3cm, y=1cm]
  % Filled band around the quadratic curve with different boundary curves
\fill[blue!10] 
(-1, 5) -- plot[domain=-2:1, samples=100] ({\x+1}, {\x*\x + 1}) -- 
plot[domain=1:-2, samples=100] ({\x+1}, {\x*\x - 0.5}) -- cycle;
  \node[anchor=west] at (2, 4) {$f({\bf w})$};
  \draw[line width=1, domain=-2:1, samples=100,dashed] plot  ({\x+1}, {\x*\x -0.5}) node[right] {$\tilde{f}({\bf w})$};
   \draw[line width=1, domain=-1:2, samples=100] plot ({\x}, {\x*\x});
  \draw[<->, thick] (1, -0.5) -- (1, 1) node[midway, right] {$\big[ l^{({\bf w})}\!,\!u^{({\bf w})} \big]$};
\end{tikzpicture}
\caption{ML methods learn model parameters ${\bf w}$ by using some estimate of $f({\bf w})$ for 
	the ultimate performance criterion $\bar{f}({\bf w})$. Using a probabilistic model, one can use $f({\bf w})$ to 
	construct confidence intervals $\big[ l^{({\bf w})},  u^{({\bf w})} \big]$ which contain $\bar{f}({\bf w})$  
	with a high probability. The best plausible performance measure for a specific choice ${\bf w}$ of model parameters 
	is $\tilde{f}({\bf w}) := l^{({\bf w})}$.} 
	\end{center}
		\end{figure}
		See also: ML, model parameters, ERM, loss, dataset, training set, risk, hypothesis, probabilistic model, probability, objective function, SRM, UCB.},first={optimism in the face of uncertainty},text={optimism in the face of uncertainty} 
}

\newglossaryentry{empgraph}
{name={federated learning network (FL network)},
	description={An FL network is an 
		undirected weighted graph whose nodes represent data generators that 
		aim to train a local (or personalized) model. Each node in an FL network 
		represents some device capable of collecting a local dataset 
		and, in turn, train a local model. FL methods learn a local hypothesis $h^{(i)}$, for 
	    each node $i \in \mathcal{V}$, such that it incurs small loss on the local datasets.
	    			\\ 
		See also: FL, graph, data, model, device, local dataset, local model, hypothesis, loss.},first={federated learning network (FL network)},text={FL network} 
}

\newglossaryentry{norm}
{name={norm},
	description={A norm is a function that maps each (vector) element 
		of a vector space to a non-negative real number. This function must be 
		homogeneous and definite, and it must satisfy the triangle inequality \cite{HornMatAnalysis}.},
	first={norm},text={norm} 
}

\newglossaryentry{dualnorm}
{name={dual norm},
description={Every norm $\left\Vert  {\cdot} \right\Vert_{}$ defined on an Euclidean space $\mathbb{R}^{d}$ 
		has an associated dual norm, which is denoted $\left\Vert  {\cdot} \right\Vert_{*}$ and defined as 
		$\left\Vert  {{\bf y}} \right\Vert_{*} := \sup_{\Vert  {{\bf x}} \Vert{} \le 1} {\bf y}^{T} {\bf x}$. 
		The dual norm measures the largest possible inner product between ${\bf y}$ 
		and any vector in the unit ball of the original norm. For further details, see 
		\cite[Sec.~A.1.6]{BoydConvexBook}.
					\\ 
		See also: norm, Euclidean space.},
	first={dual norm},
	text={dual norm}
}

\newglossaryentry{geometricmedian}{
	name={geometric median (GM)},
	description={The GM of a set of input vectors ${\bf x}^{(1)}, \ldots, {\bf x}^{(m)}$ 
		in $\mathbb{R}^{d}$ is a point ${\bf z} \in \mathbb{R}^{d}$ that 
		minimizes the sum of distances to the vectors \cite{BoydConvexBook} such that 
		\begin{equation} 
			\label{equ_geometric_median}
		{\bf z} \in \argmin_{{\bf y} \in \mathbb{R}^{d}} \sum_{r=1}^{m} \left\Vert  {{\bf y} - {\bf x}^{(r)}} \right\Vert_{2}.
		\end{equation} 
	Figure~\ref{opt_cond_GM} illustrates a fundamental property of the GM:
	If ${\bf z}$ does not coincide with any of the input vectors, then the unit vectors pointing 
	from ${\bf z}$ to each ${\bf x}^{(r)}$ must sum to zero - this is the zero-subgradient  
	(optimality) condition of \eqref{equ_geometric_median}. It turns out that the solution to 
	\eqref{equ_geometric_median} cannot be arbitrarily pulled away from trustworthy input vectors as long as they 
	are the majority \cite[Th. 2.2]{Lopuhaae1991}.
	  	\begin{figure}[H]
  		\begin{center}
			\begin{tikzpicture}[scale=2, thick, >=stealth]
%				% Central model w
				\coordinate (w) at (3,0);
				\fill (w) circle (1.2pt) node[below right] {${\bf z}$};
				% Good nodes
				\coordinate (w2) at (0.5,0.3);
				\coordinate (w3) at (0.7,0.7);
				\fill (w2) circle (1pt) node[above left] {${\bf x}^{(1)}$};
				\fill (w3) circle (1pt) node[above left] {${\bf x}^{(2)}$};
%				%	\fill (wk) circle (1pt) node[above left] {$\mathbf{w}^{(k)}$};
%				% Dashed lines from w to good nodes
				\draw[dashed] (w) -- (w2);
				\draw[dashed] (w) -- (w3);
%				% Draw unit vectors (scaled to 1cm)
				\draw[->, thick, red] (w) -- ($(w)!1cm!(w2)$) ;
				\draw[->, thick, red] (w) -- ($(w)!1cm!(w3)$) node[pos=0.9, right,yshift=7pt] {$\frac{{\bf x}^{(2)}- {\bf z}}{\left\Vert  {{\bf x}^{(2)}-{\bf z}} \right\Vert_{2}}$};
%				\node at (-0.2,1.4) {\textbf{Clean}};
				\coordinate (w4) at (5,0.2);
				\node at (5,0.4) {\textbf{Perturbed}};
				\fill (w4) circle (1pt) node[below left] {${\bf x}^{(3)}$};
				\draw[->, thick, red] (w) -- ($(w)!1cm!(w4)$) ;
%		% Optional dotted line from w to bad
		\end{tikzpicture}
		\caption{\label{opt_cond_GM}
			Consider a solution ${\bf z}$ of \eqref{equ_geometric_median} that does not coincide 
			with any of the input vectors. The optimality condition for \eqref{equ_geometric_median} 
			requires that the unit vectors from ${\bf z}$ to the input vectors sum to zero.}
			\end{center}
	\end{figure}
		See also: subgradient.
},
	first={geometric median},
	text={GM}
}


\newglossaryentry{explanation}
{name={explanation},
	description={One approach to make ML methods transparent is to provide an 
		explanation along with the prediction delivered by an 
		ML method. Explanations can take on many different forms. An explanation 
		could be some natural text or some quantitative measure for the importance 
		of individual features of a data point \cite{Molnar2019}. We can also 
		use visual forms of explanations, such as intensity plots for image classification \cite{GradCamPaper}.
					\\ 
		See also: ML, prediction, feature, data point, classification.},
	first={explanation},text={explanation} 
}

\newglossaryentry{risk}
{name={risk},
	description={Consider a hypothesis $h$ used to predict the label 
		$y$ of a data point based on its features ${\bf x}$. We measure 
		the quality of a particular prediction using a loss function $L\left(({\bf x},y),h \right)$. 
		If we interpret data points as the realizations of i.i.d. RVs, 
		also the $L\left(({\bf x},y),h \right)$ becomes the realization 
		of an RV. The i.i.d.\ assumption allows us to define the risk of a hypothesis 
		as the expected loss $\mathbb{E}  \big\{L\left(({\bf x},y),h \right) \big\}$. 
		Note that the risk of $h$ depends on both the specific choice for the loss function and the 
		probability distribution of the data points.
					\\ 
		See also: hypothesis, label, data point, feature, prediction, loss function, realization, i.i.d. RV, i.i.d.\ assumption, loss, probability distribution.},
	first={risk},text={risk} 
}

\newglossaryentry{actfun}
{name={activation function},
	description={Each artificial neuron within an ANN is 
		assigned an activation function $\sigma(\cdot)$ that maps a weighted combination of 
		the neuron inputs $x_{1},\ldots,x_{d}$ to a single output 
		value $a = \sigma\big(w_{1} x_{1}+\ldots+w_{d} x_{d} \big)$. 
		Note that each neuron is parametrized by the weights $w_{1},\ldots,w_{d}$.
					\\ 
		See also: ANN, weights.},
first={activation function},text={activation function} 
}

\newglossaryentry{distributedalgorithm}
{name={distributed algorithm},
	description={A distributed algorithm is an algorithm designed for 
		a special type of computer, i.e., a collection of interconnected computing devices (or nodes). 
		These devices communicate and coordinate their local computations by exchanging 
		messages over a network \cite{IntroDistAlg}, \cite{ParallelDistrBook}. Unlike a classical algorithm, 
		which is implemented on a single device, a distributed algorithm is 
		executed concurrently on multiple devices with computational capabilities. 
		Similar to a classical algorithm, a distributed algorithm can be modeled as a 
		set of potential executions. However, each execution in the distributed setting involves 
		both local computations and message-passing events. A generic execution might look as 
		follows:
		\[
		\begin{array}{l}
			\text{Node 1: } {\rm input}_1, s_1^{(1)}, s_2^{(1)}, \ldots, s_{T_1}^{(1)}, {\rm output}_1; \\
			\text{Node 2: } {\rm input}_2, s_1^{(2)}, s_2^{(2)}, \ldots, s_{T_2}^{(2)}, {\rm output}_2; \\
			\quad \vdots \\
			\text{Node N: } {\rm input}_N, s_1^{(N)}, s_2^{(N)}, \ldots, s_{T_N}^{(N)}, {\rm output}_N.
		\end{array}
		\]
		Each device $i$ starts from its own local input and performs a sequence of 
		intermediate computations $s_{k}^{(i)}$ at discrete time instants $k = 1, \dots, T_i$. 
		These computations may depend on both the previous local computations at the device 
		and the messages received from other devices. One important application of distributed 
		algorithms is in FL where a network of devices collaboratively trains a personal model 
		for each device. 
					\\ 
		See also: algorithm, device, FL, model.
		},
	first={distributed algorithm}, text={distributed algorithm}
}


\newglossaryentry{algorithm}
{name={algorithm}, plural={algorithms},
  description={An algorithm is a precise, step-by-step specification for 
  	how to produce an output from a given input within a finite number of computational steps \cite{Cormen:2022aa}. 
    For example, an algorithm for training a linear model explicitly describes how to 
	transform a given training set into model parameters through a sequence of gradient steps. 
    This informal characterization can be formalized rigorously via different mathematical models \cite{Sipser2013}. 
    One very simple model of an algorithm is a collection of possible executions. Each execution is a sequence in the form of
    $${\rm input},s_1,s_2,\ldots,s_T,{\rm output}$$ 
    that respects the constraints inherent to the computer executing the algorithm.
	Algorithms may be deterministic, where each input results in a single execution,
	or randomized, where executions can vary probabilistically. Randomized algorithms 
	can thus be analyzed by modeling execution sequences as outcomes of random experiments, 
	viewing the algorithm as a stochastic process \cite{BertsekasProb}, \cite{RandomizedAlgos}, \cite{Gallager13}.
	Crucially, an algorithm encompasses more than just a mapping from input to output; it also includes 
	the intermediate computational steps $s_1,\ldots,s_T$. 
	%. In \textbf{online algorithms}, these intermediate computational steps  can dynamically incorporate additional input data as the execution progresses.
				\\ 
		See also: linear model, training set, model parameters, gradient step, model.
	},
	first={algorithm},text={algorithm} 
}

\newglossaryentry{onlinelearning}
{name={online learning},
	description={
		Some ML methods  are designed to process data in a sequential 
		manner, updating their model parameters as new data points become available—one at a time. 
		A typical example is time series data, such as daily minimum and maximum temperatures 
		recorded by a FMI weather station. These values form a chronological sequence 
		of observations. In online learning, the hypothesis (or its model parameters) is refined 
		incrementally with each newly observed data point, without revisiting past data.  \\ 
		See also: ML, data, model parameters, data point, FMI, hypothesis, online GD, online algorithm. 
	},
	first={online learning},text={online learning} 
}

\newglossaryentry{onlinealgorithm}
{name={online algorithm},
	description={An online algorithm processes input data incrementally, 
		receiving data points sequentially and making decisions or producing outputs (or decisions) immediately 
		without having access to the entire input in advance \cite{PredictionLearningGames}, \cite{HazanOCO}. 
		Unlike an offline algorithm, which has the entire input available from the start, an online algorithm 
		must handle uncertainty about future inputs and cannot revise past decisions. Similar to an 
		offline algorithm, we also represent an online algorithm formally as a collection of possible 
		executions. However, the execution sequence for an online algorithm has a distinct structure:
		$${\rm in}_{1},s_1,{\rm out}_{1},{\rm in}_{2},s_2,{\rm out}_{2},\ldots,{\rm in}_{T},s_T,{\rm out}_{T}.$$ 
		Each execution begins from an initial state (i.e., \(\text{in}_{1}\)) and proceeds through alternating 
		computational steps, outputs (or decisions), and inputs. Specifically, at step \(k\), 
		the algorithm performs a computational step \(s_{k}\), generates an output \(\text{out}_{k}\), 
		and then subsequently receives the next input (data point) \(\text{in}_{k+1}\). A 
		notable example of an online algorithm in ML is online GD, which incrementally 
		updates model parameters as new data points arrive. 
					\\ 
		See also: algorithm, data, data point, uncertainty, ML, online GD, model parameters, online learning.
	},
	first={online algorithm},text={online algorithm} 
}



%\newglossaryentry{transparency}
%{name={transparency},
%	description={Transparency is a key requirement for 
%		trustworthy AI \cite{HLEGTrustworhtyAI}. In the context of ML methods, 
%		such as ERM-based methods, transparency is mainly used synonymously 
%		for explainability \cite{gallese2023ai,JunXML2020}. However, in the wide 
%		context of AI systems, transparency also includes providing information 
%		about limitations and reliability of the AI system. As a point in case, logistic regression provides a 
%		quantitative measure of the reliability of a classification in the form of the value $|h({\bf x})|$. 
%		Transparency also includes the user interface, by requiring to clearly indicate when a user is 
%		interaction with an AI system. Another component of transparency is the documentation 
%		of the system’s purpose, design choices, and intended use cases \cite{Shahriari2017,DatasheetData2021,10.1145/3287560.3287596}. },
%	first={transparency},text={transparency} 
%}

\newglossaryentry{transparency}
{name={transparency},
	description={Transparency is a fundamental requirement for 
		trustworthy AI \cite{HLEGTrustworhtyAI}. In the context of ML 
		methods, transparency is often used interchangeably with explainability 
		\cite{JunXML2020}, \cite{gallese2023ai}. However, in the broader scope of AI 
		systems, transparency extends beyond explainability and includes providing information 
		about the system’s limitations, reliability, and intended use. 
		In medical diagnosis systems, transparency requires disclosing the confidence level 
		for the predictions delivered by a trained model. In credit scoring, 
		AI-based lending decisions should be accompanied by explanations of 
		contributing factors, such as income level or credit history. These explanations 
		allow humans (e.g., a loan applicant) to understand and contest automated decisions. 
		Some ML methods inherently offer transparency. For example, logistic regression 
		provides a quantitative measure of classification reliability through the value $|h({\bf x})|$. 
		Decision trees are another example, as they allow human-readable decision rules \cite{rudin2019stop}.
		Transparency also requires a clear indication when a user is engaging with an AI system. 
		For example, AI-powered chatbots should notify users that they are interacting with an 
		automated system rather than a human. Furthermore, transparency encompasses comprehensive 
		documentation detailing the purpose and design choices underlying the AI system. 
		For instance, model datasheets \cite{DatasheetData2021} and AI system cards \cite{10.1145/3287560.3287596} 
		help practitioners understand the intended use cases and limitations of an AI system \cite{Shahriari2017}.
					\\ 
		See also: trustworthy AI, ML, explainability, AI, prediction, model, logistic regression, classification, decision tree.},
	first={transparency}, text={transparency} 
}



\newglossaryentry{sensattr}
{name={sensitive attribute}, plural={sensitive attributes},
	description={ML revolves around learning a hypothesis map that allows 
		us to predict the label of a data point from its features. In some 
		applications, we must ensure that the output delivered by an ML system does 
		not allow us to infer sensitive attributes of a data point. Which part 
		of a data point is considered a sensitive attribute is a design 
		choice that varies across different application domains.
					\\ 
		See also: ML, hypothesis, label, data point, feature.},
	first={sensitive attribute},text={sensitive attribute} 
}


\newglossaryentry{sbm}
{name={stochastic block model (SBM)},
	description={The SBM is a 
		probabilistic generative model for an undirected graph $\mathcal{G} = \big( \mathcal{V}, \mathcal{E} \big)$ 
		with a given set of nodes $\mathcal{V}$ \cite{AbbeSBM2018}. In its most basic variant, 
		the SBM generates a graph by first randomly assigning each node $i \in \mathcal{V}$ to 
		a cluster index $c_{i} \in \{1,\ldots,k\}$. A pair of different nodes in the 
		graph is connected by an edge with probability $p_{i,i'}$ that depends 
		solely on the labels $c_{i}, c_{i'}$. 
		The presence of edges between different pairs of 
		nodes is statistically independent.
					\\ 
		See also: model, graph, cluster, probability, label. },
	first={stochastic block model (SBM)},text={SBM} 
}

\newglossaryentry{deepnet}
{name={deep net}, plural={deep nets},
	description={A deep net is an ANN with a (relatively) large number of 
	hidden layers. Deep learning is an umbrella term for ML methods that use a deep 
	net as their model \cite{Goodfellow-et-al-2016}.
				\\ 
		See also: ANN, ML, model.},
	first={deep net},text={deep net} 
}

\newcommand{\gaussiancenter}{3}

\newglossaryentry{baseline}
{name={baseline},
    description={Consider some ML method that produces a learned 
    	hypothesis (or trained model) $\hat{h} \in \mathcal{H}$. We evaluate the quality of a trained model 
    by computing the average loss on a test set. But how can we assess 
    whether the resulting test set performance is sufficiently good? How can we 
    determine if the trained model performs close to optimal and there is little point 
    in investing more resources (for data collection or computation) to improve it? 
    To this end, it is useful to have a reference (or baseline) level against which 
    we can compare the performance of the trained model. Such a reference value 
    might be obtained from human performance, e.g., the misclassification rate of dermatologists 
    who diagnose cancer from visual inspection of skin \cite{SkinHumanAI}. Another source for a baseline is an existing, 
    but for some reason unsuitable, ML method. For example, the existing ML method 
    might be computationally too expensive for the intended ML application. 
    Nevertheless, its test set error can still serve as a baseline. Another, somewhat more principled, 
    approach to constructing a baseline is via a probabilistic model. In many cases, given a probabilistic model $p({\bf x},y)$,  
    we can precisely determine the minimum achievable risk among any hypotheses
    (not even required to belong to the hypothesis space $\mathcal{H}$) \cite{LC}. 
    This minimum achievable risk (referred to as the Bayes risk) is the risk 
    of the Bayes estimator for the label $y$ of a data point, given
    its features ${\bf x}$. Note that, for a given choice of loss function, the 
    Bayes estimator (if it exists) is completely determined by the probability distribution $p({\bf x},y)$ \cite[Ch. 4]{LC}. 
    However, computing the Bayes estimator and Bayes risk presents two 
    main challenges:
    \begin{enumerate}[label=\arabic*)]
    	\item The probability distribution $p({\bf x},y)$ is unknown and 
    needs to be estimated.
    	\item Even if $p({\bf x},y)$ is known, 
    it can be computationally too expensive to compute the Bayes risk exactly \cite{cooper1990computational}. 
   \end{enumerate}
A widely used probabilistic model is the multivariate normal distribution $\left( {\bf x},y \right) \sim \mathcal{N}({\bm \mu},{\bm \Sigma})$ 
for data points characterized by numeric features and labels.
Here, for the squared error loss, the Bayes estimator is given by the posterior 
mean $\mu_{y|{\bf x}}$ of the label $y$, given the 
features ${\bf x}$ \cite{LC}, \cite{GrayProbBook}. The corresponding Bayes risk 
is given by the posterior variance 
$\sigma^{2}_{y|{\bf x}}$ (see Fig. \ref{fig_post_baseline_dict}).
	\begin{figure}[H]
		\begin{center}
		\begin{tikzpicture}
			% Axes
			\draw[->] (-1,0) -- (7,0) node[right] {$y$}; % x-axis
			% Gaussian distribution centered at \gaussiancenter with variance 1
			\draw[thick,domain=-1:7,smooth,variable=\x] 
			  plot ({\x}, {2*exp(-0.5*((\x-\gaussiancenter)^2))});
			% Dashed line indicating the mean of the Gaussian
			\draw[dashed] (\gaussiancenter,0) -- (\gaussiancenter,2.5);
			\node[anchor=south] at ([yshift=-5pt] \gaussiancenter,2.5) {\small $\mu_{y|{\bf x}}$};
			% Double arrow indicating the variance
			\draw[<->,thick] (\gaussiancenter-1,1) -- (\gaussiancenter+1,1.0);
			\node[anchor=west] at ([yshift=2pt] \gaussiancenter,1.2) {\small $\sigma_{y|{\bf x}}$};
			% Posterior variance label
			%\node[anchor=south east] at (\gaussiancenter-0.5,1.8) {\small Posterior Variance};
			% x-axis marks with crosses
			  % x-axis marks with crosses
  			\foreach \x in {0.5} {
				\node[red] at (\x, 0) {\bf \large $\times$};
 			 }
  % h(x) label for the first cross
  			\node[anchor=north] at (0.5,-0.2) {\small $\hat{h}({\bf x})$};
		  \end{tikzpicture}
		\end{center}
		\caption{If the features and the label of a data point are drawn from a multivariate normal distribution, we 
		can achieve the minimum risk (under squared error loss) by using the Bayes estimator $\mu_{y|{\bf x}}$ 
		to predict the label $y$ of a data point with features ${\bf x}$. The corresponding 
		minimum risk is given by the posterior variance $\sigma^{2}_{y|{\bf x}}$. We can use 
		this quantity as a baseline for the average loss of a trained model $\hat{h}$. \label{fig_post_baseline_dict}}
	\end{figure}
		See also: ML, hypothesis, model, loss, test set, data, probabilistic model, minimum, risk, hypothesis space, Bayes risk, Bayes estimator, label, data point, feature, loss function, probability distribution, multivariate normal distribution, squared error loss, mean, variance.},
    first={baseline},text={baseline}
}

\newglossaryentry{spectrogram}
{name={spectrogram},
	description={
		A spectrogram represents the time-frequency distribution of the energy of a time signal $x(t)$.  
		Intuitively, it quantifies the amount of signal energy present within a specific time segment 
		$[t_{1},t_{2}] \subseteq \mathbb{R}$ and frequency interval $[f_{1},f_{2}]\subseteq \mathbb{R}$. 
		Formally, the spectrogram of a signal is defined as the squared magnitude of its 
		short-time Fourier transform (STFT) \cite{cohen1995time}.
        Fig. \ref{fig:spectrogram_dict} depicts a time signal along with its spectrogram. 
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{assets/spectrogram.png}
		\caption{Left: A time signal consisting of two modulated Gaussian pulses. Right: An intensity 
		plot of the spectrogram.
		\label{fig:spectrogram_dict}}
	\end{figure}
        The intensity plot of its spectrogram can serve as an image of a signal. A 
		simple recipe for audio signal classification is to feed this signal image 
		into deep nets originally developed for image classification and object detection \cite{Li:2022aa}. 
		It is worth noting that, beyond the spectrogram, several alternative representations exist 
		for the time-frequency distribution of signal energy \cite{TimeFrequencyAnalysisBoashash}, \cite{MallatBook}.
					\\ 
		See also: classification, deep net.
		}, 
	first={spectrogram},text={spectrogram} 
}

\newglossaryentry{graphclustering}
{name={graph clustering},
	description={Graph clustering aims at 
		clustering data points that are represented as the nodes 
		of a graph $\mathcal{G}$. The edges of $\mathcal{G}$ represent 
		pairwise similarities between data points. Sometimes we
		can quantify the extend of these similarities by an edge weight \cite{FlowSpecClustering2021}, \cite{Luxburg2007}.
					\\ 
		See also: graph, clustering, data point, edge weight. }, 
	first={graph clustering},text={graph clustering} 
}

\newglossaryentry{specclustering}
{name={spectral clustering},
	description={Spectral clustering is a particular instance of 
		graph clustering, i.e., it clusters data points 
		represented as the nodes $i=1,\ldots,n$ of a graph $\mathcal{G}$. 
		Spectral clustering uses the eigenvectors of the Laplacian matrix ${\bf L}^{(\mathcal{G})}$ 
		to construct feature vectors ${\bf x}^{(i)} \in \mathbb{R}^{d}$ 
		for each node (i.e., for each data point) $i=1,\ldots,n$. We can feed these feature vectors 
		into Euclidean space-based clustering methods, such as $k$-means 
		or soft clustering via GMM. Roughly speaking, the feature vectors of nodes 
		belonging to a well-connected subset (or cluster) of nodes in $\mathcal{G}$ are located 
		nearby in the Euclidean space $\mathbb{R}^{d}$ (see Fig. \ref{fig_lap_mtx_specclustering_dict}). 
		\begin{figure}[H]
			\begin{center}
				\begin{minipage}{0.4\textwidth}
			\begin{tikzpicture}
				% Define the style for filled nodes
				\begin{scope}[every node/.style={circle, fill=black, inner sep=0pt, minimum size=0.3cm}]
					% Define nodes
					\node (1) at (0,0) {};
					\node (2) [below left=of 1, xshift=-0.2cm, yshift=-1cm] {};
					\node (3) [below right=of 1, xshift=0.2cm, yshift=-1cm] {};
					\node (4) [below=of 1, yshift=0.5cm] {}; % Isolated node
				\end{scope}
				% Draw edges
				\draw (1) -- (2);
				\draw (1) -- (3);
				% Add labels (separate from filled nodes)
				\node[above=0.2cm] at (1) {$i=1$};
				\node[left=0.3cm] at (2) {$2$};
				\node[right=0.3cm] at (3) {$3$};
				\node[below=0.2cm] at (4) {$4$};
			\end{tikzpicture}
				\end{minipage} 
				\hspace*{5mm}
				\begin{minipage}{0.4\textwidth}
					\begin{equation} 
						{\bf L}^{(\mathcal{G})}\!=\!
						\begin{pmatrix} 
							2 & -1 & -1 & 0 \\ 
							-1 & 1 & 0 & 0 \\  
							-1 & 0 & 1 & 0 \\ 
							0 & 0 & 0 & 0 
						\end{pmatrix}\!=\!\mathbf{V} {\bm \Lambda} \mathbf{V}^{T}  
						\nonumber
					\end{equation} 
				\end{minipage}
				\vspace*{20mm}\\
				  \begin{minipage}{0.4\textwidth}
				\begin{tikzpicture}[scale=3]
%					% Axes
					\draw[->] (-0.2, 0) -- (1.2, 0) node[right] {$v^{(1)}_{i}$};
					\draw[->] (0, -0.2) -- (0, 1.2) node[above] {$v^{(2)}_{i}$};
%					
%					% Tailored tick marks and labels
%					\draw (0,0) node[below left] {$0$};
%					\draw (1/sqrt(3), 0) node[below] {$\frac{1}{\sqrt{3}}$} -- ++(0,0.05);
%					\draw (0, 1) node[left] {$1$} -- ++(0.05,0);
%					
%					 Data points
					\filldraw[blue] (0.577, 0) circle (0.03cm) node[above right] {$i=1,2,3$};
					\filldraw[blue] (0.577, 0) circle (0.03cm); % Second point overlaps
					\filldraw[blue] (0.577, 0) circle (0.03cm); % Third point overlaps
					\filldraw[red] (0, 1) circle (0.03cm) node[above right] {$4$};
%					% Grid for reference
%					\draw[dashed, gray] (1/sqrt(3), 0) -- (1/sqrt(3), 1);
%					\draw[dashed, gray] (0, 1) -- (1, 1);
				\end{tikzpicture}
				\end{minipage} 
    		\begin{minipage}{0.4\textwidth}
										\begin{align}
											& \mathbf{V} = \big( {\bf v}^{(1)},{\bf v}^{(2)},{\bf v}^{(3)},{\bf v}^{(4)} \big) \nonumber \\
											&	\mathbf{v}^{(1)}\!=\!\frac{1}{\sqrt{3}} \begin{pmatrix} 1 \\ 1 \\ 1 \\ 0 \end{pmatrix}, \,
												\mathbf{v}^{(2)}\!=\!\begin{pmatrix} 0 \\ 0 \\ 0 \\ 1 \end{pmatrix} \nonumber 
												\end{align}
				\end{minipage} 
				\caption{\label{fig_lap_mtx_specclustering_dict} {\bf Top.} Left: An undirected graph 
					$\mathcal{G}$ with four nodes $i=1,2,3,4$, each representing a data point. Right: The Laplacian matrix 
					${\bf L}^{(\mathcal{G})}  \in \mathbb{R}^{4 \times 4}$ and its EVD. 
					{\bf Bottom.} Left: A scatterplot of data points using the feature vectors 
					${\bf x}^{(i)} = \big( v^{(1)}_{i},v^{(2)}_{i} \big)^{T}$. 
					Right: Two eigenvectors ${\bf v}^{(1)},{\bf v}^{(2)} \in \mathbb{R}^{d}$ 
					corresponding to the eigenvalue $\lambda=0$ of the Laplacian matrix ${\bf L}^{(\mathcal{G})}$. 
					} 
			\end{center}
		\end{figure}
		See also: clustering, graph clustering, data point, graph, eigenvector, Laplacian matrix, feature vector, Euclidean space, $k$-means, soft clustering, GMM, cluster, EVD, scatterplot, eigenvalue.
	\newpage}, 
	first={spectral clustering},text={spectral clustering} 
}

\newglossaryentry{flowbasedclustering}
{name={flow-based clustering},
	description={Flow-based clustering groups the nodes 
		of an undirected graph by applying $k$-means clustering to node-wise 
		feature vectors. These feature vectors are built from network flows between 
		carefully selected sources and destination nodes \cite{FlowSpecClustering2021}. 
					\\ 
		See also: clustering, graph, $k$-means, feature vector.}, 
	first={flow-based clustering},text={flow-based clustering} 
}



\newglossaryentry{esterr}
{name={estimation error},
	description={Consider data points, each with feature vector ${\bf x}$ and label 
		$y$. In some applications, we can model the relation between the feature vector and the label
		of a data point as $y = \bar{h}({\bf x}) + \varepsilon$. Here, we 
		use some true underlying hypothesis $\bar{h}$ and a noise term $\varepsilon$ 
		which summarizes any modeling or labeling errors. The estimation error incurred by an ML 
		method that learns a hypothesis $\widehat{h}$, e.g., using ERM, is defined as 
		$\widehat{h}({\bf x}) - \bar{h}({\bf x})$, for some feature vector. 
		For a parametric hypothesis space, which consists of hypothesis maps determined by 
		model parameters ${\bf w}$, we can define the estimation error as $\Delta {\bf w} = \widehat{{\bf w}} - \overline{{\bf w}}$ \cite{kay}, \cite{hastie01statisticallearning}.
					\\ 
		See also: data point, feature vector, label, hypothesis, ML, ERM, hypothesis space, model parameters.},
	first={estimation error},text={estimation error} 
}


\newglossaryentry{dob}
{name={degree of belonging},
	description={Degree of belonging is a number that indicates the extent to which a data point 
		belongs to a cluster \cite[Ch. 8]{MLBasics}. The degree of belonging can be 
		interpreted as a soft cluster assignment. Soft clustering methods can 
		encode the degree of belonging by a real number in the interval $[0,1]$. 
		Hard clustering is obtained as the extreme case when the degree of belonging 
		only takes on values $0$ or $1$.
					\\ 
		See also: data point, cluster, soft clustering, hard clustering.}, first={degree of belonging},text={degree of belonging} 
}

\newglossaryentry{msee}
{name={mean squared estimation error (MSEE)},
	description={Consider an ML method that 
		learns model parameters $\widehat{{\bf w}}$ based on some dataset $\mathcal{D}$. 
		If we interpret the data points in $\mathcal{D}$ as i.i.d. realizations of an RV ${\bf z}$, 
		we define the estimation error $\Delta {\bf w} := \widehat{w} - \overline{{\bf w}}$. 
		Here, $\overline{{\bf w}}$ denotes the true model parameters of the probability distribution 
		of ${\bf z}$. The MSEE is 
		defined as the expectation $\mathbb{E}  \big\{ \big\| \Delta {\bf w} \big\|^{2} \big\}$ of the 
		squared Euclidean norm of the estimation error \cite{LC}, \cite{kay}.
					\\ 
		See also: ML, model parameters, dataset, data point, i.i.d., realization, RV, estimation error, probability distribution, expectation, norm, mean.},
	first={mean squared estimation error (MSEE)},text={MSEE} 
}

\newglossaryentry{gtvmin}
{name={generalized total variation minimization (GTVMin)},
	description={GTVMin is an instance of RERM 
		using the GTV of local model parameters as a regularizer \cite{ClusteredFLTVMinTSP}.
					\\ 
		See also: RERM, GTV, model parameters, regularizer.},
	first={generalized total variation minimization (GTVMin)},text={GTVMin} 
}

\newglossaryentry{regression}
{name={regression},
	description={Regression problems revolve around the 
		prediction of a numeric label solely from the features of a data point \cite[Ch. 2]{MLBasics}.
					\\ 
		See also: label, feature, data point.},
	first={regression},text={regression} 
}

\newglossaryentry{acc}
{name={accuracy},
	description={Consider data points characterized by features ${\bf x} \in \mathcal{X}$ and 
		a categorical label $y$ which takes on values from a finite label space $\mathcal{Y}$. The 
		accuracy of a hypothesis $h: \mathcal{X} \rightarrow \mathcal{Y}$, when applied 
		to the data points in a dataset $\mathcal{D} = \big\{ \big({\bf x}^{(1)}, y^{(1)} \big), \ldots, \big({\bf x}^{(m)},y^{(m)}\big) \big\}$, 
		is then defined as $1 - (1/m)\sum_{r=1}^{m} L^{(0/1)}\left(\big({\bf x}^{(r)},y^{(r)}\big),h \right)$ using the $0/1$ loss $L^{(0/1)}\left(\cdot,\cdot \right)$.
					\\ 
		See also: data point, feature, label space, hypothesis, dataset, $0/1$ loss.},
	first={accuracy},text={accuracy} 
}





\newglossaryentry{expert}
{name={expert},
	description={ML aims to learn a hypothesis $h$ that accurately predicts the label 
		of a data point based on its features. We measure the prediction error using 
		some loss function. Ideally, we want to find a hypothesis that incurs minimal loss 
		on any data point. We can make this informal goal precise via the i.i.d.\ assumption 
		and by using the Bayes risk as the baseline for the (average) loss of a hypothesis. 
		An alternative approach to obtaining a baseline is to use the hypothesis $h'$ learned 
		by an existing ML method. We refer to this hypothesis $h'$ as an expert \cite{PredictionLearningGames}. Regret minimization methods learn a hypothesis
		that incurs a loss comparable to the best expert \cite{PredictionLearningGames}, \cite{HazanOCO}.
					\\ 
		See also: ML, hypothesis, label, data point, feature, prediction, loss function, loss, i.i.d.\ assumption, Bayes risk, baseline, regret.},
	first={expert},text={expert} 
}

\newglossaryentry{nfl}
{name={networked federated learning (NFL)},
	description={NFL refers 
		to methods that learn personalized models in a distributed fashion. These methods learn from local datasets 
		that are related by an intrinsic network structure.
					\\ 
		See also: model, local dataset, FL.},
 first={networked federated learning (NFL)},text={NFL} 
}




\newglossaryentry{regret}
{name={regret},
	description={The regret of a hypothesis $h$ relative to 
		another hypothesis $h'$, which serves as a baseline, 
		is the difference between the loss incurred by $h$ and the loss 
		incurred by $h'$ \cite{PredictionLearningGames}. 
		The baseline hypothesis $h'$ is also referred to as an expert.
					\\ 
		See also: hypothesis, baseline, loss, expert.},
	first={regret},text={regret} 
}

\newglossaryentry{strcvx}
{name={strongly convex},
	description={A continuously differentiable real-valued 
		function $f({\bf x})$ is strongly convex with coefficient $\sigma$ if $f({\bf y}) \geq f({\bf x}) + \nabla f({\bf x})^{T} ({\bf y} - {\bf x}) + (\sigma/2) \left\Vert  {{\bf y} - {\bf x}} \right\Vert_{2}^{2}$ \cite{nesterov04},\cite[Sec. B.1.1]{CvxAlgBertsekas}.
					\\ 
		See also: differentiable, convex.},
	first={strongly convex},text={strongly convex} 
}

\newglossaryentry{differentiable}
{name={differentiable},
	description={A real-valued function $f: \mathbb{R}^{d} \rightarrow \mathbb{R}$ 
		is differentiable if it can, at any point, be approximated locally by a linear 
		function. The local linear approximation at the point $\mathbf{x}$ is determined 
		by the gradient $\nabla f ( \mathbf{x})$ \cite{RudinBookPrinciplesMatheAnalysis}.
					\\ 
		See also: gradient.},
	first={differentiable},text={differentiable} 
}

\newglossaryentry{gradient}
{name={gradient}, plural={gradients},
	description={For a real-valued function 
	$f: \mathbb{R}^{d} \rightarrow \mathbb{R}: {\bf w} \mapsto f({\bf w})$, 
	if a vector ${\bf g}$ exists such that 
	$\lim_{{\bf w} \rightarrow {\bf w}'} \frac{f({\bf w}) - \big(f({\bf w}')+ {\bf g}^{T} ({\bf w}- {\bf w}') \big) }{\| {\bf w}-{\bf w}'\|}=0$, 
	it is referred to as the gradient of $f$ at ${\bf w}'$. If it exists, the gradient is unique and 
	denoted $\nabla f({\bf w}')$ or $\nabla f({\bf w})\big|_{{\bf w}'}$ \cite{RudinBookPrinciplesMatheAnalysis}.},
	first={gradient},text={gradient} 
}

\newglossaryentry{subgradient}
{name={subgradient}, plural={subgradients},
description={For a real-valued function $f: \mathbb{R}^{d} \rightarrow \mathbb{R}: {\bf w} \mapsto f({\bf w})$, 
		a vector ${\bf a}$ such that $f({\bf w}) \geq  f({\bf w}') +\big({\bf w}-{\bf w}' \big)^{T} {\bf a}$ is 
		referred to as a subgradient of $f$ at ${\bf w}'$ \cite{BertCvxAnalOpt}, \cite{BertsekasNonLinProgr}.},
	first={subgradient},text={subgradient} 
}

\newglossaryentry{fedprox}
{name={FedProx},
	description={FedProx refers to an iterative FL algorithm that alternates between separately training local models and combining the updated local model parameters. In contrast to FedAvg, which uses 
		SGD to train local models, FedProx uses a proximal operator for the training \cite{FedProx2020}.
					\\ 
		See also: FL, algorithm, local model, model parameters, FedAvg, SGD, proximal operator.}, 
	first = {FedProx}, text={FedProx} 
}

\newglossaryentry{relu}
{name={rectified linear unit (ReLU)},
	description={The ReLU is 
		a popular choice for the activation function of a neuron within an ANN. It is defined 
		as $\sigma(z) = \max\{0,z\}$, with $z$ being the weighted input of the artificial 
		neuron.
					\\ 
		See also: activation function, ANN.}, first = {rectified linear unit (ReLU)}, text={ReLU} 
}

\newglossaryentry{hypothesis}
{name={hypothesis},
	description={A hypothesis refers to a map (or function) $h: \mathcal{X} \rightarrow \mathcal{Y}$ from the 
		feature space $\mathcal{X}$ to the label space $\mathcal{Y}$. 
		Given a data point with features ${\bf x}$, we use a hypothesis map $h$
		to estimate (or approximate) the label $y$ using the prediction  
		$\hat{y} = h({\bf x})$. ML is all about learning (or finding) a 
		hypothesis map $h$ such that $y \approx h({\bf x})$ 
		for any data point (having features ${\bf x}$ and label $y$).
					\\ 
		See also: feature space, label space, data point, feature, label, prediction, ML.},
	first={hypothesis},text={hypothesis}  
}



\newglossaryentry{vcdim}
{name={Vapnik–Chervonenkis dimension (VC dimension)},
	description={The VC dimension of an infinite hypothesis space is a widely-used measure 
		for its size. We refer to the literature (see \cite{ShalevMLBook}) for a precise definition of VC dimension 
		as well as a discussion of its basic properties and use in ML.
					\\ 
		See also: hypothesis space, ML.},
	first={Vapnik–Chervonenkis dimension (VC dimension)},text={VC dimension}  
}

\newglossaryentry{effdim}
{name={effective dimension},
	description={The effective dimension $d_{\rm eff} \left( \mathcal{H} \right)$ of 
		an infinite hypothesis space $\mathcal{H}$ is a measure of its size. Loosely speaking, the 
		effective dimension is equal to the effective number of independent tunable model parameters. 
		These parameters might be the coefficients used in a linear map or the 
		weights and bias terms of an ANN.
					\\ 
		See also: hypothesis space, model parameters, parameters, weights, ANN.},
	first={effective dimension},text={effective dimension}  
}

\newglossaryentry{labelspace}
{name={label space},
	description={Consider an ML application that involves data points characterized by features 
		and labels. The label space is constituted by all potential values that the label 
		of a data point can take on. Regression methods, aiming at predicting numeric labels, often
		 use the label space $\mathcal{Y} = \mathbb{R}$. Binary classification methods use a label space 
 		that consists of two different elements, e.g., $\mathcal{Y} =\{-1,1\}$, $\mathcal{Y}=\{0,1\}$, 
		or $\mathcal{Y} = \{ \mbox{``cat image''}, \mbox{``no cat image''} \}$.
					\\ 
		See also: ML, data point, feature, label, regression, classification.}, first={label space},text={label space}  
}

\newglossaryentry{prediction}
{name={prediction}, plural={predictions},
	description={A prediction is an estimate or approximation for some 
		quantity of interest. ML revolves around learning or finding a hypothesis map $h$ 
		that reads in the features ${\bf x}$ of a data point and delivers a prediction 
		$\widehat{y} := h({\bf x})$ for its label $y$.
					\\ 
		See also: ML, hypothesis, feature, data point, label.},
	first={prediction},text={prediction}  
}


\newglossaryentry{histogram}
{name={histogram},
	description={Consider a dataset $\mathcal{D}$ that consists of $m$ data points 
		${\bf z}^{(1)},\ldots,{\bf z}^{(m)}$, each of them belonging to some 
		cell $[-U,U] \times \ldots \times [-U,U] \subseteq \mathbb{R}^{d}$ with side 
		length $U$. We partition this cell evenly into smaller elementary cells with side 
		length $\Delta$. The histogram of $\mathcal{D}$ assigns each elementary cell to 
		the corresponding fraction of data points in $\mathcal{D}$ that fall into this 
		elementary cell. A visual example of such a histogram is provided in Fig.~\ref{fig:histogram}.\\
		\begin{figure}[H]
		\centering
		\begin{tikzpicture}
		\pgfplotsset{compat=1.18}
		\begin{axis}[
		    ybar,
		    ymin=0,
		    ymax=6,
		    bar width=22pt,
		    width=10cm,
		    height=6cm,
		    xlabel={Value},
		    ylabel={Frequency},
		    ytick={1,2,3,4,5,6},
		    xtick={1,2,3,4,5},
		    xticklabels={{[0,1)}, {[1,2)}, {[2,3)}, {[3,4)}, {[4,5)}},
		    enlarge x limits=0.15,
		    title={Histogram of Sample Data}
			]
		\addplot+[fill=blue!40] coordinates {(1,2) (2,5) (3,4) (4,3) (5,1)};
		\end{axis}
		\end{tikzpicture}
		\caption{A histogram representing the frequency of data points falling within discrete value ranges (i.e., bins). Each bar height shows the count of samples in the corresponding interval.}
		\label{fig:histogram}
		\end{figure}
		See also: dataset, data point, sample.
	},
	first={histogram},text={histogram}  
}

\newglossaryentry{bootstrap}
{name={bootstrap},
	description={For the analysis of ML methods, it is often useful to interpret 
		a given set of data points $\mathcal{D} = \big\{ {\bf z}^{(1)},\ldots,{\bf z}^{(m)}\big\}$ 
		as realizations of i.i.d. RVs with a common probability distribution $p({\bf z})$. In general, we 
		do not know $p({\bf z})$ exactly, but we need to estimate it. The bootstrap uses the 
		histogram of $\mathcal{D}$ as an estimator for the underlying probability distribution $p({\bf z})$. 
				\\
		See also: ML, data point, realization, i.i.d., RV, probability distribution, histogram.
	},
	first={bootstrap},text={bootstrap}  
}

\newglossaryentry{featurespace}
{name={feature space},
	description={
		The feature space of a given ML application or method is 
		constituted by all potential values that the feature vector of a data point can 
		take on. A widely used choice for the feature space is the Euclidean space $\mathbb{R}^{d}$, 
		with the dimension $d$ being the number of individual features of a data point.
				\\
		See also: feature, ML, feature vector, data point, feature, Euclidean space.},
	first={feature space},text={feature space}  
}


\newglossaryentry{missingdata}
{name={missing data},
	description={Consider a dataset constituted by data points collected via 
		some physical device. Due to imperfections and failures, some of the feature 
		or label values of data points might be corrupted or simply missing. 
		Data imputation aims at estimating these missing values \cite{Abayomi2008DiagnosticsFM}. 
		We can interpret data imputation as an ML problem where the label of a data point is 
		the value of the corrupted feature.
				\\
		See also: dataset, data point, device, feature, label, data, ML. },
	first={missing data},text={missing data}  
}


\newglossaryentry{psd}
{name={positive semi-definite (psd)},
	description=
	{A (real-valued) symmetric matrix $\mathbf{Q} = \mathbf{Q}^{T} \in \mathbb{R}^{d \times d}$ 
	 is referred to as psd if ${\bf x}^{T} \mathbf{Q} {\bf x} \geq 0$ for every vector ${\bf x} \in \mathbb{R}^{d}$. 
	 The property of being psd can be extended from matrices to (real-valued) 
	 symmetric kernel maps $K: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ 
	 (with $K({\bf x},{\bf x}') = K({\bf x}',{\bf x})$)
	 as follows: For any finite set of feature vectors ${\bf x}^{(1)},\dots,{\bf x}^{(m)}$, 
	 the resulting matrix $\mathbf{Q} \in \mathbb{R}^{m \times m}$ with 
	entries $Q_{r,r'} = K\big({\bf x}^{(r)},{\bf x}^{(r')}\big)$ 
	is psd \cite{LearningKernelsBook}.
			\\
		See also: kernel, feature vector.},
	first={positive semi-definite (psd)},text={psd}  
}

\newglossaryentry{feature}
{name={feature}, plural={features},
	description={A feature of a data point is one of its properties that can be 
		measured or computed easily without the need for human supervision. For example, if a data point 
		is a digital image (e.g., stored as a \texttt{.jpeg} file), then we could use the red-green-blue intensities 
		of its pixels as features. Domain-specific synonyms for the term feature are "covariate," "explanatory variable," 
		"independent variable," "input (variable)," "predictor (variable)," or "regressor" \cite{Gujarati2021}, \cite{Dodge2003}, \cite{Everitt2022}. 
				\\
		See also: data point.
		}, first={feature},
		text={feature}  
}

\newglossaryentry{featurevec}
{name={feature vector}, plural={feature vectors},
	description={Feature vector refers to a vector ${\bf x} = \big(x_{1},\ldots,x_{d}\big)^{T}$ 
	whose entries are individual features $x_{1},\ldots,x_{d}$. Many ML methods 
	use feature vectors that belong to some finite-dimensional Euclidean space $\mathbb{R}^{d}$. 
	For some ML methods, however, it can be more convenient to work with feature 
	vectors that belong to an infinite-dimensional vector space (e.g., see kernel method). 
			\\
		See also: feature, ML, Euclidean space, kernel method.
		}, first={feature vector},text={feature vector}  
}


\newglossaryentry{label}
{name={label}, plural={labels},
	description={A higher-level fact or quantity of interest associated with a data point. 
		For example, if the data point is an image, the label could indicate whether the 
		image contains a cat or not. Synonyms for label, commonly used in specific domains, 
		include "response variable," "output variable," and "target" \cite{Gujarati2021}, \cite{Dodge2003}, \cite{Everitt2022}.
				\\
		See also: data point.
 },
	first={label},text={label}  
}


\newglossaryentry{data}
{name={data},
	 description={Data refers to objects that carry information. These 
	 	objects can be either concrete physical objects (such as persons or animals) 
	 	or abstract concepts (such as numbers). We often use representations (or 
	 	approximations) of the original data that are more convenient for data processing. 
	 	These approximations are based on different data models, with the relational data 
	 	model being one of the most widely used \cite{codd1970relational}.
				\\
		See also: model.}, 
	text={data}
}

\newglossaryentry{dataset}
{name={dataset}, plural={datasets},
	description={A dataset refers to a collection of data points. These 
		data points carry information about some quantity of interest (or label) within 
		an ML application. ML methods use datasets for model training (e.g., via ERM)
		and model validation. Note that our notion of a dataset is very flexible, as 
		it allows for very different types of data points. Indeed, data points can be concrete 
		physical objects (such as humans or animals) or abstract objects (such as numbers). 
		As a case in point, Fig.\ \ref{fig_cows_dataset} depicts a dataset that consists of cows as 
		data points. 
		\begin{figure}[H]
				\begin{center}
		\label{fig:cowsintheswissalps}
		\includegraphics[width=0.5\textwidth]{assets/Cows_in_the_Swiss_Alps.jpg}
		  \end{center}
		\caption{\label{fig_cows_dataset}“Cows in the Swiss Alps” by User:Huhu Uet is licensed under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)}
	  \end{figure}
       Quite often, an ML engineer does not have direct access to a dataset. Indeed, accessing the 
       dataset in Fig.\ \ref{fig_cows_dataset} would require us to visit the cow herd in the Alps. Instead, 
       we need to use an approximation (or representation) of the dataset which is more convenient 
       to work with. Different mathematical models have been developed for the representation (or approximation) 
       of datasets \cite{silberschatz2019database}, \cite{abiteboul1995foundations}, \cite{hoberman2009data}, \cite{ramakrishnan2002database}. 
       One of the most widely adopted data model is the relational model, which organizes data 
       as a table (or relation) \cite{codd1970relational}, \cite{silberschatz2019database}.
		A table consists of rows and columns:
		\begin{itemize} 
		\item Each row of the table represents a single data point.
		\item Each column of the table corresponds to a specific attribute of the data point. 
		ML methods can use attributes as features and labels of the data point.
		\end{itemize}
		For example, Table \ref{tab:cowdata} shows a representation of the dataset in Fig.\ \ref{fig_cows_dataset}. 
		In the relational model, the order of rows is irrelevant, and each attribute (i.e., column) must be 
		precisely defined with a domain, which specifies the set of possible values. In ML applications, 
		these attribute domains become the feature space and the label space.
		\begin{table}[H]
			\centering
			\begin{tabular}{lcccc}
				\hline
				\textbf{Name} & \textbf{Weight} & \textbf{Age} & \textbf{Height} & \textbf{Stomach temperature} \\
				\hline
				Zenzi & 100 & 4 & 100 & 25 \\
				Berta & 140 & 3 & 130 & 23 \\
				Resi  & 120 & 4 & 120 & 31 \\
				\hline
			\end{tabular}
			\caption{A relation (or table) that represents the dataset in Fig.\ \ref{fig_cows_dataset}.}
			\label{tab:cowdata}
		\end{table}
 While the relational model is useful for the study of many ML applications, it may be 
 insufficient regarding the requirements for trustworthy AI. Modern 
 approaches like datasheets for datasets provide more comprehensive 
 documentation, including details about the dataset’s collection process, intended 
 use, and other contextual information \cite{DatasheetData2021}.
 		\\
		See also: data point, label, ML, model, ERM, validation, data, feature, feature space, label space, trustworthy AI.},first={dataset},text={dataset}  
}

\newglossaryentry{predictor}
{name={predictor},
	description={A predictor is a real-valued hypothesis map. 
		Given a data point with features ${\bf x}$, the value 
		$h({\bf x}) \in \mathbb{R}$ is used as a prediction for the true 
		numeric label $y \in \mathbb{R}$ of the data point.
				\\
		See also: hypothesis, data point, feature, prediction, label. },first={predictor},text={predictor}  
}

\newglossaryentry{labeled datapoint}
{name={labeled datapoint}, plural={labeled datapoints},
 description={A data point whose label is known or has been determined 
 	by some means which might require human labor.
			\\
		See also: data point, label.},
 first={labeled datapoint},text={labeled datapoint}  
}

\newglossaryentry{rv}
{name={random variable (RV)}, plural={RVs},
 description={An RV is a function that maps from 
 	a probability space $\mathcal{P}$ to a value space \cite{GrayProbBook}, \cite{BillingsleyProbMeasure}. 
 	The probability space consists of elementary events and is equipped with a probability 
 	measure that assigns probabilities to subsets of $\mathcal{P}$. 
 	Different types of RVs include  
 	\begin{itemize} 
 	\item {binary RVs}, which map each elementary event to an element of a binary set (e.g., $\{-1,1\}$ or $\{\text{cat}, \text{no cat}\}$; 
 	\item {real-valued RVs}, which take values in the real numbers $\mathbb{R}$;  
 	\item {vector-valued RVs}, which map elementary events to the Euclidean space $\mathbb{R}^{d}$.  
 	\end{itemize} 
 	Probability theory uses the concept of measurable spaces to rigorously define 
 	and study the properties of (large) collections of RVs \cite{BillingsleyProbMeasure}.
			\\
		See also: probability space, probability, Euclidean space.}, first={random variable (RV)},text={RV}  }
 
 \newglossaryentry{probspace}{
 	name={probability space}, 
 	description={A probability space is a mathematical 
 		model of a physical process (i.e., a random experiment) with an uncertain outcome. 
 	   Formally, a probability space $\mathcal{P}$ is a triplet $(\Omega, \mathcal{F}, P)$ where
 		\begin{itemize} 
 		\item  $\Omega$ is a sample space containing all possible elementary outcomes of a random experiment;
 		\item  $\mathcal{F}$ is a sigma-algebra, i.e., a collection of subsets of $\Omega$ (called events) that satisfies 
 		certain closure properties under set operations;
 		\item $P$ is a probability measure, i.e., a function that assigns a probability $P(\mathcal{A}) \in [0,1]$ 
 		to each event $\mathcal{A} \in \mathcal{F}$. The function must satisfy $P(\Omega) = 1$ and 	$
 		P\left(\bigcup_{i=1}^{\infty} \mathcal{A}_i\right) = \sum_{i=1}^{\infty} P(\mathcal{A}_i)$ for any 
 		countable sequence of pairwise disjoint events $\mathcal{A}_1, \mathcal{A}_2, \dots$ in $\mathcal{F}$.
 		\end{itemize}
 		Probability spaces provide the foundation for defining RVs and to reason about 
 		uncertainty in ML applications \cite{GrayProbBook}, \cite{BillingsleyProbMeasure}, \cite{ross2013first}.
				\\
		See also: probability, model, RV, uncertainty, ML.},  
 	first={probability space}, 
 	text={probability space}
 }
 
	
\newglossaryentry{realization}
{name={realization}, plural={realizations},
	description={Consider an RV $x$ which maps each element 
	(i.e., outcome or elementary event) $\omega \in \mathcal{P}$ of a probability space $\mathcal{P}$ 
	to an element $a$ of a measurable space $\mathcal{N}$ \cite{RudinBookPrinciplesMatheAnalysis}, \cite{HalmosMeasure}, \cite{BillingsleyProbMeasure}. 
	A realization of $x$ is any element $a' \in \mathcal{N}$ such that there is 
	an element $\omega' \in \mathcal{P}$ with $x(\omega') = a'$.
			\\
		See also: RV, probability space.}, first={realization},text={realization}  }

\newglossaryentry{trainset}
{name={training set}, plural={training sets},
description={A training set is a dataset $\mathcal{D}$ which consists of some data points used in ERM 
	to learn a hypothesis $\hat{h}$. The average loss of $\hat{h}$ on the 
	training set is referred to as the training error. The comparison of the training error with the 
	validation error of $\hat{h}$ allows us to diagnose the ML method and informs how to improve 
	the validation error (e.g., using a different hypothesis space or collecting more data points) \cite[Sec. 6.6]{MLBasics}.
			\\
		See also: dataset, data point, ERM, hypothesis, loss, training error, validation error, ML, hypothesis space.},first={training set},text={training set}  
}

\newglossaryentry{netmodel}
{name={networked model},
  description={A networked model over an FL network $\mathcal{G} = \left( \mathcal{V},\mathcal{E} \right)$ assigns 
   a local model (i.e., a hypothesis space) to each node $i \in \mathcal{V}$ of the FL network $\mathcal{G}$.
   		\\
		See also: model, FL network, local model, hypothesis space.}, 
   first={networked model},text={networked model}  
}

\newglossaryentry{batch}
{
	name={batch},
	description={In the context of SGD, a batch refers to a randomly 
	chosen subset of the overall training set. We use the data points in this subset 
	to estimate the gradient of training error and, in turn, to update the model parameters.
			\\
		See also: SGD, training set, data point, gradient, training error, model parameters.}, 
	first={batch},text={batch}  
}

\newglossaryentry{netdata}
{
	name={networked data},
	description={Networked data consists of local datasets 
	that are related by some notion of pairwise similarity. We can represent networked 
	data using a graph whose nodes carry local datasets and edges encode 
	pairwise similarities. One example of networked data arises in FL applications 
	where local datasets are generated by spatially distributed devices.
			\\
		See also: data, local dataset, graph, FL, device.}, 
	first={networked data},text={networked data}  
}

\newglossaryentry{trainerr}
{
	name={training error},
	description={The average loss of a hypothesis when 
		predicting the labels of the data points in a training set. 
		We sometimes refer by training error also to minimal average loss 
		which is achieved by a solution of ERM.
				\\
		See also: loss, hypothesis, label, data point, training set, ERM.},first={training error},text={training error}  
}

\newglossaryentry{datapoint}
{name={data point}, plural={data points},
description={A data point is any object that conveys information \cite{coverthomas}. Data points might be 
		students, radio signals, trees, forests, images, RVs, real numbers, or proteins. We characterize data points 
		using two types of properties. One type of property is referred to as a feature. Features are properties of a 
		data point that can be measured or computed in an automated fashion. 
		A different kind of property is referred to as a label. The label of 
		a data point represents some higher-level fact (or quantity of interest). In 
		contrast to features, determining the label of a data point typically 
		requires human experts (or domain experts). Roughly speaking, ML aims to predict 
		the label of a data point based solely on its features. 
				\\
		See also: data, RV, feature, label, ML.
		}, first={data point},text={data point}  
}


\newglossaryentry{valerr}
{name={validation error}, plural={validation errors},
 description={Consider a hypothesis $\hat{h}$ which is 
 	obtained by some ML method, e.g., using ERM on a training set. The average loss 
 	of $\hat{h}$ on a validation set, which is different from the training set, is referred 
 	to as the validation error.
			\\
		See also: hypothesis, ML, ERM, training set, loss, validation set, validation.},first={validation error},text={validation error}  
}

\newglossaryentry{validation} 
{name={validation},
	description={Consider a hypothesis $\hat{h}$ that has been 
		learned via some ML method, e.g., by solving ERM on a training set $\mathcal{D}$. 
		Validation refers to the practice of evaluating the loss incurred by the 
		hypothesis $\hat{h}$ on a set of 
		data points that are not contained in the training set $\mathcal{D}$.
				\\
		See also: hypothesis, ML, ERM, training set, loss, data point. },first={validation},text={validation}  
}

\newglossaryentry{quadfunc}
{name={quadratic function},
	description={A function $f: \mathbb{R}^{d} \rightarrow \mathbb{R}$ of the form 
	$$f({\bf w}) =  {\bf w}^{T} \mathbf{Q} \mathbf{w} + \mathbf{q}^{T} {\bf w}+a,$$ with 
	some matrix $\mathbf{Q} \in \mathbb{R}^{d \times d}$, vector ${\bf q} \in \mathbb{R}^{d}$, 
	and scalar $a \in \mathbb{R}$. },first={quadratic function},text={quadratic function}  
}

\newglossaryentry{valset}
{name={validation set},
  description={A set of data points used to estimate 
  	the risk of a hypothesis $\hat{h}$ that has been learned by some 
  	ML method (e.g., solving ERM). The average loss of $\hat{h}$ 
  	on the validation set is referred to as the validation error and can be used to diagnose an 
  	ML method (see \cite[Sec. 6.6]{MLBasics}). The comparison between training error 
  	and validation error can inform directions for improvement of the ML method (such as 
  	using a different hypothesis space).
			\\
		See also: data point, risk, hypothesis, ML, ERM, loss, validation, validation error, training error, hypothesis space.},first={validation set},text={validation set}  
}

\newglossaryentry{testset}
{name={test set},
	description={A set of data points that have  
		been used neither to train a model (e.g., via ERM) nor in a validation set 
		to choose between different models.
				\\
		See also: data point, model, ERM, validation set.},first={test set},text={test set}  
}


\newglossaryentry{modelsel}
{name={model selection},
	description={In ML, model selection refers to the 
		process of choosing between different candidate models. In its most 
		basic form, model selection amounts to: 1) training each candidate model; 
		2) computing the validation error for each trained model; and 3) choosing the model 
		with the smallest validation error \cite[Ch. 6]{MLBasics}. 
				\\
		See also: ML, model, validation error.},first={model selection},text={model selection}  
}





\newglossaryentry{linclass}{name={linear classifier}, description={
	    Consider data points characterized by numeric features ${\bf x} \in \mathbb{R}^{d}$ 
	    and a label $y \in \mathcal{Y}$ from some finite label space $\mathcal{Y}$. 
		A linear classifier is characterized by having decision regions that are 
		separated by hyperplanes in $\mathbb{R}^{d}$ \cite[Ch. 2]{MLBasics}.
				\\
		See also: data point, feature, label, label space, classifier, decision region.},first={linear classifier},text={linear classifier} }

\newglossaryentry{erm}{name={empirical risk minimization (ERM)}, description={ERM is the optimization problem of finding 
		a hypothesis (out of a model) with the minimum average loss (or empirical risk) on a given dataset 
		$\mathcal{D}$ (i.e., the training set). Many ML methods are obtained from 
		empirical risk via specific design choices for the dataset, model, and loss \cite[Ch. 3]{MLBasics}.
				\\
		See also: hypothesis, model, minimum, loss, empirical risk, dataset, training set, ML.},
	first={empirical risk minimization (ERM)},text={ERM} }

\newglossaryentry{multilabelclass}{name={multi-label classification}, description={Multi-label 
		classification problems and methods use data points 
		that are characterized by several labels. As an example, consider a data point 
		representing a picture with two labels. One label indicates the presence of a human 
		in this picture and another label indicates the presence of a car.
				\\
		See also: label, classification, data point.},
	    first={multi-label classification},text={multi-label classification} }


\newglossaryentry{ssl}{
		name={semi-supervised learning (SSL)}, 
		description={SSL methods use unlabeled data points
	to support the learning of a hypothesis from labeled datapoints \cite{SemiSupervisedBook}. 
	This approach is particularly useful for ML applications that offer a large amount of 
	unlabeled data points, but only a limited number of labeled datapoints.
			\\
		See also: data point, hypothesis, labeled datapoint, ML.}, 
		first={semi-supervised learning (SSL)},text={SSL} }
	
	
\newglossaryentry{objfunc}{name={objective function}, plural={objective functions}, 
	description={An objective function is a map that assigns a numeric 
		objective value $f({\bf w})$ to each choice ${\bf w}$ of some variable that we want to 
		optimize (see Fig.\ \ref{fig_obj_func}). In the context of ML, the optimization variable could 
		be the model parameters of a hypothesis $h^{({\bf w})}$. 
		Common objective functions include the risk (i.e., expected loss) or the empirical risk 
		(i.e., average loss over a training set). ML methods apply optimization 
		techniques, such as gradient-based methods, to find the choice ${\bf w}$ with the 
		optimal value (e.g., the minimum or the maximum) of the objective function.
		\\
		\begin{figure}[H]
			\begin{center}
			\begin{tikzpicture}[scale=1.0]
				% Axes
				\draw[->] (-0.5,0) -- (4.5,0) node[right] {${\bf w}$};
				\draw[->] (0,-0.5) -- (0,3.5);
				% Objective function curve
				\draw[thick,domain=0.3:4,smooth,variable=\x] 
				plot ({\x}, {0.5*(\x-2)^2 + 0.5});
				% Label the curve
				\node at (3.5,2.8) {$f({\bf w})$};
			\end{tikzpicture} 
			\end{center}
		\caption{An objective function maps each possible value ${\bf w}$ of an optimization variable, such 
		as the model parameters of an ML model, to a value that measures the usefulness 
	   of ${\bf w}$.\label{fig_obj_func}}
		\end{figure} 
		See also: ML, model parameters, hypothesis, risk, loss, empirical risk, training set, gradient-based methods, minimum, maximum, model, loss function.},first={objective function},text={objective function} }
	
\newglossaryentry{regularizer}{name={regularizer}, description={A regularizer 
		assigns each hypothesis $h$ from a hypothesis space $\mathcal{H}$ a quantitative 
		measure $\mathcal{R}\big\{ h \big\}$ for how much its prediction error on a training set might 
		differ from its prediction errors on data points outside the training set. Ridge regression 
		uses the regularizer $\mathcal{R}\big\{ h \big\} := \left\Vert  {{\bf w}} \right\Vert_{2}^{2}$ for linear hypothesis maps $h^{({\bf w})}({\bf x}) := {\bf w}^{T} {\bf x}$ \cite[Ch. 3]{MLBasics}. 
		Lasso uses the regularizer $\mathcal{R}\big\{ h \big\} := \left\Vert  {{\bf w}} \right\Vert_{1}$ 
		for linear hypothesis maps $h^{({\bf w})}({\bf x}) := {\bf w}^{T} {\bf x}$ \cite[Ch. 3]{MLBasics}.
				\\
		See also: hypothesis, hypothesis space, prediction, training set, data point, ridge regression, Lasso. },first={regularizer},text={regularizer} }


\newglossaryentry{regularization}{name={regularization}, description={
		A key challenge of modern ML applications is that they often 
		use large models, which have an effective dimension in the order of billions. 
		Training a high-dimensional model using basic ERM-based methods
		is prone to overfitting, i.e., the learned hypothesis performs well on the training set 
		but poorly outside the training set. Regularization refers to modifications of a given instance 
		of ERM in order to avoid overfitting, i.e., to ensure that the learned hypothesis performs 
		not much worse outside the training set. There are three routes for implementing 
		regularization: 
		\begin{enumerate}[label=\arabic*)]
			\item {Model pruning:} We prune the original model $\mathcal{H}$ to obtain a 
			smaller model $\mathcal{H}'$. For a parametric model, the pruning can be 
			implemented via constraints on the model parameters (such as $w_{1} \in [0.4,0.6]$ for 
			the weight of feature $x_{1}$ in linear regression).
			\item {Loss penalization:} We modify the objective function of ERM by adding a 
			penalty term to the training error. The penalty term estimates how much larger the expected loss (or risk) 
			is compared to the average loss on the training set. 
			\item {Data augmentation:} We can enlarge the training set $\mathcal{D}$ by adding 
			perturbed copies of the original data points in $\mathcal{D}$. One example for such 
			a perturbation is to add the realization of an RV to the feature vector 
			of a data point. 
		\end{enumerate} 
		Fig. \ref{fig_equiv_dataaug_penal_dict} illustrates the above three routes to regularization. 
		These routes are closely related and sometimes fully equivalent. Data augmentation using Gaussian RVs 
		to perturb the feature vectors in the training set of linear regression 
		has the same effect as adding the penalty 
		$\lambda \left\Vert  {{\bf w}} \right\Vert_{2}^2$ to the training error (which is nothing but ridge regression). 
        The decision on which route to use for regularization can be based on the 
        available computational infrastructure. For example, it might be much easier to 
        implement data augmentation than model pruning. 
		\begin{figure}[H]
			\begin{center} 
				\begin{tikzpicture}[scale = 1]
					% Axes
					\draw[->, very thick] (0,0.5) -- (7.7,0.5) node[right] {feature $x$};       % X-axis
					\draw[->, very thick] (0.5,0) -- (0.5,4.2) node[above] {label $y$};   % Y-axis
					\draw[color=black, thick, dashed, domain = -1: 6.2, variable = \x]  plot ({\x},{\x*0.4 + 2.0}) ;     
					\draw[color=black, thick, dashed, domain = -1: 6.2, variable = \x]  plot ({\x},{\x*0.6 + 2.0}) ;     
					            % Add a lasso around the two dashed lines
	          % Ellipse around the two dashed lines
					\draw[blue, thick] (5, 4.5) ellipse [x radius=0.2cm, y radius=1cm];
					\node at (5, 5.8) [text=black, font=\small] {$\{ h: h(x)\!=\!w_{1}x\!+\!w_{0}; w_{1} \in [0.4,0.6]\}$};
					\node at (6.7,4.5) {$h(x)$};    
					\coordinate (l1)   at (1.2, 2.48);
					\coordinate (l2) at (1.4, 2.56);
					\coordinate (l3)   at (1.7,  2.68);
					\coordinate (l4)   at (2.2, 2.2*0.4+2.0);
					\coordinate (l5) at (2.4, 2.4*0.4+2.0);
					\coordinate (l6)   at (2.7,  2.7*0.4+2.0);
					\coordinate (l7)   at (3.9,  3.9*0.4+2.0);
					\coordinate (l8) at (4.2, 4.2*0.4+2.0);
					\coordinate (l9)   at (4.5,  4.5*0.4+2.0);
					\coordinate (n1)   at (1.2, 1.8);
					\coordinate (n2) at (1.4, 1.8);
					\coordinate (n3)   at (1.7,  1.8);
					\coordinate (n4)   at (2.2, 3.8);
					\coordinate (n5) at (2.4, 3.8);
					\coordinate (n6)   at (2.7,  3.8);
					% augemented data point obtained by perturbing feature, not touching label value 
					\coordinate (n7)   at (3.9, 2.6);
					\coordinate (n8) at (4.2, 2.6);
					\coordinate (n9)   at (4.5,  2.6);
					\node at (n1)  [circle,draw,fill=red,minimum size=6pt,scale=0.6, name=c1] {};
					\node at (n2)  [circle,draw,fill=blue,minimum size=6pt, scale=0.6, name=c2] {};
					\node at (n3)  [circle,draw,fill=red,minimum size=6pt,scale=0.6,  name=c3] {};
					\node at (n4)  [circle,draw,fill=red,minimum size=12pt, scale=0.6, name=c4] {};  
					\node at (n5)  [circle,draw,fill=blue,minimum size=12pt,scale=0.6,  name=c5] {};
					\node at (n6)  [circle,draw,fill=red,minimum size=12pt, scale=0.6, name=c6] {};  
					\node at (n7)  [circle,draw,fill=red,minimum size=12pt,scale=0.6,  name=c7] {};
					\node at (n8)  [circle,draw,fill=blue,minimum size=12pt, scale=0.6, name=c8] {};
					\node at (n9)  [circle,draw,fill=red,minimum size=12pt, scale=0.6, name=c9] {};
					\draw [<->] ($ (n7) + (0,-0.3) $)  --  ($ (n9) + (0,-0.3) $) node [pos=0.4, below] {$\sqrt{\alpha}$}; ; 
					\draw[<->, color=red, thick] (l1) -- (c1);  
					\draw[<->, color=blue, thick] (l2) -- (c2);  
					\draw[<->, color=red, thick] (l3) -- (c3);  
					\draw[<->, color=red, thick] (l4) -- (c4);  
					\draw[<->, color=blue, thick] (l5) -- (c5);  
					\draw[<->, color=red, thick] (l6) -- (c6);  
					\draw[<->, color=red, thick] (l7) -- (c7);  
					\draw[<->, color=blue, thick] (l8) -- (c8);  
					\draw[<->, color=red, thick] (l9) -- (c9);  
					\draw[fill=blue] (6.2, 3.7)  circle (0.1cm) node [black,xshift=2.3cm] {original training set $\mathcal{D}$};
					\draw[fill=red] (6.2, 3.2)  circle (0.1cm) node [black,xshift=1.3cm] {augmented};
					\node at (4.6,1.2)  [minimum size=12pt, font=\fontsize{12}{0}\selectfont, text=blue] {$\frac{1}{m} \sum_{r=1}^m L\left(\left( {\bf x}^{(r)}, y^{(r)} \right),h \right)$};
					\node at (7.8,1.2)  [minimum size=12pt, font=\fontsize{12}{0}\selectfont, text=red] {$+\alpha \mathcal{R}\big\{ h \big\}$};
				\end{tikzpicture}
				\caption{Three approaches to regularization: 1) data augmentation; 2) loss penalization; and 3) model 
				pruning (via constraints on model parameters). \label{fig_equiv_dataaug_penal_dict} }
			\end{center}
		\end{figure} 
		See also: ML, model, effective dimension, ERM, overfitting, hypothesis, training set, model parameters, feature, linear regression, loss, objective function, training error, risk, data augmentation, data point, realization, RV, feature vector, Gaussian RV, ridge regression, label.
		},first={regularization},text={regularization} }
	

\newglossaryentry{rerm}{
	name={regularized empirical risk minimization (RERM)}, 
	description={Basic ERM learns a hypothesis (or trains a model) $h \in \mathcal{H}$ 
		based solely on the empirical risk $\widehat{L}\big(h|\mathcal{D}\big)$ incurred on a training set $\mathcal{D}$. 
		To make ERM less prone to overfitting, we can implement regularization by 
		including a (scaled) regularizer $\mathcal{R}\big\{ h \big\}$ in the learning objective. 
		This leads to RERM such that
		\begin{equation}
			\label{equ_def_rerm}
			\hat{h} \in \argmin_{h \in \mathcal{H}} \widehat{L}\big(h|\mathcal{D}\big) + \alpha \mathcal{R}\big\{ h \big\}.
		\end{equation}
		The parameter $\alpha \geq 0$ controls the regularization strength. 
		For $\alpha = 0$, we recover standard ERM without regularization. As $\alpha$ increases, the 
		learned hypothesis is increasingly biased toward small values of $\mathcal{R}\big\{ h \big\}$. 
		The component $\alpha \mathcal{R}\big\{ h \big\}$ in the objective function of \eqref{equ_def_rerm} 
		can be intuitively understood as a surrogate for the increased average loss that may 
		occur when predicting labels for data points outside the training set. This intuition  
		can be made precise in various ways. For example, consider a linear model trained using squared error loss 
		and the regularizer $\mathcal{R}\big\{ h \big\} = \left\Vert  {{\bf w}} \right\Vert_{2}^{2}$. 
		In this setting, $\alpha \mathcal{R}\big\{ h \big\}$ corresponds to the expected increase in loss 
		caused by adding Gaussian RVs to the feature vectors in the training set 
		\cite[Ch. 3]{MLBasics}.
		A principled construction for the regularizer $\mathcal{R}\big\{ h \big\}$ 
		arises from approximate upper bounds on the generalization error. The resulting 
		RERM instance is known as SRM \cite[Sec. 7.2]{ShalevShwartz2009}.
				\\
		See also: ERM, hypothesis, model, empirical risk, training set, overfitting, regularization, regularizer, objective function, loss, label, data point, linear model, squared error loss, Gaussian RV, feature vector, generalization, SRM.
	}, 
	first={regularized empirical risk minimization (RERM)},
	text={RERM} 
}


\newglossaryentry{generalization}{name={generalization}, 
	description={
		Generalization refers to the ability of a model trained on a training set to make accurate 
		predictions on new, unseen data points. This is a central goal of ML and AI: 
		to learn patterns that extend beyond the training set. Most ML systems 
		use ERM to learn a hypothesis $\hat{h} \in \mathcal{H}$ by minimizing 
		the average loss over a training set of data points ${\bf z}^{(1)}, \ldots, {\bf z}^{(m)}$, 
		denoted as $\mathcal{D}^{(\rm train)}$. However, success on the training set does not guarantee success on 
		unseen data - this discrepancy is the challenge of generalization. \\[1mm] To study generalization 
		mathematically, we need to formalize the notion of ``unseen'' data. A widely used 
		approach is to assume a probabilistic model for data generation, such as the i.i.d.\ assumption. 
		Here, we interpret data points as independent RVs with an identical 
		probability distribution $p({\bf z})$. This probability distribution, which is assumed fixed but unknown, 
		allows us to define risk of a trained model $\hat{h}$ as the expected loss
		\[
		\bar{L} \big( \hat{h} \big) =\mathbb{E} _{{\bf z} \sim p({\bf z})} \big\{ L(\hat{h}, {\bf z}) \big\}.
		\]
		The difference between risk $\bar{L} \big( \hat{h} \big) $ and empirical risk $\widehat{L}\big(\hat{h}|\mathcal{D}^{(\rm train)}\big)$ 
		is known as the generalization gap. Tools from probability theory, such as concentration inequalities 
		and uniform convergence, allow us to bound this gap under certain conditions \cite{ShalevMLBook}.\\[1mm]
		{\bf Generalization without probability.} Probability theory is one way to study how well a 
		model generalizes beyond the training set, but it is not the only way. Another option is to use 
		simple, deterministic changes to the data points in the training set. The basic idea is that a 
		good model $\hat{h}$ should be robust: its prediction $\hat{h}({\bf x})$ 
		should not change much if we slightly change the features ${\bf x}$ of a data point ${\bf z}$. 
		\\[1mm] For example, an object detector trained on smartphone photos should still detect the object if a few 
		random pixels are masked \cite{OnePixelAttack}. Similarly, it should deliver the same result if we rotate 
		the object in the image \cite{MallatUnderstandingDeepLearning}. 
		  \begin{figure}[H]
		                   	\centering
		                   	\begin{tikzpicture}[scale=0.8]
							   \draw[lightblue, fill=lightblue, opacity=0.5] (3, 2) ellipse (6cm and 2cm);
								\node[black] at (6, 3) {$p({\bf z})$};
		                   		\fill[blue] (1, 3) circle (4pt) node[below, xshift=0pt, yshift=0pt] {${\bf z}^{(1)}$};
		                   		\fill[blue] (5, 1) circle (4pt) node[below] {${\bf z}^{(2)}$};
		                   		\fill[blue] (1.6, 3) circle (3pt);
		                   		\fill[blue] (0.4, 3) circle (3pt);
		                   		\draw[<->, thin] (1, 3) -- (1.6, 3);
		                   		\draw[<->, thin] (1, 3) -- (0.4, 3);
		                   		\fill[blue] (5.6, 1) circle (3pt);
		                   		\fill[blue] (4.4, 1) circle (3pt);
		                   		\draw[<->, thin] (5, 1) -- (5.6, 1);
		                   		\draw[<->, thin] (5, 1) -- (4.4, 1);
		                   		\draw[black, thick, domain=0:6, smooth] plot (\x, {- 1*\x + 5});
		                   		\node[black] at (3, 2.5) [right] {$\hat{h}$};
		                   	\end{tikzpicture}
		                   	\caption{Two data points ${\bf z}^{(1)},{\bf z}^{(2)}$ that are used as a training set 
		                   		to learn a hypothesis $\hat{h}$ via ERM. We can evaluate $\hat{h}$ 
		                   		outside $\mathcal{D}^{(\rm train)}$ either by an i.i.d.\ assumption with some underlying probability distribution $p({\bf z})$ 
		                   		or by perturbing the data points.}
		                   	\label{fig:polynomial_fit_dict}
		                   \end{figure}
		See also: ML, AI, ERM, model, hypothesis, loss, empirical risk, data point, training set, probabilistic model, i.i.d.\ assumption, data, i.i.d., realization, probability distribution, risk, RV, prediction.
		},
	first={generalization},
	text={generalization} 
}

\newglossaryentry{gengap}
{name = {generalization gap}, 
	description={The difference between the performance of a trained model 
		on the training set and its performance on other data points (such as those in a validation set). 
		\\
		See also: hypothesis, decision tree, generalization, gradient-based methods, ERM, models, smooth, loss functions, GD, model parameters, empirical risk, gradient, loss, gradient step.
	}, 
	first={generalization gap}, 
	text={generalization gap}} 
	
\newglossaryentry{concentrationinequ}
{name = {concentration inequality}, 
	description={An upper bound on the probability that a RV deviates 
		more than a prescribed amount from its expectation \cite{Wain2019}. 
	}, 
	first={concentration inequality},
	firstplural={concentration inequalities},  
	plural={concentration inequalities},  
	text={concentration inequality}} 



\newglossaryentry{boosting}
{name = {boosting}, 
	description={Boosting is an iterative optimization method to learn an accurate 
		hypothesis map (or strong learner) by sequentially combining less accurate 
		hypothesis maps (referred to as weak learners) \cite[Ch. 10]{hastie01statisticallearning}.
		For example, weak learners are shallow decision trees which are combined to 
		obtain a deep decision tree. Boosting can be understood as a generalization 
		of gradient-based methods for ERM using parametric models and smooth loss functions 
		\cite{Friedman2001}. Just like GD iteratively updates model parameters to reduce the empirical risk, 
		boosting iteratively combines (e.g., by summation) hypothesis maps to reduce the empirical risk. 
		A widely-used instance of the generic boosting idea is referred to as gradient boosting, which 
		uses gradients of the loss function for combining the weak learners \cite{Friedman2001}. 
		\begin{figure}[H]
			\begin{center}
				\begin{tikzpicture}[scale=1.2]
					% Axes
					\draw[->] (-0.5,0) -- (5.5,0) node[right] {$h$};
					\draw[->] (0,-0.5) -- (0,4.5) node[above] {$L\left({\bf z},h \right)$};
					\draw[thick,domain=0.2:5,smooth,variable=\x,blue!60] plot ({\x},{(4 - 1.3*\x + 0.15*\x*\x)});
					\foreach \x/\label in {0.7/$h^{(0)}$, 1.5/$h^{(1)}$, 2.3/$h^{(2)}$, 3.0/$h^{(3)}$} {
						\draw[dashed, gray] (\x, 0) -- (\x, {4 - 1.3*\x + 0.15*\x*\x}); % helper line
						\filldraw[black] (\x, {4 - 1.3*\x + 0.15*\x*\x}) circle (2pt);   % point
						\node[below] at (\x, -0.1) {\label};                             % label
					}
				\end{tikzpicture}
			\end{center} 
			\caption{Boosting methods construct a sequence of hypothesis maps $h^{(0)},h^{(1)},\ldots$ 
				            that are increasingly strong learners (i.e., incurring a smaller loss).}
     	\end{figure} 
     	See also: hypothesis, decision tree, generalization, gradient-based methods, ERM, models, smooth, loss functions, GD, model parameters, empirical risk, gradient, loss, gradient step.
		}, 
	first={boosting}, 
	text={boosting}} 

	
\newglossaryentry{gtv}
{name={generalized total variation (GTV)}, 
description={GTV is a 
		measure of the variation of trained local models $h^{(i)}$ 
		(or their model parameters $\mathbf{w}^{(i)}$) assigned to the nodes $i=1,\ldots,n$ 
		of an undirected weighted graph $\mathcal{G}$ with edges $\mathcal{E}$. Given a measure $d^{(h,h')}$ 
		for the discrepancy between hypothesis maps $h,h'$, the GTV is 
		\begin{equation} 
			\nonumber
			\sum_{\{i,i'\}\in \mathcal{E}} A_{i,i'} 
			d^{(h^{(i)},h^{(i')})}.
		\end{equation}
		Here, $A_{i,i'}>0$ denotes the weight of the undirected edge $\{i,i'\}\in \mathcal{E}$.
				\\
		See also: local model, model parameters, graph, discrepancy, hypothesis.
		},
		first={GTV},
		text={GTV} 
}
	
\newglossaryentry{srm}{
	name={structural risk minimization (SRM)}, 
	description={SRM is an
		instance of RERM, with which the model $\mathcal{H}$ can be expressed 
		as a countable union of submodels such that $\mathcal{H} = \bigcup_{n=1}^{\infty} \mathcal{H}^{(n)}$. 
		Each submodel $\mathcal{H}^{(n)}$ permits the derivation of an approximate upper bound 
		on the generalization error incurred when applying ERM to train $\mathcal{H}^{(n)}$. 
		These individual bounds—one for each submodel—are then combined to form a regularizer 
		used in the RERM objective. 
        These approximate upper bounds (one for each $\mathcal{H}^{(n)}$) are then combined 
		to construct a regularizer for RERM \cite[Sec.\ 7.2]{ShalevMLBook}.
				\\
		See also: RERM, model, generalization, ERM, regularizer, risk.},
		first={structural risk minimization (SRM)},text={SRM}
 }

 \newglossaryentry{rlm}{
 	name={regularized loss minimization (RLM)},
 	description={See RERM.},
 	text={RLM}
 }
 

\newglossaryentry{datapoisoning}{name={data poisoning}, description={Data 
		poisoning refers to the intentional manipulation (or fabrication) of data points to 
		steer the training of an ML model \cite{Liu2021}, \cite{PoisonGAN}. The protection against 
		data poisoning is particularly important in distributed ML applications where datasets are decentralized.
				\\
		See also: data, data point, ML, model, dataset.},first={data poisoning},text={data poisoning} }
	
	
\newglossaryentry{backdoor}{name={backdoor}, description={A backdoor attack refers 
		to the intentional manipulation of the training process underlying an ML method. This manipulation 
		can be implemented by perturbing the training set (i.e., through data poisoning) or via the 
		optimization algorithm used by an ERM-based method. The goal of a 
		backdoor attack is to nudge the learned hypothesis $\hat{h}$ 
		towards specific predictions for a certain range of feature values. This range of feature 
		values serves as a key (or trigger) to unlock a backdoor in the sense of 
		delivering anomalous predictions. The key ${\bf x}$ and the corresponding 
		anomalous prediction $\hat{h}({\bf x})$ are only known to the attacker.
				\\
		See also: ML, training set, data poisoning, algorithm, ERM, hypothesis, prediction, feature.},
	first={backdoor},text={backdoor} }


\newglossaryentry{clustasspt}{name={clustering assumption}, description={The 
		clustering assumption postulates that data points in a dataset form a (small) number of 
		groups or clusters. Data points in the same cluster are more similar to each 
		other than those outside the cluster \cite{SemiSupervisedBook}. We obtain different 
		clustering methods by using different notions of similarity between data points.
				\\
		See also: clustering, data point, dataset, cluster.},first={clustering assumption},text={clustering assumption} }
	
\newglossaryentry{dosattack}{name={denial-of-service attack}, description={A 
		denial-of-service attack aims (e.g., via data poisoning) to steer the training of a model 
		such that it performs poorly for typical data points.
				\\
		See also: data poisoning, model, data point.},
	first={denial-of-service attack},text={denial-of-service attack} }

\newglossaryentry{netexpfam}{name={networked exponential families (nExpFam)}, 
	description={A collection of exponential 
		families, each of them assigned to a node of an FL network. The model parameters are coupled 
	   via the network structure by requiring them to have a small GTV \cite{JungNetExp2020}.
	   		\\
		See also: FL network, model parameters, GTV.},first={networked exponential family (nExpFam)},text={nExpFam} }
	 


\newglossaryentry{scatterplot}{name={scatterplot}, description={A 
		visualization technique that depicts data points by markers in a two-dimensional plane. 
		Fig. \ref{fig_scatterplot_temp_FMI_dict} depicts an example of a scatterplot.  
		\begin{figure}[H]
			\begin{center}
				\begin{tikzpicture}[scale=1]
					\tikzset{x=2cm,y=2cm,every path/.style={>=latex},node style/.style={circle,draw}}
					\begin{axis}[axis x line=none,
						axis y line=none,
						ylabel near ticks,
						xlabel near ticks,
						enlarge y limits=true,
						xmin=-5, xmax=30,
						ymin=-5, ymax=30,
						width=6cm, height=6cm ]
						\addplot[only marks] table [x=mintmp, y=maxtmp, col sep = semicolon] {assets/FMIData1.csv};
						\node at (axis cs:26,2) [anchor=west] {$x$};
						\node at (axis cs:0,30) [anchor=west] {$y$};
						\draw[->] (axis cs:-5,0) -- (axis cs:30,0);
						\draw[->] (axis cs:0,-5) -- (axis cs:0,30);
					\end{axis}
				\end{tikzpicture}
				\vspace*{-10mm}
			\end{center}
			\caption{A scatterplot with circle markers, where the data points represent daily weather conditions in Finland. 
				Each data point is characterized by its minimum daytime temperature $x$ 
				as the feature and its maximum daytime temperature $y$ as the label. 
				The temperatures have been measured at the FMI weather station Helsinki Kaisaniemi 
				during 1.9.2024 - 28.10.2024.}
			\label{fig_scatterplot_temp_FMI_dict}
			\vspace*{-3mm}
			\end{figure}
		A scatterplot can enable the visual inspection of data points that are naturally 
			represented by feature vectors in high-dimensional spaces.
		See also: data point, minimum, feature, maximum, label, FMI, feature vector, dimensionality reduction.
		},first={scatterplot},text={scatterplot} }


\newglossaryentry{stepsize}{name={step size}, description={
		See learning rate.}, 
	first={step size},text={step size} }

\newglossaryentry{learnrate}{name={learning rate}, description={Consider 
		an iterative ML method for finding or learning a useful hypothesis $h \in \mathcal{H}$. 
		Such an iterative method repeats similar computational (update) steps that adjust or 
		modify the current hypothesis to obtain an improved hypothesis. One 
		well-known example of such an iterative learning method is GD and its variants, SGD and 
		projected GD. A key parameter of an iterative method is the learning rate. 
		The learning rate controls the extent to which the current hypothesis 
		can be modified during a single iteration. A well-known example of such a parameter 
		is the step size used in GD \cite[Ch. 5]{MLBasics}.
				\\
		See also: ML, hypothesis, GD, SGD, projected GD, step size.},
	first={learning rate},text={learning rate} }

\newglossaryentry{featuremap}{name={feature map}, description={Feature map refers to a map 
		that transforms the original features of a data point into new features. The 
		so-obtained new features might be preferable over the original features for 
		several reasons. For example, the arrangement of data points might become 
		simpler (or more linear) in the new feature space, allowing the use of linear models 
		in the new features. This idea is a main driver for the development of kernel methods \cite{LearningKernelsBook}. 
		Moreover, the hidden layers of a deep net can be interpreted as a trainable feature map 
		followed by a linear model in the form of the output layer. Another reason for learning a feature map
		could be that learning a small number of new features helps to avoid overfitting and 
		ensures interpretability \cite{Ribeiro2016}. The special case of a feature map delivering 
		two numeric features is particularly useful for data visualization. Indeed, we can depict 
		data points in a scatterplot by using two features as the coordinates of a data point.
				\\
		See also: feature, data point, feature space, linear model, kernel method, deep net, overfitting, interpretability, data, scatterplot.},
	first={feature map},text={feature map} }
	
 
  \newglossaryentry{lasso}{name={least absolute shrinkage and selection operator (Lasso)}, 
	description={The Lasso is an 
		instance of SRM. It learns the weights ${\bf w}$ of a linear map 
		$h({\bf x}) = {\bf w}^{T} {\bf x}$ based on a training set. 
		Lasso is obtained from linear regression by adding the scaled $\ell_{1}$-norm 
		$\alpha \left\Vert  {{\bf w}} \right\Vert_{1}$ to the average squared error loss incurred on the training set. 
				\\
		See also: SRM, weights, training set, linear regression, norm, squared error loss.
	},
	first={Lasso},text={Lasso} }
 
 \newglossaryentry{simgraph}{name={similarity graph}, 
 	description={Some ML applications generate data points that 
 		are related by a domain-specific notion of similarity. These similarities can be 
 		represented conveniently using a similarity graph $\mathcal{G} = \big(\mathcal{V} := \{1,\ldots,m\},\mathcal{E}\big)$. 
 		The node $r \in \mathcal{V}$ represents the $r$-th data point. Two 
 		nodes are connected by an undirected edge if the corresponding data points are similar. 
				\\
		See also: ML, data point, graph.
 	},
 	first={similarity graph},text={similarity graph} }
 
 
 \newglossaryentry{kld}{name={Kullback-Leibler divergence (KL divergence)}, 
 	description={
 		 The KL divergence is a quantitative 
 		 measure of how much one probability distribution is different from another probability distribution \cite{coverthomas}.  
		 		\\
		See also: probability distribution.
 	},
 	first={Kullback-Leibler divergence (KL divergence)},text={KL divergence} }

\newglossaryentry{LapMat}{
	name={Laplacian matrix},
	description={The structure of a graph $\mathcal{G}$, with 
		nodes $i=1,\ldots,n$, can be analyzed using the properties of 
		special matrices that are associated with $\mathcal{G}$. One such matrix is the 
		graph Laplacian matrix ${\bf L}^{(\mathcal{G})} \in \mathbb{R}^{n \times n}$, 
		which is defined for an undirected and weighted graph \cite{Luxburg2007}, \cite{Ng2001}. 
		It is defined element-wise as (see Fig. \ref{fig_lap_mtx_dict})
	\begin{equation}
		L^{(\mathcal{G})}_{i,i'} := \begin{cases} - A_{i,i'} & \mbox{ for } i\neq i', \{i,i'\}\!\in\!\mathcal{E}, \\ 
			\sum_{i'' \neq i} A_{i,i''} & \mbox{ for } i = i', \\ 
							0 & \mbox{ else.} \end{cases}
	 \end{equation}
  Here, $A_{i,i'}$ denotes the edge weight of an edge $\{i,i'\} \in \mathcal{E}$. 
  \begin{figure}[H]
  	\begin{center}
    \begin{minipage}{0.45\textwidth}
	\begin{tikzpicture}
%	 				% 		% Left part - Graph
	 	 		\begin{scope}[every node/.style={circle, draw, minimum size=1cm}]
	 					 			\node (1) at (0,0) {1};
	 					 			\node (2) [below left=of 1] {2};
	 					 			\node (3) [below right=of 1] {3};
	 					 		   \draw (1) -- (2);
	 					 			\draw (1) -- (3);
	 					 		\end{scope}
	 				 	\end{tikzpicture}
	 			 	\end{minipage} 
	 			 	\hspace*{-15mm}
 		 		\begin{minipage}{0.45\textwidth}
	 			 	 \begin{equation} 
	 				 		 {\bf L}^{(\mathcal{G})} = \begin{pmatrix} 2 & -1& -1 \\ -1& 1 & 0 \\  -1 & 0 & 1 \end{pmatrix}  
	 				 		 \nonumber
	 				 		 \end{equation} 
	 			 \end{minipage}
	 	 \caption{\label{fig_lap_mtx_dict} Left: Some undirected graph $\mathcal{G}$ with three nodes $i=1,2,3$. 
	 		 	Right: The Laplacian matrix ${\bf L}^{(\mathcal{G})}  \in \mathbb{R}^{3 \times 3}$ of $\mathcal{G}$.} 
	 		 	\end{center}
	 		\end{figure}
		See also: graph, edge weight.
	%		
	},
	first={Laplacian matrix},
	text={Laplacian matrix}
}

\newglossaryentry{algconn}{
	name={algebraic connectivity},
	description={The algebraic connectivity of an undirected graph 
		is the second-smallest eigenvalue $\lambda_{2}$ of its Laplacian matrix. A graph is connected if and only if 
		$\lambda_{2} >0$. 
				\\
		See also: graph, eigenvalue, Laplacian matrix.
	},
	first={algebraic connectivity},
	text={algebraic connectivity}
}




\newglossaryentry{cfwmaxmin}{name ={Courant–Fischer–Weyl min-max characterization}, 
description={Consider a psd 
	matrix $\mathbf{Q} \in \mathbb{R}^{d \times d}$ with 
	EVD (or spectral decomposition), 
	$$ \mathbf{Q} = \sum_{j=1}^{d} \lambda_{j} {\bf u}^{(j)} \big(  {\bf u}^{(j)}  \big)^{T}.$$ 
	Here, we use the ordered (in increasing fashion) eigenvalues 
	\begin{equation}
		\nonumber
	%	\label{equ_def_order_eigvals_LapMat}  
		 \lambda_{1}  \leq  \ldots \leq \lambda_{n}. 
	\end{equation}
	The Courant–Fischer–Weyl min-max characterization \cite[Th. 8.1.2]{GolubVanLoanBook} 
	represents the eigenvalues of $\mathbf{Q}$ as the solutions to certain optimization problems.
			\\
		See also: psd, EVD, eigenvalue.}, 
first = {Courant–Fischer–Weyl min-max characterization (CFW)}, text={CFW}}

\newglossaryentry{kernel}{name={kernel}, 
	description={Consider data points characterized by a feature vector ${\bf x} \in \mathcal{X}$ 
	with a generic feature space $\mathcal{X}$. A (real-valued) kernel $K: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ 
	assigns each pair of feature vectors ${\bf x}, {\bf x}' \in \mathcal{X}$ a real number $K\big({\bf x},{\bf x}'\big)$. 
	The value $K\big({\bf x},{\bf x}'\big)$ is often interpreted as a measure for the similarity between ${\bf x}$ 
	and ${\bf x}'$. Kernel methods use a kernel to transform the feature vector ${\bf x}$ to a new feature vector ${\bf z} = K\big({\bf x},\cdot\big)$. 
         This new feature vector belongs to a linear feature space $\mathcal{X}'$ which is (in general)  
          different from the original feature space $\mathcal{X}$. The feature space $\mathcal{X}'$ has 
          a specific mathematical structure, i.e., it is a reproducing kernel Hilbert space \cite{LearningKernelsBook}, \cite{LampertNowKernel}.
          		\\
		See also: data point, feature vector, feature space, kernel method, Hilbert space.
          },
	first={kernel},text={kernel} }
	
\newglossaryentry{kernelmethod}{name={kernel method}, plural={kernel methods}, 
	description={A kernel method is an ML method that uses a 
	kernel $K$ to map the original (i.e., raw) feature vector ${\bf x}$ of a 
	data point to a new (transformed) feature vector ${\bf z} = K\big({\bf x},\cdot\big)$ \cite{LearningKernelsBook}, \cite{LampertNowKernel}.
	The motivation for transforming the feature vectors is that, by using a suitable kernel, 
	the data points have a "more pleasant" geometry in the transformed feature space. 
	For example, in a binary classification problem, using transformed feature vectors ${\bf z}$ might 
	allow us to use linear models, even if the data points are not linearly 
	separable in the original feature space (see Fig. \ref{fig_linsep_kernel_dict}). 
	\begin{figure}[H]
\begin{center}
 \begin{tikzpicture}[auto,scale=0.6]
        % Left rectangle (\mathcal{X})
       % \draw [thick] (-9,-3) rectangle (-2,4) node [anchor=east,above] {$\mathcal{X}$};
        \draw [thick] (-6,2) circle (0.1cm) node[anchor=west] {\hspace*{0mm}${\bf x}^{(5)}$};
       \draw [thick] (-8,1.6) circle (0.1cm) node[anchor=west] {\hspace*{0mm}${\bf x}^{(4)}$};
        \draw [thick] (-7.4,-1.7) circle (0.1cm) node[anchor=west] {\hspace*{0mm}${\bf x}^{(3)}$};
        \draw [thick] (-6,-1.9) circle (0.1cm) node[anchor=west] {\hspace*{0mm}${\bf x}^{(2)}$};
        \draw [thick] (-6.5,0.0) rectangle ++(0.1cm,0.1cm) node[anchor=west,above] {\hspace*{0mm}${\bf x}^{(1)}$};
%
%        % Right rectangle (\mathcal{X}')
      % \draw [thick] (0,-4) rectangle (7,3) node [anchor=east,above] {$\mathcal{X}'$};
        \draw [thick] (4,0) circle (0.1cm) node[anchor=north] {\hspace*{0mm}${\bf z}^{(5)}$};
        \draw [thick] (5,0) circle (0.1cm) node[anchor=north] {\hspace*{0mm}${\bf z}^{(4)}$};
        \draw [thick] (6,0) circle (0.1cm) node[anchor=north] {\hspace*{0mm}${\bf z}^{(3)}$};
        \draw [thick] (7,0) circle (0.1cm) node[anchor=north] {\hspace*{0mm}${\bf z}^{(2)}$};
        \draw [thick] (2,0) rectangle ++(0.1cm,0.1cm) node[anchor=west,above] {\hspace*{0mm}${\bf z}^{(1)}$};
%
%        % Arrow from left rectangle to right rectangle
       \draw[->,bend left=30] (-3,0) to node[midway,above] {${\bf z} = K\big({\bf x},\cdot\big)$} (1,0);
    \end{tikzpicture}
\end{center}
\caption{
Five data points characterized by feature vectors ${\bf x}^{(r)}$ 
and labels $y^{(r)} \in \{ \circ, \square \}$, for $r=1,\ldots,5$. 
With these feature vectors, there is no way to separate the two classes 
by a straight line (representing the decision boundary of a linear classifier). 
In contrast, the transformed feature vectors ${\bf z}^{(r)} = K\big({\bf x}^{(r)},\cdot\big)$ 
allow us to separate the data points using a linear classifier.  \label{fig_linsep_kernel_dict}}
\end{figure}
		See also: kernel, ML, feature vector, data point, feature space, classification, linear model, label, decision boundary, linear classifier.
},first={kernel method},text={kernel method} }
	

\newglossaryentry{cm}{name={confusion matrix}, 
	description={Consider data points, which are characterized 
		by features ${\bf x}$ and label $y$, having values from the finite 
		label space $\mathcal{Y} = \{1,\ldots,k\}$. For a given hypothesis $h$, 
		the confusion matrix is a $k \times k$ matrix with rows representing the elements of 
		$\mathcal{Y}$. The columns of a confusion matrix correspond to the prediction $h({\bf x})$. 
		The $(c,c')$-th entry of the confusion matrix is the fraction of 
		data points with label $y\!=\! c$ and resulting in a prediction $h({\bf x})\!=\!c'$.
				\\
		See also: data point, feature, label, label space, hypothesis, prediction.},
	first={confusion matrix},text={confusion matrix} }


\newglossaryentry{featuremtx}{name={feature matrix}, 
	description={Consider a dataset $\mathcal{D}$ 
		with $m$ data points with feature vectors ${\bf x}^{(1)},\ldots,{\bf x}^{(m)} \in \mathbb{R}^{d}$. It is convenient to 
		collect the individual feature vectors into a feature 
		matrix ${\bf X} := \big({\bf x}^{(1)},\ldots,{\bf x}^{(m)}\big)^{T}$ 
		of size $m \times d$.
				\\
		See also: dataset, data point, feature vector, feature.},
	first={feature matrix},text={feature matrix} }

\newglossaryentry{dbscan}{name={density-based spatial clustering of applications with noise (DBSCAN)}, 
	description={DBSCAN refers to a clustering algorithm for data points that are characterized by numeric feature vectors. 
		Like $k$-means and soft clustering via GMM, also DBSCAN uses the Euclidean 
		distances between feature vectors to determine the clusters. However, in contrast to $k$-means 
		and GMM, DBSCAN uses a different notion of similarity between data points. 
		DBSCAN considers two data points as similar if they are connected 
		via a sequence (i.e., path) of close-by intermediate data points. Thus, DBSCAN might consider 
		two data points as similar (and therefore belonging to the same cluster) even if 
		their feature vectors have a large Euclidean distance.
				\\
		See also: clustering, algorithm, data point, feature vector, $k$-means, soft clustering, GMM, cluster.},
	first={density-based spatial clustering of applications with noise (DBSCAN)},text={DBSCAN} }

\newglossaryentry{fl}{name={federated learning (FL)}, description={FL 
		is an umbrella term for ML methods that train models in a collaborative 
		fashion using decentralized data and computation.
				\\
		See also: ML, model, data.},first={federated learning (FL)},text={FL} }
	
\newglossaryentry{cfl}
{name={clustered federated learning (CFL)}, 
description={CFL trains local models for the 
 	devices in a FL application by using a clustering assumption, i.e., the devices 
 	of an FL network form clusters. Two devices in the same cluster generate 
 	local datasets with similar statistical properties. CFL pools the local datasets of devices 
 	in the same cluster to obtain a training set for a cluster-specific model. 
 	GTVMin clusters devices implicitly by enforcing approximate similarity of model parameters 
 	across well-connected nodes of the FL network.\\ 
 	See also: local model, device, FL, clustering assumption, FL network, cluster, local dataset, training set, model, GTVMin, model parameters.},
	first={clustered federated learning (CFL)},
	text={CFL} }

\newglossaryentry{iid}
{name={independent and identically distributed (i.i.d.)}, 
  description={It can be useful to 
		interpret data points ${\bf z}^{(1)},\ldots,{\bf z}^{(m)}$ 
		as realizations of i.i.d. RVs with 
		a common probability distribution. If these RVs are continuous-valued, their joint pdf 
		is $p\big({\bf z}^{(1)},\ldots,{\bf z}^{(m)} \big) = \prod_{r=1}^{m} p \big({\bf z}^{(r)}\big)$, 
		with $p({\bf z})$ being the common 
		marginal pdf of the underlying RVs.
				\\
		See also: data point, realization, RV, probability distribution, pdf.},
	first={independent and identically distributed (i.i.d.)},
	text={i.i.d.} 
}


\newglossaryentry{outlier}{name={outlier}, description={Many ML methods 
		are motivated by the i.i.d.\ assumption, which interprets data points as realizations of 
		i.i.d. RVs with a common probability distribution. The i.i.d.\ assumption is useful for applications  
		where the statistical properties of the data generation process are stationary (or time-invariant) \cite{Brockwell91}. 
		However, in some applications the data consists of a majority of regular data points 
		that conform with an i.i.d.\ assumption as well as a small number of data points that have fundamentally different 
        statistical properties compared to the regular data points. We refer to a data point that 
        substantially deviates from the statistical properties of most data points as an 
        outlier. Different methods for outlier detection use different measures for this deviation. 
        Stastistical learning theory studies fundamental limits on the ability to mitigate outliers reliably \cite{doi:10.1137/0222052}, \cite{10.1214/20-AOS1961}.
        		\\
		See also: ML, i.i.d.\ assumption, data point, realization, i.i.d., RV, probability distribution, data.},
	          first={outlier},text={outlier} }

\newglossaryentry{decisionregion}{name={decision region}, plural={decision regions}, description={Consider 
		a hypothesis map $h$ that delivers values from a finite set $\mathcal{Y}$. 
		For each label value (i.e., category) $a \in \mathcal{Y}$, the hypothesis $h$ 
		determines a subset of feature values ${\bf x} \in \mathcal{X}$ that result 
		in the same output $h({\bf x})=a$. We refer to this subset as a decision 
		region of the hypothesis $h$.
				\\
		See also: hypothesis, label, feature.},first={decision region},text={decision region} }

\newglossaryentry{decisionboundary}{name={decision boundary}, description={Consider a 
		hypothesis map $h$ that reads in a feature vector 
		${\bf x} \in \mathbb{R}^{d}$ and delivers a value from a finite set $\mathcal{Y}$. 
		The decision boundary of $h$ is the set of vectors ${\bf x} \in \mathbb{R}^{d}$ 
		that lie between different decision regions. More precisely, a 
		vector ${\bf x}$ belongs to the decision boundary if and only 
		if each neighborhood $\{ {\bf x}': \| {\bf x} - {\bf x}' \| \leq \varepsilon \}$, 
		for any $\varepsilon >0$, contains at least two vectors with different function values.
				\\
		See also: hypothesis, feature, decision region, neighborhood.},first={decision boundary},text={decision boundary} }


\newglossaryentry{euclidspace}{name={Euclidean space}, description={The 
		Euclidean space $\mathbb{R}^{d}$ of dimension $d \in \mathbb{N}$ consists 
		of vectors ${\bf x}= \big(x_{1},\ldots,x_{d}\big)$, with $d$ 
		real-valued entries $x_{1},\ldots,x_{d} \in \mathbb{R}$. Such an Euclidean 
		space is equipped with a geometric structure defined by the inner product 
		${\bf x}^{T} {\bf x}' = \sum_{j=1}^{d} x_{j} x'_{j}$ 
		between any two vectors ${\bf x},{\bf x}' \in \mathbb{R}^{d}$ \cite{RudinBookPrinciplesMatheAnalysis}.},first={Euclidean space},text={Euclidean space} }

\newglossaryentry{eerm}{name={explainable empirical risk minimization (EERM)}, description={EERM is an 
		instance of SRM that adds a regularization term to the 
		average loss in the objective function of ERM. 
		The regularization term is chosen to favor hypothesis maps that are intrinsically 
		explainable for a specific user. This user is characterized by their predictions provided 
		for the data points in a training set \cite{Zhang:2024aa}.
				\\
		See also: SRM, regularization, loss, objective function, ERM, hypothesis, prediction, data point, training set.},first={explainable empirical risk minimization (EERM)},text={EERM} }
	
	
\newglossaryentry{kmeans}{name={$k$-means}, description={The $k$-means algorithm 
		is a hard clustering method which assigns each data point of a dataset 
		to precisely one of $k$ different clusters. The method alternates between updating 
		the cluster assignments (to the cluster with the nearest mean) and, given the 
		updated cluster assignments, re-calculating the cluster means \cite[Ch. 8]{MLBasics}.
				\\
		See also: mean, algorithm, hard clustering, data point, dataset, cluster.},first={$k$-means},text={$k$-means} }


\newglossaryentry{xml}{name={explainable machine learning (explainable ML)}, description={Explainable 
		ML methods aim at complementing each prediction with an explanation of 
		how the prediction has been obtained. The construction of an explicit explanation 
		might not be necessary if the ML method uses a sufficiently simple (or interpretable) model \cite{rudin2019stop}.
				\\
		See also: ML, prediction, explanation, model.},first={explainable ML},text={explainable ML} }

\newglossaryentry{fmi}{name={Finnish Meteorological Institute (FMI)}, description={The
		FMI is a government agency responsible for gathering 
		and reporting weather data in Finland.
				\\
		See also: data.},first={Finnish Meteorological Institute (FMI)},text={FMI} }
	
\newglossaryentry{samplemean}{name={sample mean}, description={The sample mean 
			${\bf m} \in \mathbb{R}^{d}$ for a given dataset, with feature vectors ${\bf x}^{(1)},\ldots,{\bf x}^{(m)} \in \mathbb{R}^{d}$, 
			is defined as 
			$${\bf m} = (1/m) \sum_{r=1}^{m} {\bf x}^{(r)}.$$ 
					\\
		See also: sample, mean, dataset, feature vector.
		},
		first={sample mean},text={sample mean} }
	
\newglossaryentry{samplecovmtx}{name={sample covariance matrix}, description={The 
		sample covariance matrix $\widehat{\bf \Sigma} \in \mathbb{R}^{d \times d}$ 
		for a given set of feature vectors ${\bf x}^{(1)},\ldots,{\bf x}^{(m)} \in \mathbb{R}^{d}$ is defined as 
		$$\widehat{\bf \Sigma} = (1/m) \sum_{r=1}^{m} ({\bf x}^{(r)}\!-\!\widehat{{\bf m}}) ({\bf x}^{(r)}\!-\!\widehat{{\bf m}})^{T}.$$ 
		Here, we use the sample mean $\widehat{{\bf m}}$. 
				\\
		See also: covariance matrix, feature vector, sample mean.
	},
	first={sample covariance matrix},text={sample covariance matrix} }

\newglossaryentry{covmtx}{name={covariance matrix}, 
	description={The covariance matrix of an RV ${\bf x} \in \mathbb{R}^{d}$ 
		is defined as $\mathbb{E}  \bigg \{ \big( {\bf x} - \mathbb{E}  \big\{ {\bf x} \big\} \big)  \big({\bf x} - \mathbb{E}  \big\{ {\bf x} \big\} \big)^{T} \bigg\}$.
				\\
		See also: RV.},
	first={covariance matrix},text={covariance matrix} }
	
\newglossaryentry{highdimregime}{name={high-dimensional regime}, description={The 
		high-dimensional regime of ERM is characterized by the effective dimension of the model 
		being larger than the sample size, i.e., the number of (labeled) data points in the training set. 
		For example, linear regression methods operate in the high-dimensional regime whenever the number $d$ of features 
		used to characterize data points exceeds the number of data points in the training set. 
		Another example of ML methods that operate in the high-dimensional regime is large ANNs, which have 
		far more tunable weights (and bias terms) than the total number of data points in the training set. 
		High-dimensional statistics is a recent main thread of probability theory that studies the 
		behavior of ML methods in the high-dimensional regime \cite{Wain2019}, \cite{BuhlGeerBook}.
				\\
		See also: ERM, effective dimension, model, sample size, data point, training set, linear regression, feature, ML, ANN, weights, probability.},
   first={high-dimensional regime},text={high-dimensional regime} }

\newglossaryentry{gmm}{name={Gaussian mixture model (GMM)}, description={A GMM 
		is a particular type of probabilistic model for a numeric vector ${\bf x}$ (e.g., 
		the features of a data point). Within a GMM, the vector ${\bf x}$ is drawn from a randomly 
		selected multivariate normal distribution $p^{(c)} = \mathcal{N}\left({\bm \mu}^{(c)},\mathbf{C}^{(c)}\right)$ with 
		$c = I$. The index $I \in \{1,\ldots,k\}$ is an RV with probabilities $p({I=c}) = p_{c}$.
	     Note that a GMM is parametrized by the probability $p_{c}$, the 
		mean vector ${\bm \mu}^{(c)}$, and the covariance matrix $\mathbf{C}^{(c)}$ for each $c=1,\ldots,k$. 
		GMMs are widely used for clustering, density estimation, and as a generative model. 
				\\
		See also: probabilistic model, feature, data point, multivariate normal distribution, RV, mean, covariance matrix, clustering, model.
	 },first={Gaussian mixture model (GMM)},text={GMM} }
 
\newglossaryentry{maxlikelihood}{name={maximum likelihood}, description={
		Consider data points $\mathcal{D}=\big\{ {\bf z}^{(1)}, \ldots, {\bf z}^{(m)} \}$ 
		that are interpreted as the realizations of i.i.d. RVs with a common probability distribution $p({{\bf z}; {\bf w}})$ which 
		depends on the model parameters ${\bf w} \in \mathcal{W} \subseteq \mathbb{R}^{n}$. 
		Maximum likelihood methods learn model parameters ${\bf w}$ by maximizing 
		the probability (density) $p({\mathcal{D}; {\bf w}}) = \prod_{r=1}^{m} p({{\bf z}^{(r)}; {\bf w}})$ 
		of the observed dataset. Thus, the maximum likelihood estimator is a 
		solution to the optimization problem $\max_{{\bf w} \in \mathcal{W}} p({\mathcal{D}; {\bf w}})$.
				\\
		See also: data point, realization, i.i.d., RV, probability distribution, model parameters, maximum, dataset.
	},first={maximum likelihood},text={maximum likelihood}}



\newglossaryentry{em}{name={expectation-maximization (EM)}, description={
		 
		Consider a probabilistic model $p({{\bf z}; {\bf w}})$ for the data points $\mathcal{D}$ generated in some 
		ML application. The maximum likelihood estimator for the model parameters ${\bf w}$ is obtained by maximizing 
		$p({\mathcal{D}; {\bf w}})$. However, the resulting optimization problem might be computationally 
		challenging. EM approximates the maximum likelihood estimator by introducing a latent 
		RV ${\bf z}$ such that maximizing $p({\mathcal{D},{\bf z}; {\bf w}})$ would be easier \cite{hastie01statisticallearning}, \cite{BishopBook}, \cite{GraphModExpFamVarInfWainJor}. Since we 
		do not observe ${\bf z}$, we need to estimate it from the observed dataset $\mathcal{D}$ 
		using a conditional expectation. The resulting estimate $\widehat{{\bf z}}$ is then used to 
		compute a new estimate $\widehat{{\bf w}}$ by solving $\max_{{\bf w}} p({\mathcal{D}, \widehat{{\bf z}}; {\bf w}})$. 
		The crux is that the conditional expectation $\widehat{{\bf z}}$ depends on the model parameters $\widehat{{\bf w}}$, 
		which we have updated based on $\widehat{{\bf z}}$. Thus, we have to re-calculate $\widehat{{\bf z}}$, 
		which, in turn, results in a new choice $\widehat{{\bf w}}$ for the model parameters. In practice, 
		we repeat the computation of the conditional expectation (i.e., the E-step) and the update 
		of the model parameters (i.e., the M-step) until some stopping criterion is met. 
				\\
		See also: probabilistic model, data point, ML, maximum likelihood, model parameters, RV, dataset, expectation, stopping criterion.
  },first={EM},text={EM}}

\newglossaryentry{ppca}{name={probabilistic principal component analysis (PPCA)}, description={PPCA 
		extends basic PCA by using a probabilistic model for data points. The probabilistic model of PPCA 
		reduces the task of dimensionality reduction to an estimation problem that can be solved using EM 
		methods.
				\\
		See also: PCA, probabilistic model, data point, EM.},first={probabilistic principal component analysis (PPCA)},text={PPCA}}
	
\newglossaryentry{polyreg}{name={polynomial regression}, description={Polynomial 
		regression aims at learning a polynomial hypothesis map to predict a numeric label based
		 on the numeric features of a data point. For data points characterized by a single 
		 numeric feature, polynomial regression uses the hypothesis space 
			$\mathcal{H}^{(\rm poly)}_{d} := \{ h(x) = \sum_{j=0}^{d-1} x^{j} w_{j} \}.$
			The quality of a polynomial hypothesis map is measured using the average squared error loss 
			incurred on a set of labeled datapoints (which we refer to as the 
			training set).
					\\
		See also: regression, hypothesis, label, feature, data point, hypothesis space, squared error loss, labeled datapoint, training set.},first={polynomial regression},text={polynomial regression}}

\newglossaryentry{linreg}{name={linear regression}, description={Linear 
		regression aims to learn a linear hypothesis map to predict a numeric label based 
		on the numeric features of a data point. The quality of a linear hypothesis map is 
		measured using the average squared error loss incurred on a set of labeled datapoints, 
		which we refer to as the training set.
				\\
		See also: regression, hypothesis, label, feature, data point,  squared error loss, labeled datapoint, training set.},first={linear regression},text={linear regression}}
        
\newglossaryentry{ridgeregression}{name={ridge regression}, description={Ridge 
		regression learns the weights ${\bf w}$ of a linear hypothesis map $h^{({\bf w})}({\bf x})= {\bf w}^{T} {\bf x}$. The quality of a particular choice for the model parameters ${\bf w}$ is measured by the sum 
		of two components. The first component is the average squared error loss incurred by $h^{({\bf w})}$ on a set of 
		labeled datapoints (i.e., the training set). The second component is the scaled squared 
		Euclidean norm $\alpha \| {\bf w} \|^{2}_{2}$ with a regularization parameter 
		$\alpha > 0$. Adding $\alpha \| {\bf w} \|^{2}_{2}$ to 
	    the average squared error loss is equivalent to replacing original data points by the realizations 
	    of (infinitely many) i.i.d. RVs centered around these data points (see regularization).
	    		\\
		See also: regression, weights, hypothesis, model parameters, squared error loss, labeled datapoint, training set, norm, regularization, data point, realization, i.i.d., RV.},first={ridge regression},text={ridge regression}}


\newglossaryentry{expectation}{name={expectation}, description={
		Consider a numeric feature vector ${\bf x} \in \mathbb{R}^{d}$ 
		which we interpret as the realization of an RV with a probability distribution $p({\bf x})$. 
		The expectation of ${\bf x}$ is defined as the integral 
		$\mathbb{E}  \{ {\bf x} \} := \int {\bf x} p({\bf x})$. Note that 
		the expectation is only defined if this integral exists, i.e., if the RV 
		is integrable \cite{RudinBookPrinciplesMatheAnalysis}, \cite{HalmosMeasure}, \cite{BillingsleyProbMeasure}.
				\\
		See also: feature vector, realization, RV, probability distribution.},first={expectation},text={expectation}}

\newglossaryentry{logreg}{name={logistic regression}, description={Logistic regression learns a 
		linear hypothesis map (or classifier) $h({\bf x}) = {\bf w}^{T} {\bf x}$ 
		to predict a binary label $y$ based on the numeric feature vector ${\bf x}$ of 
		a data point. The quality of a linear hypothesis map is measured by the average logistic loss 
		on some labeled datapoints (i.e., the training set).
				\\
		See also: regression, hypothesis, classifier, label, feature vector, data point, logistic loss, labeled datapoint, training set.},
		first={logistic regression},text={logistic regression}}
	
\newglossaryentry{logloss}{name={logistic loss}, description={Consider 
		a data point characterized by the features ${\bf x}$ and a binary label $y \in \{-1,1\}$. 
		We use a real-valued hypothesis $h$ to predict the label $y$ 
		from the features ${\bf x}$. The logistic loss incurred by this prediction is 
		defined as 
	\begin{equation} 
		\label{equ_log_loss_gls_dict}
		L\left(({\bf x},y),h \right) :=  \log ( 1 + \exp(- y h({\bf x}))).
\end{equation}
Carefully note that the expression \eqref{equ_log_loss_gls_dict} 
for the logistic loss applies only for the label space $\mathcal{Y} = \{ -1,1\}$ and when using 
the thresholding rule \eqref{equ_def_threshold_bin_classifier_dict}. 
		\\
		See also: data point, feature, label, hypothesis, loss, prediction, label space.},first={logistic loss},text={logistic loss}}
	
\newglossaryentry{hingeloss}{name={hinge loss}, description={Consider a data point 
		characterized by a feature vector ${\bf x} \in \mathbb{R}^{d}$ and a 
		binary label $y \in \{-1,1\}$. The hinge loss incurred by a real-valued 
		hypothesis map $h({\bf x})$ is defined as 
		\begin{equation} 
			\label{equ_hinge_loss_gls_dict}
				L\left(({\bf x},y),h \right) := \max \{ 0 , 1 - y h({\bf x}) \}. 
			\end{equation}
\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
    \begin{axis}[
        axis lines=middle,
        xlabel={$\truelabelh({\bf x})$},
        ylabel={$L\left(({\bf x},y),h \right)$},
 	xlabel style={at={(axis description cs:1.,0.3)}, anchor=north},  % Adjusted to be relative to axis end
        ylabel style={at={(axis description cs:0.5,1.1)}, anchor=center}, % Corrected to vertical position, rotated for readability
        xmin=-3.5, xmax=3.5,
        ymin=-0.5, ymax=2.5,
        xtick={-3, -2, -1, 0, 1, 2, 3},
        ytick={0, 1, 2},
        domain=-3:3,
        samples=100,
        width=10cm, height=6cm,
        grid=both,
        major grid style={line width=.2pt, draw=gray!50},
        minor grid style={line width=.1pt, draw=gray!20},
        legend pos=south west % Positions legend at the bottom left
    ]
        \addplot[blue, thick] {max(0, 1-x)};
     %   \addlegendentry{$\max(0, 1-x)$}
    \end{axis}
\end{tikzpicture}
\caption{A regularized variant of the hinge loss is used by the SVM \cite{LampertNowKernel}.}
\label{fig_hingeloss}
\end{center}
\end{figure} 	    
		See also: data point, feature vector, label, loss, hypothesis, SVM.
		},first={hinge loss},text={hinge loss}}

\newglossaryentry{iidasspt}{name={independent and identically distributed assumption (i.i.d.\ assumption)}, description={The i.i.d. 
		assumption interprets data points of a dataset as the 
		realizations of i.i.d. RVs.
				\\
		See also: i.i.d., data point, dataset, realization, RV.},first={independent and identically distributed assumption (i.i.d.\ assumption)},text={i.i.d.\ assumption} }

\newglossaryentry{hypospace}{name={hypothesis space}, plural={hypothesis spaces}, description={Every 
		practical ML method uses a hypothesis space (or model) $\mathcal{H}$. The hypothesis space 
		of an ML method is a subset of all possible maps from the feature space to the label space. 
		The design choice of the hypothesis space should take into account available computational resources and 
		statistical aspects. If the computational infrastructure allows for efficient matrix operations, and there 
		is an (approximately) linear relation between a set of features and a label, a useful choice for the 
		hypothesis space might be the linear model.
				\\
		See also: ML, hypothesis, model, feature space, label space, statistical aspects, feature, label, linear model.},first={hypothesis space},text={hypothesis space} }
	
\newglossaryentry{model}{name={model}, plural={models}, description={In the context of ML, 
		the term model typically refers to the hypothesis space underlying an 
		ML method \cite{MLBasics}, \cite{ShalevMLBook}. However, the term is also used in other 
		fields but with a different meaning. For example, a probabilistic model refers to a parametrized 
		set of probability distributions.
				\\
		See also: ML, hypothesis space, probabilistic model, probability distribution.},first={model},text={model} }

\newglossaryentry{modelparams}{name={model parameters}, 
	description={Model parameters are quantities that 
	are used to select a specific hypothesis map from a model. 
	We can think of a list of model parameters as a unique identifier for a hypothesis 
	map, similar to how a social security number identifies a person in Finland.
			\\
		See also: model, parameters, hypothesis.},
	first={model parameters},text={model parameters} }

\newglossaryentry{ai}{name={artificial intelligence (AI)}, description={
		AI refers to systems that behave rationally in the sense of 
		maximizing a long-term reward. The ML-based approach to AI is to train a model for  
		predicting optimal actions. These predictions are computed from observations about the state of the 
		environment. The choice of loss function sets AI applications apart from more basic ML applications. 
		AI systems rarely have access to a labeled training set that allows the average loss to be measured for any possible choice of model parameters. 
		Instead, AI systems use observed reward signals to obtain a (point-wise) estimate for the 
		loss incurred by the current choice of model parameters.
				\\
		See also: reward, ML, model, loss function, training set, loss, model parameters.},first={AI},text={AI} }

\newglossaryentry{reward}{name={reward}, description={A reward refers to some observed 
		(or measured) quantity that allows us to estimate the loss incurred by the prediction 
		(or decision) of a hypothesis $h({\bf x})$. For example, in an 
		ML application to self-driving vehicles, $h({\bf x})$ could represent 
		the current steering direction of a vehicle. We could construct a reward from the 
		measurements of a collision sensor that indicate if the vehicle is moving towards 
		an obstacle. We define a low reward for the steering direction 
	$h({\bf x})$ if the vehicle moves dangerously towards an obstacle.
			\\
		See also: loss, prediction, hypothesis, ML.},
	first={reward}, text={reward}} 

\newglossaryentry{hardclustering}
{name={hard clustering}, 
 description={Hard clustering 
		refers to the task of partitioning a given set of data points into (a few) non-overlapping clusters. 
		The most widely used hard clustering method is $k$-means.
				\\
		See also: clustering, data point, cluster, $k$-means.},
 first={hard clustering},
 text={hard clustering} 
}
	
\newglossaryentry{softclustering}
{name={soft clustering}, 
 description={Soft clustering refers to the task of partitioning a given set of data points 
                      into (a few) overlapping clusters. Each data point is assigned to several different clusters 
                      with varying degrees of belonging. Soft clustering methods determine the degree of belonging (or soft cluster assignment) 
                      for each data point and each cluster. A principled approach to soft clustering is by 
                      interpreting data points as i.i.d. realizations of a GMM. We then obtain a natural 
                      choice for the degree of belonging as the conditional probability of a data point belonging to a specific mixture component.
				\\
		See also: clustering, data point, cluster, degree of belonging, i.i.d., realization, GMM, probability.},
 first={soft clustering},
 text={soft clustering} 
}
	
\newglossaryentry{clustering}{name={clustering}, description={Clustering methods decompose a given 
		set of data points into a few subsets, which are referred to as clusters. 
		Each cluster consists of data points that are more similar to each 
		other than to data points outside the cluster. Different clustering methods 
		use different measures for the similarity between data points and different 
		forms of cluster representations. The clustering method $k$-means uses the 
		average feature vector of a cluster (i.e., the cluster mean) as its representative. 
		A popular soft clustering method based on GMM represents 
		a cluster by a multivariate normal distribution.
				\\
		See also: data point, cluster, $k$-means, feature, mean, soft clustering, GMM, multivariate normal distribution.},first={clustering},text={clustering} }
	
\newglossaryentry{cluster}{name={cluster}, plural={clusters}, description={A cluster is a subset of 
		data points that are more similar to each other than to the data points outside the cluster. 
		The quantitative measure of similarity between data points is a design choice. If data points 
		are characterized by Euclidean feature vectors ${\bf x} \in \mathbb{R}^{d}$, 
		we can define the similarity between two data points via the Euclidean distance between 
		their feature vectors. An example of such clusters is shown in Fig.~\ref{fig:clusters}.\\
		\begin{figure}[H]
		\centering
		\begin{tikzpicture}
		\pgfplotsset{compat=1.18}
		\begin{axis}[
		    width=10cm,
		    height=8cm,
		    xlabel={$x_1$},
		    ylabel={$x_2$},
		    title={Clusters of Data Points},
		    xmin=0, xmax=10,
		    ymin=0, ymax=10,
		    axis lines=left,
		    legend style={at={(0.5,-0.25)}, anchor=north, legend columns=3}
		]
		% Cluster 1 
		\addplot[only marks, color=blue, mark=*, mark size=3pt] coordinates {
		    (1,1) (2,1.2) (1.8,2) (2.2,1.5) (1.5,2.5)
		};
		% Cluster 2 
		\addplot[only marks, color=red, mark=square*, mark size=3pt] coordinates {
		    (7,8) (8,7.5) (7.5,8.5) (8.2,7.8) (7.7,7)
		};
		% Cluster 3 
		\addplot[only marks, color=green!60!black, mark=triangle*, mark size=3pt] coordinates {
		    (5,3) (5.5,3.2) (5.2,2.8) (4.8,3.5) (5.1,3.1)
		};
		\legend{Cluster 1, Cluster 2, Cluster 3}
		\end{axis}
		\end{tikzpicture}
		\caption{Illustration of three clusters in a two-dimensional feature space. Each cluster groups data points that are more similar to each other than to those in other clusters, based on the Euclidean distance.}
		\label{fig:clusters}
		\end{figure}
		See also: data point, feature vector, feature space.
		},
		first={cluster},text={cluster} }

\newglossaryentry{huberloss}{name={Huber loss}, description={The 
		Huber loss unifies the squared error loss and the absolute error loss.
				\\
		See also: loss, squared error loss, absolute error loss.},first={Huber loss},text={Huber loss} }

\newglossaryentry{svm}{name={support vector machine (SVM)}, description={The 
		SVM is a binary classification method that 
		learns a linear hypothesis map. Thus, like linear regression and logistic regression, 
		it is also an instance of ERM for the linear model. However, the 
		SVM uses a different loss function from the one used in those methods. As illustrated in 
		Fig. \ref{fig_svm_gls_dict}, it aims to maximally separate data points from 
		the two different classes in the feature space (i.e., maximum margin principle). 
		Maximizing this separation is equivalent to minimizing a regularized 
		variant of the hinge loss \eqref{equ_hinge_loss_gls_dict} \cite{BishopBook}, \cite{LampertNowKernel}, \cite{Cristianini_Shawe-Taylor_2000}.
		\begin{figure}[H]
			\begin{center}
				\begin{tikzpicture}[auto,scale=0.8]
					\draw [thick] (1,2) circle (0.1cm)node[anchor=west] {\hspace*{0mm}${\bf x}^{(5)}$};
					\draw [thick] (0,1.6) circle (0.1cm)node[anchor=west] {\hspace*{0mm}${\bf x}^{(4)}$};
					\draw [thick] (0,3) circle (0.1cm)node[anchor=west] {\hspace*{0mm}${\bf x}^{(3)}$};
					\draw [thick] (2,1) circle (0.1cm)node[anchor=east,above] {\hspace*{0mm}${\bf x}^{(6)}$};
					\node[] (B) at (-2,0) {support vector};
					\draw[->,dashed] (B) to (1.9,1) ; 
					\draw [|<->|,thick] (2.05,0.95)  -- (2.75,0.25)node[pos=0.5] {$\xi$} ; 
					\draw [thick] (1,-1.5) -- (4,1.5) node [right] {$h^{({\bf w})}$} ; 
					\draw [thick] (3,-1.9) rectangle ++(0.1cm,0.1cm) node[anchor=west,above]  {\hspace*{0mm}${\bf x}^{(2)}$};
					\draw [thick] (4,.-1) rectangle ++(0.1cm,0.1cm) node[anchor=west,above] {\hspace*{0mm}${\bf x}^{(1)}$};
				\end{tikzpicture}
				\caption{The SVM learns a hypothesis (or classifier) $h^{({\bf w})}$ with 
					minimal average soft-margin hinge loss. Minimizing this loss is equivalent 
					to maximizing the margin $\xi$ between the decision boundary of $h^{({\bf w})}$ 
					and each class of the training set.}
				\label{fig_svm_gls_dict}
			\end{center}
		\end{figure}
		The above basic variant of SVM is only useful if the data points from different categories can be  
		(approximately) linearly separated. For an ML application where the categories are not 
		derived from a kernel.
				\\
		See also: classification, hypothesis, linear regression, logistic regression, ERM, linear model, loss function, data point, feature space, maximum, hinge loss, SVM, classifier, loss, decision boundary, training set, ML, kernel.
},first={support vector machine (SVM)},text={SVM} }

\newglossaryentry{eigenvalue}{name={eigenvalue}, plural={eigenvalues}, description={We refer to a 
		number $\lambda \in \mathbb{R}$ as an eigenvalue of a square matrix $\mathbf{A} \in \mathbb{R}^{d \times d}$ 
		if there is a non-zero vector ${\bf x} \in \mathbb{R}^{d} \setminus \{ \mathbf{0} \}$ such that $\mathbf{A} {\bf x} = \lambda {\bf x}$.
			},first={eigenvalue},text={eigenvalue} }
	
\newglossaryentry{eigenvector}{name={eigenvector}, plural={eigenvectors}, description={An 
		eigenvector of a matrix $\mathbf{A} \in \mathbb{R}^{d \times d}$ 
		is a non-zero vector ${\bf x} \in \mathbb{R}^{d} \setminus \{ \mathbf{0} \}$ 
		such that $\mathbf{A} {\bf x} = \lambda {\bf x}$ with some eigenvalue $\lambda$.
				\\
		See also: eigenvalue.},first={eigenvector},text={eigenvector} }

\newglossaryentry{evd}{name={eigenvalue decomposition (EVD)}, 
	description={The EVD
		for a square matrix ${\bf A} \in \mathbb{R}^{d \times d}$ 
		is a factorization of the form 
		$${\bf A} = \mathbf{V} {\bm \Lambda} \mathbf{V}^{-1}.$$ 
		The columns of the matrix $\mathbf{V} = \big( {\bf v}^{(1)},\ldots,{\bf v}^{(d)} \big)$ are the 
		eigenvectors of the matrix $\mathbf{V}$. The diagonal matrix 
		${\bm \Lambda} = {\rm diag} \big\{ \lambda_{1},\ldots,\lambda_{d} \big\}$ 
		contains the eigenvalues $\lambda_{j}$ corresponding to the eigenvectors ${\bf v}^{(j)}$. 
		Note that the above decomposition exists only if the matrix ${\bf A}$ is diagonalizable.
				\\
		See also: eigenvector, eigenvalue.},first={eigenvalue decomposition (EVD)},text={EVD} }

\newglossaryentry{svd}{name={singular value decomposition (SVD)}, 
  	description={The SVD  
  		for a matrix ${\bf A} \in \mathbb{R}^{m \times d}$ 
		is a factorization of the form 
		$${\bf A} = \mathbf{V} {\bm \Lambda} \mathbf{U}^{T},$$ 
		with orthonormal matrices $\mathbf{V} \in \mathbb{R}^{m \times m}$ 
		and $\mathbf{U} \in \mathbb{R}^{d \times d}$ \cite{GolubVanLoanBook}. 
		The matrix ${\bm \Lambda} \in \mathbb{R}^{m \times d}$ is 
		only non-zero along the main diagonal, whose entries $\Lambda_{j,j}$ 
		are non-negative and referred to as singular values.
	},first={singular value decomposition (SVD)},text={SVD} }


\newglossaryentry{tv}{name={total variation}, description={See GTV.},
	first={total variation},text={total variation} }

 \newglossaryentry{cvxclustering}{name={convex clustering}, 
 	description={Consider a dataset 
 	${\bf x}^{(1)},\ldots,{\bf x}^{(m)} \in \mathbb{R}^{d}$. 
 	Convex clustering learns vectors ${\bf w}^{(1)},\ldots,{\bf w}^{(m)}$ by 
 	minimizing 
 	$$ \sum_{r=1}^{m} \left\Vert  {{\bf x}^{(r)} - {\bf w}^{(r)}} \right\Vert_{2}^2 + 
 	\alpha \sum_{i,i' \in \mathcal{V}} \left\Vert  {{\bf w}^{(i)} - {\bf w}^{(i')}} \right\Vert_{p}.$$ 
	Here, $ \left\Vert  {{\bf u}} \right\Vert_{p} := \big( \sum_{j=1}^{d} |u_{j}|^{p} \big)^{1/p}$ 
	denotes the $p$-norm (for $p\geq1$).  
	It turns out that many of the optimal vectors $\widehat{{\bf w}}^{(1)},\ldots,\widehat{{\bf w}}^{(m)}$ 
	coincide. A cluster then consists of those data points $r \in \{1,\ldots,m\}$ 
	with identical $\widehat{{\bf w}}^{(r)}$ \cite{JMLR:v22:18-694}, \cite{Pelckmans2005}. 
			\\
		See also: dataset, convex, clustering, norm, cluster, data point.
 	  },
 		first={convex clustering},text={convex clustering} }


\newglossaryentry{gdmethods}{name={gradient-based methods}, 
	description={Gradient-based 
		methods are iterative techniques for finding the minimum (or maximum) 
		of a differentiable objective function of the model parameters. These 
		methods construct a sequence of approximations to an optimal choice for 
		model parameters that results in a minimum (or maximum) value of the objective function. 
		As their name indicates, gradient-based methods use the gradients of the objective function 
		evaluated during previous iterations to construct new, (hopefully) improved model parameters. 
		One important example of a gradient-based method is GD.
				\\
		See also: gradient, minimum, maximum, differentiable, objective function, model parameters, GD.},
		first={gradient-based methods},text={gradient-based methods} }

\newglossaryentry{sgd}{name={subgradient descent}, description={Subgradient 
		descent is a generalization of GD that does not require differentiability of the 
		function to be minimized. This generalization is obtained by replacing the concept 
		of a gradient with that of a subgradient. Similar to gradients, also subgradients 
		allow us to construct local approximations of an objective function. The objective function 
		might be the empirical risk $\widehat{L}\big( h^{({\bf w})} \big| \mathcal{D} \big)$ viewed 
		as a function of the model parameters ${\bf w}$ that select a hypothesis $h^{({\bf w})} \in \mathcal{H}$.
				\\
		See also: subgradient, generalization, GD, gradient, objective function, empirical risk, model parameters, hypothesis.},first={subgradient descent},text={subgradient descent} }
	
\newglossaryentry{stochGD}{name={stochastic gradient descent (SGD)}, description={SGD 
		is obtained from GD by replacing the gradient of the objective function 
		with a stochastic approximation. A main application of SGD
		is to train a parametrized model via ERM on a training set $\mathcal{D}$ that 
		is either very large or not readily available (e.g., when data points are stored 
		in a database distributed all over the planet). To evaluate the gradient of the 
		empirical risk (as a function of the model parameters ${\bf w}$), 
		we need to compute a sum $\sum_{r=1}^{m} \nabla_{{\bf w}} L\left({\bf z}^{(r)},{\bf w} \right)$  
		over all data points in the training set. We obtain a stochastic 
		approximation to the gradient by replacing the sum $\sum_{r=1}^{m} \nabla_{{\bf w}} L\left({\bf z}^{(r)},{\bf w} \right)$ 
		with a sum $\sum_{r \in \mathcal{B}} \nabla_{{\bf w}} L\left({\bf z}^{(r)},{\bf w} \right)$ 
		over a randomly chosen subset $\mathcal{B} \subseteq \{1,\ldots,m\}$ (see Fig. \ref{fig_sgd_approx_dict}). 
		We often refer to these randomly chosen data points as a batch. 
		The batch size $|\mathcal{B}|$ is an important parameter of SGD. 
		SGD with $|\mathcal{B}|> 1$ is referred to as mini-batch SGD \cite{Bottou99}. 		
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[scale=1.5, >=stealth]
				\draw[thick, blue, domain=0.5:2.5, samples=100] plot (\x, {(\x-1.5)^2 + 1});
				\node[blue,above] at (0.5, 2) {$\sum_{r=1}^{m}$};
				\draw[thick, red, domain=1:3, samples=100] plot (\x, {(\x-2)^2 + 0.5});
				\node[red] at (3.3, 1.5) {$\sum_{r \in \mathcal{B}}$};
			\end{tikzpicture}
		\caption{SGD for ERM approximates the gradient 
		$\sum_{r=1}^{m} \nabla_{{\bf w}} L\left({\bf z}^{(r)},{\bf w} \right)$ 
		by replacing the 
		sum over all data points in the training set (indexed by $r=1,\ldots,m$) 
		with a sum over a randomly chosen subset $\mathcal{B} \subseteq \{1,\ldots,m\}$.\label{fig_sgd_approx_dict}}
		\end{figure}
		See also: GD, gradient, objective function, model, ERM, training set, data point, empirical risk, model parameters, batch.
},first={stochastic gradient descent (SGD)},text={SGD} }


\newglossaryentry{onlineGD}{name={online gradient descent (online GD)}, description={
Consider  an ML method that learns model parameters 
${\bf w}$ from some parameter space $\mathcal{W} \subseteq \mathbb{R}^{d}$. 
The learning process uses data points ${\bf z}^{(t)}$ that arrive at consecutive time-instants $t=1,2,\ldots$. 
Let us interpret the data points ${\bf z}^{(t)}$ as i.i.d. copies 
of an RV ${\bf z}$. The risk $\mathbb{E} \{ L\left({\bf z},{\bf w} \right) \}$ of a 
hypothesis $h^{({\bf w})}$ can then (under mild conditions) be obtained as the limit 
$\lim_{T\rightarrow \infty} (1/T)\sum_{t=1}^{T} L\left({\bf z}^{(t)},{\bf w} \right)$. 
We might use this limit as the objective function for learning the model parameters ${\bf w}$. 
Unfortunately, this limit can only be evaluated if we wait infinitely long in order to collect all data points. 
Some ML applications require methods that learn online, i.e., as soon as a new data point ${\bf z}^{(t)}$ 
arrives at time $t$, we update the current model parameters ${\bf w}^{(t)}$. Note that 
the new data point ${\bf z}^{(t)}$ contributes the component $L\left({\bf z}^{(t)},{\bf w} \right)$ 
to the risk. As its name suggests, online GD updates ${\bf w}^{(t)}$ via a (projected) gradient step such that
\begin{equation} 
\label{equ_def_ogd_dict}
 {\bf w}^{(t+1)} := P_{\mathcal{W}}\big( {\bf w}^{(t)} - \eta_{t} \nabla_{{\bf w}} L\left({\bf z}^{(t)},{\bf w} \right)\big) . 
\end{equation} 
Note that \eqref{equ_def_ogd_dict} is a gradient step for the current component $L\left({\bf z}^{(t)},\cdot \right)$ 
of the risk. The update \eqref{equ_def_ogd_dict} ignores all the previous components $L\left({\bf z}^{(t')},\cdot \right)$, 
for $t' < t$. It might therefore happen that, compared to ${\bf w}^{(t)}$, the updated model parameters 
${\bf w}^{(t+1)}$ increase the retrospective average loss $\sum_{t'=1}^{t-1} L\left({\bf z}^{(t')},\cdot \right)$. 
However, for a suitably chosen learning rate $\eta_{t}$, online GD can be shown 
to be optimal in practically relevant settings. By optimal, we mean that the model parameters 
${\bf w}^{(T+1)}$ delivered by online GD after observing $T$ data points ${\bf z}^{(1)},\ldots, {\bf z}^{(T)}$ 
are at least as good as those delivered by any other learning method \cite{HazanOCO}, \cite{GDOptimalRakhlin2012}. 
\begin{figure}[H]
	\begin{center}
\begin{tikzpicture}[x=1.5cm,scale=1.5, every node/.style={font=\footnotesize}]
	\draw[->] (0.5, 0) -- (5.5, 0) node[below] {};
	\foreach \x in {1, 2, 3, 4, 5} {
		\draw (\x, 0.1) -- (\x, -0.1) node[below] {$t=\x$};
	}
	\foreach \x/\y in {1/2.5, 2/1.8, 3/2.3, 4/1.5, 5/2.0} {
		\fill[black] (\x, \y) circle (2pt) node[above right] {${\bf z}^{(\x)}$};
	}
	\foreach \x/\y in {1/1.0, 2/1.6, 3/1.8, 4/2.2, 5/1.9} {
		\fill[blue] (\x, \y) circle (2pt) node[below left] {${\bf w}^{(\x)}$};
	}
	\foreach \x/\y/\z in {1/2.5/1.0, 2/1.8/1.6, 3/2.3/2.0, 4/1.5/1.8, 5/2.0/1.9} {
		\draw[dashed, gray] (\x, \y) -- (\x, \z);
	}
	\end{tikzpicture}
\end{center} 
\caption{An instance of online GD that updates the model parameters ${\bf w}^{(t)}$ 
using the data point ${\bf z}^{(t)} = x^{(t)}$ arriving at time $t$. 
This instance uses the squared error loss $L\left({\bf z}^{(t)},w \right) = (x^{(t)} - w)^{2}$.
}
\end{figure}
		See also: ML, model parameters, parameter space, data point, i.i.d., RV, risk, hypothesis, objective function, GD, gradient step, loss, learning rate, squared error loss.},
first={online gradient descent (online GD)},text={online GD}}

\newglossaryentry{pca}{name={principal component analysis (PCA)}, description={PCA 
		determines a linear feature map such that the new features 
		allow us to reconstruct the original features with the minimum reconstruction error \cite{MLBasics}.
				\\
		See also: feature map, feature, minimum.},first={principal component analysis (PCA)},text={PCA} }
	
\newglossaryentry{loss}{name={loss}, description={ML methods use a 
		loss function $L\left({\bf z},h \right)$ to measure the error incurred 
		by applying a specific hypothesis to a specific data point. With a
		slight abuse of notation, we use the term loss for both the loss function $L$ 
		itself and the specific value $L\left({\bf z},h \right)$, for a data point ${\bf z}$ 
		and hypothesis $h$.
				\\
		See also: ML, loss function, hypothesis, data point.},first={loss},text={loss} }

\newglossaryentry{lossfunc}{name={loss function}, description={A loss function is a map 
		$$L: \mathcal{X} \times \mathcal{Y} \times \mathcal{H} \rightarrow \mathbb{R}_{+}: \big( \big({\bf x},y\big),
		 h\big) \mapsto  L\left(({\bf x},y),h \right).$$
		It assigns a non-negative real number (i.e., the loss) $L\left(({\bf x},y),h \right)$
		to a pair that consists of a data point, with features ${\bf x}$ 
		and label $y$, and a hypothesis $h \in \mathcal{H}$. The 
		value $L\left(({\bf x},y),h \right)$ quantifies the discrepancy 
		between the true label $y$ and the prediction $h({\bf x})$. 
		Lower (closer to zero) values $L\left(({\bf x},y),h \right)$ indicate a smaller 
		discrepancy between prediction $h({\bf x})$ and label $y$. 
		Fig. \ref{fig_loss_function_gls_dict} depicts a loss function for a given data point, 
		with features ${\bf x}$ and label $y$, as a function of the hypothesis $h \in \mathcal{H}$. 
		\begin{figure}[H]
			\begin{center}
				\begin{tikzpicture}[scale = 0.7]
					\begin{axis}
						[axis x line=center,
						axis y line=center,
						xlabel={},
						xlabel style={below right},
						ylabel style={above right},
						xtick=\empty,
						ytick=\empty,
						xmin=-4,
						xscale = 1.4, 
						xmax=4,
						ymin=-0.5,
						ymax=2.5
						]
						\addplot [smooth, ultra thick] table [x=a, y=b, col sep=comma] {assets/logloss.csv};    
					\end{axis}
					\node [above] at (1,5) {$L\left(({\bf x},y),h \right)$};
					\node [above] at (10,1) {hypothesis $h$};
						\node [right] at (4,6) {loss};
				\end{tikzpicture}
			\end{center}
			\vspace*{-7mm}
			\caption{Some loss function $L\left(({\bf x},y),h \right)$ for a fixed data point, with 
				feature vector ${\bf x}$ and label $y$, and a varying hypothesis $h$. 
				ML methods try to find (or learn) a hypothesis that incurs minimal loss.}
			\label{fig_loss_function_gls_dict}
	\end{figure}
		See also: loss, data point, feature, label, hypothesis, prediction, feature vector, ML.
 },first={loss function},text={loss function} }

\newglossaryentry{decisiontree}{name={decision tree}, plural={decision trees}, description={A 
		decision tree is a flow-chart-like representation of a hypothesis map $h$. 
		More formally, a decision tree is a directed graph containing a root node that reads 
		in the feature vector ${\bf x}$ of a data point. The root node then forwards 
		the data point to one of its child nodes based on some elementary test on the features ${\bf x}$. 
		If the receiving child node is not a leaf node, i.e., it has itself child nodes, 
	  it represents another test. Based on the test result, the data point is forwarded 
	   to one of its descendants. This testing and forwarding of the data point is continued 
	  until the data point ends up in a leaf node (having no child nodes). 
\begin{figure}[H]
\begin{minipage}{.45\textwidth}
	\scalebox{1}{
\begin{tikzpicture}
	\node[fill=black, circle, inner sep=2pt, label=above:{$\| {\bf x}-\mathbf{u} \| \leq \varepsilon$?}] (A) {};	
	\node[fill=black, circle, inner sep=2pt, below left=1.5cm and 1cm of A, label=left:{$h({\bf x}) = \hat{y}_1$}] (B) {};
	\node[fill=black, circle, inner sep=2pt, below right=1.5cm and 1cm of A, label=right:{$\| {\bf x} - \mathbf{v} \| \leq \varepsilon$?}] (C) {};
	\node[fill=black, circle, inner sep=2pt, below left=1.5cm and 1cm of C, label=left:{$h({\bf x}) = \hat{y}_2$}] (D) {};
	\node[fill=black, circle, inner sep=2pt, below right=1.5cm and 1cm of C, label=right:{$h({\bf x}) =\hat{y}_3$}] (E) {};
	\draw[line width=1.5pt, ->] (A) -- (B) node[midway, left] {no};
	\draw[line width=1.5pt, ->] (A) -- (C) node[midway, right] {yes};
	\draw[line width=1.5pt, ->] (C) -- (D) node[midway, left] {no};
	\draw[line width=1.5pt, ->] (C) -- (E) node[midway, right] {yes};
\end{tikzpicture}
	}
\end{minipage}	
\hspace*{15mm}
\begin{minipage}{.45\textwidth}
	\hspace*{15mm}
	\begin{tikzpicture}
		\draw (-2,2) rectangle (2,-2);
		\begin{scope}
			\clip (-0.5,0) circle (1cm);
			\clip (0.5,0) circle (1cm);
			\fill[color=gray] (-2,1.5) rectangle (2,-1.5);
		\end{scope}
		\draw (-0.5,0) circle (1cm);
		\draw (0.5,0) circle (1cm);
		\draw[fill] (-0.5,0) circle [radius=0.025];
		\node [below right, red] at (-0.5,0) {$\hat{y}_{3}$};
		\node [below left, blue] at (-0.7,0) {$\hat{y}_{2}$};
		\node [above left] at (-0.7,1) {$\hat{y}_{1}$};
		\node [left] at (-0.4,0) {$\mathbf{u}$};
		\draw[fill] (0.5,0) circle [radius=0.025];
		\node [right] at (0.6,0) {$\mathbf{v}$};
	\end{tikzpicture}
\end{minipage}
	\caption{Left: A decision tree is a flow-chart-like representation of a piece-wise constant hypothesis $h: \mathcal{X} \rightarrow \mathbb{R}$.  Each piece is a decision region $\mathcal{R}_{\hat{y}} := \big\{ {\bf x} \in  \mathcal{X}: h({\bf x}) = \hat{y} \big\}$. 
		The depicted decision tree can be applied to numeric feature vectors, i.e., $\mathcal{X} \subseteq \mathbb{R}^{d}$. It is  parametrized by the threshold $\varepsilon>0$ and the vectors ${\bf u}, {\bf v} \in \mathbb{R}^{d}$. 
		Right: A decision tree partitions  
		the feature space $\mathcal{X}$ into decision regions. Each decision region  
		$\mathcal{R}_{\hat{y}} \!\subseteq\!\mathcal{X}$ corresponds to a specific leaf node in the decision tree.}
	\label{fig_decision_tree}
\end{figure} 
		See also: hypothesis, graph, feature vector, data point, feature, decision region, feature space.
	  }
	  ,first={decision tree},text={decision tree} }

\newglossaryentry{API} 
{name={application programming interface (API)},
		description={			
			An  API is a formal mechanism that 
			allows software components to interact in a structured and modular way \cite{RestfulBook2013}.
			In the context of ML, APIs are commonly used to provide access to a trained ML model. 
			Users—whether humans or machines—can submit the feature vector of a data point and receive 
			a corresponding prediction. Suppose a trained ML model is defined 
			as $\widehat{h}(x) := 2 x + 1$. Through an API, a user 
			can input $x = 3$ and receive the output $\widehat{h}(3) = 7$ 
			without knowledge of the detailed structure of the ML model or its training. 
			In practice, the model is typically deployed on a server connected to the internet. 
			Clients send requests containing feature values to the server, which responds with 
			the computed prediction $\widehat{h}({\bf x})$. APIs promote modularity 
			in ML system design, i.e., one team can develop and train the model, while another team
			handles integration and user interaction. Publishing a trained model via an API also 
			offers practical advantages: 
			\begin{itemize} 
				\item The server can centralize computational resources which are required to compute predictions. 
		        \item The internal structure of the model remains hidden (which is useful for protecting IP or trade secrets). 
		    \end{itemize} 
			However, APIs are not without risk. Techniques such as model inversion can potentially reconstruct a 
			model from its predictions on carefully selected feature vectors.
					\\
		See also: ML, model, feature vector, data point, prediction, feature, model inversion.
			},
		first={application programming interface (API)},
		text={API}
}

\newglossaryentry{modelinversion}{name={model inversion},description={TBD.},first={model inversion},text={model inversion}}


\newglossaryentry{hilbertspace}{
	name={Hilbert space},
	description={A Hilbert space is a complete inner 
		product space \cite{introhilbertbook}. That is, it is a vector space equipped 
		with an inner product between pairs of vectors, and it satisfies the additional requirement 
		of completeness, i.e., every Cauchy sequence of vectors converges to a limit within the space. 
		A canonical example of a Hilbert space is the Euclidean space $\mathbb{R}^{d}$, 
		for some dimension $d$, consisting of vectors ${\bf u} = \big(u_1, \ldots, u_{d}\big)^T$ 
		and the standard inner product ${\bf u}^T {\bf v}$.
				\\
		See also: Euclidean space.},
	first={Hilbert space},
	text={Hilbert space}
}



\newglossaryentry{sample}{name={sample}, plural={samples}, description={A 
		finite sequence (or list) of data points ${\bf z}^{(1)},\ldots,{\bf z}^{(m)}$ that 
		is obtained or interpreted as the realization of $m$ i.i.d. RVs 
		with a common probability distribution $p({\bf z})$. The length $m$ of 
		the sequence is referred to as the sample size.
				\\
		See also: data point, realization, i.i.d., RV, probability distribution, sample size.},first={sample},text={sample}}
	
\newglossaryentry{samplesize}
{name={sample size},
  description={The number of individual data points 
		contained in a dataset.
				\\
		See also: data point, dataset.},
  first={sample size},
  text={sample size}
}

\newglossaryentry{ann}{
	name={artificial neural network (ANN)}, plural={ANNs},
	description={An ANN 
		is a graphical (signal-flow) representation of a function that maps 
		features of a data point at its input to a prediction 
		for the corresponding label at its output. The fundamental unit of an 
		ANN is the artificial neuron, which applies an activation function to its 
		weighted inputs. The outputs of these neurons serve as inputs for other neurons, 
		forming interconnected layers.
				\\
		See also: feature, data point, prediction, label, activation function.},
	first={artificial neural network (ANN)},
	text={ANN}
}


\newglossaryentry{randomforest}
{name={random forest},
	description={A random forest is a set of different decision trees. 
		Each of these decision trees is obtained by fitting a perturbed copy of 
		the original dataset.
				\\
		See also: decision tree, dataset.},first = {random forest}, text={random forest}
}

\newglossaryentry{bagging}
{name={bagging (or bootstrap aggregation)},
description={Bagging (or bootstrap aggregation) 
		is a generic technique to improve (the robustness of) a given ML method. 
		The idea is to use the bootstrap to generate perturbed copies of a given dataset 
		and then to learn a separate hypothesis for each copy. We then predict the 
		label of a data point by combining or aggregating the individual predictions 
		of each separate hypothesis. For hypothesis maps delivering numeric label 
		values, this aggregation could be implemented by computing the average of individual 
		predictions.
				\\
		See also: ML, bootstrap, dataset, hypothesis, label, data point, prediction.},
		first={bagging (or bootstrap aggregation)},
		text={bagging}}

\newglossaryentry{gd}
{name={gradient descent (GD)},
description={GD 
is an iterative method for finding the minimum of a differentiable 
function $f({\bf w})$ of a vector-valued argument ${\bf w} \in \mathbb{R}^{d}$. 
Consider a current guess or approximation ${\bf w}^{(k)}$ for the minimum of 
the function $f({\bf w})$. We would like to find a new (better) vector ${\bf w}^{(k+1)}$ 
that has a smaller objective value $f({\bf w}^{(k+1)}) < f\big({\bf w}^{(k)}\big)$ than 
the current guess ${\bf w}^{(k)}$. We can achieve this typically by using a gradient step
		\begin{equation} 
			\label{equ_def_GD_step_dict}
			{\bf w}^{(k\!+\!1)} = {\bf w}^{(k)} - \eta \nabla f({\bf w}^{(k)})
		\end{equation} 
		with a sufficiently small step size $\eta\!>\!0$. Fig. \ref{fig_basic_GD_step_dict} illustrates the effect of 
		a single GD step \eqref{equ_def_GD_step_dict}.
		\begin{figure}[H]
			\begin{center}
				\begin{tikzpicture}[scale=0.8]
					\draw[loosely dotted] (-4,0) grid (4,4);
					\draw[blue, ultra thick, domain=-4.1:4.1] plot (\x,  {(1/4)*\x*\x});
					\draw[red, thick, domain=2:4.7] plot (\x,  {2*\x - 4});
					\draw[<-] (4,4) -- node[right] {$\nabla f({\bf w}^{(k)})$} (4,2);
					\draw[->] (4,4) -- node[above] {$-\eta \nabla f({\bf w}^{(k)})$} (2,4);
					\draw[<-] (4,2) -- node[below] {$1$} (3,2) ;
					\draw[->] (-4.25,0) -- (4.25,0) node[right] {${\bf w}$};
					\draw[->] (0,-2pt) -- (0,4.25) node[above] {$f({\bf w})$};
					\draw[shift={(0,0)}] (0pt,2pt) -- (0pt,-2pt) node[below] {$\overline{{\bf w}}$};
					\draw[shift={(4,0)}] (0pt,2pt) -- (0pt,-2pt) node[below] {${\bf w}^{(k)}$};
					\draw[shift={(2,0)}] (0pt,2pt) -- (0pt,-2pt) node[below] {${\bf w}^{(k\!+\!1)}$};
					\foreach \y/\ytext in {1/1, 2/2, 3/3, 4/4}
					\draw[shift={(0,\y)}] (2pt,0pt) -- (-2pt,0pt) node[left] {$\ytext$};  
				\end{tikzpicture}
			\end{center}
			\caption{A single gradient step \eqref{equ_def_GD_step_dict} towards the minimizer $\overline{{\bf w}}$ of $f({\bf w})$.}
			\label{fig_basic_GD_step_dict}
		\end{figure}
		See also: minimum, differentiable, gradient step, step size, gradient.		
		},first={gradient descent (GD)},text={GD}}

\newglossaryentry{abserr}
{name={absolute error loss},
 description={Consider a data point with features ${\bf x} \in \mathcal{X}$ and numeric 
			label $y \in \mathbb{R}$. The absolute error loss 
			incurred by a hypothesis $h: \mathcal{X} \rightarrow \mathbb{R}$ 
			is defined as $|y - h({\bf x})|$, i.e., the absolute difference between 
			the prediction $h({\bf x})$ and the true label $y$.
					\\
		See also: data point, feature, label, loss, hypothesis, prediction.},
 first={absolute error loss},
 text={absolute error loss}
}

\newglossaryentry{device}{name={device}, plural={devices}, description={
				Any physical system that can be used to store and process data. In the context of ML, 
				we typically mean a computer that is able to read in data points from different 
				sources and, in turn, to train an ML model using these data points.
						\\
		See also: data, ML, data point, model.},
				first={device},text={device}}

\newglossaryentry{llm}{name={large language model (LLM)},description={
	LLMs is an umbrella term for ML methods 
	that process and generate human-like text. These methods typically 
	use deep nets with billions (or even trillions) of parameters. 
	A widely used choice for the network architecture is referred to as 
	Transformers \cite{vaswani2017attention}. The training of LLMs is often  
	based on the task of predicting a few words that are intentionally removed 
	from a large text corpus. Thus, we can construct labeled datapoints 
	simply by selecting some words of a text as labels and the remaining 
	words as features of data points. This construction requires 
	very little human supervision and allows for generating sufficiently 
	large training sets for LLMs.
			\\
		See also: ML, deep net, parameters, labeled datapoint, label, feature, data point, training set, model.},
					first={large language model (LLM)},text={LLM}}


\newglossaryentry{huberreg}{name={Huber regression},description={
			Huber regression refers to ERM-based methods 
			that use the Huber loss as a measure of the prediction error. 
			Two important special cases of Huber regression are least absolute deviation regression and 
			linear regression. Tuning the threshold parameter of the Huber loss allows the user
			to trade the robustness of the absolute error loss 
			against the computational benefits of the smooth squared error loss.
					\\
		See also: regression, ERM, Huber loss, prediction, regression, least absolute deviation regression, linear regression, absolute error loss, smooth, squared error loss.},
			first={Huber regression},text={Huber regression}}


\newglossaryentry{ladregression}{name={least absolute deviation regression},description={
		Least absolute deviation regression is 
		an instance of ERM using the absolute error loss. It is a special case of 
		Huber regression.
				\\
		See also: ERM, absolute error loss, Huber regression.},
		first={least absolute deviation regression},text={least absolute deviation regression}}


\newglossaryentry{metric}
{name={metric},
	description={In its most general form, a metric is a quantitative measure used to compare or evaluate objects. 
		In mathematics, a metric measures the distance between two points and must follow specific rules: 
		the distance is always non-negative, zero only if the points are the same, symmetric, and it satisfies the 
		triangle inequality \cite{RudinBookPrinciplesMatheAnalysis}. In ML, a metric is a quantitative measure 
		of how well a model performs. Examples include accuracy, precision, and the average $0/1$ loss 
		on a test set \cite{Goodfellow-et-al-2016,BishopBook}. A loss function is used to train models, 
		while a metric is used to compare trained models. 
		\\ See also: loss function, loss, model selection.},
	first={metric}, text={metric}}

\newglossaryentry{bayesrisk}{name={Bayes risk},description={Consider a probabilistic model with a 
joint probability distribution $p({\bf x},y)$ for the features ${\bf x}$ 
and label $y$ of a data point. The Bayes risk 
is the minimum possible risk that can be achieved by any hypothesis 
$h: \mathcal{X} \rightarrow \mathcal{Y}$. Any hypothesis that achieves 
the Bayes risk is referred to as a Bayes estimator \cite{LC}.
		\\
		See also: probabilistic model, probability distribution, feature, label, data point, risk, minimum, hypothesis, Bayes estimator.},first={Bayes risk},text={Bayes risk}}
	
\newglossaryentry{bayesestimator}{name={Bayes estimator},description={Consider 
a probabilistic model with a joint probability distribution $p({\bf x},y)$ for the features ${\bf x}$ and label 
$y$ of a data point. For a given loss function $L\left(\cdot,\cdot \right)$, we refer to a hypothesis 
$h$ as a Bayes estimator if its risk $\mathbb{E} \{L\left(\left( {\bf x},y \right),h \right)\}$ is the 
minimum \cite{LC}. Note that the property of a hypothesis being a Bayes estimator depends on 
the underlying probability distribution and the choice for the loss function $L\left(\cdot,\cdot \right)$.
		\\
		See also: probabilistic model, probability distribution, feature, label, data point, loss function, hypothesis, risk, minimum.},
		first={Bayes estimator},text={Bayes estimator}}


\newglossaryentry{weights}{name={weights},
	description={Consider a parametrized hypothesis space $\mathcal{H}$. 
		We use the term weights for numeric model parameters that are 
		used to scale features or their transformations in order to compute $h^{({\bf w})} \in \mathcal{H}$. A linear model uses weights ${\bf w}=\big(w_{1},\ldots,w_{d}\big)^{T}$ to compute 
		the linear combination $h^{({\bf w})}({\bf x})= {\bf w}^{T} {\bf x}$. 
		Weights are also used in ANNs to form linear combinations of features or the 
		outputs of neurons in hidden layers.
				\\
		See also: hypothesis space, model parameters, feature, linear model, ANN.},first={weights},text={weights}}
	
\newglossaryentry{probdist}{name={probability distribution}, plural={probability distributions},
	description={To analyze ML methods, it can be useful 
		to interpret data points as i.i.d. realizations of an RV. The typical 
		properties of such data points are then governed by the probability distribution 
		of this RV. The probability distribution of a binary RV $y \in \{0,1\}$ 
		is fully specified by the probabilities $p({y = 0})$ and 
		$p({y=1})\!=\!1\!-\!p({y=0})$. The probability 
		distribution of a real-valued RV $x \in \mathbb{R}$ might be specified 
		by a pdf $p(x)$ such that $p({ x \in [a,b] }) \approx  p(a) |b-a|$. 
	    In the most general case, a probability distribution is defined by a probability measure \cite{GrayProbBook}, \cite{BillingsleyProbMeasure}.
	    		\\
		See also: ML, data point, i.i.d., realization, RV, probability, pdf.},first={probability distribution},text={probability distribution}}
    
    
\newglossaryentry{pdf}{name={probability density function (pdf)},
	description={The pdf $p(x)$ 
		of a real-valued RV $x \in \mathbb{R}$ is a particular representation of its probability distribution. 
		If the pdf exists, it can be used to compute the probability that $x$ takes on a value 
		from a (measurable) set $\mathcal{B} \subseteq \mathbb{R}$ via $p({x \in \mathcal{B}}) = \int_{\mathcal{B}} p(x') d x'$ \cite[Ch. 3]{BertsekasProb}. 
		The pdf of a vector-valued RV ${\bf x} \in \mathbb{R}^{d}$ (if it exists) 
        allows us to compute the probability of ${\bf x}$ belonging to a (measurable) region $\mathcal{R}$ via 
        $p({{\bf x} \in \mathcal{R}}) = \int_{\mathcal{R}} p({\bf x}') d x_{1}' \ldots d x_{d}' $ \cite[Ch. 3]{BertsekasProb}.
        		\\
		See also: RV, probability distribution, probability.},
first={probability density function (pdf)},text={pdf}}


\newglossaryentry{parameters}{name={parameters},
	description={The parameters of an ML model are tunable 
		(i.e., learnable or adjustable) quantities that allow us to choose between different hypothesis maps. 
		For example, the linear model $\mathcal{H} := \{h^{({\bf w})}: h^{({\bf w})}(x)= w_{1} x + w_{2}\}$ 
		consists of all hypothesis maps $h^{({\bf w})}(x)= w_{1} x + w_{2}$ 
		with a particular choice for the parameters ${\bf w} = \big(w_{1},w_{2}\big)^{T} \in \mathbb{R}^{2}$. 
		Another example of parameters is the weights assigned to the connections 
		between neurons of an ANN.
				\\
		See also: ML, model, hypothesis, linear model, weights, ANN.},first={parameters},text={parameters}}

\newglossaryentry{lln}{name={law of large numbers},
	description={The law of large numbers refers to the 
		convergence of the average of an increasing (large) number of i.i.d. RVs 
		to the mean of their common probability distribution. Different instances of the 
		law of large numbers are obtained by using different notions of convergence \cite{papoulis}.
				\\
		See also: i.i.d., RV, mean, probability distribution.},first={law of large numbers},text={law of large numbers}}
    
\newglossaryentry{stopcrit}{name={stopping criterion},
	description={Many ML methods use iterative algorithms that construct a 
		sequence of model parameters (such as the weights of a linear map or 
		the weights of an ANN). These parameters (hopefully) converge to an optimal choice 
		for the model parameters. In practice, given finite computational 
		resources, we need to stop iterating after a finite number of repetitions. 
		A stopping criterion is any well-defined condition required for stopping 
		the iteration.
				\\
		See also: ML, algorithm, model parameters, weights, ANN.},first={stopping criterion},text={stopping criterion}}

\newglossaryentry{kCV}{name={$k$-fold cross-validation ($k$-fold CV)},
	description={$k$-fold CV is a 
		method for learning and validating a hypothesis using a given dataset. 
		This method divides the dataset evenly into $k$ subsets or folds 
		and then executes $k$ repetitions of model training (e.g., via ERM) and validation. 
		Each repetition uses a different fold as the validation set and the remaining $k-1$ folds 
		as a training set. The final output is the average of the validation errors obtained 
		from the $k$ repetitions.
				\\
		See also: hypothesis, dataset, model, ERM, validation, validation set, training set, validation error.},first={$k$-fold cross-validation ($k$-fold CV)},text={$k$-fold CV}}
	
\newglossaryentry{renyidiv}{name={R\'enyi divergence}, 
	sort={Renyi},
	description={The R\'enyi divergence measures the (dis)similarity 
		between two probability distributions \cite{RenyiInfo95}.
				\\
		See also: probability distribution.}, 
	first = {R\'enyi divergence}, text = {R\'enyi divergence}} 

\newglossaryentry{nonsmooth}{name={non-smooth},
	description={We refer to a function as non-smooth if it is not 
		smooth \cite{nesterov04}.
				\\
		See also: smooth.},first={non-smooth},text={non-smooth}}

\newglossaryentry{convex}{name={convex},
	description={A subset $\mathcal{C} \subseteq \mathbb{R}^{d}$ of the 
		Euclidean space $\mathbb{R}^{d}$ is referred to as convex if it contains 
		the line segment between any two points ${\bf x}, {\bf y}\!\in\!\mathcal{C}$ in that set. A function 
		$f\!:\!\mathbb{R}^{d}\!\rightarrow\!\mathbb{R}$ 
		is convex if its epigraph $\big\{ \big( {\bf w}^{T},t \big)^{T}\!\in\!\mathbb{R}^{d\!+\!1}\!:\!t\!\geq\!f({\bf w}) \}$ 
		is a convex set \cite{BoydConvexBook}. We illustrate one example of a convex set 
		and a convex function in Fig. \ref{fig_convex_set_function}. 
		\begin{figure}[H]
		\begin{center}
			\begin{tikzpicture}
				\fill[blue!20, opacity=0.5] (-3,0) ellipse (2 and 1.2); 
				\draw[thick] (-3,0) ellipse (2 and 1.2);
				\filldraw[black] (-3.7,0.2) circle (2pt) node[left] {${\bf w}$};
				\filldraw[black] (-2.3,-0.5) circle (2pt) node[right] {${\bf w}'$};
				\draw[thick] (-3.7,0.2) -- (-2.3,-0.5);
				\node at (-1.2,-1.0) {$\mathcal{C}$};
				\begin{scope}[shift={(5,-1)}]
					\draw[thick, domain=-2:2, smooth, variable=\x] 
					plot ({\x}, {0.5*\x*\x});
					\fill[blue!30, opacity=0.5] 
					plot[domain=-1.5:1.5, smooth] ({\x}, {0.5*\x*\x}) -- 
					(2, {0.5*2*2}) -- 
					(-2, {0.5*2*2}) -- 
					cycle;
					\node at (0,-0.4) {$f({\bf w})$};
				\end{scope}
			\end{tikzpicture}
			\vspace*{-8mm}
			\end{center}
			\caption{Left: A convex set $\mathcal{C} \subseteq \mathbb{R}^{d}$. 
				Right: A convex function $f: \mathbb{R}^{d} \rightarrow \mathbb{R}$.\label{fig_convex_set_function}}
		\end{figure}
		See also: Euclidean space.},first={convex},text={convex}}


\newglossaryentry{smooth}{name={smooth},
	description={A real-valued function $f: \mathbb{R}^{d} \rightarrow \mathbb{R}$ 
		is smooth if it is differentiable and its gradient $\nabla f({\bf w})$ is continuous at all ${\bf w} \in \mathbb{R}^{d}$  \cite{nesterov04}, \cite{CvxBubeck2015}. A smooth function $f$ is referred to as $\beta$-smooth if the gradient 
		$\nabla f({\bf w})$ is Lipschitz continuous with Lipschitz constant $\beta$, i.e., 
		$$\| \nabla f({\bf w}) - \nabla f({\bf w}') \| \leq \beta \| {\bf w} - {\bf w}' \| \mbox{, for any } {\bf w},{\bf w}' \in \mathbb{R}^{d}.$$ 
		The constant $\beta$ quantifies the amount of smoothness of the function $f$: the smaller the $\beta$, 
		the smoother $f$ is. Optimization problems with a smooth objective function can be solved effectively by gradient-based methods. 
	    Indeed, gradient-based methods approximate the objective function locally around a current choice ${\bf w}$ 
	    using its gradient. This approximation works well if the gradient does 
	    not change too rapidly. We can make this informal claim precise by studying the effect of a single 
	    gradient step with step size $\eta=1/\beta$ (see Fig. \ref{fig_gd_smooth_dict}). 
	    \begin{figure}[H] 
	    	\begin{center} 
	    	\begin{tikzpicture}[scale=0.8, x=0.7cm,y=0.05cm]
	    		% Parameter to shift the quadratic curve horizontally
	    		\def\hshift{0.5} % Change this value to shift the curve horizontally
	    		% Define the function (only the increasing part of x^2 for x >= 0)
	    		\draw[thick, domain=\hshift:8+\hshift, smooth, variable=\x] plot ({\x}, {\x^2}); %node[right] {$f(x) = x^2$};
	    		% Define points for the tangents
	    		\coordinate (w) at (\hshift,{\hshift*\hshift}); % Point w on the curve (left end of the plot)
	    		\coordinate (wkplus1) at (4+\hshift,{(4+\hshift)^2}); % Point w^{k+1} on the curve (x=1 + hshift, y=1)
	    		\coordinate (wk) at (8+\hshift,{(8+\hshift)^2}); % Point w^k on the curve (right end of the plot)
	    		% Calculate the slopes for the tangents
  				\draw[line width=1pt, transform canvas={yshift=-2pt}] (wk) -- +(-1, -{2*(8 + \hshift)} ) -- +(1, {2*(8 + \hshift)}); % Tangent at w^k with positive slope
 				\draw[line width=1pt, transform canvas={yshift=-2pt}] (w) -- +(-1, -{2*\hshift} ) -- +(1, {2*\hshift} )  node[below] {$\nabla f({\bf w})$};% Tangent at w with slope 0 (since derivative at hshift = 0)
%	    		% Draw filled circles at points w^k, w, and w^{k+1}
	    		\filldraw (wk) circle (2pt) node[above left] {${\bf w}^{(k)}$} node[below right] {$\nabla f({\bf w}^{(k)})$} ;
	    		\filldraw (w) circle (2pt) node[above right] {${\bf w}$} ;
	    		\filldraw (wkplus1) circle (2pt) node[below right] {${\bf w}^{(k+1)}\!=\!{\bf w}^{(k)}\!-\!(1/\beta)\nabla f({\bf w}^{(k)})$};
	    		    % Draw horizontal rulers to mark the function values at wk and wk_plus1
	    		\draw[dashed] (wk) -- ($(8,0) + (wk)$) ; %node[left] {$f({\bf w}^{(k)})$};
	    		\draw[dashed] (wkplus1) -- ($(12,0) + (wkplus1)$) ; %node[left] {$f({\bf w}^{(k+1)})$};
	    		 \draw[<->, thick] ($(4,0) + (wk)$) -- ($(8,0) + (wkplus1)$) 
	    		node[midway, right] {$ f\big({\bf w}^{(k)}\big)\!-\!f\big({\bf w}^{(k+1)}\big)\!\geq\!\frac{1}{2\beta}\left\Vert  {\nabla f({\bf w}^{(k)})} \right\Vert_{2}^{2}$};
%	    		% Label the curve
%	    		\node at (2, 4) {};
	    	\end{tikzpicture}
	    	\end{center}
	    	\caption{Consider an objective function $f({\bf w})$ that is $\beta$-smooth. 
	    		Taking a gradient step, with step size $\eta = 1/\beta$, decreases the 
	    		objective by at least $\frac{1}{2\beta}\left\Vert  {\nabla f({\bf w}^{(k)})} \right\Vert_{2}^{2}$ \cite{nesterov04}, \cite{CvxBubeck2015}, \cite{CvxAlgBertsekas}. 
	    		Note that the step size $\eta = 1/\beta$ becomes larger for smaller $\beta$. Thus, 
	    		for smoother objective functions (i.e., those with smaller $\beta$), 
				we can take larger steps. \label{fig_gd_smooth_dict}}
	    	\end{figure}
		See also: differentiable, gradient, objective function, gradient-based methods, gradient step, step size.
	    },first={smooth},text={smooth}}

\newglossaryentry{paramspace}{name={parameter space},
		description={The parameter space $\mathcal{W}$ of 
		an ML model $\mathcal{H}$ is the set of all feasible choices for the 
		model parameters (see Fig. \ref{fig_param_space_dict}). Many important ML methods 
		use a model that is parametrized by vectors of the Euclidean space $\mathbb{R}^{d}$. 
		Two widely used examples of parametrized models are linear models 
		and deep nets. The parameter space is then often a subset $\mathcal{W} \subseteq \mathbb{R}^{d}$, 
		e.g., all vectors ${\bf w} \in \mathbb{R}^{d}$ with a norm smaller than one.
		\begin{figure}[H]
			\begin{center}
			\begin{tikzpicture}
				% Left part: Ellipse representing parameter space (with two dots)
				\node[ellipse, minimum width=3cm, minimum height=2cm, draw, thick] (paramspace) {};
				\node[below=0.1cm of paramspace] {parameter space $\mathcal{W}$};
				% Two dots inside the left ellipse
				\node[black, circle, inner sep=2pt, fill] (theta1) at ($(paramspace.north west) + (1, -1)$) {};
				\node[left=0.01cm of theta1] {${\bf w}$};
				\node[black, circle, inner sep=2pt, fill] (theta2) at ($(paramspace.south east) + (-1.5, 1)$) {};
				\node[left=0.01cm of theta2] {${\bf w}'$};
				% Right part: Ellipse containing two smaller plots
				\node[ellipse, minimum width=7cm, minimum height=3cm, draw, thick, right=4cm of paramspace] (plotcloud) {};
				\node[above=0.2cm of plotcloud] {model $\mathcal{H}$};
				% Axis for first smaller plot
				\node (plot1start) at ($(plotcloud.south west) + (0.2, 0.2)$) {};
				%\draw[thick, ->] (plot1start) -- ++(2, 0) node[anchor=north] {${\bf x}$};
				%\draw[thick, ->] (plot1start) -- ++(0, 1.5) node[anchor=east] {$y$};
				% Simple plot line in first smaller plot
				\draw[thick, red] (plot1start) .. controls ++(0.8, 1) and ++(-0.8, -0.8) .. ($(plotcloud.south west) + (2.8, 0.8)$) node[anchor=west] {$h^{({\bf w})}$};
				% Axis for second smaller plot
				\node (plot2start) at ($(plotcloud.south west) + (1.0, 1.2)$) {};
			%	\draw[thick, ->] (plot2start) -- ++(2, 0) node[anchor=north] {${\bf x}$};
			%	\draw[thick, ->] (plot2start) -- ++(0, 1.5) node[anchor=east] {$y$};
				% Simple plot line in second smaller plot
				\draw[thick, blue] (plot2start) .. controls ++(0.8, 0.5) and ++(-0.8, -0.8) .. ($(plotcloud.south west) + (2.8, 2.1)$) node[anchor=west] {$h^{({\bf w}')}$};
				% Connect the two dots in the parameter space to the two plots
				\draw[thick, ->, bend right=20] (theta1) to ($(plot1start) + (0,0)$);
				\draw[thick, ->, bend left=20] (theta2) to (plot2start);
			\end{tikzpicture}
			\end{center} 
			\caption{The parameter space $\mathcal{W}$ of an ML model $\mathcal{H}$ consists of all 
			feasible choices for the model parameters. Each choice ${\bf w}$ for the model parameters 
			selects a hypothesis map $h^{({\bf w})} \in \mathcal{H}$.
				 \label{fig_param_space_dict}} 
\end{figure}
		See also: ML, model, model parameters, Euclidean space, linear model, deep net, norm, hypothesis.},
			first={parameter space},text={parameter space}}

\newglossaryentry{datanorm}{name={data normalization},
	description={Data normalization refers to transformations 
		applied to the feature vectors of data points to improve the ML method's 
		statistical aspects or computational aspects. For example, in linear regression with gradient-based methods using 
		a fixed learning rate, convergence depends on controlling the norm of feature vectors 
		in the training set. A common approach is to normalize feature vectors such that their 
		norm does not exceed one \cite[Ch.\ 5]{MLBasics}.
				\\
		See also: data, feature vector, data point, ML, statistical aspects, computational aspects, linear regression, gradient-based methods, learning rate, norm, training set.},
	first={data normalization},text={data normalization}}

\newglossaryentry{dataaug}{name={data augmentation},
	description={Data augmentation methods add synthetic data points 
		to an existing set of data points. These synthetic data points are obtained by 
		perturbations (e.g., adding noise to physical measurements) or transformations 
		(e.g., rotations of images) of the original data points. These perturbations and 
		transformations are such that the resulting synthetic data points should 
		still have the same label. As a case in point, a rotated cat image is still 
		a cat image even if their feature vectors (obtained by stacking pixel color intensities) 
		are very different (see Fig. \ref{fig_symmetry_dataaug_dict}). Data augmentation can be an 
		efficient form of regularization.
		\begin{figure}[H]
		\begin{center}
			\begin{tikzpicture}
				% Define shift macros locally
				\newcommand{\xshift}{0.5}
				\newcommand{\yshift}{2}
				% Define the shifted curves
				% Define the shifted curves
  				\draw[very thick, blue] plot[smooth, tension=1] coordinates {(0,0) (2,1) (4,0) (6,-1) (8,0)};
  				\node[blue, right] at (0,0) {\textbf{cat}};
  				\draw[very thick, red, dashed] plot[smooth, tension=1] coordinates {(0 + \xshift,0 + \yshift) (2 + \xshift,1 + \yshift) (4 + \xshift,0 + \yshift) (6 + \xshift,-1 + \yshift) (8 + \xshift,0 + \yshift)};
  				\node[red, right] at (8 + \xshift,0 + \yshift) {\textbf{no cat}};
				\fill[blue] (2,1) circle (2pt) node[above] {${\bf x}^{(1)}$};
				\fill[blue] (6,-1) circle (2pt) node[above] {${\bf x}^{(2)}$};
				  % Draw a bent arrow connecting the two points with custom in and out angles
				  \draw[->, thin, >=latex, line width=0.5pt] (2,1) to[out=240, in=240] node[midway, below] {$\mathcal{T}^{(\eta)}$} (6,-1);
			  \end{tikzpicture}
			  \vspace*{-11mm}
		\end{center}
		\caption{Data augmentation exploits intrinsic symmetries of data points in 
		       some feature space $\mathcal{X}$. We can represent a symmetry by 
		     an operator $\mathcal{T}^{(\eta)}: \mathcal{X} \rightarrow \mathcal{X}$,
		     parametrized by some number $\eta \in \mathbb{R}$. For example, $\mathcal{T}^{(\eta)}$ 
		    might represent the effect of rotating a cat image by $\eta$ degrees. A data point 
		    with feature vector ${\bf x}^{(2)} = \mathcal{T}^{(\eta)} \big({\bf x}^{(1)} \big)$ must 
		    have the same label $y^{(2)}=y^{(1)}$ as a data point 
		     with feature vector ${\bf x}^{(1)}$.\label{fig_symmetry_dataaug_dict}}
		 \end{figure}
		See also: data, data point, label, feature vector, regularization, feature space. },first={data augmentation},text={data augmentation}}
	
	
\newglossaryentry{localdataset}{name={local dataset}, plural={local datasets}, description={The concept of a local dataset is 
		in between the concept of a data point and a dataset. A local dataset consists of several 
		individual data points, which are characterized by features and labels. 
		In contrast to a single dataset used in basic ML methods, a local dataset is also 
		related to other local datasets via different notions of similarity. These similarities 
		might arise from probabilistic models or communication infrastructure and 
		are encoded in the edges of an FL network.
				\\
		See also: dataset, data point, feature, label, ML, probabilistic model, FL network.},first={local dataset},text={local dataset}}
	
\newglossaryentry{localmodel}{name={local model}, plural={local models}, description={Consider a collection of devices that are represented 
		as nodes $\mathcal{V}$ of an FL network. A local model $\mathcal{H}^{(i)}$ 
		is a hypothesis space assigned to a node $i \in \mathcal{V}$. Different nodes might be 
		assigned different hypothesis spaces, i.e., in general $\mathcal{H}^{(i)} \neq \mathcal{H}^{(i')}$ for different 
		nodes $i, i' \in \mathcal{V}$. 
				\\
		See also: device, FL network, model, hypothesis space. },
		first={local model},
		text={local model}
		}
	
\newglossaryentry{mutualinformation}
{name={mutual information (MI)},
 description={The MI $I \left( {\bf x};y\right)$ 
 	between two RVs ${\bf x}$, $y$ defined on the same probability space 
 	is given by \cite{coverthomas} $$I \left( {\bf x};y\right) := 
	\mathbb{E}  \left\{ \log \frac{p ({\bf x},y)}{p({\bf x})p(y)} \right\}.$$ 
	It is a measure of how well we can estimate $y$ based 
	solely on ${\bf x}$. A large value of $I \left( {\bf x};y\right)$ indicates that 
	$y$ can be well predicted solely from ${\bf x}$. This prediction could be obtained by a 
		hypothesis learned by an ERM-based ML method. 
				\\
		See also: RV, probability space, prediction, hypothesis, ERM, ML.
	 }, first={MI}, text={MI} 
}

\newglossaryentry{zerogradientcondition}{name={zero-gradient condition},
	description={Consider the unconstrained 
		optimization problem $\min_{{\bf w} \in \mathbb{R}^{d}} f({\bf w})$  with 
			a smooth and convex objective function $f({\bf w})$. A necessary and 
			sufficient condition for a vector $\widehat{{\bf w}} \in \mathbb{R}^{d}$ 
			to solve this problem is that the gradient $\nabla f \big( \widehat{{\bf w}} \big)$ 
			is the zero vector such that
			$$ \nabla f \big( \widehat{{\bf w}} \big) = \mathbf{0} \Leftrightarrow  f \big( \widehat{{\bf w}} \big) = \min_{{\bf w} \in \mathbb{R}^{d}} f({\bf w}) .$$ 
					\\
		See also: smooth, convex, objective function, gradient.}, 
			first={zero-gradient condition},text={zero-gradient condition}}


\newglossaryentry{edgeweight}{name={edge weight},
	description={Each edge $\{i,i'\}$ of an FL network is 
		assigned a non-negative edge weight $A_{i,i'}\geq0$. 
		A zero edge weight $A_{i,i'}=0$ indicates the absence 
		of an edge between nodes $i, i' \in \mathcal{V}$.
				\\
		See also: FL network.}, 
	first={edge weight},text={edge weight}}


\newglossaryentry{dataminprinc}{name={data minimization principle},
	description={European data protection regulation 
		includes a data minimization principle. This principle requires a data controller to 
		limit the collection of personal information to what is directly relevant and necessary 
		to accomplish a specified purpose. The data should be retained only for as long as 
		necessary to fulfill that purpose \cite[Article 5(1)(c)]{GDPR2016}, \cite{EURegulation2018}.
				\\
		See also: data.}, 
	first={data minimization principle},text={data minimization principle}}



\documentclass{article}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{amsmath, amssymb}
\usepackage[margin=2.5cm]{geometry}

\begin{document}

\section*{Generalization}


 Generalization refers to the ability of a model trained on a trainset to make accurate
 predictions on new, unseen data points. This is a central goal of machine learning (ML) and ai:
 to learn patterns that extend beyond the trainset. Most machine learning (ML) systems
 use erm to learn a hypothesis $\hat{h} \in \mathcal{H}$ by minimizing
 the average loss over a trainset of data points ${\bf z}^{(1)}, \ldots, {\bf z}^{(m)}$,
 denoted as $\mathcal{D}^{(\rm train)}$. However, success on the trainset does not guarantee success on
 unseen data - this discrepancy is the challenge of generalization. To study generalization
 mathematically, we need to formalize the notion of ``unseen'' data. A widely used
 approach is to assume a probabilistic model for data generation, such as the iidasspt.
 Here, we interpret data points as independent random variable (RV)s with an identical
 probdist $p({\bf z})$. This probdist, which is assumed fixed but unknown,
 allows us to define risk of a trained model $\hat{h}$ as the expected loss
 \[
 \risk{\hat{h}} := \expect_{{\bf z} \sim p({\bf z})} \big\{ L(\hat{h}, {\bf z}) \big\}.
 \]
 The difference between risk $\risk{\hat{h}}$ and empirical risk $\emprisk{\hat{h}}{\mathcal{D}^{(\rm train)}}$
 is known as the generalized total variation (GTV). Tools from probability theory, such as concentrationinequs
 and uniform convergence, allow us to bound this gap under certain conditions \cite{ShalevMLBook}.\\
 {\bf Generalization without probability.} \Gls{probability} theory is one way to study how well a
 model generalizes beyond the trainset, but it is not the only way. Another option is to use
 simple, deterministic changes to the data points in the trainset. The basic idea is that a
 good model $\hat{h}$ should be robust: its prediction $\hat{h}({\bf x})$
 should not change much if we slightly change the features ${\bf x}$ of a data point ${\bf z}$.
 For example, an object detector trained on smartphone photos should still detect the object if a few
 random pixels are masked \cite{OnePixelAttack}. Similarly, it should deliver the same result if we rotate
 the object in the image \cite{MallatUnderstandingDeepLearning}.
 \begin{figure}[H]
 \centering
 \includegraphics[width=0.8\linewidth]{blog_posts/images/generalization_tikz.png}
 \caption{Two data points ${\bf z}^{(1)},{\bf z}^{(2)}$ that are used as a trainset
 to learn a hypothesis $\hat{h}$ via erm. We can evaluate $\hat{h}$
 outside $\mathcal{D}^{(\rm train)}$ either by an iidasspt with some underlying probdist $p({\bf z})$
 or by perturbing the data points.}
 \label{fig:polynomial_fit_dict}
 \end{figure}
 See also: machine learning (ML), ai, erm, model, hypothesis, loss, empirical risk, data point, trainset, probabilistic model, iidasspt, data, iid, realization, probdist, risk, random variable (RV), prediction.
 

\end{document}

% !TeX spellcheck = en_GB

\section*{Lists of Symbols}
%\label{ch_list_of_symbols}

\vspace*{-2mm}
\section*{Sets and Functions} 

\begin{align} 
	&a \in \mathcal{A} & \quad & \parbox{.75\textwidth}{This statement indicates that the object $a$ is an element of the set $\mathcal{A}$.} \nonumber \\[5mm]
	&a \defeq b & \quad & \mbox{This statement defines $a$ to be shorthand for $b$. } \nonumber \\[5mm]
	&|\mathcal{A}| & \quad & \mbox{The cardinality (number of elements) of a finite set $\mathcal{A}$.} \nonumber \\[5mm]
	&\mathcal{A} \subseteq \mathcal{B}& \quad & \mbox{$\mathcal{A}$ is a subset of $\mathcal{B}$.} \nonumber \\[5mm]
	&\mathcal{A} \subset \mathcal{B}& \quad & \mbox{$\mathcal{A}$ is a strict subset of $\mathcal{B}$.} \nonumber \\[5mm]
	&\mathbb{N} & \quad & \mbox{The natural numbers $1,2,\ldots$.} \nonumber \\[5mm]
	&\mathbb{R}  &\quad &\mbox{The real numbers $x$ \cite{RudinBook}.} \nonumber \\[5mm]
	&\mathbb{R}_{+}  &\quad &\mbox{The non-negative real numbers $x\geq0$.} \nonumber \\[5mm]
	&\mathbb{R}_{++}  &\quad &\mbox{The positive real numbers $x> 0$.} \nonumber
\end{align} 

\newpage
\begin{align}
		&\{0,1\}& \quad & \mbox{The set consisting of the two real numbers $0$ and $1$.} \nonumber \\[5mm]
	&[0,1] &\quad &\mbox{The closed interval of real numbers $x$ with $0 \leq x \leq 1$. } \nonumber \\[5mm]
    &\argmin_{\weights} f(\weights) &\quad &\mbox{The set of minimizers for a real-valued function $f(\weights)$.  } \nonumber \\[5mm]
    &\sphere{\nrnodes} &\quad &\mbox{The set of unit-norm vectors in $\mathbb{R}^{\nrnodes+1}$.  } \nonumber \\[5mm]
	 &\log a &\quad &\mbox{The logarithm of the positive number $a \in \mathbb{R}_{++}$.  } \nonumber \\[5mm]
	 &h(\cdot)\!:\!\mathcal{A}\!\rightarrow\!\mathcal{B} :  a \!\mapsto\!h(a) &\quad &\parbox{.75\textwidth}{
	 	A function (map) that accepts any element $a \in \mathcal{A}$ from a set $\mathcal{A}$ 
	 	as input and delivers a well-defined element $h(a) \in \mathcal{B}$ of a set $\mathcal{B}$. 
	 	The set $\mathcal{A}$ is the domain of the function $h$ and the set $\mathcal{B}$ is the 
	 	codomain of $h$. ML aims at finding (or learning) a function $\hypothesis$ (\emph{\gls{hypothesis}}) 
	 	that reads in the \gls{feature}s $\featurevec$ of a \gls{datapoint} and delivers a \gls{prediction} $h(\featurevec)$
	 	for its \gls{label} $\truelabel$.} \nonumber  \\[5mm]	
	 	&\nabla f(\weights) & \quad & \parbox{.75\textwidth}{The \gls{gradient} of a \gls{differentiable} real-valued function 
	 	$f: \mathbb{R}^{\featuredim}\rightarrow \mathbb{R}$ is the vector 
	 	$\nabla f(\weights) = \big( \frac{\partial f}{\partial \weight_{1}},\ldots,\frac{\partial f}{\partial \weight_{\featuredim}}  \big)^{T} \in \mathbb{R}^{\featuredim}$ \cite[Ch. 9]{RudinBookPrinciplesMatheAnalysis}.}   \nonumber
\end{align} 
\section*{Matrices and Vectors} 

\begin{align} 
	 &\featurevec=\big(\feature_{1},\ldots,\feature_{\featuredim})^{T} &\quad & \parbox{.75\textwidth}{A vector of length $\featuredim$ with its 
		$\featureidx$-th entry being $\feature_{\featureidx}$.} \nonumber \\[4mm]
	&\mathbb{R}^{\featuredim} & \quad &  \parbox{.75\textwidth}{The set of vectors $\featurevec=\big(\feature_{1},\ldots,\feature_{\featurelen}\big)^{T}$ consisting of $\featuredim$ real-valued entries $\feature_{1},\ldots,\feature_{\featurelen} \in \mathbb{R}$.} \nonumber \\[4mm]Â 
	&\mathbf{I}_{\modelidx \times \featuredim}  & \quad &  \parbox{.75\textwidth}{A generalized identity matrix 
		with $\modelidx$ rows and $\featuredim$ columns. The entries of $\mathbf{I}_{\modelidx \times \featuredim} \in \mathbb{R}^{\modelidx \times \featuredim}$ 
		are equal to $1$ along the main diagonal and equal to $0$ otherwise. }	\nonumber \\[4mm]%For example, $\mathbf{I}_{1 \times 2} = \big(1, 0\big)$ and $\mathbf{I}_{2 \times 1}= \begin{pmatrix} 1 \\ 0 \end{pmatrix}$.} 
	&\mathbf{I}_{\dimlocalmodel}, \mathbf{I} & \quad &  \parbox{.75\textwidth}{A square identity 
		matrix of size $\dimlocalmodel \times \dimlocalmodel$. If the size is clear from the 
		context, we drop the subscript.} \nonumber \\[4mm]
	&\normgeneric{\featurevec}{2}  &\quad & \parbox{.75\textwidth}{The Euclidean (or $\ell_{2}$) norm of the vector 
		$\featurevec=\big(\feature_{1},\ldots,\feature_{\featurelen}\big)^{T} \in \mathbb{R}^{\featuredim}$ defined as $ \| \featurevec \|_{2} \defeq \sqrt{\sum_{\featureidx=1}^{\featuredim} \feature_{\featureidx}^{2}}$.} \nonumber \\[4mm] 
	&\normgeneric{\featurevec}{}  & \quad &  \parbox{.75\textwidth}{Some norm of the vector $\featurevec \in \mathbb{R}^{\featuredim}$ \cite{GolubVanLoanBook}. Unless specified otherwise, we mean the Euclidean norm $\normgeneric{\featurevec}{2}$.} \nonumber \\[4mm]
	&\featurevec^{T} &\quad & \parbox{.75\textwidth}{The transpose of a matrix that has the vector 
		$\featurevec \in \mathbb{R}^{\dimlocalmodel}$ as its single column.}  \nonumber \\[4mm]
	&\mathbf{X}^{T} &\quad & \parbox{.75\textwidth}{The transpose of a matrix $\mathbf{X} \in \mathbb{R}^{\samplesize \times \featurelen}$. 
		A square real-valued matrix $\mathbf{X} \in \mathbb{R}^{\samplesize \times \samplesize}$ 
		is called symmetric if $\mathbf{X} = \mathbf{X}^{T}$. }  \nonumber\\[4mm]
	&\mathbf{0}= \big(0,\ldots,0\big)^{T}  & \quad &  \parbox{.75\textwidth}{The vector in $\mathbb{R}^{\dimlocalmodel}$ with each entry equal to zero.} \nonumber\\[4mm]
	&\mathbf{1}= \big(1,\ldots,1\big)^{T}  & \quad &  \parbox{.75\textwidth}{The vector in $\mathbb{R}^{\dimlocalmodel}$ with each entry equal to one.} \nonumber
\end{align} 
\newpage
\begin{align} 
	&\big(\vv^{T},\vw^{T} \big)^{T}  & \quad &  \parbox{.75\textwidth}{The vector of length $\featurelen+\featurelen'$ 
		obtained by concatenating the entries of vector $\vv \in \mathbb{R}^{\featurelen}$ with the entries of $\vw \in \mathbb{R}^{\featurelen'}$.} \nonumber \\[4mm]
	&	{\rm span}\{ \mathbf{B} \}  & \quad &  \parbox{.75\textwidth}{The span of a matrix $\mathbf{B} \in \mathbb{R}^{a \times b}$, 
		which is the subspace of all linear combinations of columns of $\mathbf{B}$, 
		${\rm span}\{ \mathbf{B} \} = \big\{  \mathbf{B} \va : \va \in \mathbb{R}^{b} \big\} \subseteq \mathbb{R}^{a}$.} \nonumber \\[4mm]
	&\determinant{\mC} &\quad & \parbox{.75\textwidth}{The determinant of the matrix $\mC$. }  \nonumber  \\[4mm]
	&\mathbf{A} \otimes \mathbf{B} &\quad & \parbox{.75\textwidth}{The Kronecker product of $\mathbf{A}$ and $\mathbf{B}$ \cite{Golub1980}. }  \nonumber
\end{align} 

\newpage
\section*{Probability Theory} 
\begin{align}
	\expect_{p} \{ f(\datapoint) \}  \quad\quad & \parbox{.75\textwidth}{The expectation of a function $f(\datapoint)$ of a \gls{rv} 
		$\datapoint$ whose \gls{probdist} is $\prob{\datapoint}$. If the \gls{probdist} is clear from context 
		we just write $\expect \{ f(\datapoint) \}$. }    \nonumber \\[4mm]             
	\prob{\featurevec,\truelabel} \quad\quad & \parbox{.75\textwidth}{A (joint) \gls{probdist} of a \gls{rv} 
		whose realizations are \gls{datapoint}s with \gls{feature}s $\featurevec$ and \gls{label} $\truelabel$.} \nonumber            \\[4mm]             
	\prob{\featurevec|\truelabel} \quad\quad & \parbox{.75\textwidth}{A conditional \gls{probdist} of a \gls{rv} 
		$\featurevec$ given the value of another \gls{rv} $\truelabel$ \cite[Sec.\ 3.5]{BertsekasProb}. } \nonumber        \\[4mm]             
	\prob{\featurevec;\weights} \quad\quad & \parbox{.75\textwidth}{A parametrized \gls{probdist} of a \gls{rv} $\featurevec$. 
		The \gls{probdist} depends on a parameter vector $\weights$. For example, $\prob{\featurevec;\weights}$ could be a 
		\gls{mvndist} with the parameter vector $\weights$ given by the entries of the mean vector $\expect \{ \featurevec \}$ 
		and the \gls{covmtx} $\expect \bigg \{ \big( \featurevec - \expect \{ \featurevec \}\big) \big( \featurevec - \expect \{ \featurevec \}\big)^{T}  \bigg\}$.} \nonumber              \\[4mm]
	\mathcal{N}(\mu, \sigma^{2}) \quad\quad & \parbox{.75\textwidth}{The \gls{probdist} of a Gaussian 
		\gls{rv} $\feature \in \mathbb{R}$ with mean (or expectation) $\mu= \expect \{ \feature \}$ 
		and variance $\sigma^{2} =   \expect \big\{  (  \feature - \mu )^2 \big\}$.} \nonumber       \\[4mm]
	\mathcal{N}(\clustermean, \mathbf{C}) \quad\quad & \parbox{.75\textwidth}{The \gls{mvndist} of a vector-valued 
		 Gaussian \gls{rv} $\featurevec \in \mathbb{R}^{\featuredim}$ with mean (or expectation) $\clustermean= \expect \{ \featurevec \}$ 
		and \gls{covmtx} $\mathbf{C} =  \expect \big\{ \big( \featurevec - \clustermean \big)\big( \featurevec - \clustermean \big)^{T} \big\}$.} \nonumber                                             
\end{align}





\newpage
\section*{Machine Learning}

\begin{align}
%	\datapoint \quad\quad & \parbox{.75\textwidth}{A \gls{datapoint} which is characterized by several properties that we 
%		divide into low-level properties (= \gls{feature}s) and high-level properties (= \gls{label}s) \cite[Ch. 2]{MLBasics}.}    \nonumber   \\[4mm] 
	\sampleidx \quad\quad & \parbox{.75\textwidth}{An index $\sampleidx=1,2,\ldots,$ that 
		enumerates \gls{datapoint}s.}    \nonumber   \\[4mm] 
	\samplesize \quad\quad &\parbox{.75\textwidth}{The number of \gls{datapoint}s in (the size of) a \gls{dataset}.} \nonumber \\[4mm] 
	\dataset \quad\quad & \parbox{.75\textwidth}{A dataset $\dataset = \{ \datapoint^{(1)},\ldots, \datapoint^{(\samplesize)} \}$ 
		is a list of individual \gls{datapoint}s $\datapoint^{(\sampleidx)}$, for $\sampleidx=1,\ldots,\samplesize$.}    \nonumber   \\[4mm] 
	\featurelen \quad\quad &\parbox{.75\textwidth}{Number of \gls{feature}s that characterize a \gls{datapoint}.} \nonumber \\[4mm] 
	\feature_{\featureidx} \quad\quad &\parbox{.75\textwidth}{The $\featureidx$-th feature of a \gls{datapoint}. The first feature of 
		a given \gls{datapoint} is denoted $\feature_{1}$, the second \gls{feature} $\feature_{2}$ and so on. } \nonumber \\[4mm] 
	\featurevec \quad\quad &\parbox{.75\textwidth}{The \gls{feature} vector $\featurevec=\big(\feature_{1},\ldots,\feature_{\featuredim}\big)^{T}$ of a \gls{datapoint} whose entries are the individual \gls{feature}s of a \gls{datapoint}.} \nonumber \\[4mm] 
	\featurespace \quad\quad & \parbox{.75\textwidth}{The \gls{featurespace} $\featurespace$ is 
		the set of all possible values that the \gls{feature}s $\featurevec$ of a \gls{datapoint} can take on.} \nonumber \\[6mm]
	\rawfeaturevec \quad\quad &\parbox{.75\textwidth}{Beside the symbol $\featurevec$, we 
		sometimes use $\rawfeaturevec$ as another symbol to denote a vector whose entries 
		are individual \gls{feature}s of a \gls{datapoint}. We need two 
		different symbols to distinguish between \emph{raw} and learnt \gls{feature}s \cite[Ch. 9]{MLBasics}.} \nonumber  \\[4mm] 
	\featurevec^{(\sampleidx)} \quad\quad &\parbox{.75\textwidth}{The \gls{feature} vector of the $\sampleidx$-th \gls{datapoint} within a \gls{dataset}. } \nonumber \\[4mm] 
	\feature_{\featureidx}^{(\sampleidx)}\quad\quad &\parbox{.75\textwidth}{The $\featureidx$-th \gls{feature} of the $\sampleidx$-th 
		\gls{datapoint} within a \gls{dataset}.} \nonumber
\end{align}        


\begin{align}
	\batch \quad\quad &\parbox{.75\textwidth}{A mini-batch (subset) of randomly chosen \gls{datapoint}s.} \nonumber
	\\[4mm] 
	\batchsize \quad\quad &\parbox{.75\textwidth}{The size of (the number of \gls{datapoint}s in) a mini-batch.} \nonumber   \\[4mm] 
	\truelabel \quad\quad &\parbox{.75\textwidth}{The \gls{label} (quantity of interest) of a \gls{datapoint}.} \nonumber   \\[4mm] 
	\truelabel^{(\sampleidx)} \quad\quad &\parbox{.75\textwidth}{The \gls{label} of the $\sampleidx$-th \gls{datapoint}.} \nonumber  \\[4mm] 
	\big(\featurevec^{(\sampleidx)},\truelabel^{(\sampleidx)}\big)  \quad\quad &\parbox{.75\textwidth}{The \gls{feature}s and \gls{label} of the $\sampleidx$-th \gls{datapoint}.} \nonumber  \\[4mm] 
	\labelspace  \quad\quad & \parbox{.75\textwidth}{The label space $\labelspace$ of 
		a ML method consists of all potential \gls{label} values that a \gls{datapoint} can 
		carry. The nominal \gls{labelspace} might be larger than the set of different \gls{label} 
		values arising in a given \gls{dataset} (e.g., a \gls{trainset}). We refer to ML problems 
		(methods) using a numeric \gls{labelspace}, such as $\labelspace=\mathbb{R}$ 
		or $\labelspace=\mathbb{R}^{3}$, as \gls{regression} problems (methods). ML problems (methods) 
		that use a discrete \gls{labelspace}, such as $\labelspace=\{0,1\}$ or $\labelspace=\{\mbox{\emph{cat}},\mbox{\emph{dog}},\mbox{\emph{mouse}}\}$ 
		are referred to as \gls{classification} problems (methods).}    \nonumber  \\[4mm] 
	\lrate  \quad\quad & \parbox{.75\textwidth}{\Gls{learnrate} (step-size) used by \gls{gdmethods}.}    \nonumber       \\[4mm] 
	\hypothesis(\cdot)  \quad\quad &\parbox{.75\textwidth}{A \gls{hypothesis} map that reads in \gls{feature}s $\featurevec$ of a \gls{datapoint} 
		and delivers a prediction $\hat{\truelabel}=\hypothesis(\featurevec)$ for its \gls{label} $\truelabel$.} \nonumber  	  \\[4mm] 
	 \labelspace^{\featurespace} \quad\quad & \parbox{.75\textwidth}{Given two sets $\featurespace$ and $\labelspace$, we denote by $ \labelspace^{\featurespace}$ the set of all possible \gls{hypothesis} maps $\hypothesis: \featurespace \rightarrow \labelspace$.} 	 \nonumber 
\end{align}                  


\begin{align}
	\hypospace  \quad\quad & \parbox{.75\textwidth}{A \gls{hypospace} or \gls{model} used by a ML method. 
		The \gls{hypospace} consists of different \gls{hypothesis} maps $\hypothesis: \featurespace \rightarrow \labelspace$ between which 
		the ML method has to choose .}    \nonumber \\[4mm]
	\effdim{\hypospace}  \quad\quad & \parbox{.75\textwidth}{The \gls{effdim} of a \gls{hypospace} $\hypospace$.}   \nonumber \\[4mm]
	\biasterm^2 \quad\quad &\parbox{.75\textwidth}{The squared \gls{bias} of a learnt \gls{hypothesis} $\learnthypothesis$ 
		delivered by a ML algorithm that is fed with \gls{datapoint}s which are modelled as 
		realizations of \gls{rv}s. If data is modelled as \gls{realization}s of \gls{rv}s, also the 
		delivered \gls{hypothesis} $\learnthypothesis$ is the \gls{realization} of a \gls{rv}.} \nonumber  \\[4mm] 
	\varianceterm \quad\quad &\parbox{.75\textwidth}{The \gls{variance} of the (parameters of the) \gls{hypothesis} 
		delivered by a ML algorithm. If the input data for this algorithm is interpreted as \gls{realization}s of \gls{rv}s, so 
		is the delivered \gls{hypothesis} a realization of a \gls{rv}.} \nonumber \\[4mm] 
	\lossfunc{(\featurevec,\truelabel)}{\hypothesis}  \quad\quad & \parbox{.75\textwidth}{The \gls{loss} incurred by predicting the 
		label $\truelabel$ of a \gls{datapoint} using the \gls{prediction} $\hat{\truelabel}=h(\featurevec)$. The 
		\gls{prediction} $\hat{\truelabel}$ is obtained from evaluating the \gls{hypothesis} $\hypothesis \in \hypospace$ for 
		the \gls{feature} vector $\featurevec$ of the \gls{datapoint}.}    \nonumber   \\[4mm] 
	\valerror \quad\quad &\parbox{.75\textwidth}{The \gls{valerr} of a \gls{hypothesis} $\hypothesis$, which is its 
		average \gls{loss} incurred over a \gls{valset}.} \nonumber \\[6mm] 
	\emperror\big(h| \dataset \big) \quad\quad &\parbox{.75\textwidth}{The \gls{emprisk} or average \gls{loss} 
		incurred by the \gls{hypothesis} $\hypothesis$ on a \gls{dataset} $\dataset$.} \nonumber                           
\end{align}     

\begin{align}
	\trainerror \quad\quad &\parbox{.75\textwidth}{The \gls{trainerr} of a \gls{hypothesis} $\hypothesis$, which is its 
		average \gls{loss} incurred over a \gls{trainset}. } \nonumber 	\\[4mm] 
	\timeidx \quad\quad &\parbox{.75\textwidth}{A discrete-time index $\timeidx=0,1,\ldots$ used to 
		enumerate sequential events (or time instants). } \nonumber \\[4mm] 
	\taskidx \quad\quad &\parbox{.75\textwidth}{An index that enumerates
		\gls{learningtask}s within a multi-task learning problem.} \nonumber      \\[4mm]     
	\regparam \quad\quad &\parbox{.75\textwidth}{A \gls{regularization} parameter that controls 
		the amount of \gls{regularization}. } \nonumber \\[4mm] 
	\eigval{\featureidx}\big( \mathbf{Q} \big) \quad\quad &\parbox{.75\textwidth}{The $\featureidx$-th 
		\gls{eigenvalue} (sorted either ascending or descending) of a \gls{psd} matrix $\mathbf{Q}$. We also 
		use the shorthand $\eigval{\featureidx}$ if the corresponding matrix is clear from context. } \nonumber \\[4mm] 
	\actfun(\cdot) \quad\quad &\parbox{.75\textwidth}{The \gls{actfun} used by an artificial neuron within an \gls{ann}.} \nonumber    	 \\[4mm]     
	\decreg{\hat{\truelabel}} \quad\quad &\parbox{.75\textwidth}{A \gls{decisionregion} within a \gls{featurespace}.  } \nonumber   \\[4mm]     
	\weights  \quad\quad & \parbox{.75\textwidth}{A parameter vector $\weights = \big(\weight_{1},\ldots,\weight_{\featuredim}\big)^{T}$ 
		of a model, e.g, the weights of a \gls{linmodel} or in a \gls{ann}.}    \nonumber \nonumber  \\[4mm]  
	\hypothesis^{(\weights)}(\cdot)  \quad\quad &\parbox{.75\textwidth}{A \gls{hypothesis} map that involves tunable \gls{modelparams} $\weight_{1},\ldots,\weight_{\featuredim}$, stacked into the vector $\weights=\big(\weight_{1},\ldots,\weight_{\featuredim} \big)^{T}$.} \nonumber    \\[4mm]    
	\featuremap(\cdot)  \quad\quad & \parbox{.75\textwidth}{A \gls{featuremap} $\featuremap: \featurespace \rightarrow \featurespace' : \featurevec \mapsto \featurevec' \defeq \featuremap\big( \featurevec \big) \in \featurespace'$.}    \nonumber    \\[4mm]    
	\kernelmap{\cdot}{\cdot} \quad\quad & \parbox{.75\textwidth}{Given an arbitrary \gls{featurespace}, 
		a \gls{kernel} is a map $\kernel: \featurespace \times \featurespace \rightarrow \mathbb{C}$ that is \gls{psd}.}    \nonumber                                                                                                                                                     
\end{align}              






\newpage
\section*{Federated Learning}

\begin{align}
 	&\graph = \pair{\nodes}{\edges} & \quad & \parbox{.75\textwidth}{An undirected graph whose nodes $\nodeidx \in \nodes$ represent 
	devices within a \gls{empgraph}. The undirected weighted edges $\edges$ represent connectivity between 
	devices and statistical similarities between their datasets and learning tasks.} \nonumber \\[7mm]
&\nodeidx \in \nodes& \quad & \parbox{.75\textwidth}{A node in the \gls{empgraph} that represents some 
	device which can access a \gls{localdataset} and train a \gls{localmodel}.} \nonumber \\[6mm]
	&\indsubgraph{\graph}{\cluster}& \quad & \parbox{.75\textwidth}{The induced sub-graph of $\graph$ using the nodes in $\cluster \subseteq \nodes$.} \nonumber \\[6mm]
	&\LapMat{\graph}   & \quad & \parbox{.75\textwidth}{The \gls{LapMat} of a graph $\graph$.} \nonumber \\[6mm]
		&\LapMat{\cluster}   & \quad & \parbox{.75\textwidth}{The \gls{LapMat} of the induced graph $\indsubgraph{\graph}{\cluster}$.} \nonumber \\[6mm]
	 &		\neighbourhood{\nodeidx}  & \quad & \parbox{.75\textwidth}{The \gls{neighbourhood} of a node $\nodeidx$ in a graph $\graph$.}    \nonumber   \\[4mm] 
	&\nodedegree{\nodeidx} & \quad & \parbox{.75\textwidth}{The weighted degree $\nodedegree{\nodeidx}\defeq \sum_{\nodeidx' \in \neighbourhood{\nodeidx}} \edgeweight_{\nodeidx,\nodeidx'}$ of node $\nodeidx$ in a graph $\graph$.}  \nonumber   \\[4mm] 
	&\maxnodedegree^{(\graph)} & \quad & \parbox{.75\textwidth}{The maximum weighted node degree of a graph $\graph$.} \nonumber\\[4mm] 
&\localdataset{\nodeidx} & \quad & \parbox{.75\textwidth}{The local \gls{dataset} $\localdataset{\nodeidx}$ carried by 
			node $\nodeidx\in \nodes$ of an \gls{empgraph}.} \nonumber \\[4mm]
&\localsamplesize{\nodeidx} & \quad & \parbox{.75\textwidth}{The number of \gls{datapoint}s (sample size) contained in the 
			local \gls{dataset} $\localdataset{\nodeidx}$ at node $\nodeidx\in \nodes$.} \nonumber 
\end{align} 
\begin{align} 
		&\featurevec^{(\nodeidx,\sampleidx)} & \quad & \parbox{.75\textwidth}{The \gls{feature}s of the $\sampleidx$-th \gls{datapoint} in 
		the local \gls{dataset} $\localdataset{\nodeidx}$.}  \nonumber   \\[4mm] 
	&\truelabel^{(\nodeidx,\sampleidx)} & \quad & \parbox{.75\textwidth}{The \gls{label} of the $\sampleidx$-th \gls{datapoint} in 
		the local \gls{dataset} $\localdataset{\nodeidx}$.} \nonumber \\[4mm]
		&\localparams{\nodeidx} & \quad & \parbox{.75\textwidth}{The local \gls{modelparams} of device $\nodeidx$ within an \gls{empgraph}.} \nonumber\\[6mm]
		&\locallossfunc{\nodeidx}{\weights} & \quad & \parbox{.75\textwidth}{The local \gls{lossfunc} used by device $\nodeidx$ 
		to measure the usefulness of some choice $\weights$ for the local \gls{modelparams}.} \nonumber \\[6mm]
	& \gtvloss{\featurevec}{\hypothesis\big(\featurevec\big)}{\hypothesis'\big(\featurevec\big)}& \quad & \parbox{.75\textwidth}{The \gls{loss} 
		incurred by a \gls{hypothesis} $\hypothesis'$ on a \gls{datapoint} with \gls{feature}s $\featurevec$ and label 
		$\hypothesis\big( \featurevec\big)$ that is obtained from another \gls{hypothesis}.} \nonumber \\[6mm]
		& 	{\rm stack} \big\{ \weights^{(\nodeidx)} \big\}_{\nodeidx=1}^{\nrnodes} & \quad & \parbox{.75\textwidth}{The vector $\bigg( \big(\weights^{(1)}  \big)^{T}, \ldots, \big(\weights^{(\nrnodes)}  \big)^{T} \bigg)^{T} \in \mathbb{R}^{\dimlocalmodel\nrnodes}$ that 
			is obtained by vertically stacking the \gls{localmodel} \gls{parameters} $\weights^{(\nodeidx)} \in \mathbb{R}^{\dimlocalmodel}$.} \nonumber  
\end{align}        



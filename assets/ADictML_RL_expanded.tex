%% ------------------------------------------------------------------
%% AUTO-GENERATED by FlattenGlossary.py
%% Source: /Users/junga1/AaltoDictionaryofML.github.io/ADictML_RL.tex
%% Repo root: /Users/junga1/AaltoDictionaryofML.github.io
%% ------------------------------------------------------------------

\newglossaryentry{mdp}
{name={Markov decision process (MDP)},
	description={An MDP \index{Markov decision process (MDP)} is a mathematical 
	        structure for the study of reinforcement learning (RL). 
			Formally, an MDP is a stochastic process that is defined by a specific choice for 
			\begin{itemize}
    			\item a state space $\mathcal{S}$;
    			\item an action space $\mathcal{A}$;
    			\item a transition function $\mathbb{P}\left(s' \mid s, a\right)$ 
			          specifying the conditional probability distribution $\mathbb{P}^{(s' \mid s, a)}$ 
					  over the next state $s' \in \mathcal{S}$, given the current state 
					  $s\in \mathcal{S}$ and action $a\in \mathcal{A}$;
    			\item a reward function $r(s, a) \in \mathbb{R}$ 
					  that assigns a numerical reward to each state-action 
					  pair $(s,a)$.
			\end{itemize}
			For a given policy $\pi$, these components define the probability distribution of a 
			sequence $$ \state_{1},\,\arm_{1},\,\reward_{1},\,\state_{2},\,\arm_{2},\,\reward_{2},\,\ldots,\,\state_{t},\,\arm_{t},\,\reward_{t}$$ 
			of random variables (RVs). The defining property of an MDP is the Markov property. 
			That is, at time $t$, the conditional probability distribution of the next state 
			$\state_{t+1}$ and reward $\reward_{t}$ depends on the 
			past only via the current state $\state_{t}$ and action $\arm_{t}$. 
			Reinforcement learning (RL) methods try to learn a policy $\pi$ that 
			maximizes the expected return 
			 \[
			 	\mathbb{E} \!\left\{ 
			 	\sum_{t=1}^{\infty} 
			 	\gamma^{t-1} \reward_{t} 
			 	\;\middle|\; \state_{1} 
			 	\right\}. 
			 \]
			 The conditioning on the initial state $\state_{1}$  indicates that the expected 
			 return is evaluated by following the policy $\pi$ from a given initial state.
			 The expected return involves the discount factor $\gamma\in (0,1)$ 
			 that determines the relative importance of future rewards compared 
			 to the immediate reward. 
			 The discount factor $\gamma$ is typically fixed for a given Markov decision process (MDP) and 
			 controls the trade-off between short-term and long-term reward. 
			 MDPs are widely used in robotics, game playing, and autonomous systems, to model 
			 decision-making problems where an agent interacts with an environment to 
			 achieve a goal\cite{SuttonEd2}, \cite{Bertsekas2012}.
		\\
		See also: reinforcement learning (RL), stochastic process, function,  reward.},
 	first={Markov decision process (MDP)},
	type=reinflearning, 
	plural={MDPs}, 
	firstplural={MDPs}, 
 	text={MDP} 
}

\newglossaryentry{statevaluefunction}
{name={state-value function},
	description={For a given Markov decision process (MDP), any policy $\pi$ naturally induces 
	             a value function \index{state-value function} 
				 $v_{\pi}:\mathcal{S}\rightarrow \mathbb{R}$. 
				 The value $v_{\pi}(s)$ is the expected 
				 return when the Markov decision process (MDP) starts in given state $s\in \mathcal{S}$ 
				  and actions are selected according to $\pi$.},
	first={state-value function},
	type=reinflearning,
	plural={state-value functions},
	firstplural={state-value functions},
	text={state-value function}
}

\newglossaryentry{valuefunction}
{name={value function},
	description={In the context of a Markov decision process (MDP), a value function \index{value function} 
				 $v: \mathcal{S}\rightarrow \mathbb{R}$ assigns to each state 
				 $s\in \mathcal{S}$ a real number $v(s)$ that quantifies 
				 the long-term desirability of being in state $s$.},
	first={value function},
	type=reinflearning,
	plural={value functions},
	firstplural={value functions},
	text={value function}
}

\newglossaryentry{policy}
{name={policy (reinforcement learning)},
	description={A policy \index{policy} is a function specifies how 
	 			 the next action $\arm_{t}$ in a Markov decision process (MDP) 
				 is chosen when the current state is $\state_{t}$. 
				 Typically, a policy is stochastic, meaning that it defines a 
				 conditional probability distribution $\mathbb{P}^{(a\mid s)}$ over the 
				 actions for a given current state. We can view 
				 a policy also as a hypothesis that uses features 
				 derived from the current state to predict the best next 
				 action \cite{SuttonEd2}.
		\\
		See also: Markov decision process (MDP), action, state.},
	first={policy},
	type=reinflearning,
	plural={policies},
	firstplural={policies},
	text={policy}
}


\newglossaryentry{bellmanoperator}
{name={Bellman operator},
	description={The Bellman operator \index{Bellman operator} $\mathcal{F}$ 
	             associated with an Markov decision process (MDP) is defined on the space of all 
				 value functions. In particular, it maps a 
				 value function $v: \mathcal{S}\rightarrow \mathbb{R}$ 
				 to another value function $v': \mathcal{S}\rightarrow \mathbb{R}$ 
				according to
				\[
		       	v'(s) =\max_{a\in \mathcal{A}}
					\Bigl(
						\mathbb{E} \{ r(s,a)\mid s,a\}
					+ \gamma\, \mathbb{E} \{ v(s') \mid s,a\}
				\Bigr),
					\]
				where $\gamma\in (0,1)$ is a discount factor and $s'$ is the 
				next state generated according to the transition function. 
				$s' \sim \mathbb{P}\left(s' \mid s,a\right)$.
				The state-value function $v^\star$ of the optimal 
				policy $\pi^\star$ is a fixed point of the Bellman operator, 
				$v^\star = \mathcal{F}v^\star$. This fixed-point equation 
			    lends naturally to the value iteration method for computing the state-value function 
			    of an optimal policy. Beside the Bellman operator 
				associated with an Markov decision process (MDP), there is also a Bellman operator 
				$\mathcal{F}^{(\pi)}$ associated with a policy $\pi$. 
				In this case, the Bellman operator is defined as
				\[
				\mathcal{F}^{(\pi)} v(s) = \mathbb{E} \{ r(s,a) \mid s,a\}
				+ \gamma\, \mathbb{E} \{ v(s') \mid s,a\},
				\]
				where $s' \sim \mathbb{P}\left(s' \mid s,a\right)$ and $a$ is selected
				according to $\pi$. The state-value function $v_{\pi}$ is a 
				fixed point of $\mathcal{F}^{(\pi)}$, $v_{\pi}= \mathcal{F}^{(\pi)}v_{\pi}$. This fixed-point equation can 
				be solved by a fixed-point iteration which is know as policy evaluation. 
				The Bellman operator is named after Richard Bellman, who introduced it in the context of
				dynamic programming \cite{Bellman1957}. The Bellman operator is a key
				concept in reinforcement learning (RL) and is used to derive algorithms for
				solving MDPs, such as value iteration and policy iteration\cite{SuttonEd2}.}, 
	first={Bellman operator},
	type=reinflearning,
	plural={Bellman operators},
	firstplural={Bellman operators},
	text={Bellman operator}
}

\newglossaryentry{policyevaluation}
{name={policy evaluation (reinforcement learning)},
 description={Policy evaluation refers to computing the state-value function $v_{\pi}$ 
		                   of a given policy $\pi$ in a Markov decision process (MDP). One widely used method is based 
		                   on the characterization of $v_{\pi}$ as a fixed point of 
		                   the Bellman operator $\mathcal{F}^{(\pi)}$.} , 
	text = {policy evaluation}, 
	first = {policy evaluation} 
}

\newglossaryentry{reinforcementlearning}
{name={reinforcement learning (RL)},
	description={RL\index{reinforcement learning (RL)} refers to an online learning setting where 
		we can only evaluate the usefulness of a single hypothesis (i.e., a specific 
		choice of model parameters) at each time step $t$. In particular, RL 
		methods apply the current hypothesis $h^{(t)}$ to the 
		feature vector ${\bf x}^{(t)}$ of the newly received data point 
		to predict the next action. 
		The usefulness of the resulting prediction $h^{(t)}({\bf x}^{(t)})$ 
		is quantified by a reward signal $r^{(t)}$ (see Fig. \ref{fig_reinforcementlearning_dict}). 
		\begin{figure}[H]
		\begin{center}
			\begin{tikzpicture}[scale=1]
			\draw[->] (-2, 0) -- (6, 0);
			\node at (6.3, 0) {$h$};
	        		% loss at time t 
			\draw[thick, blue, domain=0:3, samples=20] plot (\x-3, {-0.2*(\x)^2 + 2});
			\node[anchor=west,yshift=4pt] at (0-3, {-0.2*(0)^2 + 2}) {$-L^{(t)}(h)$};
			% Marker and hypothesis label for h^(t)
			\filldraw[blue] (1.5-3, {-0.2*(1.5)^2 + 2}) circle (2pt);
			\node[anchor=north] at (1.5-3, -0.3) {$h^{(t)}$};		
			\draw[dotted] (1.5-3, 0) -- (1.5-3, {-0.2*(1.5)^2 + 2});
			%%% time t+1
			\draw[thick, red, domain=0:5, samples=20, dashed] plot (\x, {-0.15*(\x - 2)^2 + 3});
			\node[anchor=west,yshift=4pt] at (3, {-0.15*(3 - 2)^2 + 3}) {$-L^{(t+1)}(h)$};
			\filldraw[red] (2, {-0.15*(2 - 2)^2 + 3}) circle (2pt);
			\node[anchor=north] at (2, -0.3) {$h^{(t+1)}$};
			\draw[dotted] (2, 0) -- (2, {-0.15*(3 - 2)^2 + 3});
			%%% time t+2
			\draw[thick, green!60!black, domain=3:5, samples=20, dotted] plot (\x+2, {-0.1*(\x - 4)^2 + 1.5});
			\node[anchor=west,yshift=4pt] at (4.5+2, {-0.1*(4.5 - 4)^2 + 1.5}) {$-L^{(t+2)}(h)$};
			\filldraw[green!60!black] (3.5+2, {-0.1*(3.5 - 4)^2 + 1.5}) circle (2pt);
			\node[anchor=north] at (3.5+2, -0.3) {$h^{(t+2)}$};
			\draw[dotted] (3.5+2, 0) -- (3.5+2, {-0.1*(3.5 - 4)^2 + 1.5});
			\end{tikzpicture}
		\caption{Three consecutive time steps $t,t+1,t+2$ with corresponding loss functions $L^{(t)},
			L^{(t+1)}, L^{(t+2)}$. During time step $t$, an RL method can evaluate the 
			loss function only for one specific hypothesis $h^{(t)}$, resulting in the reward 
			signal $r^{(t)}=-L^{(t)}(h^{(t)})$. \label{fig_reinforcementlearning_dict}}
		\end{center}
		\end{figure}
		In general, the reward depends also on the 
		previous predictions $h^{(t')}\big({\bf x}^{(t')}\big)$ 
		for $t' < t$. The goal of RL is to learn $h^{(t)}$, for 
		each time step $t$, such that the (possibly discounted) cumulative reward 
		is maximized \cite{MLBasics}, \cite{SuttonEd2}.
		\\
		See also: reward, loss function, machine learning (ML).},
	first={reinforcement learning (RL)},
	type=reinflearning,
	text={RL}
}

\newglossaryentry{action}
{name={action},
	description={An action\index{action} refers to a decision taken by an AI 
	             system at a given time step $t$ that influences the observed 
				 reward signal. The actions are elements of an 
				 action space $\mathcal{A}$ and are typically denoted by 
				 $\arm_{t} \in \mathcal{A}$. The action $\arm_{t}$ is selected 
				 based on the feature vector ${\bf x}^{(t)}$ (that collects 
				 all available observations) and the current 
				 hypothesis $h^{(t)}$. 
		         RL uses online learning methods 
				 to learn a hypothesis $h^{(t)}$ that predicts 
		         an (nearly) optimal action. The usefulness of the 
				 prediction $\arm_{t}$ is evaluated indirectly 
				 through the resulting reward signal $r^{(t)}$.
		         In the special case of a MAB, the set of possible actions is 
				 finite and each action corresponds to selecting one arm. In more
				 general RL settings, the action space may be continuous.
		\\
		See also: reinforcement learning (RL), reward, loss function, MAB, hypothesis.},
	first={action},
	type=reinflearning,
	plural={actions},
	firstplural={actions},
	text={action}
}

\newglossaryentry{actionspace}
{name={action space},
	description={See\index{action space} action.} ,
	first={action space},
	type=reinflearning,
	plural={action spaces},
	firstplural={action spaces},
	text={action space}
}

\newglossaryentry{mab}
{name={multiarmed bandit (MAB)},
	description={A MAB \index{multiarmed bandit (MAB)} is a precise 
		         formulation of a sequential decision-making task under uncertainty. 
				 At each time step $t$, one must choose an action from an finite 
				 action space $\mathcal{A}$. Choosing action 
				 $a$ at time $t$ yields a reward $r^{(a,t)}$. 
				 Each MAB induces a machine learning (ML) problem: Learn a hypothesis that 
				 predicts the optimal action $\arm_{t}$ at time $t$. 
				 This prediction must be based on the actions 
				 and rewards received up to time $t-1$ 
				  \cite{Bubeck2012}, \cite{SuttonEd2}. 
		See also: reward, regret.},
	first={MAB},
	type=reinflearning,
	text={MAB}
}

\newglossaryentry{stochmab}
{name={stochastic multiarmed bandit (MAB)},
	description={An stochastic MAB \index{stochastic multiarmed bandit (MAB)} 
	             is a stochastic process that is obtained from a MAB. In particular, 
				 the reward $r^{(a,t)}$ is modelled as a random variable (RV)		 
				 with an unknown probability distribution $\mathbb{P}^{\big(r^{(a,t)}\big)}$ \cite{Bubeck2012,HazanOCO}. 
				 In the simplest setting, the probability distribution $\mathbb{P}^{\big(r^{(a,t)}\big)}$ 
				 does not depend on $t$, i.e., it is time invariant. \\ 
		See also: reward, regret.},
	first={stochastic MAB},
	type=reinflearning,
	text={stochastic MAB}
}

\newglossaryentry{regret}
{name={regret},
	description={The regret\index{regret} of a hypothesis $h$ relative to 
		another hypothesis $h'$, which serves as a baseline, 
		is the difference between the loss incurred by $h$ and the loss 
		incurred by $h'$ \cite{PredictionLearningGames}. 
		The baseline hypothesis $h'$ is also referred to as an expert.
					\\ 
		See also: baseline, loss, expert.},
	first={regret},
	type=reinflearning,
	text={regret} 
}
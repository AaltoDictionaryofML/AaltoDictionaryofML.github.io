%% ------------------------------------------------------------------
%% AUTO-GENERATED by FlattenGlossary.py
%% Source: /Users/junga1/AaltoDictionaryofML.github.io/ADictML_RL.tex
%% Repo root: /Users/junga1/AaltoDictionaryofML.github.io
%% ------------------------------------------------------------------

\newglossaryentry{mdp}
{name={Markov decision process (MDP)},
	description={An MDP\index{Markov decision process (MDP)} is a mathematical 
	        structure for the study of reinforcement learning (RL). 
		Formally, an MDP is a stochastic process that is defined by a specific choice for 
		\begin{itemize}
    			\item a state space $\mathcal{S}$;
    			\item an action space $\mathcal{A}$;
    			\item a transition function $\mathbb{P}\left(s' \mid s, a\right)$ 
				specifying the conditional probability distribution $\mathbb{P}^{(s' \mid s, a)}$
				over the next state $s' \in \mathcal{S}$, given the current state 
				$s\in \mathcal{S}$ and action $a\in \mathcal{A}$;
    			\item a reward function $r(s, a) \in \mathbb{R}$ 
				that assigns a numerical reward to each state-action
				pair $(s,a)$.
		\end{itemize}
		For a given policy $\pi$, these components define the probability distribution of a sequence
		$$ \state_{1},\,\arm_{1},\,\reward_{1},\,\state_{2},\,\arm_{2},\,\reward_{2},\,\ldots,\,\state_{t},\,\arm_{t},\,\reward_{t}$$ 
		of random variables (RVs). The defining property of an MDP is the Markov property. 
		That is, at time instant $t$, the conditional probability distribution of the next state 
		$\state_{t+1}$ and reward $\reward_{t}$ depends on the 
		past only via the current state $\state_{t}$ and action $\arm_{t}$. 
		Reinforcement learning (RL) methods try to learn a policy $\pi$ that 
		maximizes the expected return 
		\[
			\mathbb{E} \!\left\{ 
			 \sum_{t=1}^{\infty} 
			 \gamma^{t-1} \reward_{t} 
			 \;\middle|\; \state_{1} 
			 \right\}. 
		\]
		The conditioning on the initial state $\state_{1}$ indicates that the expected 
		return is evaluated by following the policy $\pi$ from a given initial state.
		The expected return involves the discount factor $\gamma\in (0,1)$ 
		that determines the relative importance of future rewards compared 
		to the immediate reward. 
		The discount factor $\gamma$ is typically fixed for a given MDP and 
		controls the trade-off between short-term and long-term reward. 
		MDPs are widely used in robotics, game playing, and autonomous systems to model 
		decision-making problems where an agent interacts with an environment to 
		achieve a goal \cite{SuttonEd2}, \cite{BertsekasDynOptII}, \cite{BertsekasDynProgVolI}.
		\\
		See also: reinforcement learning (RL), stochastic process, function, reward.},
 	first={Markov decision process (MDP)},
	type=reinflearning, 
	plural={MDPs}, 
	firstplural={Markov decision processes (MDPs)}, 
 	text={MDP} 
}

\newglossaryentry{statevaluefunction}
{name={state-value function},
	description={For a given Markov decision process (MDP), any policy $\pi$ naturally induces 
		a value function\index{state-value function} 
		$v_{\pi}:\mathcal{S}\rightarrow \mathbb{R}$. 
		The value $v_{\pi}(s)$ is the expected 
		return when the Markov decision process (MDP) starts in a given state $s\in \mathcal{S}$ 
		and actions are selected according to $\pi$.
		\\
		See also: Markov decision process (MDP), value function, state. },
	first={state-value function},
	type=reinflearning,
	plural={state-value functions},
	firstplural={state-value functions},
	text={state-value function}
}

\newglossaryentry{valuefunction}
{name={value function},
	description={In the context of an Markov decision process (MDP), a value function\index{value function} 
		$v: \mathcal{S}\rightarrow \mathbb{R}$ assigns to each state 
		$s\in \mathcal{S}$ a real number $v(s)$ that quantifies 
		the long-term desirability of being in state $s$.
		\\
		See also: state. },
	first={value function},
	type=reinflearning,
	plural={value functions},
	firstplural={value functions},
	text={value function}
}

\newglossaryentry{policy}
{name={policy (reinforcement learning)},
	description={A policy\index{policy} is a function that specifies how 
	 	the next action $\arm_{t}$ in an Markov decision process (MDP) 
		is chosen when the current state is $\state_{t}$. 
		Typically, a policy is stochastic, meaning that it defines a 
		conditional probability distribution $\mathbb{P}^{(a\mid s)}$ over the 
		actions for a given current state. We can view 
		a policy also as a hypothesis that uses features 
		derived from the current state to predict the best next 
		action \cite{SuttonEd2}.
		\\
		See also: action, Markov decision process (MDP), state.},
	first={policy},
	type=reinflearning,
	plural={policies},
	firstplural={policies},
	text={policy}
}

\newglossaryentry{bellmanoperator}
{name={Bellman operator},
	description={The Bellman operator\index{Bellman operator} $\mathcal{F}$ 
		associated with a Markov decision process (MDP) is defined on the space of all 
		value functions. In particular, it maps a 
		value function $v: \mathcal{S}\rightarrow \mathbb{R}$ 
		to another value function $v': \mathcal{S}\rightarrow \mathbb{R}$ 
		according to
		\[
		       	v'(s) =\max_{a\in \mathcal{A}}
			\Bigl(
			\mathbb{E} \{ r(s,a)\mid s,a\}
			+ \gamma\, \mathbb{E} \{ v(s') \mid s,a\}
			\Bigr)
		\]
		where $\gamma\in (0,1)$ is a discount factor and $s'$ is the 
		next state generated according to the transition function, 
		$s' \sim \mathbb{P}\left(s' \mid s,a\right)$.
		The state-value function $v^\star$ of the optimal 
		policy $\pi^\star$ is a fixed point of the Bellman operator, 
		$v^\star = \mathcal{F}v^\star$. This fixed-point equation 
		naturally lends itself to the value iteration method for computing the state-value function 
		of an optimal policy. Beside the Bellman operator 
		associated with an Markov decision process (MDP), there is also a Bellman operator 
		$\mathcal{F}^{(\pi)}$ associated with a policy $\pi$. 
		In this case, the Bellman operator is defined as
		\[
			\mathcal{F}^{(\pi)} v(s) = \mathbb{E} \{ r(s,a) \mid s,a\}
			+ \gamma\, \mathbb{E} \{ v(s') \mid s,a\}
		\]
		where $s' \sim \mathbb{P}\left(s' \mid s,a\right)$ and $a$ is selected
		according to $\pi$. The state-value function $v_{\pi}$ is a 
		fixed point of $\mathcal{F}^{(\pi)}$, $v_{\pi}= \mathcal{F}^{(\pi)}v_{\pi}$. 
		This fixed-point equation can be solved by a fixed-point iteration that is known as policy evaluation. 
		The Bellman operator is named after Richard Bellman, who introduced it in the context of
		dynamic programming \cite{Bellman1957}. The Bellman operator is a key
		concept in reinforcement learning (RL) and is used to derive algorithms for
		solving Markov decision processes (MDPs), such as value iteration and policy iteration\cite{SuttonEd2}.
		\\
		See also: Markov decision process (MDP), value function, policy, value iteration, contractive operator, Banach's fixed-point theorem. }, 
	first={Bellman operator},
	type=reinflearning,
	plural={Bellman operators},
	firstplural={Bellman operators},
	text={Bellman operator}
}

\newglossaryentry{policyevaluation}
{name={policy evaluation (reinforcement learning)},
	description={Policy evaluation\index{policy evaluation} refers to computing 
		the state-value function $v_{\pi}$ 
		of a given policy $\pi$ in an Markov decision process (MDP). One widely used method, referred to 
		as iterative policy evaluation, is based on the characterization of 
		$v_{\pi}$ as a fixed point of the Bellman operator 
		$\mathcal{F}^{(\pi)}$. In particular, starting from an initial 
		value function $\valuefunc_{0}$, we iteratively apply the Bellman operator
		$\mathcal{F}^{(\pi)}$ to obtain a sequence of value functions 
		$\valuefunc_{1}, \,\valuefunc_{2}, \,\ldots$ according to 
		\[
		\valuefunc_{t+1} = \mathcal{F}^{(\pi)} \valuefunc_{t}, \quad t=0,\,1,\,2,\,\ldots
		\]
		Under mild conditions, this fixed-point iteration converges to 
		$v_{\pi}$ as $t\rightarrow \infty$ \cite[Sec. 4.2]{SuttonEd2}.
		\\
		See also: policy, state-value function, Markov decision process (MDP). }, 
	first={policy evaluation},
	type=reinflearning,
	text={policy evaluation}	 
}

\newglossaryentry{valueiteration}
{name={value iteration},
	description={Consider an Markov decision process (MDP) with the associated Bellman operator $\mathcal{F}$. The 
		state-value function $v^\star$ of the optimal policy 
		is a fixed point of $\mathcal{F}$, i.e., $v^\star = \mathcal{F}v^\star$. 
		Value iteration\index{value iteration} is the fixed-point iteration for computing
		$v^\star$ by repeatedly applying $\mathcal{F}$ to an initial 
		value function $\valuefunc_{0}$ \cite[Sec. 4.4]{SuttonEd2}.
		\\
		See also: state-value function, fixed-point iteration, value function. }, 
	first={value iteration},
	type=reinflearning,
	plural={value iterations},
	firstplural={value iterations},
	text={value iteration} 
}

\newglossaryentry{reinforcementlearning}
{name={reinforcement learning (RL)},
	description={RL\index{reinforcement learning (RL)} refers to an online learning setting where 
		we can only evaluate the usefulness of a single hypothesis (i.e., a specific
		choice of model parameters) at each time step $t$. In particular, RL 
		methods apply the current hypothesis $h^{(t)}$ to the 
		feature vector ${\bf x}^{(t)}$ of the newly received data point
		to predict the next action. 
		The usefulness of the resulting prediction $h^{(t)}({\bf x}^{(t)})$ 
		is quantified by a reward signal $r^{(t)}$ (see Fig. \ref{fig_reinforcementlearning_dict}). 
		\begin{figure}[H]
		\begin{center}
			\begin{tikzpicture}[scale=1]
			\draw[->] (-2, 0) -- (6, 0);
			\node at (6.3, 0) {$h$};
	        		% loss at time t 
			\draw[thick, blue, domain=0:3, samples=20] plot (\x-3, {-0.2*(\x)^2 + 2});
			\node[anchor=west,yshift=4pt] at (0-3, {-0.2*(0)^2 + 2}) {$-L^{(t)}(h)$};
			% Marker and hypothesis label for h^(t)
			\filldraw[blue] (1.5-3, {-0.2*(1.5)^2 + 2}) circle (2pt);
			\node[anchor=north] at (1.5-3, -0.3) {$h^{(t)}$};		
			\draw[dotted] (1.5-3, 0) -- (1.5-3, {-0.2*(1.5)^2 + 2});
			%%% time t+1
			\draw[thick, red, domain=0:5, samples=20, dashed] plot (\x, {-0.15*(\x - 2)^2 + 3});
			\node[anchor=west,yshift=4pt] at (3, {-0.15*(3 - 2)^2 + 3}) {$-L^{(t+1)}(h)$};
			\filldraw[red] (2, {-0.15*(2 - 2)^2 + 3}) circle (2pt);
			\node[anchor=north] at (2, -0.3) {$h^{(t+1)}$};
			\draw[dotted] (2, 0) -- (2, {-0.15*(3 - 2)^2 + 3});
			%%% time t+2
			\draw[thick, green!60!black, domain=3:5, samples=20, dotted] plot (\x+2, {-0.1*(\x - 4)^2 + 1.5});
			\node[anchor=west,yshift=4pt] at (4.5+2, {-0.1*(4.5 - 4)^2 + 1.5}) {$-L^{(t+2)}(h)$};
			\filldraw[green!60!black] (3.5+2, {-0.1*(3.5 - 4)^2 + 1.5}) circle (2pt);
			\node[anchor=north] at (3.5+2, -0.3) {$h^{(t+2)}$};
			\draw[dotted] (3.5+2, 0) -- (3.5+2, {-0.1*(3.5 - 4)^2 + 1.5});
			\end{tikzpicture}
		\caption{Three consecutive time steps $t,t+1,t+2$ with corresponding loss functions $L^{(t)},
			L^{(t+1)}, L^{(t+2)}$. During time step $t$, an RL method can evaluate the 
			loss function only for one specific hypothesis $h^{(t)}$, resulting in the reward 
			signal $r^{(t)}=-L^{(t)}(h^{(t)})$. \label{fig_reinforcementlearning_dict}}
		\end{center}
		\end{figure}
		In general, the reward depends also on the 
		previous predictions $h^{(t')}\big({\bf x}^{(t')}\big)$ 
		for $t' < t$. The goal of RL is to learn $h^{(t)}$, for 
		each time step $t$, such that the (possibly discounted) cumulative reward 
		is maximized \cite{MLBasics}, \cite{SuttonEd2}.
		\\
		See also: reward, loss function, machine learning (ML).},
	first={reinforcement learning (RL)},
	type=reinflearning,
	text={RL}
}

\newglossaryentry{action}
{name={action},
	description={An action\index{action} refers to a decision taken by an artificial intelligence system (AI system)
		at a given time step $t$ that influences the observed 
		reward signal. The actions are elements of an 
		action space $\mathcal{A}$ and are typically denoted by 
		$\arm_{t} \in \mathcal{A}$. The action $\arm_{t}$ is selected 
		based on the feature vector ${\bf x}^{(t)}$ (which collects 
		all available observations) and the current 
		hypothesis $h^{(t)}$. 
		Reinforcement learning (RL) uses online learning methods 
		to learn a hypothesis $h^{(t)}$ that predicts 
		a (nearly) optimal action. The usefulness of the 
		prediction $\arm_{t}$ is evaluated indirectly 
		through the resulting reward signal $r^{(t)}$.
		In the special case of an MAB, the set of possible actions is 
		finite and each action corresponds to selecting one arm. In more
		general reinforcement learning (RL) settings, the action space may be continuous.
		\\
		See also: reward, hypothesis, reinforcement learning (RL), MAB, loss function.},
	first={action},
	type=reinflearning,
	plural={actions},
	firstplural={actions},
	text={action}
}

\newglossaryentry{actionspace}
{name={action space},
	description={See\index{action space} action.} ,
	first={action space},
	type=reinflearning,
	plural={action spaces},
	firstplural={action spaces},
	text={action space}
}

\newglossaryentry{mab}
{name={multiarmed bandit (MAB)},
	description={An MAB\index{multiarmed bandit (MAB)} is a precise 
		formulation of a sequential decision-making task under uncertainty. 
		At each time step $t$, one must choose an action from an finite 
		action space $\mathcal{A}$. Choosing action 
		$a$ at time $t$ yields a reward $r^{(a,t)}$. 
		Each MAB induces an machine learning (ML) problem, i.e., learn a hypothesis that 
		predicts the optimal action $\arm_{t}$ at time $t$. 
		This prediction must be based on the actions 
		and rewards received up to time $t-1$ 
		\cite{SuttonEd2}, \cite{Bubeck2012}.
			\\ 
		See also: reward, regret.},
	first={MAB},
	type=reinflearning,
	text={MAB}
}

\newglossaryentry{stochmab}
{name={stochastic multiarmed bandit (stochastic MAB)},
	description={A stochastic MAB\index{stochastic multiarmed bandit (stochastic MAB)} 
		is a stochastic process that is obtained from an MAB. In particular, 
		the reward $r^{(a,t)}$ is modeled as a random variable (RV)		 
		with an unknown probability distribution $\mathbb{P}^{\big(r^{(a,t)}\big)}$ 
		\cite{HazanOCO}, \cite{Bubeck2012}. 
		In the simplest setting, the probability distribution $\mathbb{P}^{\big(r^{(a,t)}\big)}$ 
		does not depend on $t$, i.e., it is time invariant. 
		\\ 
		See also: reward, regret.},
	first={stochastic multiarmed bandit (stochastic MAB)},
	type=reinflearning,
	text={stochastic MAB}
}

\newglossaryentry{regret}
{name={regret},
	description={The regret\index{regret} of a hypothesis $h$ relative to 
		another hypothesis $h'$, which serves as a baseline, 
		is the difference between the loss incurred by $h$ and the loss 
		incurred by $h'$ \cite{PredictionLearningGames}. 
		The baseline hypothesis $h'$ is also referred to as an expert.
					\\ 
		See also: baseline, loss, expert.},
	first={regret},
	type=reinflearning,
	text={regret} 
}
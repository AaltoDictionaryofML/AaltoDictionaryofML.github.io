%% ------------------------------------------------------------------
%% AUTO-GENERATED by FlattenGlossary.py
%% Source: /Users/junga1/AaltoDictionaryofML.github.io/ADictML_RL.tex
%% Repo root: /Users/junga1/AaltoDictionaryofML.github.io
%% ------------------------------------------------------------------

\newglossaryentry{mdp}
{name={Markov decision process (MDP)},
	description={An MDP \index{Markov decision process (MDP)} is a mathematical 
	        structure for the study of reinforcement learning (RL). 
			Formally, an MDP is a stochastic process 
			which is defined by a specific choice for 
			\begin{itemize}
    			\item a state space $\mathcal{S}$;
    			\item an action space $\mathcal{A}$;
    			\item a transition function $\mathbb{P}\left(s' \mid s, a\right)$ 
			specifying the probability distribution over the next state 
			$s' \in \mathcal{S}$, given the current state 
			$s\in \mathcal{S}$ and action $a\in \mathcal{A}$;
    			\item a reward function $r(s, a) \in \mathbb{R}$ 
			that assigns a numerical reward to each 
			state-action pair $(s,a)$.
		\end{itemize}
		These components define the probability distribution of a sequence
		$$ \state_{1},\,\arm_{1},\,\reward_{1},\,\state_{2},\,\arm_{2},\,\reward_{2},\,\ldots,\,\state_{t},\,\arm_{t},\,\reward_{t}$$ 
		of random variables (RVs). The defining property of an MDP is the Markov property. That is, at 
		time instant $t$, the conditional probability distribution of the 
		next state $\state_{t+1}$ and reward $\reward_{t}$ 
		depends on the past only via the current state $\state_{t}$ and action $\arm_{t}$. 
		\\
		See also: reinforcement learning (RL), stochastic process, function, probability distribution, reward, prediction.},
 	first={Markov decision process (MDP)},
	type=reinflearning, 
	plural={MDPs}, 
	firstplural={MDPs}, 
 	text={MDP} 
}

\newglossaryentry{reinforcementlearning}
{name={reinforcement learning (RL)},
	description={RL\index{reinforcement learning (RL)} refers to an online learning setting where 
		we can only evaluate the usefulness of a single hypothesis (i.e., a choice of model parameters) 
		at each time step $t$. In particular, RL methods apply the current hypothesis 
		$h^{(t)}$ to the feature vector ${\bf x}^{(t)}$ of the 
		newly received data point. The usefulness of the resulting prediction 
		$h^{(t)}({\bf x}^{(t)})$ is quantified by a reward 
		signal $r^{(t)}$ (see Fig. \ref{fig_reinforcementlearning_dict}). 
		\begin{figure}[H]
		\begin{center}
			\begin{tikzpicture}[scale=1]
			\draw[->] (-2, 0) -- (6, 0);
			\node at (6.3, 0) {$h$};
	        		% loss at time t 
			\draw[thick, blue, domain=0:3, samples=20] plot (\x-3, {-0.2*(\x)^2 + 2});
			\node[anchor=west,yshift=4pt] at (0-3, {-0.2*(0)^2 + 2}) {$-L^{(t)}(h)$};
			% Marker and hypothesis label for h^(t)
			\filldraw[blue] (1.5-3, {-0.2*(1.5)^2 + 2}) circle (2pt);
			\node[anchor=north] at (1.5-3, -0.3) {$h^{(t)}$};		
			\draw[dotted] (1.5-3, 0) -- (1.5-3, {-0.2*(1.5)^2 + 2});
			%%% time t+1
			\draw[thick, red, domain=0:5, samples=20, dashed] plot (\x, {-0.15*(\x - 2)^2 + 3});
			\node[anchor=west,yshift=4pt] at (3, {-0.15*(3 - 2)^2 + 3}) {$-L^{(t+1)}(h)$};
			\filldraw[red] (2, {-0.15*(2 - 2)^2 + 3}) circle (2pt);
			\node[anchor=north] at (2, -0.3) {$h^{(t+1)}$};
			\draw[dotted] (2, 0) -- (2, {-0.15*(3 - 2)^2 + 3});
			%%% time t+2
			\draw[thick, green!60!black, domain=3:5, samples=20, dotted] plot (\x+2, {-0.1*(\x - 4)^2 + 1.5});
			\node[anchor=west,yshift=4pt] at (4.5+2, {-0.1*(4.5 - 4)^2 + 1.5}) {$-L^{(t+2)}(h)$};
			\filldraw[green!60!black] (3.5+2, {-0.1*(3.5 - 4)^2 + 1.5}) circle (2pt);
			\node[anchor=north] at (3.5+2, -0.3) {$h^{(t+2)}$};
			\draw[dotted] (3.5+2, 0) -- (3.5+2, {-0.1*(3.5 - 4)^2 + 1.5});
			\end{tikzpicture}
		\caption{Three consecutive time steps $t,t+1,t+2$ with corresponding loss functions $L^{(t)},
		L^{(t+1)}, L^{(t+2)}$. During time step $t$, an RL method can evaluate the 
		loss function only for one specific hypothesis $h^{(t)}$, resulting in the reward 
		signal $r^{(t)}=-L^{(t)}(h^{(t)})$. \label{fig_reinforcementlearning_dict}}
		\end{center}
		\end{figure}
		In general, the reward depends also on the 
		previous predictions $h^{(t')}\big({\bf x}^{(t')}\big)$ 
		for $t' < t$. The goal of RL is to learn $h^{(t)}$, for 
		each time step $t$, such that the (possibly discounted) cumulative reward 
		is maximized \cite{MLBasics}, \cite{SuttonEd2}.
		\\
		See also: reward, loss function, machine learning (ML).},
	first={reinforcement learning (RL)},
	type=reinflearning, 
	text={RL}
}

\newglossaryentry{action}
{name={action},
	description={An action\index{action} refers to a decision taken by an AI 
	             system at a given time step $t$ that influences the observed 
				 reward signal. The actions are elements of an 
				 action space $\mathcal{A}$ and are typically denoted by 
				 $\arm_{t} \in \mathcal{A}$. The action $\arm_{t}$ is selected 
				 based on the feature vector ${\bf x}^{(t)}$ (that collects 
				 all available observations) and the current 
				 hypothesis $h^{(t)}$. 
		         RL uses online learning methods 
				 to learn a hypothesis $h^{(t)}$ that predicts 
		         an (nearly) optimal action. The usefulness of the 
				 prediction $\arm_{t}$ is evaluated indirectly 
				 through the resulting reward signal $r^{(t)}$.
		         In the special case of a MAB, the set of possible actions is 
				 finite and each action corresponds to selecting one arm. In more
				 general RL settings, the action space may be continuous.
		\\
		See also: reinforcement learning (RL), reward, loss function, MAB, hypothesis.},
	first={action},
	type=reinflearning,
	plural={actions},
	firstplural={actions},
	text={action}
}


\newglossaryentry{actionspace}
{name={action space},
	description={See\index{action space} action.} ,
	first={action space},
	type=reinflearning,
	plural={action spaces},
	firstplural={action spaces},
	text={action space}
}

\newglossaryentry{mab}
{name={multiarmed bandit (MAB)},
	description={An MAB \index{multiarmed bandit (MAB)} problem is a precise 
	formulation of a sequential decision-making task under uncertainty. At each 
	discrete time step $t$, a learner selects one of several possible 
	actions—called arms—from a finite set $\mathcal{A}$. Pulling arm $a$ at time 
	$t$ yields a reward $r^{(a,t)}$ that is drawn from an unknown 
	probability distribution $\mathbb{P}\left(r^{(a,t)}\right)$. We obtain different classes 
	of MAB problems by placing different restrictions on this probability distribution. In the simplest 
	setting, the probability distribution $\mathbb{P}\left(r^{(a,t)}\right)$ does not depend on $t$. 
		Given an MAB problem, the goal is to construct machine learning (ML) methods that maximize the cumulative 
		reward over time by strategically balancing exploration (i.e., gathering information 
		about uncertain arms) and exploitation (i.e., selecting arms known to perform well). 
		MAB problems form an important special case of reinforcement learning (RL) problems \cite{Bubeck2012}, \cite{SuttonEd2}.
					\\ 
		See also: reward, regret.},
	first={MAB},
	type=reinflearning, 
	text={MAB}
}

\newglossaryentry{regret}
{name={regret},
	description={The regret\index{regret} of a hypothesis $h$ relative to 
		another hypothesis $h'$, which serves as a baseline, 
		is the difference between the loss incurred by $h$ and the loss 
		incurred by $h'$ \cite{PredictionLearningGames}. 
		The baseline hypothesis $h'$ is also referred to as an expert.
					\\ 
		See also: baseline, loss, expert.},
	first={regret},
	type=reinflearning, 
	text={regret} 
}

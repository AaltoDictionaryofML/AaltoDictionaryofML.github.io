<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Glossary Network</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css" rel="stylesheet" />
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js"></script>
    <script type="text/javascript" src="https://unpkg.com/vis-network/standalone/umd/vis-network.min.js"></script>
    <style>
      #mynetwork {
        width: 100%;
        height: 1000px;
        background-color: #ffffff;
        border: 1px solid lightgray;
      }
    </style>
  </head>
  <body>
    <center><h1>The Aalto Dictionary of Machine Learning</h1></center>
    <div id="mynetwork"></div>
    <script type="text/javascript">
      const nodes = new vis.DataSet([
  {
    "id": 1,
    "label": "minimum",
    "title": "Given a set of real numbers, the minimum is the smallest of those numbers.",
    "color": "lightcoral"
  },
  {
    "id": 2,
    "label": "epigraph",
    "title": "The epigraph of a real-valued function is the set of points lying on or above its : \\[ {epi}(f) = \\{ ({x}, t) {R}^n {R} \\,|\\, f({x}) t \\}. \\] A function is if and only if its epigraph is a set , . {figure}[H] {tikzpicture}[scale=1.0] {axis}[ axis lines = middle, xlabel = , ylabel = {}, xmin=-2, xmax=2, ymin=0, ymax=4.5, samples=100, domain=-1.5:1.5, thick, width=8cm, height=6cm, grid=none, axis on top, ] [blue, thick, domain=-1.5:1.5] {x^2} node [pos=0.85, anchor=south west, xshift=5pt] {}; [ name path=f, draw=none, ytick=, domain=-1.5:1.5, ] {x^2}; (axis cs:-1.5,4) -- (axis cs:1.5,4); [ blue!20, opacity=0.6, draw=none, ] fill between [ of=f and top, soft clip={domain=-1.5:1.5}, ]; at (axis cs:-1.0,2.3) {}; {axis} {tikzpicture} {Epigraph of the function (i.e., shaded area).} {figure} See also: , .",
    "color": "lightgreen"
  },
  {
    "id": 3,
    "label": "maximum",
    "title": "The maximum of a set of real numbers is the greatest element in that set, if such an element exists. A set has a maximum if it is bounded above and attains its {RudinBookPrinciplesMatheAnalysis}. \\\\ See also: .",
    "color": "lightblue"
  },
  {
    "id": 4,
    "label": "supremum (or least upper bound)",
    "title": "The supremum of a set of real numbers is the smallest number that is greater than or equal to every element in the set. More formally, a real number is the supremum of a set if: 1) is an upper bound of ; and 2) no number smaller than is an upper bound of . Every non-empty set of real numbers that is bounded above has a supremum, even if it does not contain its supremum as an element {RudinBookPrinciplesMatheAnalysis}.",
    "color": "lightblue"
  },
  {
    "id": 5,
    "label": "discrepancy",
    "title": "Consider an application with represented by an . methods use a discrepancy measure to compare maps from {localmodel} at nodes connected by an edge in the . \\\\ See also: , , , , .",
    "color": "khaki"
  },
  {
    "id": 6,
    "label": "FedRelax",
    "title": "An . \\\\ See also: , .",
    "color": "khaki"
  },
  {
    "id": 7,
    "label": "FedAvg",
    "title": "FedAvg refers to a family of iterative {algorithm}. It uses a server-client setting and alternates between client-wise {localmodel} re-training, followed by the aggregation of updated at the server . The local update at client at time starts from the current provided by the server and typically amounts to executing few iterations of . After completing the local updates, they are aggregated by the server (e.g., by averaging them). Fig.\\ {fig_single_iteration_fedavg} illustrates the execution of a single iteration of FedAvg. {figure} {center} {tikzpicture}[>=Stealth, node distance=1cm and 1.5cm, every node/.style={font=}] {server} = [circle, fill=black, minimum size=6pt, inner sep=0pt] {client} = [circle, draw=black, minimum size=6pt, inner sep=0pt] (label1) at (0,3.5) {broadcast}; (label2) {local update}; (label3) {aggregate}; (s1) at (label1 |- 0,2.5) {}; (c1l) at () {}; (c1r) at () {}; (dots1) at () {}; (s1) -- (c1l) node[midway,left] {}; (s1) -- (c1r) node[midway,right] {}; (s1) -- (dots1); (s2) at (label2 |- 0,2.5) {}; (c2l) at () {}; (c2r) at () {}; (dots2) at () {}; {}; {}; (s3) at (label3 |- 0,2.5) {}; {}; (c3l) at () {}; (c3r) at () {}; (dots3) at () {}; (c3l) -- (s3) node[midway,left] {}; (c3r) -- (s3) node[midway,right] {}; (dots3) -- (s3); {tikzpicture} {center} {Illustration of a single iteration of FedAvg which consisting of broadcasting by the server, local updates at clients and their aggregation by the server. {fig_single_iteration_fedavg}} {figure} \\\\ See also: , , .",
    "color": "lightcoral"
  },
  {
    "id": 8,
    "label": "FedGD",
    "title": "An that can be implemented as message passing across an . \\\\ See also: , , , , .",
    "color": "khaki"
  },
  {
    "id": 9,
    "label": "FedSGD",
    "title": "An that can be implemented as message passing across an . \\\\ See also: , , , , , .",
    "color": "khaki"
  },
  {
    "id": 10,
    "label": "dimensionality reduction",
    "title": "Dimensionality reduction refers to methods that learn a transformation of a (typically large) set of raw {feature} into a smaller set of informative {feature} . Using a smaller set of {feature} is beneficial in several ways: {itemize} {Statistical benefit:} It typically reduces the risk of , as reducing the number of {feature} often reduces the of a . {Computational benefit:} Using fewer {feature} means less computation for the training of {model}. As a case in point, methods need to invert a matrix whose size is determined by the number of {feature}. {Visualization:} Dimensionality reduction is also instrumental for visualization. For example, we can learn a transformation that delivers two {feature} which we can use, in turn, as the coordinates of a . Fig.\\ {fig:dimred-scatter} depicts the of hand-written digits that are placed according transformed {feature}. Here, the {datapoint} are naturally represented by a large number of grayscale values (one value for each pixel). {itemize} {figure}[H] {tikzpicture}[scale=1] (-0.5,0) -- (5.5,0) node[right] {}; (0,-0.5) -- (0,4.5) node[above] {}; // in { 1.2/0.5/3, 0.8/2.0/8, 2.5/1.8/1, 3.8/3.5/6, 4.2/0.7/9, 2.8/3.0/7, 1.5/3.8/2 }{ at (,) {}; } {tikzpicture} {Example of dimensionality reduction: High-dimensional image data (e.g., high-resolution images of hand-written digits) embedded into 2D using learned {feature} and visualized in a .} {fig:dimred-scatter} {figure} See also: , , , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 11,
    "label": "machine learning (ML)",
    "title": "ML aims to predict a from the {feature} of a . ML methods achieve this by learning a from a (or ) through the minimization of a , . One precise formulation of this principle is . Different ML methods are obtained from different design choices for {datapoint} (i.e., their {feature} and ), the , and the {MLBasics}. \\\\ See also: , , , , , , , .",
    "color": "orange"
  },
  {
    "id": 12,
    "label": "feature learning",
    "title": "Consider an application with {datapoint} characterized by raw {feature} . learning refers to the task of learning a map : ': ', that reads in raw {feature} of a and delivers new {feature} from a new . Different learning methods are obtained for different design choices of , for a of potential maps , and for a quantitative measure of the usefulness of a specific . For example, uses , with , and a \\{ : {R}^{} \\!\\! {R}^{'}\\!:\\!'\\!\\! { with some } \\!\\! {R}^{' } \\}. measures the usefulness of a specific map by the linear reconstruction error incurred on a such that _{ {R}^{ '}} _{=1}^{} { ^{()} - ^{()}}{2}^{2}. \\\\ See also: , , , , , , , .",
    "color": "lightcoral"
  },
  {
    "id": 13,
    "label": "autoencoder",
    "title": "An autoencoder is an method that simultaneously learns an encoder map and a decoder map . It is an instance of using a computed from the reconstruction error . \\\\ See also: , , .",
    "color": "orange"
  },
  {
    "id": 14,
    "label": "vertical federated learning (VFL)",
    "title": "VFL refers to applications where {device} have access to different {feature} of the same set of {datapoint} . Formally, the underlying global is \\[ ^{({global})} \\{ (^{(1)}, ^{(1)}), , (^{()}, ^{()}) \\}. \\] We denote by , for , the complete {featurevec} for the {datapoint}. Each observes only a subset of {feature}, resulting in a with {featurevec} \\[ ^{(,)} = ( ^{()}_{_{1}}, , ^{()}_{_{}} )^{T}. \\] Some of the {device} might also have access to the {label} , for , of the global . One potential application of VFL is to enable collaboration between different healthcare providers. Each provider collects distinct types of measurements\u2014such as blood values, electrocardiography, and lung X-rays\u2014for the same patients. Another application is a national social insurance system, where health records, financial indicators, consumer behavior, and mobility are collected by different institutions. VFL enables joint learning across these parties while allowing well-defined levels of . {figure}[H] {center} {tikzpicture}[every node/.style={anchor=base}] {0} {1.6} {3.2} {4.8} {6.4} {0} {-1.2} {-2.4} {-3.6} / in {1/1, 2/2, 4/} { {}{-1.2*(-1)} (x1) at (0,) {}; (x2) at (1.6,) {}; (dots) at (3.2,) {}; (x3) at (4.8,) {}; (y) at (6.4,) {}; } (-0.6,0.6) rectangle (6.9,-4.2); at (3.1,0.9) {}; (-0.9,0.9) rectangle (2.1,-4.0); at (0.25,1.0) {}; () rectangle (); at () {}; {tikzpicture} {center} {VFL uses {localdataset} that are derived from the {datapoint} of a common global . The {localdataset} differ in the choice of {feature} used to characterize the {datapoint}.{fig_vertical_FL}} {figure} See also: , , , , , , , , , .",
    "color": "salmon"
  },
  {
    "id": 15,
    "label": "explainability",
    "title": "Consider a trained (or learned ) , which maps the of a to the . LIME is a technique for explaining the behavior of , locally around a with . The is given in the form of a local approximation of (see Fig. {fig_lime}). This approximation can be obtained by an instance of with a carefully designed . In particular, the consists of {datapoint} with close to and the (pseudo-) . Note that we can use a different for the approximation from the original . For example, we can use a to approximate (locally) a . Another widely-used choice for is the . {figure}[H] {center} {tikzpicture} {axis}[ axis lines=middle, xlabel={}, ylabel={}, xtick=, ytick=, xmin=0, xmax=6, ymin=0, ymax=6, domain=0:6, samples=100, width=10cm, height=6cm, clip=false ] {2 + sin(deg(x))} node[pos=0.85, above right,yshift=3pt] {}; coordinates {(3,0) (3,6)}; {2 + sin(deg(3))} node[pos=0.9, above] {}; coordinates {(3, {2 + sin(deg(3))})}; at (axis cs:3,-0.3) {}; {axis} {tikzpicture} {center} {To explain a trained , around a given , we can use a local approximation . } {fig_lime} {figure} See also: , , , , , , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 16,
    "label": "linear model",
    "title": "Consider {datapoint}, each characterized by a numeric . A linear is a which consists of all linear maps such that {equation} {equ_def_lin_model_hypspace_dict} {} \\{ ()= ^{T} : {R}^{} \\}. {equation} Note that {equ_def_lin_model_hypspace_dict} defines an entire family of {hypospace}, which is parametrized by the number of {feature} that are linearly combined to form the . The design choice of is guided by (e.g., reducing means less computation), (e.g., increasing might reduce error), and . A linear using few carefully chosen {feature} tends to be considered more interpretable , . \\\\ See also: , , , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 17,
    "label": "Gaussian random variable (Gaussian RV)",
    "title": "A standard Gaussian is a real-valued with , , {equation} p(x) = {1}{{2}} ^{-x^2/2}. {equation} Given a standard Gaussian , we can construct a general Gaussian with and via . The of a Gaussian is referred to as the normal distribution, denoted . \\\\ A Gaussian random vector with and can be constructed as \\[ {A} + { }, \\] where is a vector of standard Gaussian {rv}, and is any matrix satisfying . The of a Gaussian random vector is referred to as the , denoted . \\\\ Gaussian random vectors arise as finite-dimensional marginals of {GaussProc}, which define consistent joint Gaussian distributions over arbitrary (potentially infinite) index sets . \\\\ Gaussian {rv} are widely used {probmodel} in the statistical analysis of methods. Their significance arises partly from the which is a mathematically precise formulation of the following rule-of-thumb: the average of a large number of independent {rv} (not necessarily Gaussian themselves) tends towards a Gaussian . \\\\ Compared to other {probdist}, the is also distinct in that-in a mathematically precise sense-represents maximum uncertainty: Among all continuous random vectors with a given covariance matrix , the Gaussian random vector maximizes differential entropy {coverthomas}. This makes Gaussian distributions a natural choice for capturing uncertainty (or lack of knowledge) in the absence of additional structural information. \\\\ See also: , , , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 18,
    "label": "Gaussian process (GP)",
    "title": "A GP is a collection of {rv} indexed by input values from some input space , such that, for any finite subset , the corresponding {rv} have a joint multivariate Gaussian distribution: \\[ ( f(^{(1)}, , ^{()} ) {N}({}, {K}). \\] For a fixed input space , a GP is fully specified (or parametrized) by {itemize} a function and a covariance function . {itemize} {Example:} We can interpret the temperature distribution across Finland (at a specific point in time) as the of a GP , where each input denotes a geographic location. Temperature observations from weather stations provide {sample} of at specific locations (see Fig.\\ {fig_gp_FMI}). A GP allows us to predict the temperature nearby weather stations and to quantify the of these predictions. {figure}[H] {center} {tikzpicture} {axis}[ axis equal, hide axis, scale=1.2, xmin=17, xmax=32, ymin=55, ymax=71, clip=true ] table [x=lon, y=lat, col sep=comma] {assets/finland_border.csv}; table [x=lon, y=lat, col sep=comma] {assets/fmi_stations_subset.csv}; (axis cs:19,59) -- (axis cs:25.5,59) node[anchor=west] {lon}; (axis cs:19,59) -- (axis cs:19,65.5) node[anchor=south] {lat}; {axis} {tikzpicture} {-15mm} {center} {We can interpret the temperature distribution over Finland as a of a GP indexed by geographic coordinates and sampled at weather stations (indicated by blue dots). {fig_gp_FMI}} {figure} See also: , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 19,
    "label": "projection",
    "title": "Consider a subset of the -dimensional . We define the projection of a vector onto as {equation} {equ_def_proj_generic_dict} {}{} = _{' } { - '}{2}. {equation} In other words, is the vector in which is closest to . The projection is only well-defined for subsets for which the above exists . \\\\ See also: , .",
    "color": "lightcoral"
  },
  {
    "id": 20,
    "label": "projected gradient descent (projected GD)",
    "title": "Consider an -based method that uses a parametrized with . Even if the of is , we cannot use basic , as it does not take into account contraints on the optimization variable (i.e., the ). Projected extends basic to handle constraints on the optimization variable (i.e., the ). A single iteration of projected consists of first taking a and then projecting the result back onto the . {figure}[H] {center} {tikzpicture}[scale=0.9] [right] at (-5.1,1.7) {} ; plot (, {(1/8)*}); [fill] (2.83,1) circle [radius=0.1] node[right] {}; (2.83,1) -- node[midway,above] {grad. step} (-1.5,1); (-1.5,1) --(-1.5,-1.5) node [below, left]{} ; (-1.5,-1.5) -- node[midway,above] {} (1,-1.5) ; [fill] (1,-1.5) circle [radius=0.1] node[below] {}; (1,-1.5) -- (3,-1.5) node[midway, above] {}; {tikzpicture} {-5mm} {center} {Projected augments a basic with a back onto the constraint set .} {fig_projected_GD_dict} {figure} See also: , , , , , , , , .",
    "color": "lightcoral"
  },
  {
    "id": 21,
    "label": "differential privacy (DP)",
    "title": "Consider some method that reads in a (e.g., the used for ) and delivers some output . The output could be either the learned or the {prediction} for specific {datapoint}. DP is a precise measure of incurred by revealing the output. Roughly speaking, an method is differentially private if the of the output does not change too much if the of one in the is changed. Note that DP builds on a for an method, i.e., we interpret its output as the of an . The randomness in the output can be ensured by intentionally adding the of an auxiliary (i.e., adding noise) to the output of the method. \\\\ See also: , , , , , , , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 22,
    "label": "stability",
    "title": "Stability is a desirable property of an method that maps a (e.g., a ) to an output . The output can be the learned or the delivered by the trained for a specific . Intuitively, is stable if small changes in the input lead to small changes in the output . Several formal notions of stability exist that enable bounds on the error or of the method (see {ShalevMLBook}). To build intuition, consider the three {dataset} depicted in Fig.~{fig_three_data_stability}, each of which is equally likely under the same -generating . Since the optimal are determined by this underlying , an accurate method should return the same (or very similar) output for all three {dataset}. In other words, any useful must be robust to variability in {realization} from the same , i.e., it must be stable. {figure}[H] {tikzpicture} {axis}[ axis lines=none, xlabel={}, ylabel={}, legend pos=north west, ymin=0, ymax=10, xtick={1,2,3,4,5}, grid style=dashed, every axis plot/.append style={very thick} ] +[only marks,mark=*] coordinates { (1,2) (2,4) (3,3) (4,5) (5,7) }; +[only marks,mark=square*] coordinates { (1,3) (2,2) (3,6) (4,4) (5,5) }; +[only marks,mark=triangle*] coordinates { (1,5) (2,7) (3,4) (4,6) (5,3) }; {axis} {tikzpicture} {Three {dataset} , , and , each sampled independently from the same -generating . A stable method should return similar outputs when trained on any of these {dataset}. {fig_three_data_stability}} {figure} See also: , , , , , , , , , , , , .",
    "color": "violet"
  },
  {
    "id": 23,
    "label": "privacy protection",
    "title": "Consider some method that reads in a and delivers some output . The output could be the learned or the obtained for a specific with {feature} . Many important applications involve {datapoint} representing humans. Each is characterized by {feature} , potentially a , and a (e.g., a recent medical diagnosis). Roughly speaking, privacy protection means that it should be impossible to infer, from the output , any of the {sensattr} of {datapoint} in . Mathematically, privacy protection requires non-invertibility of the map . In general, just making non-invertible is typically insufficient for privacy protection. We need to make sufficiently non-invertible. \\\\ See also: , , , , , , , .",
    "color": "salmon"
  },
  {
    "id": 24,
    "label": "privacy leakage",
    "title": "Consider an application that processes a and delivers some output, such as the {prediction} obtained for new {datapoint}. Privacy leakage arises if the output carries information about a private (or sensitive) of a (which might be a human) of . Based on a for the generation, we can measure the privacy leakage via the between the output and the senstive . Another quantitative measure of privacy leakage is . The relations between different measures of privacy leakage have been studied in the literature (see ). \\\\ See also: , , , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 25,
    "label": "probabilistic model",
    "title": "A probabilistic interprets {datapoint} as {realization} of {rv} with a joint . This joint typically involves which have to be manually chosen or learned via statistical inference methods such as estimation . \\\\ See also: , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 26,
    "label": "mean",
    "title": "The mean of an , taking values in an , is its . It is defined as the Lebesgue integral of with respect to the underlying (e.g., see or ), i.e., \\[ \\{\\} = _{{R}^{}} \\, {d}P(). \\] We also use the term to refer to the average of a finite sequence . However, these two definitions are essentially the same. Indeed, we can use the sequence to construct a discrete , with the index being chosen uniformly at random from the set . The mean of is precisely the average . \\\\ See also: , , , .",
    "color": "lightblue"
  },
  {
    "id": 27,
    "label": "variance",
    "title": "The variance of a real-valued is defined as the of the squared difference between and its . We extend this definition to vector-valued {rv} as . \\\\ See also: , .",
    "color": "lightblue"
  },
  {
    "id": 28,
    "label": "nearest neighbor (NN)",
    "title": "NN methods learn a whose function value is solely determined by the NNs within a given . Different methods use different metrics for determining the NNs. If {datapoint} are characterized by numeric {featurevec}, we can use their Euclidean distances as the metric. \\\\ See also: , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 29,
    "label": "neighborhood",
    "title": "The neighborhood of a node is the subset of nodes constituted by the of . \\\\ See also: .",
    "color": "lightgreen"
  },
  {
    "id": 30,
    "label": "neighbors",
    "title": "The neighbors of a node within an are those nodes that are connected (via an edge) to node . \\\\ See also: .",
    "color": "lightgreen"
  },
  {
    "id": 31,
    "label": "bias",
    "title": "Consider an method using a parametrized . It learns the using the =\\{ {^{()}}{^{()}} \\}_{=1}^{}. To analyze the properties of the method, we typically interpret the {datapoint} as {realization} of {rv}, ^{()} = ^{({})}( ^{()} ) + {}^{()}, =1,,. We can then interpret the method as an estimator computed from (e.g., by solving ). The (squared) bias incurred by the estimate is then defined as . \\\\ See also: , , , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 32,
    "label": "classification",
    "title": "Classification is the task of determining a discrete-valued label for a given , based solely on its features . The label belongs to a finite set, such as or , and represents the category to which the corresponding belongs. \\\\ See also: .",
    "color": "salmon"
  },
  {
    "id": 33,
    "label": "privacy funnel",
    "title": "The privacy funnel is a method for learning privacy-friendly {feature} of {datapoint} . \\\\ See also: , .",
    "color": "lightgreen"
  },
  {
    "id": 34,
    "label": "condition number",
    "title": "The condition number of a positive definite matrix is the ratio between the largest and the smallest of . The condition number is useful for the analysis of methods. The computational complexity of for crucially depends on the condition number of the matrix , with the of the . Thus, from a computational perspective, we prefer {feature} of {datapoint} such that has a condition number close to . \\\\ See also: , , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 35,
    "label": "classifier",
    "title": "A classifier is a (i.e., a map) used to predict a taking values from a finite . We might use the function value itself as a for the . However, it is customary to use a map that delivers a numeric quantity. The is then obtained by a simple thresholding step. For example, in a binary problem with {labelspace} , we might use a real-valued map as a classifier. A can then be obtained via thresholding, {equation} {equ_def_threshold_bin_classifier_dict} =1 { for } ()\\!\\!0 { and } =-1 { otherwise.} {equation} We can characterize a classifier by its {decisionregion} , for every possible value . \\\\ See also: , , , , , .",
    "color": "salmon"
  },
  {
    "id": 36,
    "label": "empirical risk",
    "title": "The empirical of a on a is the average incurred by when applied to the {datapoint} in . \\\\ See also: , , , , .",
    "color": "violet"
  },
  {
    "id": 37,
    "label": "node degree",
    "title": "The degree of a node in an undirected is the number of its , i.e., . \\\\ See also: , .",
    "color": "lightgreen"
  },
  {
    "id": 38,
    "label": "graph",
    "title": "A graph is a pair that consists of a node set and an edge set . In its most general form, a graph is specified by a map that assigns each edge a pair of nodes . One important family of graphs is simple undirected graphs. A simple undirected graph is obtained by identifying each edge with two different nodes . Weighted graphs also specify numeric for each edge . \\\\ See also: .",
    "color": "lightgreen"
  },
  {
    "id": 39,
    "label": "uncertainty",
    "title": "Uncertainty refers to the degree of confidence\u2014or lack thereof\u2014associated with a quantity such as a , parameter estimate, or observed . In , uncertainty arises from various sources, including noisy , limited training {sample}, or ambiguity in assumptions. theory offers a principled framework for representing and quantifying such uncertainty. \\\\ See also: , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 40,
    "label": "upper confidence bound (UCB)",
    "title": "Consider an application that requires selecting, at each time step , an action from a finite set of alternatives . The utility of selecting action is quantified by a numeric signal . A widely used for this type of sequential decision-making problem is the stochastic setting . In this , the is viewed as the of an with unknown . Ideally, we would always choose the action with the largest expected , but these {mean} are unknown and must be estimated from observed . Simply choosing the action with the largest estimate can lead to suboptimal outcomes due to estimation . The UCB strategy addresses this by selecting actions not only based on their estimated {mean} but also by incorporating a term that reflects the in these estimates\u2014favoring actions with high potential and high . Theoretical guarantees for the performance of UCB strategies, including logarithmic bounds, are established in . \\\\ See also: , , , , , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 41,
    "label": "multi-armed bandit (MAB)",
    "title": "A MAB problem models a repeated decision-making scenario in which, at each time step , a learner must choose one out of several possible actions, often referred to as arms, from a finite set . Each arm yields a stochastic drawn from an unknown with . The learner\u2019s goal is to maximize the cumulative over time by strategically balancing exploration (i.e., gathering information about uncertain arms) and exploitation (i.e., selecting arms known to perform well). This balance is quantified by the notion of , which measures the performance gap between the learner's strategy and the optimal strategy that always selects the best arm. MAB problems form a foundational in , reinforcement learning, and sequential experimental design . \\\\ See also: , , , , .",
    "color": "lightblue"
  },
  {
    "id": 42,
    "label": "optimism in the face of uncertainty",
    "title": "methods learn according to some performance criterion . However, they usually cannot access directly but rely on an estimate (or approximation) of . As a case in point, -based methods use the average on a given (i.e., the ) as an estimate for the of a . Using a , one can construct a confidence interval for each choice for the . One simple construction is , , with being a measure of the (expected) deviation of from . We can also use other constructions for this interval as long as they ensure that with a sufficiently high . An optimist chooses the according to the most favorable - yet still plausible - value of the performance criterion. Two examples of methods that use such an optimistic construction of an are {ShalevMLBook} and methods for sequential decision making {Bubeck2012}. {figure}[H] {center} {tikzpicture}[x=3cm, y=1cm] (-1, 5) -- plot[domain=-2:1, samples=100] ({+1}, { + 1}) -- plot[domain=1:-2, samples=100] ({+1}, { - 0.5}) -- cycle; at (2, 4) {}; plot ({+1}, { -0.5}) node[right] {}; plot ({}, {}); (1, -0.5) -- (1, 1) node[midway, right] {}; {tikzpicture} { methods learn by using some estimate of for the ultimate performance criterion . Using a , one can use to construct confidence intervals which contain with a high probability. The best plausible performance measure for a specific choice of is .} {center} {figure} See also: , , , , , , , , , , , , .",
    "color": "violet"
  },
  {
    "id": 43,
    "label": "federated learning network (FL network)",
    "title": "An network is an undirected weighted whose nodes represent generators that aim to train a local (or personalized) . Each node in an network represents some capable of collecting a and, in turn, train a . methods learn a local , for each node , such that it incurs small on the {localdataset}. \\\\ See also: , , , , , , , , .",
    "color": "khaki"
  },
  {
    "id": 44,
    "label": "norm",
    "title": "A norm is a function that maps each (vector) element of a vector space to a non-negative real number. This function must be homogeneous and definite, and it must satisfy the triangle inequality .",
    "color": "orange"
  },
  {
    "id": 45,
    "label": "dual norm",
    "title": "Every defined on an has an associated dual , which is denoted and defined as . The dual measures the largest possible inner product between and any vector in the unit ball of the original . For further details, see {BoydConvexBook}. \\\\ See also: , .",
    "color": "orange"
  },
  {
    "id": 46,
    "label": "explanation",
    "title": "One approach to make methods transparent is to provide an explanation along with the delivered by an method. Explanations can take on many different forms. An explanation could be some natural text or some quantitative measure for the importance of individual {feature} of a . We can also use visual forms of explanations, such as intensity plots for image . \\\\ See also: , , , , .",
    "color": "salmon"
  },
  {
    "id": 47,
    "label": "risk",
    "title": "Consider a used to predict the of a based on its {feature} . We measure the quality of a particular using a . If we interpret {datapoint} as the {realization} of {rv}, also the becomes the of an . The allows us to define the risk of a as the expected . Note that the risk of depends on both the specific choice for the and the of the {datapoint}. \\\\ See also: , , , , , , , , , , .",
    "color": "violet"
  },
  {
    "id": 48,
    "label": "activation function",
    "title": "Each artificial neuron within an is assigned an activation function that maps a weighted combination of the neuron inputs to a single output value . Note that each neuron is parametrized by the . \\\\ See also: , .",
    "color": "lightblue"
  },
  {
    "id": 49,
    "label": "distributed algorithm",
    "title": "A distributed is an designed for a special type of computer, i.e., a collection of interconnected computing devices (or nodes). These devices communicate and coordinate their local computations by exchanging messages over a network , . Unlike a classical , which is implemented on a single , a distributed is executed concurrently on multiple {device} with computational capabilities. Similar to a classical , a distributed can be modeled as a set of potential executions. However, each execution in the distributed setting involves both local computations and message-passing events. A generic execution might look as follows: \\[ {array}{l} {Node 1: } { input}_1, s_1^{(1)}, s_2^{(1)}, , s_{T_1}^{(1)}, { output}_1; \\\\ {Node 2: } { input}_2, s_1^{(2)}, s_2^{(2)}, , s_{T_2}^{(2)}, { output}_2; \\\\ \\\\ {Node N: } { input}_N, s_1^{(N)}, s_2^{(N)}, , s_{T_N}^{(N)}, { output}_N. {array} \\] Each starts from its own local input and performs a sequence of intermediate computations at discrete time instants . These computations may depend on both the previous local computations at the and the messages received from other {device}. One important application of distributed {algorithm} is in where a network of {device} collaboratively trains a personal for each . \\\\ See also: , , , .",
    "color": "khaki"
  },
  {
    "id": 50,
    "label": "algorithm",
    "title": "An algorithm is a precise, step-by-step specification for how to produce an output from a given input within a finite number of computational steps . For example, an algorithm for training a explicitly describes how to transform a given into through a sequence of {gradstep}. This informal characterization can be formalized rigorously via different mathematical {model} . One very simple of an algorithm is a collection of possible executions. Each execution is a sequence in the form of { input},s_1,s_2,,s_T,{ output} that respects the constraints inherent to the computer executing the algorithm. Algorithms may be deterministic, where each input results in a single execution, or randomized, where executions can vary probabilistically. Randomized algorithms can thus be analyzed by modeling execution sequences as outcomes of random experiments, viewing the algorithm as a stochastic process , , . Crucially, an algorithm encompasses more than just a mapping from input to output; it also includes the intermediate computational steps . \\\\ See also: , , , , .",
    "color": "lightcoral"
  },
  {
    "id": 51,
    "label": "online learning",
    "title": "Some methods are designed to process in a sequential manner, updating their as new {datapoint} become available\u2014one at a time. A typical example is time series data, such as daily minimum and maximum temperatures recorded by a weather station. These values form a chronological sequence of observations. In online learning, the (or its ) is refined incrementally with each newly observed , without revisiting past . \\\\ See also: , , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 52,
    "label": "online algorithm",
    "title": "An online processes input incrementally, receiving {datapoint} sequentially and making decisions or producing outputs (or decisions) immediately without having access to the entire input in advance , . Unlike an offline , which has the entire input available from the start, an online must handle about future inputs and cannot revise past decisions. Similar to an offline , we also represent an online formally as a collection of possible executions. However, the execution sequence for an online has a distinct structure: { in}_{1},s_1,{ out}_{1},{ in}_{2},s_2,{ out}_{2},,{ in}_{T},s_T,{ out}_{T}. Each execution begins from an initial state (i.e., \\({in}_{1}\\)) and proceeds through alternating computational steps, outputs (or decisions), and inputs. Specifically, at step \\(\\), the performs a computational step \\(s_{}\\), generates an output \\({out}_{}\\), and then subsequently receives the next input () \\({in}_{+1}\\). A notable example of an online in is , which incrementally updates as new {datapoint} arrive. \\\\ See also: , , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 53,
    "label": "transparency",
    "title": "Transparency is a fundamental requirement for . In the context of methods, transparency is often used interchangeably with , . However, in the broader scope of systems, transparency extends beyond and includes providing information about the system\u2019s limitations, reliability, and intended use. In medical diagnosis systems, transparency requires disclosing the confidence level for the {prediction} delivered by a trained . In credit scoring, -based lending decisions should be accompanied by explanations of contributing factors, such as income level or credit history. These explanations allow humans (e.g., a loan applicant) to understand and contest automated decisions. Some methods inherently offer transparency. For example, provides a quantitative measure of reliability through the value . {decisiontree} are another example, as they allow human-readable decision rules . Transparency also requires a clear indication when a user is engaging with an system. For example, -powered chatbots should notify users that they are interacting with an automated system rather than a human. Furthermore, transparency encompasses comprehensive documentation detailing the purpose and design choices underlying the system. For instance, datasheets and system cards help practitioners understand the intended use cases and limitations of an system . \\\\ See also: , , , , , , , , .",
    "color": "salmon"
  },
  {
    "id": 54,
    "label": "sensitive attribute",
    "title": "revolves around learning a map that allows us to predict the of a from its {feature}. In some applications, we must ensure that the output delivered by an system does not allow us to infer sensitive attributes of a . Which part of a is considered a sensitive attribute is a design choice that varies across different application domains. \\\\ See also: , , , , .",
    "color": "salmon"
  },
  {
    "id": 55,
    "label": "stochastic block model (SBM)",
    "title": "The SBM is a probabilistic generative for an undirected with a given set of nodes . In its most basic variant, the SBM generates a by first randomly assigning each node to a index . A pair of different nodes in the is connected by an edge with that depends solely on the {label} . The presence of edges between different pairs of nodes is statistically independent. \\\\ See also: , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 56,
    "label": "deep net",
    "title": "A deep net is an with a (relatively) large number of hidden layers. Deep learning is an umbrella term for methods that use a deep net as their . \\\\ See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 57,
    "label": "baseline",
    "title": "Consider some method that produces a learned (or trained ) . We evaluate the quality of a trained by computing the average on a . But how can we assess whether the resulting performance is sufficiently good? How can we determine if the trained performs close to optimal and there is little point in investing more resources (for collection or computation) to improve it? To this end, it is useful to have a reference (or baseline) level against which we can compare the performance of the trained . Such a reference value might be obtained from human performance, e.g., the misclassification rate of dermatologists who diagnose cancer from visual inspection of skin . Another source for a baseline is an existing, but for some reason unsuitable, method. For example, the existing method might be computationally too expensive for the intended application. Nevertheless, its error can still serve as a baseline. Another, somewhat more principled, approach to constructing a baseline is via a . In many cases, given a , we can precisely determine the achievable among any hypotheses (not even required to belong to the ) . This achievable (referred to as the ) is the of the for the of a , given its {feature} . Note that, for a given choice of , the (if it exists) is completely determined by the {LC}. However, computing the and presents two main challenges: {enumerate}[label=)] The is unknown and needs to be estimated. Even if is known, it can be computationally too expensive to compute the exactly . {enumerate} A widely used is the for {datapoint} characterized by numeric {feature} and {label}. Here, for the , the is given by the posterior of the , given the {feature} , . The corresponding is given by the posterior (see Fig. {fig_post_baseline_dict}). {figure}[H] {center} {tikzpicture} (-1,0) -- (7,0) node[right] {}; plot ({}, {2*exp(-0.5*((-)^2))}); (,0) -- (,2.5); at ([yshift=-5pt] ,2.5) { }; (-1,1) -- (+1,1.0); at ([yshift=2pt] ,1.2) { }; in {0.5} { at (, 0) { }; } at (0.5,-0.2) { }; {tikzpicture} {center} {If the {feature} and the of a are drawn from a , we can achieve the (under ) by using the to predict the of a with {feature} . The corresponding is given by the posterior . We can use this quantity as a baseline for the average of a trained . {fig_post_baseline_dict}} {figure} See also: , , , , , , , , , , , , , , , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 58,
    "label": "spectrogram",
    "title": "A spectrogram represents the time-frequency distribution of the energy of a time signal . Intuitively, it quantifies the amount of signal energy present within a specific time segment and frequency interval . Formally, the spectrogram of a signal is defined as the squared magnitude of its short-time Fourier transform (STFT) . Fig. {fig:spectrogram_dict} depicts a time signal along with its spectrogram. {figure}[H] {assets/spectrogram.png} {Left: A time signal consisting of two modulated Gaussian pulses. Right: An intensity plot of the spectrogram. {fig:spectrogram_dict}} {figure} The intensity plot of its spectrogram can serve as an image of a signal. A simple recipe for audio signal is to feed this signal image into {deepnet} originally developed for image and object detection . It is worth noting that, beyond the spectrogram, several alternative representations exist for the time-frequency distribution of signal energy , . \\\\ See also: , .",
    "color": "lightblue"
  },
  {
    "id": 59,
    "label": "graph clustering",
    "title": "aims at {datapoint} that are represented as the nodes of a . The edges of represent pairwise similarities between {datapoint}. Sometimes we can quantify the extend of these similarities by an , . \\\\ See also: , , , .",
    "color": "lightgreen"
  },
  {
    "id": 60,
    "label": "spectral clustering",
    "title": "Spectral is a particular instance of , i.e., it clusters {datapoint} represented as the nodes of a . Spectral uses the {eigenvector} of the to construct {featurevec} for each node (i.e., for each ) . We can feed these {featurevec} into -based methods, such as or via . Roughly speaking, the {featurevec} of nodes belonging to a well-connected subset (or ) of nodes in are located nearby in the (see Fig. {fig_lap_mtx_specclustering_dict}). {figure}[H] {center} {minipage}{0.4} {tikzpicture} {scope}[every node/.style={circle, fill=black, inner sep=0pt, minimum size=0.3cm}] (1) at (0,0) {}; (2) [below left=of 1, xshift=-0.2cm, yshift=-1cm] {}; (3) [below right=of 1, xshift=0.2cm, yshift=-1cm] {}; (4) [below=of 1, yshift=0.5cm] {}; {scope} (1) -- (2); (1) -- (3); at (1) {}; at (2) {}; at (3) {}; at (4) {}; {tikzpicture} {minipage} {5mm} {minipage}{0.4} {equation} {}\\!=\\! {pmatrix} 2 & -1 & -1 & 0 \\\\ -1 & 1 & 0 & 0 \\\\ -1 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 0 {pmatrix}\\!=\\!{V} { } {V}^{T} {equation} {minipage} {20mm}\\\\ {minipage}{0.4} {tikzpicture}[scale=3] (-0.2, 0) -- (1.2, 0) node[right] {}; (0, -0.2) -- (0, 1.2) node[above] {}; (0.577, 0) circle (0.03cm) node[above right] {}; (0.577, 0) circle (0.03cm); (0.577, 0) circle (0.03cm); (0, 1) circle (0.03cm) node[above right] {}; {tikzpicture} {minipage} {minipage}{0.4} {align} & {V} = ( ^{(1)},^{(2)},^{(3)},^{(4)} ) \\\\ & {v}^{(1)}\\!=\\!{1}{{3}} {pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 {pmatrix}, \\, {v}^{(2)}\\!=\\!{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 {pmatrix} {align} {minipage} {{fig_lap_mtx_specclustering_dict} { Top.} Left: An undirected with four nodes , each representing a . Right: The and its . { Bottom.} Left: A of {datapoint} using the {featurevec} . Right: Two {eigenvector} corresponding to the of the . } {center} {figure} See also: , , , , , , , , , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 61,
    "label": "flow-based clustering",
    "title": "Flow-based groups the nodes of an undirected by applying to node-wise {featurevec}. These {featurevec} are built from network flows between carefully selected sources and destination nodes . \\\\ See also: , , , .",
    "color": "lightgreen"
  },
  {
    "id": 62,
    "label": "estimation error",
    "title": "Consider {datapoint}, each with and . In some applications, we can model the relation between the and the of a as . Here, we use some true underlying and a noise term which summarizes any modeling or labeling errors. The estimation error incurred by an method that learns a , e.g., using , is defined as , for some . For a parametric , which consists of maps determined by , we can define the estimation error as , . \\\\ See also: , , , , , , , .",
    "color": "orange"
  },
  {
    "id": 63,
    "label": "degree of belonging",
    "title": "Degree of belonging is a number that indicates the extent to which a belongs to a {MLBasics}. The degree of belonging can be interpreted as a soft assignment. methods can encode the degree of belonging by a real number in the interval . is obtained as the extreme case when the degree of belonging only takes on values or . \\\\ See also: , , , .",
    "color": "lightgreen"
  },
  {
    "id": 64,
    "label": "mean squared estimation error (MSEE)",
    "title": "Consider an method that learns based on some . If we interpret the {datapoint} in as {realization} of an , we define the . Here, denotes the true of the of . The MSEE is defined as the of the squared Euclidean of the , . \\\\ See also: , , , , , , , , , , , .",
    "color": "orange"
  },
  {
    "id": 65,
    "label": "generalized total variation minimization (GTVMin)",
    "title": "GTVMin is an instance of using the of local as a . \\\\ See also: , , , .",
    "color": "lightgreen"
  },
  {
    "id": 66,
    "label": "regression",
    "title": "Regression problems revolve around the prediction of a numeric solely from the {feature} of a {MLBasics}. \\\\ See also: , , .",
    "color": "salmon"
  },
  {
    "id": 67,
    "label": "accuracy",
    "title": "Consider {datapoint} characterized by {feature} and a categorical which takes on values from a finite . The accuracy of a , when applied to the {datapoint} in a , is then defined as using the . \\\\ See also: , , , , , , .",
    "color": "salmon"
  },
  {
    "id": 68,
    "label": "expert",
    "title": "aims to learn a that accurately predicts the of a based on its {feature}. We measure the error using some . Ideally, we want to find a that incurs minimal on any . We can make this informal goal precise via the and by using the as the for the (average) of a . An alternative approach to obtaining a is to use the learned by an existing method. We refer to this as an expert . minimization methods learn a that incurs a comparable to the best expert , . \\\\ See also: , , , , , , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 69,
    "label": "networked federated learning (NFL)",
    "title": "NFL refers to methods that learn personalized {model} in a distributed fashion. These methods learn from {localdataset} that are related by an intrinsic network structure. \\\\ See also: , , .",
    "color": "salmon"
  },
  {
    "id": 70,
    "label": "regret",
    "title": "The regret of a relative to another , which serves as a , is the difference between the incurred by and the incurred by . The is also referred to as an . \\\\ See also: , , , .",
    "color": "lightblue"
  },
  {
    "id": 71,
    "label": "strongly convex",
    "title": "A continuously real-valued function is strongly with coefficient if ,{CvxAlgBertsekas}. \\\\ See also: , .",
    "color": "violet"
  },
  {
    "id": 72,
    "label": "differentiable",
    "title": "A real-valued function is differentiable if it can, at any point, be approximated locally by a linear function. The local linear approximation at the point is determined by the . \\\\ See also: .",
    "color": "violet"
  },
  {
    "id": 73,
    "label": "gradient",
    "title": "For a real-valued function , if a vector exists such that , it is referred to as the gradient of at . If it exists, the gradient is unique and denoted or .",
    "color": "violet"
  },
  {
    "id": 74,
    "label": "subgradient",
    "title": "For a real-valued function , a vector such that is referred to as a subgradient of at , .",
    "color": "orange"
  },
  {
    "id": 75,
    "label": "FedProx",
    "title": "FedProx refers to an iterative that alternates between separately training {localmodel} and combining the updated local . In contrast to , which uses to train {localmodel}, FedProx uses a for the training . \\\\ See also: , , , , , , .",
    "color": "lightcoral"
  },
  {
    "id": 76,
    "label": "rectified linear unit (ReLU)",
    "title": "The ReLU is a popular choice for the of a neuron within an . It is defined as , with being the weighted input of the artificial neuron. \\\\ See also: , .",
    "color": "lightblue"
  },
  {
    "id": 77,
    "label": "hypothesis",
    "title": "A hypothesis refers to a map (or function) from the to the . Given a with {feature} , we use a hypothesis map to estimate (or approximate) the using the . is all about learning (or finding) a hypothesis map such that for any (having {feature} and ). \\\\ See also: , , , , , , .",
    "color": "orange"
  },
  {
    "id": 78,
    "label": "Vapnik\u2013Chervonenkis dimension (VC dimension)",
    "title": "The VC dimension of an infinite is a widely-used measure for its size. We refer to the literature (see ) for a precise definition of VC dimension as well as a discussion of its basic properties and use in . \\\\ See also: , .",
    "color": "orange"
  },
  {
    "id": 79,
    "label": "effective dimension",
    "title": "The effective dimension of an infinite is a measure of its size. Loosely speaking, the effective dimension is equal to the effective number of independent tunable . These might be the coefficients used in a linear map or the and bias terms of an . \\\\ See also: , , , , .",
    "color": "lightblue"
  },
  {
    "id": 80,
    "label": "label space",
    "title": "Consider an application that involves {datapoint} characterized by {feature} and {label}. The space is constituted by all potential values that the of a can take on. methods, aiming at predicting numeric {label}, often use the space . Binary methods use a space that consists of two different elements, e.g., , , or . \\\\ See also: , , , , , .",
    "color": "salmon"
  },
  {
    "id": 81,
    "label": "prediction",
    "title": "A prediction is an estimate or approximation for some quantity of interest. revolves around learning or finding a map that reads in the {feature} of a and delivers a prediction for its . \\\\ See also: , , , , .",
    "color": "salmon"
  },
  {
    "id": 82,
    "label": "histogram",
    "title": "Consider a that consists of {datapoint} , each of them belonging to some cell with side length . We partition this cell evenly into smaller elementary cells with side length . The histogram of assigns each elementary cell to the corresponding fraction of {datapoint} in that fall into this elementary cell. A visual example of such a histogram is provided in Fig.~{fig:histogram}.\\\\ {figure}[H] {tikzpicture} {compat=1.18} {axis}[ ybar, ymin=0, ymax=6, bar width=22pt, width=10cm, height=6cm, xlabel={Value}, ylabel={Frequency}, ytick={1,2,3,4,5,6}, xtick={1,2,3,4,5}, xticklabels={{[0,1)}, {[1,2)}, {[2,3)}, {[3,4)}, {[4,5)}}, enlarge x limits=0.15, title={Histogram of Sample Data} ] +[fill=blue!40] coordinates {(1,2) (2,5) (3,4) (4,3) (5,1)}; {axis} {tikzpicture} {A histogram representing the frequency of {datapoint} falling within discrete value ranges (i.e., bins). Each bar height shows the count of {sample} in the corresponding interval.} {fig:histogram} {figure} See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 83,
    "label": "bootstrap",
    "title": "For the analysis of methods, it is often useful to interpret a given set of {datapoint} as {realization} of {rv} with a common . In general, we do not know exactly, but we need to estimate it. The bootstrap uses the of as an estimator for the underlying . \\\\ See also: , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 84,
    "label": "feature space",
    "title": "The space of a given application or method is constituted by all potential values that the of a can take on. A widely used choice for the space is the , with the dimension being the number of individual {feature} of a . \\\\ See also: , , , , , .",
    "color": "orange"
  },
  {
    "id": 85,
    "label": "missing data",
    "title": "Consider a constituted by {datapoint} collected via some physical . Due to imperfections and failures, some of the or values of {datapoint} might be corrupted or simply missing. imputation aims at estimating these missing values . We can interpret imputation as an problem where the of a is the value of the corrupted . \\\\ See also: , , , , , , .",
    "color": "salmon"
  },
  {
    "id": 86,
    "label": "feature",
    "title": "A feature of a is one of its properties that can be measured or computed easily without the need for human supervision. For example, if a is a digital image (e.g., stored as a {.jpeg} file), then we could use the red-green-blue intensities of its pixels as features. Domain-specific synonyms for the term feature are \"covariate,\" \"explanatory variable,\" \"independent variable,\" \"input (variable),\" \"predictor (variable),\" or \"regressor\" , , . \\\\ See also: .",
    "color": "salmon"
  },
  {
    "id": 87,
    "label": "feature vector",
    "title": "vector refers to a vector whose entries are individual {feature} . Many methods use vectors that belong to some finite-dimensional . For some methods, however, it can be more convenient to work with vectors that belong to an infinite-dimensional vector space (e.g., see ). \\\\ See also: , , , .",
    "color": "lightgreen"
  },
  {
    "id": 88,
    "label": "label",
    "title": "A higher-level fact or quantity of interest associated with a . For example, if the is an image, the label could indicate whether the image contains a cat or not. Synonyms for label, commonly used in specific domains, include \"response variable,\" \"output variable,\" and \"target\" , , . \\\\ See also: .",
    "color": "salmon"
  },
  {
    "id": 89,
    "label": "data",
    "title": "Data refers to objects that carry information. These objects can be either concrete physical objects (such as persons or animals) or abstract concepts (such as numbers). We often use representations (or approximations) of the original data that are more convenient for data processing. These approximations are based on different data {model}, with the relational data being one of the most widely used . \\\\ See also: .",
    "color": "lightblue"
  },
  {
    "id": 90,
    "label": "dataset",
    "title": "A dataset refers to a collection of {datapoint}. These {datapoint} carry information about some quantity of interest (or ) within an application. methods use datasets for training (e.g., via ) and . Note that our notion of a dataset is very flexible, as it allows for very different types of {datapoint}. Indeed, {datapoint} can be concrete physical objects (such as humans or animals) or abstract objects (such as numbers). As a case in point, Fig.\\ {fig_cows_dataset} depicts a dataset that consists of cows as {datapoint}. {figure}[H] {center} {fig:cowsintheswissalps} {assets/Cows_in_the_Swiss_Alps.jpg} {center} {{fig_cows_dataset}\u201cCows in the Swiss Alps\u201d by User:Huhu Uet is licensed under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/).} {figure} Quite often, an engineer does not have direct access to a dataset. Indeed, accessing the dataset in Fig.\\ {fig_cows_dataset} would require us to visit the cow herd in the Alps. Instead, we need to use an approximation (or representation) of the dataset which is more convenient to work with. Different mathematical {model} have been developed for the representation (or approximation) of datasets , , , . One of the most widely adopted data is the relational , which organizes as a table (or relation) , . A table consists of rows and columns: {itemize} Each row of the table represents a single . Each column of the table corresponds to a specific attribute of the . methods can use attributes as {feature} and {label} of the . {itemize} For example, Table {tab:cowdata} shows a representation of the dataset in Fig.\\ {fig_cows_dataset}. In the relational , the order of rows is irrelevant, and each attribute (i.e., column) must be precisely defined with a domain, which specifies the set of possible values. In applications, these attribute domains become the and the . {table}[H] {tabular}{lcccc} & & & & \\\\ Zenzi & 100 & 4 & 100 & 25 \\\\ Berta & 140 & 3 & 130 & 23 \\\\ Resi & 120 & 4 & 120 & 31 \\\\ {tabular} {A relation (or table) that represents the dataset in Fig.\\ {fig_cows_dataset}.} {tab:cowdata} {table} While the relational is useful for the study of many applications, it may be insufficient regarding the requirements for . Modern approaches like datasheets for datasets provide more comprehensive documentation, including details about the dataset\u2019s collection process, intended use, and other contextual information . \\\\ See also: , , , , , , , , , , .",
    "color": "salmon"
  },
  {
    "id": 91,
    "label": "predictor",
    "title": "A predictor is a real-valued map. Given a with {feature} , the value is used as a for the true numeric of the . \\\\ See also: , , , , .",
    "color": "salmon"
  },
  {
    "id": 92,
    "label": "labeled datapoint",
    "title": "A whose is known or has been determined by some means which might require human labor. \\\\ See also: , .",
    "color": "lightgreen"
  },
  {
    "id": 93,
    "label": "random variable (RV)",
    "title": "An RV is a function that maps from a to a value space , . The consists of elementary events and is equipped with a measure that assigns probabilities to subsets of . Different types of RVs include {itemize} {binary RVs}, which map each elementary event to an element of a binary set (e.g., or ; {real-valued RVs}, which take values in the real numbers ; {vector-valued RVs}, which map elementary events to the . {itemize} theory uses the concept of measurable spaces to rigorously define and study the properties of (large) collections of RVs . \\\\ See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 94,
    "label": "networked model",
    "title": "A networked over an assigns a (i.e., a ) to each node of the . \\\\ See also: , , , .",
    "color": "khaki"
  },
  {
    "id": 95,
    "label": "batch",
    "title": "In the context of , a batch refers to a randomly chosen subset of the overall . We use the {datapoint} in this subset to estimate the of and, in turn, to update the . \\\\ See also: , , , , , .",
    "color": "violet"
  },
  {
    "id": 96,
    "label": "networked data",
    "title": "Networked consists of {localdataset} that are related by some notion of pairwise similarity. We can represent networked using a whose nodes carry {localdataset} and edges encode pairwise similarities. One example of networked arises in applications where {localdataset} are generated by spatially distributed {device}. \\\\ See also: , , , , .",
    "color": "khaki"
  },
  {
    "id": 97,
    "label": "training error",
    "title": "The average of a when predicting the {label} of the {datapoint} in a . We sometimes refer by training error also to minimal average which is achieved by a solution of . \\\\ See also: , , , , , .",
    "color": "orange"
  },
  {
    "id": 98,
    "label": "data point",
    "title": "A point is any object that conveys information . points might be students, radio signals, trees, forests, images, {rv}, real numbers, or proteins. We characterize points using two types of properties. One type of property is referred to as a . {feature} are properties of a point that can be measured or computed in an automated fashion. A different kind of property is referred to as a . The of a point represents some higher-level fact (or quantity of interest). In contrast to {feature}, determining the of a point typically requires human experts (or domain experts). Roughly speaking, aims to predict the of a point based solely on its {feature}. \\\\ See also: , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 99,
    "label": "validation error",
    "title": "Consider a which is obtained by some method, e.g., using on a . The average of on a , which is different from the , is referred to as the error. \\\\ See also: , , , , , , .",
    "color": "orange"
  },
  {
    "id": 100,
    "label": "validation",
    "title": "Consider a that has been learned via some method, e.g., by solving on a . Validation refers to the practice of evaluating the incurred by the on a set of {datapoint} that are not contained in the . \\\\ See also: , , , , , .",
    "color": "orange"
  },
  {
    "id": 101,
    "label": "quadratic function",
    "title": "A function of the form f() = ^{T} {Q} {w} + {q}^{T} +a, with some matrix , vector , and scalar .",
    "color": "violet"
  },
  {
    "id": 102,
    "label": "validation set",
    "title": "A set of {datapoint} used to estimate the of a that has been learned by some method (e.g., solving ). The average of on the set is referred to as the and can be used to diagnose an method (see {MLBasics}). The comparison between and can inform directions for improvement of the method (such as using a different ). \\\\ See also: , , , , , , , , , .",
    "color": "orange"
  },
  {
    "id": 103,
    "label": "test set",
    "title": "A set of {datapoint} that have been used neither to train a (e.g., via ) nor in a to choose between different {model}. \\\\ See also: , , , .",
    "color": "orange"
  },
  {
    "id": 104,
    "label": "model selection",
    "title": "In , selection refers to the process of choosing between different candidate {model}. In its most basic form, selection amounts to: 1) training each candidate ; 2) computing the for each trained ; and 3) choosing the with the smallest {MLBasics}. \\\\ See also: , , .",
    "color": "orange"
  },
  {
    "id": 105,
    "label": "linear classifier",
    "title": "Consider {datapoint} characterized by numeric {feature} and a from some finite . A linear is characterized by having {decisionregion} that are separated by hyperplanes in {MLBasics}. \\\\ See also: , , , , , .",
    "color": "salmon"
  },
  {
    "id": 106,
    "label": "generalization",
    "title": "Generalization refers to the ability of a trained on a to make accurate {prediction} on new, unseen {datapoint}. This is a central goal of and : to learn patterns that extend beyond the . Most systems use to learn a by minimizing the average over a of {datapoint} , denoted as . However, success on the does not guarantee success on unseen - this discrepancy is the challenge of generalization. \\\\ To study generalization mathematically, we need to formalize the notion of ``unseen'' . A widely used approach is to assume a for generation, such as the . Here, we interpret {datapoint} as independent {rv} with an identical . This , which is assumed fixed but unknown, allows us to define of a trained as the expected \\[ {}=_{ p()} \\{ (, ) \\}. \\] The difference between and is known as the . Tools from probability theory, such as {concentrationinequ} and uniform convergence, allow us to bound this gap under certain conditions .\\\\ { Generalization without .} theory is one way to study how well a generalizes beyond the , but it is not the only way. Another option is to use simple, deterministic changes to the {datapoint} in the . The basic idea is that a good should be robust: its should not change much if we slightly change the {feature} of a . \\\\[1mm] For example, an object detector trained on smartphone photos should still detect the object if a few random pixels are masked . Similarly, it should deliver the same result if we rotate the object in the image . {figure}[H] {tikzpicture}[scale=0.8] (3, 2) ellipse (6cm and 2cm); at (6, 3) {}; (1, 3) circle (4pt) node[below, xshift=0pt, yshift=0pt] {}; (5, 1) circle (4pt) node[below] {}; (1.6, 3) circle (3pt); (0.4, 3) circle (3pt); (1, 3) -- (1.6, 3); (1, 3) -- (0.4, 3); (5.6, 1) circle (3pt); (4.4, 1) circle (3pt); (5, 1) -- (5.6, 1); (5, 1) -- (4.4, 1); plot (, {- 1* + 5}); at (3, 2.5) [right] {}; {tikzpicture} {Two {datapoint} that are used as a to learn a via . We can evaluate outside either by an with some underlying or by perturbing the {datapoint}.} {fig:polynomial_fit_dict} {figure} See also: , , , , , , , , , , , , , , , , , .",
    "color": "violet"
  },
  {
    "id": 107,
    "label": "generalized total variation (GTV)",
    "title": "The difference between the performance of a trained on the and its performance on other {datapoint} (such as those in a ). \\\\ See also: , , , , , {model}, , {lossfunc}, , , , , , .",
    "color": "violet"
  },
  {
    "id": 108,
    "label": "structural risk minimization (SRM)",
    "title": "SRM is an instance of , with which the can be expressed as a countable union of submodels such that . Each submodel permits the derivation of an approximate upper bound on the error incurred when applying to train . These individual bounds\u2014one for each submodel\u2014are then combined to form a used in the objective. These approximate upper bounds (one for each ) are then combined to construct a for {ShalevMLBook}. \\\\ See also: , , , , , .",
    "color": "violet"
  },
  {
    "id": 109,
    "label": "algebraic connectivity",
    "title": "The algebraic connectivity of an undirected is the second-smallest of its . A is connected if and only if . \\\\ See also: , , .",
    "color": "lightgreen"
  },
  {
    "id": 110,
    "label": "kernel",
    "title": "Consider a matrix with (or spectral decomposition), = _{=1}^{} {} ^{()} ( ^{()} )^{T}. Here, we use the ordered (in increasing fashion) {eigenvalue} {equation} {1} {}. {equation} The Courant\u2013Fischer\u2013Weyl min-max characterization {GolubVanLoanBook} represents the {eigenvalue} of as the solutions to certain optimization problems. \\\\ See also: , , .",
    "color": "lightgreen"
  },
  {
    "id": 111,
    "label": "confusion matrix",
    "title": "Consider {datapoint}, which are characterized by {feature} and , having values from the finite . For a given , the confusion matrix is a matrix with rows representing the elements of . The columns of a confusion matrix correspond to the . The -th entry of the confusion matrix is the fraction of {datapoint} with and resulting in a . \\\\ See also: , , , , , .",
    "color": "salmon"
  },
  {
    "id": 112,
    "label": "outlier",
    "title": "Many methods are motivated by the , which interprets {datapoint} as {realization} of {rv} with a common . The is useful for applications where the statistical properties of the generation process are stationary (or time-invariant) . However, in some applications the consists of a majority of regular {datapoint} that conform with an as well as a small number of {datapoint} that have fundamentally different statistical properties compared to the regular {datapoint}. We refer to a that substantially deviates from the statistical properties of most {datapoint} as an outlier. Different methods for outlier detection use different measures for this deviation. Stastistical learning theory studies fundamental limits on the ability to mitigate outliers reliably , . \\\\ See also: , , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 113,
    "label": "soft clustering",
    "title": "Soft refers to the task of partitioning a given set of {datapoint} into (a few) overlapping {cluster}. Each is assigned to several different {cluster} with varying degrees of belonging. Soft methods determine the (or soft assignment) for each and each . A principled approach to soft is by interpreting {datapoint} as {realization} of a . We then obtain a natural choice for the as the conditional of a belonging to a specific mixture component. \\\\ See also: , , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 114,
    "label": "clustering",
    "title": "Clustering methods decompose a given set of {datapoint} into a few subsets, which are referred to as {cluster}. Each consists of {datapoint} that are more similar to each other than to {datapoint} outside the . Different clustering methods use different measures for the similarity between {datapoint} and different forms of representations. The clustering method uses the average vector of a (i.e., the cluster ) as its representative. A popular method based on represents a by a . \\\\ See also: , , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 115,
    "label": "eigenvalue",
    "title": "We refer to a number as an eigenvalue of a square matrix if there is a non-zero vector such that .",
    "color": "lightgreen"
  },
  {
    "id": 116,
    "label": "principal component analysis (PCA)",
    "title": "PCA determines a linear such that the new {feature} allow us to reconstruct the original {feature} with the reconstruction error . \\\\ See also: , , .",
    "color": "lightcoral"
  },
  {
    "id": 117,
    "label": "model inversion",
    "title": "TBD.",
    "color": "khaki"
  },
  {
    "id": 118,
    "label": "sample",
    "title": "A finite sequence (or list) of {datapoint} that is obtained or interpreted as the of {rv} with a common . The length of the sequence is referred to as the . \\\\ See also: , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 119,
    "label": "artificial neural network (ANN)",
    "title": "An ANN is a graphical (signal-flow) representation of a function that maps {feature} of a at its input to a for the corresponding at its output. The fundamental unit of an ANN is the artificial neuron, which applies an to its weighted inputs. The outputs of these neurons serve as inputs for other neurons, forming interconnected layers. \\\\ See also: , , , , .",
    "color": "lightblue"
  },
  {
    "id": 120,
    "label": "random forest",
    "title": "A random forest is a set of different {decisiontree}. Each of these {decisiontree} is obtained by fitting a perturbed copy of the original . \\\\ See also: , .",
    "color": "salmon"
  },
  {
    "id": 121,
    "label": "bagging (or bootstrap aggregation)",
    "title": "Bagging (or bootstrap aggregation) is a generic technique to improve (the robustness of) a given method. The idea is to use the to generate perturbed copies of a given and then to learn a separate for each copy. We then predict the of a by combining or aggregating the individual {prediction} of each separate . For maps delivering numeric values, this aggregation could be implemented by computing the average of individual {prediction}. \\\\ See also: , , , , , , .",
    "color": "salmon"
  },
  {
    "id": 122,
    "label": "device",
    "title": "Any physical system that can be used to store and process . In the context of , we typically mean a computer that is able to read in {datapoint} from different sources and, in turn, to train an using these {datapoint}. \\\\ See also: , , , .",
    "color": "khaki"
  }
]);
      const edges = new vis.DataSet([
  {
    "from": 2,
    "to": 38
  },
  {
    "from": 3,
    "to": 4
  },
  {
    "from": 5,
    "to": 96
  },
  {
    "from": 5,
    "to": 43
  },
  {
    "from": 5,
    "to": 77
  },
  {
    "from": 6,
    "to": 49
  },
  {
    "from": 7,
    "to": 50
  },
  {
    "from": 7,
    "to": 1
  },
  {
    "from": 8,
    "to": 49
  },
  {
    "from": 8,
    "to": 43
  },
  {
    "from": 9,
    "to": 49
  },
  {
    "from": 9,
    "to": 43
  },
  {
    "from": 10,
    "to": 79
  },
  {
    "from": 10,
    "to": 11
  },
  {
    "from": 10,
    "to": 89
  },
  {
    "from": 10,
    "to": 86
  },
  {
    "from": 10,
    "to": 98
  },
  {
    "from": 10,
    "to": 47
  },
  {
    "from": 11,
    "to": 88
  },
  {
    "from": 11,
    "to": 98
  },
  {
    "from": 11,
    "to": 77
  },
  {
    "from": 11,
    "to": 86
  },
  {
    "from": 12,
    "to": 11
  },
  {
    "from": 12,
    "to": 86
  },
  {
    "from": 12,
    "to": 98
  },
  {
    "from": 12,
    "to": 84
  },
  {
    "from": 12,
    "to": 116
  },
  {
    "from": 12,
    "to": 1
  },
  {
    "from": 12,
    "to": 90
  },
  {
    "from": 13,
    "to": 11
  },
  {
    "from": 14,
    "to": 90
  },
  {
    "from": 14,
    "to": 122
  },
  {
    "from": 14,
    "to": 89
  },
  {
    "from": 14,
    "to": 23
  },
  {
    "from": 14,
    "to": 86
  },
  {
    "from": 14,
    "to": 98
  },
  {
    "from": 14,
    "to": 87
  },
  {
    "from": 14,
    "to": 88
  },
  {
    "from": 15,
    "to": 77
  },
  {
    "from": 15,
    "to": 87
  },
  {
    "from": 15,
    "to": 98
  },
  {
    "from": 15,
    "to": 81
  },
  {
    "from": 15,
    "to": 46
  },
  {
    "from": 15,
    "to": 88
  },
  {
    "from": 15,
    "to": 56
  },
  {
    "from": 15,
    "to": 16
  },
  {
    "from": 16,
    "to": 87
  },
  {
    "from": 16,
    "to": 81
  },
  {
    "from": 16,
    "to": 98
  },
  {
    "from": 16,
    "to": 86
  },
  {
    "from": 17,
    "to": 93
  },
  {
    "from": 17,
    "to": 26
  },
  {
    "from": 17,
    "to": 27
  },
  {
    "from": 17,
    "to": 11
  },
  {
    "from": 17,
    "to": 25
  },
  {
    "from": 17,
    "to": 3
  },
  {
    "from": 17,
    "to": 18
  },
  {
    "from": 17,
    "to": 39
  },
  {
    "from": 18,
    "to": 26
  },
  {
    "from": 18,
    "to": 39
  },
  {
    "from": 18,
    "to": 93
  },
  {
    "from": 18,
    "to": 118
  },
  {
    "from": 19,
    "to": 1
  },
  {
    "from": 20,
    "to": 19
  },
  {
    "from": 21,
    "to": 11
  },
  {
    "from": 21,
    "to": 90
  },
  {
    "from": 21,
    "to": 24
  },
  {
    "from": 21,
    "to": 54
  },
  {
    "from": 21,
    "to": 98
  },
  {
    "from": 21,
    "to": 25
  },
  {
    "from": 21,
    "to": 93
  },
  {
    "from": 21,
    "to": 81
  },
  {
    "from": 22,
    "to": 11
  },
  {
    "from": 22,
    "to": 90
  },
  {
    "from": 22,
    "to": 81
  },
  {
    "from": 22,
    "to": 98
  },
  {
    "from": 22,
    "to": 106
  },
  {
    "from": 22,
    "to": 47
  },
  {
    "from": 22,
    "to": 89
  },
  {
    "from": 22,
    "to": 118
  },
  {
    "from": 23,
    "to": 11
  },
  {
    "from": 23,
    "to": 90
  },
  {
    "from": 23,
    "to": 81
  },
  {
    "from": 23,
    "to": 98
  },
  {
    "from": 23,
    "to": 88
  },
  {
    "from": 23,
    "to": 54
  },
  {
    "from": 23,
    "to": 86
  },
  {
    "from": 24,
    "to": 11
  },
  {
    "from": 24,
    "to": 90
  },
  {
    "from": 24,
    "to": 86
  },
  {
    "from": 24,
    "to": 98
  },
  {
    "from": 24,
    "to": 25
  },
  {
    "from": 24,
    "to": 89
  },
  {
    "from": 24,
    "to": 21
  },
  {
    "from": 24,
    "to": 81
  },
  {
    "from": 25,
    "to": 98
  },
  {
    "from": 25,
    "to": 93
  },
  {
    "from": 26,
    "to": 93
  },
  {
    "from": 27,
    "to": 93
  },
  {
    "from": 28,
    "to": 77
  },
  {
    "from": 28,
    "to": 90
  },
  {
    "from": 28,
    "to": 98
  },
  {
    "from": 28,
    "to": 87
  },
  {
    "from": 28,
    "to": 30
  },
  {
    "from": 29,
    "to": 30
  },
  {
    "from": 30,
    "to": 43
  },
  {
    "from": 31,
    "to": 11
  },
  {
    "from": 31,
    "to": 90
  },
  {
    "from": 31,
    "to": 98
  },
  {
    "from": 31,
    "to": 93
  },
  {
    "from": 32,
    "to": 98
  },
  {
    "from": 32,
    "to": 88
  },
  {
    "from": 33,
    "to": 86
  },
  {
    "from": 33,
    "to": 98
  },
  {
    "from": 34,
    "to": 115
  },
  {
    "from": 34,
    "to": 11
  },
  {
    "from": 34,
    "to": 86
  },
  {
    "from": 34,
    "to": 98
  },
  {
    "from": 35,
    "to": 77
  },
  {
    "from": 35,
    "to": 88
  },
  {
    "from": 35,
    "to": 80
  },
  {
    "from": 35,
    "to": 81
  },
  {
    "from": 35,
    "to": 32
  },
  {
    "from": 36,
    "to": 47
  },
  {
    "from": 36,
    "to": 77
  },
  {
    "from": 36,
    "to": 90
  },
  {
    "from": 36,
    "to": 98
  },
  {
    "from": 37,
    "to": 38
  },
  {
    "from": 37,
    "to": 30
  },
  {
    "from": 39,
    "to": 81
  },
  {
    "from": 39,
    "to": 98
  },
  {
    "from": 39,
    "to": 11
  },
  {
    "from": 39,
    "to": 89
  },
  {
    "from": 39,
    "to": 118
  },
  {
    "from": 40,
    "to": 11
  },
  {
    "from": 40,
    "to": 25
  },
  {
    "from": 40,
    "to": 41
  },
  {
    "from": 40,
    "to": 93
  },
  {
    "from": 40,
    "to": 26
  },
  {
    "from": 40,
    "to": 89
  },
  {
    "from": 40,
    "to": 39
  },
  {
    "from": 40,
    "to": 70
  },
  {
    "from": 41,
    "to": 26
  },
  {
    "from": 41,
    "to": 70
  },
  {
    "from": 41,
    "to": 51
  },
  {
    "from": 42,
    "to": 11
  },
  {
    "from": 42,
    "to": 90
  },
  {
    "from": 42,
    "to": 47
  },
  {
    "from": 42,
    "to": 77
  },
  {
    "from": 42,
    "to": 25
  },
  {
    "from": 42,
    "to": 108
  },
  {
    "from": 42,
    "to": 40
  },
  {
    "from": 43,
    "to": 38
  },
  {
    "from": 43,
    "to": 89
  },
  {
    "from": 43,
    "to": 122
  },
  {
    "from": 43,
    "to": 77
  },
  {
    "from": 45,
    "to": 44
  },
  {
    "from": 46,
    "to": 11
  },
  {
    "from": 46,
    "to": 81
  },
  {
    "from": 46,
    "to": 98
  },
  {
    "from": 46,
    "to": 32
  },
  {
    "from": 46,
    "to": 86
  },
  {
    "from": 47,
    "to": 77
  },
  {
    "from": 47,
    "to": 88
  },
  {
    "from": 47,
    "to": 98
  },
  {
    "from": 47,
    "to": 81
  },
  {
    "from": 47,
    "to": 93
  },
  {
    "from": 47,
    "to": 86
  },
  {
    "from": 48,
    "to": 119
  },
  {
    "from": 49,
    "to": 50
  },
  {
    "from": 49,
    "to": 122
  },
  {
    "from": 50,
    "to": 16
  },
  {
    "from": 51,
    "to": 11
  },
  {
    "from": 51,
    "to": 89
  },
  {
    "from": 51,
    "to": 77
  },
  {
    "from": 51,
    "to": 98
  },
  {
    "from": 51,
    "to": 52
  },
  {
    "from": 51,
    "to": 1
  },
  {
    "from": 51,
    "to": 3
  },
  {
    "from": 52,
    "to": 50
  },
  {
    "from": 52,
    "to": 89
  },
  {
    "from": 52,
    "to": 39
  },
  {
    "from": 52,
    "to": 98
  },
  {
    "from": 52,
    "to": 11
  },
  {
    "from": 52,
    "to": 51
  },
  {
    "from": 53,
    "to": 11
  },
  {
    "from": 53,
    "to": 15
  },
  {
    "from": 53,
    "to": 32
  },
  {
    "from": 53,
    "to": 81
  },
  {
    "from": 54,
    "to": 11
  },
  {
    "from": 54,
    "to": 77
  },
  {
    "from": 54,
    "to": 88
  },
  {
    "from": 54,
    "to": 98
  },
  {
    "from": 54,
    "to": 86
  },
  {
    "from": 55,
    "to": 38
  },
  {
    "from": 55,
    "to": 88
  },
  {
    "from": 56,
    "to": 119
  },
  {
    "from": 56,
    "to": 11
  },
  {
    "from": 57,
    "to": 11
  },
  {
    "from": 57,
    "to": 77
  },
  {
    "from": 57,
    "to": 103
  },
  {
    "from": 57,
    "to": 89
  },
  {
    "from": 57,
    "to": 25
  },
  {
    "from": 57,
    "to": 1
  },
  {
    "from": 57,
    "to": 47
  },
  {
    "from": 57,
    "to": 88
  },
  {
    "from": 57,
    "to": 98
  },
  {
    "from": 57,
    "to": 26
  },
  {
    "from": 57,
    "to": 27
  },
  {
    "from": 57,
    "to": 86
  },
  {
    "from": 58,
    "to": 32
  },
  {
    "from": 58,
    "to": 56
  },
  {
    "from": 59,
    "to": 38
  },
  {
    "from": 59,
    "to": 114
  },
  {
    "from": 59,
    "to": 98
  },
  {
    "from": 60,
    "to": 114
  },
  {
    "from": 60,
    "to": 59
  },
  {
    "from": 60,
    "to": 38
  },
  {
    "from": 60,
    "to": 98
  },
  {
    "from": 60,
    "to": 113
  },
  {
    "from": 60,
    "to": 115
  },
  {
    "from": 60,
    "to": 87
  },
  {
    "from": 60,
    "to": 1
  },
  {
    "from": 61,
    "to": 114
  },
  {
    "from": 61,
    "to": 38
  },
  {
    "from": 61,
    "to": 87
  },
  {
    "from": 62,
    "to": 87
  },
  {
    "from": 62,
    "to": 88
  },
  {
    "from": 62,
    "to": 98
  },
  {
    "from": 62,
    "to": 77
  },
  {
    "from": 62,
    "to": 11
  },
  {
    "from": 63,
    "to": 98
  },
  {
    "from": 63,
    "to": 113
  },
  {
    "from": 64,
    "to": 11
  },
  {
    "from": 64,
    "to": 90
  },
  {
    "from": 64,
    "to": 93
  },
  {
    "from": 64,
    "to": 62
  },
  {
    "from": 64,
    "to": 44
  },
  {
    "from": 64,
    "to": 98
  },
  {
    "from": 64,
    "to": 26
  },
  {
    "from": 66,
    "to": 88
  },
  {
    "from": 66,
    "to": 98
  },
  {
    "from": 66,
    "to": 86
  },
  {
    "from": 66,
    "to": 81
  },
  {
    "from": 67,
    "to": 88
  },
  {
    "from": 67,
    "to": 80
  },
  {
    "from": 67,
    "to": 77
  },
  {
    "from": 67,
    "to": 90
  },
  {
    "from": 67,
    "to": 98
  },
  {
    "from": 67,
    "to": 86
  },
  {
    "from": 68,
    "to": 11
  },
  {
    "from": 68,
    "to": 77
  },
  {
    "from": 68,
    "to": 88
  },
  {
    "from": 68,
    "to": 98
  },
  {
    "from": 68,
    "to": 81
  },
  {
    "from": 68,
    "to": 57
  },
  {
    "from": 68,
    "to": 70
  },
  {
    "from": 68,
    "to": 86
  },
  {
    "from": 70,
    "to": 77
  },
  {
    "from": 70,
    "to": 57
  },
  {
    "from": 70,
    "to": 68
  },
  {
    "from": 71,
    "to": 72
  },
  {
    "from": 72,
    "to": 73
  },
  {
    "from": 75,
    "to": 50
  },
  {
    "from": 75,
    "to": 7
  },
  {
    "from": 76,
    "to": 48
  },
  {
    "from": 76,
    "to": 119
  },
  {
    "from": 77,
    "to": 84
  },
  {
    "from": 77,
    "to": 80
  },
  {
    "from": 77,
    "to": 98
  },
  {
    "from": 77,
    "to": 88
  },
  {
    "from": 77,
    "to": 81
  },
  {
    "from": 77,
    "to": 11
  },
  {
    "from": 77,
    "to": 86
  },
  {
    "from": 78,
    "to": 11
  },
  {
    "from": 79,
    "to": 119
  },
  {
    "from": 79,
    "to": 31
  },
  {
    "from": 80,
    "to": 11
  },
  {
    "from": 80,
    "to": 88
  },
  {
    "from": 80,
    "to": 98
  },
  {
    "from": 80,
    "to": 66
  },
  {
    "from": 80,
    "to": 32
  },
  {
    "from": 80,
    "to": 86
  },
  {
    "from": 81,
    "to": 11
  },
  {
    "from": 81,
    "to": 77
  },
  {
    "from": 81,
    "to": 98
  },
  {
    "from": 81,
    "to": 88
  },
  {
    "from": 81,
    "to": 86
  },
  {
    "from": 82,
    "to": 90
  },
  {
    "from": 82,
    "to": 98
  },
  {
    "from": 82,
    "to": 118
  },
  {
    "from": 83,
    "to": 11
  },
  {
    "from": 83,
    "to": 82
  },
  {
    "from": 83,
    "to": 98
  },
  {
    "from": 83,
    "to": 93
  },
  {
    "from": 84,
    "to": 86
  },
  {
    "from": 84,
    "to": 11
  },
  {
    "from": 84,
    "to": 87
  },
  {
    "from": 84,
    "to": 98
  },
  {
    "from": 85,
    "to": 90
  },
  {
    "from": 85,
    "to": 122
  },
  {
    "from": 85,
    "to": 86
  },
  {
    "from": 85,
    "to": 88
  },
  {
    "from": 85,
    "to": 89
  },
  {
    "from": 85,
    "to": 11
  },
  {
    "from": 85,
    "to": 98
  },
  {
    "from": 86,
    "to": 98
  },
  {
    "from": 86,
    "to": 91
  },
  {
    "from": 87,
    "to": 86
  },
  {
    "from": 87,
    "to": 11
  },
  {
    "from": 88,
    "to": 98
  },
  {
    "from": 90,
    "to": 88
  },
  {
    "from": 90,
    "to": 11
  },
  {
    "from": 90,
    "to": 100
  },
  {
    "from": 90,
    "to": 89
  },
  {
    "from": 90,
    "to": 98
  },
  {
    "from": 90,
    "to": 84
  },
  {
    "from": 90,
    "to": 80
  },
  {
    "from": 90,
    "to": 86
  },
  {
    "from": 91,
    "to": 77
  },
  {
    "from": 91,
    "to": 98
  },
  {
    "from": 91,
    "to": 81
  },
  {
    "from": 91,
    "to": 88
  },
  {
    "from": 91,
    "to": 86
  },
  {
    "from": 92,
    "to": 98
  },
  {
    "from": 92,
    "to": 88
  },
  {
    "from": 94,
    "to": 43
  },
  {
    "from": 95,
    "to": 73
  },
  {
    "from": 95,
    "to": 97
  },
  {
    "from": 95,
    "to": 98
  },
  {
    "from": 96,
    "to": 89
  },
  {
    "from": 96,
    "to": 38
  },
  {
    "from": 96,
    "to": 122
  },
  {
    "from": 97,
    "to": 77
  },
  {
    "from": 97,
    "to": 88
  },
  {
    "from": 97,
    "to": 98
  },
  {
    "from": 98,
    "to": 89
  },
  {
    "from": 98,
    "to": 86
  },
  {
    "from": 98,
    "to": 88
  },
  {
    "from": 98,
    "to": 11
  },
  {
    "from": 98,
    "to": 93
  },
  {
    "from": 99,
    "to": 77
  },
  {
    "from": 99,
    "to": 11
  },
  {
    "from": 99,
    "to": 102
  },
  {
    "from": 99,
    "to": 100
  },
  {
    "from": 100,
    "to": 77
  },
  {
    "from": 100,
    "to": 11
  },
  {
    "from": 100,
    "to": 98
  },
  {
    "from": 102,
    "to": 47
  },
  {
    "from": 102,
    "to": 77
  },
  {
    "from": 102,
    "to": 11
  },
  {
    "from": 102,
    "to": 100
  },
  {
    "from": 102,
    "to": 99
  },
  {
    "from": 102,
    "to": 97
  },
  {
    "from": 102,
    "to": 98
  },
  {
    "from": 103,
    "to": 102
  },
  {
    "from": 103,
    "to": 98
  },
  {
    "from": 104,
    "to": 11
  },
  {
    "from": 104,
    "to": 99
  },
  {
    "from": 105,
    "to": 88
  },
  {
    "from": 105,
    "to": 80
  },
  {
    "from": 105,
    "to": 35
  },
  {
    "from": 105,
    "to": 98
  },
  {
    "from": 105,
    "to": 86
  },
  {
    "from": 106,
    "to": 11
  },
  {
    "from": 106,
    "to": 77
  },
  {
    "from": 106,
    "to": 89
  },
  {
    "from": 106,
    "to": 25
  },
  {
    "from": 106,
    "to": 47
  },
  {
    "from": 106,
    "to": 36
  },
  {
    "from": 106,
    "to": 107
  },
  {
    "from": 106,
    "to": 81
  },
  {
    "from": 106,
    "to": 98
  },
  {
    "from": 106,
    "to": 93
  },
  {
    "from": 106,
    "to": 5
  },
  {
    "from": 106,
    "to": 86
  },
  {
    "from": 107,
    "to": 102
  },
  {
    "from": 107,
    "to": 77
  },
  {
    "from": 107,
    "to": 106
  },
  {
    "from": 107,
    "to": 36
  },
  {
    "from": 107,
    "to": 73
  },
  {
    "from": 107,
    "to": 98
  },
  {
    "from": 108,
    "to": 106
  },
  {
    "from": 108,
    "to": 47
  },
  {
    "from": 109,
    "to": 38
  },
  {
    "from": 109,
    "to": 115
  },
  {
    "from": 110,
    "to": 115
  },
  {
    "from": 111,
    "to": 88
  },
  {
    "from": 111,
    "to": 80
  },
  {
    "from": 111,
    "to": 77
  },
  {
    "from": 111,
    "to": 81
  },
  {
    "from": 111,
    "to": 98
  },
  {
    "from": 111,
    "to": 86
  },
  {
    "from": 112,
    "to": 11
  },
  {
    "from": 112,
    "to": 89
  },
  {
    "from": 112,
    "to": 98
  },
  {
    "from": 112,
    "to": 93
  },
  {
    "from": 113,
    "to": 114
  },
  {
    "from": 113,
    "to": 98
  },
  {
    "from": 113,
    "to": 63
  },
  {
    "from": 114,
    "to": 86
  },
  {
    "from": 114,
    "to": 26
  },
  {
    "from": 114,
    "to": 113
  },
  {
    "from": 114,
    "to": 98
  },
  {
    "from": 116,
    "to": 1
  },
  {
    "from": 116,
    "to": 86
  },
  {
    "from": 118,
    "to": 98
  },
  {
    "from": 118,
    "to": 93
  },
  {
    "from": 119,
    "to": 98
  },
  {
    "from": 119,
    "to": 81
  },
  {
    "from": 119,
    "to": 88
  },
  {
    "from": 119,
    "to": 48
  },
  {
    "from": 119,
    "to": 86
  },
  {
    "from": 120,
    "to": 90
  },
  {
    "from": 121,
    "to": 11
  },
  {
    "from": 121,
    "to": 83
  },
  {
    "from": 121,
    "to": 90
  },
  {
    "from": 121,
    "to": 77
  },
  {
    "from": 121,
    "to": 88
  },
  {
    "from": 121,
    "to": 98
  },
  {
    "from": 121,
    "to": 81
  },
  {
    "from": 122,
    "to": 89
  },
  {
    "from": 122,
    "to": 11
  },
  {
    "from": 122,
    "to": 98
  },
  {
    "from": 122,
    "to": 26
  }
]);
      const container = document.getElementById("mynetwork");
      const data = { nodes: nodes, edges: edges };
      const options = {
        nodes: {
          shape: "dot",
          size: 20,
          font: { size: 14, color: "#000" }
        },
        edges: {
          arrows: "to",
          color: "gray",
          smooth: true
        },
        physics: {
          enabled: true,
          solver: "forceAtlas2Based",
          stabilization: {
            enabled: true,
            iterations: 200,
            fit: true
          }
        },
        interaction: {
          navigationButtons: true,
          keyboard: true,
          zoomView: true,
          dragView: true
        }
      };
      const network = new vis.Network(container, data, options);
    </script>
  </body>
</html>
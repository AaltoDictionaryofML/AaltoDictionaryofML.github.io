<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Glossary Network</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css" rel="stylesheet" />
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js"></script>
    <script type="text/javascript" src="https://unpkg.com/vis-network/standalone/umd/vis-network.min.js"></script>
    <style>
      #mynetwork {
        width: 100%;
        height: 1000px;
        background-color: #ffffff;
        border: 1px solid lightgray;
      }
    </style>
  </head>
  <body>
    <center><h1>The Aalto Dictionary of Machine Learning</h1></center>
    <div id="mynetwork"></div>
    <script type="text/javascript">
      const nodes = new vis.DataSet([
  {
    "id": 1,
    "label": "pseudoinverse",
    "title": "The Moore\u2013Penrose pseudoinverse of a matrix generalizes the notion of an . The pseudoinverse arises naturally within when applied to a with arbitrary {label} and {hastie01statisticallearning}. The learned by are given by \\[ {}^{()} = (^T + )^{-1} ^ , > 0. \\] We can then define the pseudoinverse via the limit {benisrael2003generalized} \\[ _{ 0^+} {}^{()} = ^+ . \\]",
    "color": "lightgreen"
  },
  {
    "id": 2,
    "label": "determinant",
    "title": "The determinant of a square matrix is a scalar that characterizes how (the orientation of) volumes in are altered by applying . [Note that a matrix represents a linear transformation on .] In particular, preserves orientation, reverses orientation, and collapses volume entirely, indicating that is non-invertible. The determinant also satisfies , and if is diagonalizable with {eigenvalue} , then . For the special cases (2D) and (3D), the determinant can be interpreted as an oriented area or volume spanned by the column vectors of . {figure} {center} {tikzpicture}[x=2cm] {scope} (0,0) -- (1,0) node[below right] {}; (0,0) -- (0,1) node[above left] {}; {scope} {scope}[shift={(2.8,0)}] (A) at (1.5,0.5); (B) at (-0.2,1.2); (0,0) -- (A) node[below right] {}; (0,0) -- (B) node[above left] {}; (0,0) -- (A) -- () -- (B) -- cycle; (A) -- (); (B) -- (); at (0.8,0.6) { }; (0.4,0.0) arc[start angle=0, end angle=35, radius=0.6]; {scope} (1.3,0.5) -- (2.4,0.5) node[midway, above] {}; {tikzpicture} {center} {figure} \\\\ See also: , .",
    "color": "orange"
  },
  {
    "id": 3,
    "label": "linear map",
    "title": "A linear map is a that satisfies additivity : , and homogeneity : for all vectors and scalars . In particular, . Any linear map can be represented as a matrix multiplication for some matrix . The collection of real-valued linear maps for a given dimension constitute a which is used in many methods. \\\\ See also: , , , .",
    "color": "khaki"
  },
  {
    "id": 4,
    "label": "vector space",
    "title": "A vector space (also called linear space) is a collection of elements (called vectors) closed under vector addition and scalar multiplication, i.e., {itemize} If , then . If and , then . In particular, . {itemize} The is a vector space. {linmodel} and {linearmap} operate within such spaces.\\\\ See also: , , .",
    "color": "khaki"
  },
  {
    "id": 5,
    "label": "stochastic",
    "title": "A process or method is called stochastic if it involves a random component or is governed by probabilistic laws. In , stochastic methods often incorporate randomness for reasons such as optimization (e.g., ) or modeling (e.g., {probmodel}). A stochastic process is a collection of {rv} indexed by time or space, which are used to model random phenomena evolving over time (e.g., noise in sensors or financial time series).\\\\ See also: , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 6,
    "label": "entropy",
    "title": "In the context of and , entropy quantifies the or randomness in a \u2019s {prediction}. Specifically, the conditional (or differential) entropy measures the unpredictability of the trained \u2019s outputs given another source of {prediction} (e.g., from a human user). Lower conditional entropy implies higher , as the \u2019s {prediction} are more aligned with the user\u2019s understanding. However, the term entropy is also used in other fields with different meanings, such as measuring disorder in thermodynamics or complexity in dynamical systems. \\\\ See also: , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 7,
    "label": "minimum",
    "title": "Given a set of real numbers, the minimum is the smallest of those numbers. Note that for some sets, such as the set of negative real numbers, the minimum does not exist.",
    "color": "lightcoral"
  },
  {
    "id": 8,
    "label": "function",
    "title": "A function is a mathematical rule that assigns each element exactly one element . We write this as , where is the domain and the co-domain of . That is, a function defines a unique output for every input .",
    "color": "salmon"
  },
  {
    "id": 9,
    "label": "map",
    "title": "We use the term map as a synonym for . \\\\ See also: .",
    "color": "lightgreen"
  },
  {
    "id": 10,
    "label": "optimization problem",
    "title": "An optimization problem is a mathematical structure consisting of an defined over an optimization variable , together with a feasible set . The co-domain is assumed to be ordered, meaning that for any two elements , we can determine whether , , or . The goal of optimization is to find those values for which the objective is extremal\u2014i.e., minimal or maximal , , . \\\\ See also: .",
    "color": "orange"
  },
  {
    "id": 11,
    "label": "optimization method",
    "title": "An optimization method is an that reads in a representation of an and delivers an (approximate) solution as its output , , . \\\\ See also: , .",
    "color": "lightblue"
  },
  {
    "id": 12,
    "label": "fixed-point iteration",
    "title": "A fixed-point iteration is an iterative method for solving a given . It constructs a sequence by repeatedly applying an operator , i.e., {equation} {equ_def_fixed_point_dict} ^{(+1)} = ^{()} {, for } =0,1,. {equation} The operator is chosen such that any of its fixed points is a solution to the given . For example, given a and , the fixed points of the operator coincide with the minimizers of . In general, for a given with solution , there are many different operators whose fixed points are . Clearly, we should use an operator in {equ_def_fixed_point_dict} that reduces the distance to a solution such that {equation} {{ ^{(+1)} - {}}{2}}_{{{equ_def_fixed_point_dict}}{=} { ^{()} - {}}{2}} { ^{()} - {}}{2}. {equation} Thus, we require to be at least non-expansive, i.e., the iteration {equ_def_fixed_point_dict} should not result in worse that have a larger distance to a solution . What is more, each iteration {equ_def_fixed_point_dict} should also make some progress, i.e., reduce the distance to a solution . This requirement can be made precise using the notion of a , . The operator is a if, for some , {equation} { \\!-\\! '}{2} {\\!-\\!'}{2} { holds for any } ,' {R}^{ }. {equation} For a , the fixed-point iteration {equ_def_fixed_point_dict} generates a sequence that converges quite rapidly. In particular {RudinBookPrinciplesMatheAnalysis}, {equation} { ^{()} - {}}{2} ^{} { ^{(0)} - {}}{2}. {equation} Here, is the distance between the initialization and the solution . It turns out that a fixed-point iteration {equ_def_fixed_point_dict} with a firmly non-expansive operator is guaranteed to converge to a fixed-point of {Bauschke:2017}. Fig. {fig_examples_nonexp_dict} depicts examples of a firmly non-expansive operator, a non-expansive operator, and a . All these operators are defined on the one-dimensional space . Another example of a firmly non-expansive operator is the of a , . {darkgreen}{rgb}{0.0, 0.5, 0.0} {figure}[H] {center} {tikzpicture}[scale=1.5] (-2,0) -- (2,0) node[right] {}; (0,-2) -- (0,2) node[above] {}; at (2.1,2.2) {}; at (1.9,-1.5) {}; at (1.5,1.2) {}; (1,-2) -- (1,2); (-2,1) -- (2,1); (-2,-1) -- (2,-1); (-1,-2) -- (-1,2); at (1,0) {}; at (0,-1) {}; plot(,{0.5* + 1}); plot(,{-}); plot(,{-1}); plot(,{}); plot(,{1}); {tikzpicture} {center} {Example of a non-expansive operator , a firmly non-expansive operator , and a contractive operator . {fig_examples_nonexp_dict}} {figure} See also: , , , , , .",
    "color": "salmon"
  },
  {
    "id": 13,
    "label": "Erd\\H{o}s-R\\'enyi graph (ER graph)",
    "title": "An ER is a for {graph} defined over a given node set . One way to define the ER is via the collection of binary {rv} , for each pair of different nodes . A specific of an ER contains an edge if and only if . The ER is parametrized by the number of nodes and the . \\\\ See also: , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 14,
    "label": "attack",
    "title": "An attack on an system refers to an intentional action\u2014either active or passive\u2014that compromises the system's integrity, availability, or confidentiality. Active attacks involve perturbing components such as {dataset} (via ) or communication links between {device} in a setting. Passive attacks, such as {privattack}, aim to infer {sensattr} without modifying the system. Depending on their goal, we distinguish between {dosattack}, attacks, and {privattack}. \\\\ See also: , , , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 15,
    "label": "privacy attack",
    "title": "A privacy on an system aims to infer {sensattr} of individuals by exploiting partial access to a trained . One form of a privacy is .\\\\ See also: , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 16,
    "label": "epigraph",
    "title": "The epigraph of a real-valued is the set of points lying on or above its : \\[ {epi}(f) = \\{ ({x}, t) {R}^n {R} \\,|\\, f({x}) t \\}. \\] A is if and only if its epigraph is a set , . {figure}[H] {tikzpicture}[scale=1.0] {axis}[ axis lines = middle, xlabel = , ylabel = {}, xmin=-2, xmax=2, ymin=0, ymax=4.5, samples=100, domain=-1.5:1.5, thick, width=8cm, height=6cm, grid=none, axis on top, ] [blue, thick, domain=-1.5:1.5] {x^2} node [pos=0.85, anchor=south west, xshift=5pt] {}; [ name path=f, draw=none, ytick=, domain=-1.5:1.5, ] {x^2}; (axis cs:-1.5,4) -- (axis cs:1.5,4); [ blue!20, opacity=0.6, draw=none, ] fill between [ of=f and top, soft clip={domain=-1.5:1.5}, ]; at (axis cs:-1.0,2.3) {}; {axis} {tikzpicture} {Epigraph of the (i.e., shaded area).} {figure} See also: , , .",
    "color": "orange"
  },
  {
    "id": 17,
    "label": "maximum",
    "title": "The maximum of a set of real numbers is the greatest element in that set, if such an element exists. A set has a maximum if it is bounded above and attains its {RudinBookPrinciplesMatheAnalysis}. \\\\ See also: .",
    "color": "lightblue"
  },
  {
    "id": 18,
    "label": "supremum",
    "title": "The supremum of a set of real numbers is the smallest number that is greater than or equal to every element in the set. More formally, a real number is the supremum of a set if: 1) is an upper bound of ; and 2) no number smaller than is an upper bound of . Every non-empty set of real numbers that is bounded above has a supremum, even if it does not contain its supremum as an element {RudinBookPrinciplesMatheAnalysis}.",
    "color": "lightblue"
  },
  {
    "id": 19,
    "label": "discrepancy",
    "title": "Consider an application with represented by an . methods use a discrepancy measure to compare {map} from {localmodel} at nodes connected by an edge in the . \\\\ See also: , , , , , .",
    "color": "orange"
  },
  {
    "id": 20,
    "label": "FedRelax",
    "title": "An . \\\\ See also: , .",
    "color": "orange"
  },
  {
    "id": 21,
    "label": "FedAvg",
    "title": "FedAvg refers to a family of iterative {algorithm}. It uses a server-client setting and alternates between client-wise {localmodel} re-training, followed by the aggregation of updated at the server . The local update at client at time starts from the current provided by the server and typically amounts to executing few iterations of . After completing the local updates, they are aggregated by the server (e.g., by averaging them). Fig.\\ {fig_single_iteration_fedavg} illustrates the execution of a single iteration of FedAvg. {figure}[H] {center} {tikzpicture}[>=Stealth, node distance=1cm and 1.5cm, every node/.style={font=}] {server} = [circle, fill=black, minimum size=6pt, inner sep=0pt] {client} = [circle, draw=black, minimum size=6pt, inner sep=0pt] (label1) at (0,3.5) {broadcast}; (label2) {local update}; (label3) {aggregate}; (s1) at (label1 |- 0,2.5) {}; (c1l) at () {}; (c1r) at () {}; (dots1) at () {}; (s1) -- (c1l) node[midway,left] {}; (s1) -- (c1r) node[midway,right] {}; (s1) -- (dots1); (s2) at (label2 |- 0,2.5) {}; (c2l) at () {}; (c2r) at () {}; (dots2) at () {}; {}; {}; (s3) at (label3 |- 0,2.5) {}; {}; (c3l) at () {}; (c3r) at () {}; (dots3) at () {}; (c3l) -- (s3) node[midway,left] {}; (c3r) -- (s3) node[midway,right] {}; (dots3) -- (s3); {tikzpicture} {center} {Illustration of a single iteration of FedAvg which consists of broadcasting by the server, local updates at clients, and their aggregation by the server. {fig_single_iteration_fedavg}} {figure} See also: , , , , .",
    "color": "lightblue"
  },
  {
    "id": 22,
    "label": "FedGD",
    "title": "An that can be implemented as message passing across an . \\\\ See also: , , , , .",
    "color": "orange"
  },
  {
    "id": 23,
    "label": "FedSGD",
    "title": "An that can be implemented as message passing across an . \\\\ See also: , , , , , .",
    "color": "orange"
  },
  {
    "id": 24,
    "label": "dimensionality reduction",
    "title": "Dimensionality reduction refers to methods that learn a transformation of a (typically large) set of raw {feature} into a smaller set of informative {feature} . Using a smaller set of {feature} is beneficial in several ways: {itemize} {Statistical benefit:} It typically reduces the risk of , as reducing the number of {feature} often reduces the of a . {Computational benefit:} Using fewer {feature} means less computation for the training of {model}. As a case in point, methods need to invert a matrix whose size is determined by the number of {feature}. {Visualization:} Dimensionality reduction is also instrumental for visualization. For example, we can learn a transformation that delivers two {feature} which we can use, in turn, as the coordinates of a . Fig.\\ {fig:dimred-scatter} depicts the of hand-written digits that are placed according transformed {feature}. Here, the {datapoint} are naturally represented by a large number of grayscale values (one value for each pixel). {itemize} {figure}[H] {tikzpicture}[scale=1] (-0.5,0) -- (5.5,0) node[right] {}; (0,-0.5) -- (0,4.5) node[above] {}; // in { 1.2/0.5/3, 0.8/2.0/8, 2.5/1.8/1, 3.8/3.5/6, 4.2/0.7/9, 2.8/3.0/7, 1.5/3.8/2 }{ at (,) {}; } {tikzpicture} {Example of dimensionality reduction: High-dimensional image data (e.g., high-resolution images of hand-written digits) embedded into 2D using learned {feature} and visualized in a .} {fig:dimred-scatter} {figure} See also: , , , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 25,
    "label": "machine learning (ML)",
    "title": "ML aims to predict a from the {feature} of a . ML methods achieve this by learning a from a (or ) through the minimization of a , . One precise formulation of this principle is . Different ML methods are obtained from different design choices for {datapoint} (i.e., their {feature} and ), the , and the {MLBasics}. \\\\ See also: , , , , , , , .",
    "color": "violet"
  },
  {
    "id": 26,
    "label": "feature learning",
    "title": "Consider an application with {datapoint} characterized by raw {feature} . learning refers to the task of learning a : ': ', that reads in raw {feature} of a and delivers new {feature} from a new . Different learning methods are obtained for different design choices of , for a of potential {map} , and for a quantitative measure of the usefulness of a specific . For example, uses , with , and a \\{ : {R}^{} \\!\\! {R}^{'}\\!:\\!'\\!\\! { with some } \\!\\! {R}^{' } \\}. measures the usefulness of a specific by the linear reconstruction error incurred on a such that _{ {R}^{ '}} _{=1}^{} { ^{()} - ^{()}}{2}^{2}. \\\\ See also: , , , , , , , , .",
    "color": "violet"
  },
  {
    "id": 27,
    "label": "autoencoder",
    "title": "An autoencoder is an method that simultaneously learns an encoder and a decoder . It is an instance of using a computed from the reconstruction error . \\\\ See also: , , , .",
    "color": "violet"
  },
  {
    "id": 28,
    "label": "vertical federated learning (VFL)",
    "title": "VFL refers to applications where {device} have access to different {feature} of the same set of {datapoint} . Formally, the underlying global is \\[ ^{({global})} \\{ (^{(1)}, ^{(1)}), , (^{()}, ^{()}) \\}. \\] We denote by , for , the complete {featurevec} for the {datapoint}. Each observes only a subset of {feature}, resulting in a with {featurevec} \\[ ^{(,)} = ( ^{()}_{_{1}}, , ^{()}_{_{}} )^{T}. \\] Some of the {device} might also have access to the {label} , for , of the global . One potential application of VFL is to enable collaboration between different healthcare providers. Each provider collects distinct types of measurements\u2014such as blood values, electrocardiography, and lung X-rays\u2014for the same patients. Another application is a national social insurance system, where health records, financial indicators, consumer behavior, and mobility are collected by different institutions. VFL enables joint learning across these parties while allowing well-defined levels of . {figure}[H] {center} {tikzpicture}[every node/.style={anchor=base}] {0} {1.6} {3.2} {4.8} {6.4} {0} {-1.2} {-2.4} {-3.6} / in {1/1, 2/2, 4/} { {}{-1.2*(-1)} (x1) at (0,) {}; (x2) at (1.6,) {}; (dots) at (3.2,) {}; (x3) at (4.8,) {}; (y) at (6.4,) {}; } (-0.6,0.6) rectangle (6.9,-4.2); at (3.1,0.9) {}; (-0.9,0.9) rectangle (2.1,-4.0); at (0.25,1.0) {}; () rectangle (); at () {}; {tikzpicture} {center} {VFL uses {localdataset} that are derived from the {datapoint} of a common global . The {localdataset} differ in the choice of {feature} used to characterize the {datapoint}.{fig_vertical_FL}} {figure} See also: , , , , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 29,
    "label": "explainability",
    "title": "Consider a trained (or learned ) , which maps the of a to the . LIME is a technique for explaining the behavior of , locally around a with . The is given in the form of a local approximation of (see Fig. {fig_lime}). This approximation can be obtained by an instance of with a carefully designed . In particular, the consists of {datapoint} with close to and the (pseudo-) . Note that we can use a different for the approximation from the original . For example, we can use a to approximate (locally) a . Another widely-used choice for is the . {figure}[H] {center} {tikzpicture} {axis}[ axis lines=middle, xlabel={}, ylabel={}, xtick=, ytick=, xmin=0, xmax=6, ymin=0, ymax=6, domain=0:6, samples=100, width=10cm, height=6cm, clip=false ] {2 + sin(deg(x))} node[pos=0.85, above right,yshift=3pt] {}; coordinates {(3,0) (3,6)}; {2 + sin(deg(3))} node[pos=0.9, above] {}; coordinates {(3, {2 + sin(deg(3))})}; at (axis cs:3,-0.3) {}; {axis} {tikzpicture} {center} {To explain a trained , around a given , we can use a local approximation . } {fig_lime} {figure} See also: , , , , , , , , , , , .",
    "color": "khaki"
  },
  {
    "id": 30,
    "label": "linear model",
    "title": "Consider {datapoint}, each characterized by a numeric . A linear is a which consists of all {linearmap} such that {equation} {equ_def_lin_model_hypspace_dict} {} \\{ ()= ^{T} : {R}^{} \\}. {equation} Note that {equ_def_lin_model_hypspace_dict} defines an entire family of {hypospace}, which is parametrized by the number of {feature} that are linearly combined to form the . The design choice of is guided by (e.g., reducing means less computation), (e.g., increasing might reduce error), and . A linear using few carefully chosen {feature} tends to be considered more interpretable , . \\\\ See also: , , , , , , , , , .",
    "color": "khaki"
  },
  {
    "id": 31,
    "label": "gradient step",
    "title": "Given a real-valued and a vector , the step updates by adding the scaled negative to obtain the new vector (see Fig. {fig_basic_GD_step_single_dict}) {equation} {equ_def_gd_basic_dict} {} - f(). {equation} Mathematically, the step is an operator that is parametrized by the and the . {figure}[H] {center} {tikzpicture}[scale=0.8] (-4,0) grid (4,4); plot (, {(1/4)*}); plot (, {2* - 4}); (4,4) -- node[right] {} (4,2); (4,4) -- node[above] {} (2,4); (4,2) -- node[below] {} (3,2) ; at (-4.1, 4.1) {}; (0pt,2pt) -- (0pt,-2pt) node[below] {}; (0pt,2pt) -- (0pt,-2pt) node[below] {}; (0pt,2pt) -- (0pt,-2pt) node[below] {}; {tikzpicture} {center} {The basic step {equ_def_gd_basic_dict} maps a given vector to the updated vector . It defines an operator .} {fig_basic_GD_step_single_dict} {figure} Note that the step {equ_def_gd_basic_dict} optimizes locally - in a whose size is determined by the - a linear approximation to the . A natural of {equ_def_gd_basic_dict} is to locally optimize the itself - instead of its linear approximation - such that {align} {equ_approx_gd_step_dict} {} = _{' {R}^{}} f(')\\!+\\!(1/){-'}{2}^2. {align} We intentionally use the same symbol for the in {equ_approx_gd_step_dict} as we used for the in {equ_def_gd_basic_dict}. The larger the we choose in {equ_approx_gd_step_dict}, the more progress the update will make towards reducing the value . Note that, much like the step {equ_def_gd_basic_dict}, also the update {equ_approx_gd_step_dict} defines an operator that is parametrized by the and the . For a , this operator is known as the of . \\\\ See also: , , , , , , , , .",
    "color": "salmon"
  },
  {
    "id": 32,
    "label": "contraction operator",
    "title": "An operator is a contraction if, for some , {equation} { \\!-\\! '}{2} {\\!-\\!'}{2} { holds for any } ,' {R}^{ }. {equation}",
    "color": "salmon"
  },
  {
    "id": 33,
    "label": "proximal operator",
    "title": "Given a , we define its proximal operator as , {f()}{}{} _{' {R}^{}} { with } > 0. As illustrated in Fig. {fig_proxoperator_opt_dict}, evaluating the proximal operator amounts to minimizing a penalized variant of . The penalty term is the scaled squared Euclidean distance to a given vector (which is the input to the proximal operator). The proximal operator can be interpreted as a of the , which is defined for a . Indeed, taking a with at the current vector is the same as applying the proximal operator of the and using . {figure}[H] {center} {tikzpicture}[scale=0.8] plot (, {(1/4)*}) node[above right] {}; plot (, {2*( - 2)*( - 2)}) node[below right] {}; (0pt,2pt) -- (0pt,-2pt) node[below] {}; {tikzpicture} {center} {A generalized updates a vector by minimizing a penalized version of the . The penalty term is the scaled squared Euclidean distance between the optimization variable and the given vector . {fig_proxoperator_opt_dict}} {figure} See also: , , , , , .",
    "color": "salmon"
  },
  {
    "id": 34,
    "label": "proximable",
    "title": "A for which the can be computed efficiently is sometimes referred to as proximable or simple . \\\\ See also: , , .",
    "color": "salmon"
  },
  {
    "id": 35,
    "label": "multivariate normal distribution",
    "title": "An undirected is connected if every non-empty subset has at least one edge connecting it to . \\\\ See also: .",
    "color": "orange"
  },
  {
    "id": 36,
    "label": "standard normal vector",
    "title": "A standard normal vector is a random vector whose entries are {gaussrv} . It is a special case of a , . \\\\ See also: , , , .",
    "color": "lightblue"
  },
  {
    "id": 37,
    "label": "statistical aspects",
    "title": "By statistical aspects of an method, we refer to (properties of) the of its output under a for the fed into the method. \\\\ See also: , , , .",
    "color": "lightblue"
  },
  {
    "id": 38,
    "label": "underfitting",
    "title": "Consider an method that uses to learn a with the on a given . Such a method is underfitting the if it is not able to learn a with a sufficiently small on the . If a method is underfitting, it will typically also not be able to learn a with a small . \\\\ See also: , , , , , , .",
    "color": "salmon"
  },
  {
    "id": 39,
    "label": "Gaussian random variable (Gaussian RV)",
    "title": "A standard Gaussian is a real-valued with , , {equation} p(x) = {1}{{2}} ^{-x^2/2}. {equation} Given a standard Gaussian , we can construct a general Gaussian with and via . The of a Gaussian is referred to as normal distribution, denoted . \\\\ A Gaussian random vector with and can be constructed as , , \\[ {A} + { }, \\] where is a vector of standard Gaussian {rv}, and is any matrix satisfying . The of a Gaussian random vector is referred to as the , denoted . \\\\ Gaussian random vectors arise as finite-dimensional marginals of {GaussProc}, which define consistent joint Gaussian distributions over arbitrary (potentially infinite) index sets . \\\\ Gaussian {rv} are widely used {probmodel} in the statistical analysis of methods. Their significance arises partly from the , which is a mathematically precise formulation of the following rule-of-thumb: the average of a large number of independent {rv} (not necessarily Gaussian themselves) tends towards a Gaussian . \\\\ Compared to other {probdist}, the is also distinct in that\u2014in a mathematically precise sense\u2014represents maximum uncertainty. Among all continuous random vectors with a given covariance matrix , the Gaussian random vector maximizes differential {coverthomas}. This makes Gaussian distributions a natural choice for capturing uncertainty (or lack of knowledge) in the absence of additional structural information. \\\\ See also: , , , , , , , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 40,
    "label": "Gaussian process (GP)",
    "title": "A GP is a collection of {rv} indexed by input values from some input space , such that, for any finite subset , the corresponding {rv} have a joint multivariate Gaussian distribution: \\[ ( f(^{(1)}, , ^{()} ) {N}({}, {K}). \\] For a fixed input space , a GP is fully specified (or parametrized) by {itemize} a and a covariance . {itemize} {Example:} We can interpret the temperature distribution across Finland (at a specific point in time) as the of a GP , where each input denotes a geographic location. Temperature observations from weather stations provide {sample} of at specific locations (see Fig.\\ {fig_gp_FMI}). A GP allows us to predict the temperature nearby weather stations and to quantify the of these predictions. {figure}[H] {center} {tikzpicture} {axis}[ axis equal, hide axis, scale=1.2, xmin=17, xmax=32, ymin=55, ymax=71, clip=true ] table [x=lon, y=lat, col sep=comma] {assets/finland_border.csv}; table [x=lon, y=lat, col sep=comma] {assets/fmi_stations_subset.csv}; (axis cs:19,59) -- (axis cs:25.5,59) node[anchor=west] {lon}; (axis cs:19,59) -- (axis cs:19,65.5) node[anchor=south] {lat}; {axis} {tikzpicture} {-15mm} {center} {We can interpret the temperature distribution over Finland as a of a GP indexed by geographic coordinates and sampled at weather stations (indicated by blue dots). {fig_gp_FMI}} {figure} See also: , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 41,
    "label": "projection",
    "title": "Consider a subset of the -dimensional . We define the projection of a vector onto as {equation} {equ_def_proj_generic_dict} {}{} = _{' } { - '}{2}. {equation} In other words, is the vector in which is closest to . The projection is only well-defined for subsets for which the above exists . \\\\ See also: , .",
    "color": "lightcoral"
  },
  {
    "id": 42,
    "label": "projected gradient descent (projected GD)",
    "title": "Consider an -based method that uses a parametrized with . Even if the of is , we cannot use basic , as it does not take into account contraints on the optimization variable (i.e., the ). Projected extends basic to handle constraints on the optimization variable (i.e., the ). A single iteration of projected consists of first taking a and then projecting the result back onto the . {figure}[H] {center} {tikzpicture}[scale=0.9] [right] at (-5.1,1.7) {} ; plot (, {(1/8)*}); [fill] (2.83,1) circle [radius=0.1] node[right] {}; (2.83,1) -- node[midway,above] {grad. step} (-1.5,1); (-1.5,1) --(-1.5,-1.5) node [below, left]{} ; (-1.5,-1.5) -- node[midway,above] {} (1,-1.5) ; [fill] (1,-1.5) circle [radius=0.1] node[below] {}; (1,-1.5) -- (3,-1.5) node[midway, above] {}; {tikzpicture} {-5mm} {center} {Projected augments a basic with a back onto the constraint set .} {fig_projected_GD_dict} {figure} See also: , , , , , , , , .",
    "color": "lightcoral"
  },
  {
    "id": 43,
    "label": "differential privacy (DP)",
    "title": "Consider some method that reads in a (e.g., the used for ) and delivers some output . The output could be either the learned or the {prediction} for specific {datapoint}. DP is a precise measure of incurred by revealing the output. Roughly speaking, an method is differentially private if the of the output does not change too much if the of one in the is changed. Note that DP builds on a for an method, i.e., we interpret its output as the of an . The randomness in the output can be ensured by intentionally adding the of an auxiliary (i.e., adding noise) to the output of the method. \\\\ See also: , , , , , , , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 44,
    "label": "robustness",
    "title": "Robustness is a key requirement for . It refers to the property of an system to maintain acceptable performance even when subjected to different forms of perturbations. These perturbations can be to the {feature} of a in order to manipulate the delivered by a trained . Robustness also includes the of -based methods against perturbations of the . Such perturbations can occur within {attack}. \\\\ See also: , , , , , , , , , , .",
    "color": "violet"
  },
  {
    "id": 45,
    "label": "stability",
    "title": "Stability is a desirable property of an method that maps a (e.g., a ) to an output . The output can be the learned or the delivered by the trained for a specific . Intuitively, is stable if small changes in the input lead to small changes in the output . Several formal notions of stability exist that enable bounds on the error or of the method (see {ShalevMLBook}). To build intuition, consider the three {dataset} depicted in Fig.~{fig_three_data_stability}, each of which is equally likely under the same -generating . Since the optimal are determined by this underlying , an accurate method should return the same (or very similar) output for all three {dataset}. In other words, any useful must be robust to variability in {realization} from the same , i.e., it must be stable. {figure}[H] {tikzpicture} {axis}[ axis lines=none, xlabel={}, ylabel={}, legend pos=north west, ymin=0, ymax=10, xtick={1,2,3,4,5}, grid style=dashed, every axis plot/.append style={very thick} ] +[only marks,mark=*] coordinates { (1,2) (2,4) (3,3) (4,5) (5,7) }; +[only marks,mark=square*] coordinates { (1,3) (2,2) (3,6) (4,4) (5,5) }; +[only marks,mark=triangle*] coordinates { (1,5) (2,7) (3,4) (4,6) (5,3) }; {axis} {tikzpicture} {Three {dataset} , , and , each sampled independently from the same -generating . A stable method should return similar outputs when trained on any of these {dataset}. {fig_three_data_stability}} {figure} See also: , , , , , , , , , , , , .",
    "color": "violet"
  },
  {
    "id": 46,
    "label": "privacy protection",
    "title": "Consider some method that reads in a and delivers some output . The output could be the learned or the obtained for a specific with {feature} . Many important applications involve {datapoint} representing humans. Each is characterized by {feature} , potentially a , and a (e.g., a recent medical diagnosis). Roughly speaking, privacy protection means that it should be impossible to infer, from the output , any of the {sensattr} of {datapoint} in . Mathematically, privacy protection requires non-invertibility of the . In general, just making non-invertible is typically insufficient for privacy protection. We need to make sufficiently non-invertible. \\\\ See also: , , , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 47,
    "label": "privacy leakage",
    "title": "Consider an application that processes a and delivers some output, such as the {prediction} obtained for new {datapoint}. Privacy leakage arises if the output carries information about a private (or sensitive) of a (which might be a human) of . Based on a for the generation, we can measure the privacy leakage via the between the output and the sensitive . Another quantitative measure of privacy leakage is . The relations between different measures of privacy leakage have been studied in the literature (see ). \\\\ See also: , , , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 48,
    "label": "probabilistic model",
    "title": "A probabilistic interprets {datapoint} as {realization} of {rv} with a joint . This joint typically involves {parameter} which have to be manually chosen or learned via statistical inference methods such as estimation . \\\\ See also: , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 49,
    "label": "mean",
    "title": "The mean of an , taking values in an , is its . It is defined as the Lebesgue integral of with respect to the underlying (e.g., see or ), i.e., \\[ \\{\\} = _{{R}^{}} \\, {d}P(). \\] We also use the term to refer to the average of a finite sequence . However, these two definitions are essentially the same. Indeed, we can use the sequence to construct a discrete , with the index being chosen uniformly at random from the set . The mean of is precisely the average . \\\\ See also: , , , .",
    "color": "lightblue"
  },
  {
    "id": 50,
    "label": "variance",
    "title": "The variance of a real-valued is defined as the of the squared difference between and its . We extend this definition to vector-valued {rv} as . \\\\ See also: , .",
    "color": "lightblue"
  },
  {
    "id": 51,
    "label": "nearest neighbor (NN)",
    "title": "NN methods learn a whose value is solely determined by the NNs within a given . Different methods use different metrics for determining the NNs. If {datapoint} are characterized by numeric {featurevec}, we can use their Euclidean distances as the metric. \\\\ See also: , , , , , .",
    "color": "khaki"
  },
  {
    "id": 52,
    "label": "neighborhood",
    "title": "The neighborhood of a node is the subset of nodes constituted by the of . \\\\ See also: .",
    "color": "orange"
  },
  {
    "id": 53,
    "label": "neighbors",
    "title": "The neighbors of a node within an are those nodes that are connected (via an edge) to node . \\\\ See also: .",
    "color": "orange"
  },
  {
    "id": 54,
    "label": "bias",
    "title": "Consider an method using a parametrized . It learns the using the =\\{ {^{()}}{^{()}} \\}_{=1}^{}. To analyze the properties of the method, we typically interpret the {datapoint} as {realization} of {rv}, ^{()} = ^{({})}( ^{()} ) + {}^{()}, =1,,. We can then interpret the method as an estimator computed from (e.g., by solving ). The (squared) bias incurred by the estimate is then defined as . \\\\ See also: , , , , , , , , .",
    "color": "violet"
  },
  {
    "id": 55,
    "label": "classification",
    "title": "Classification is the task of determining a discrete-valued for a given , based solely on its {feature} . The belongs to a finite set, such as or , and represents the category to which the corresponding belongs. \\\\ See also: , , .",
    "color": "lightgreen"
  },
  {
    "id": 56,
    "label": "privacy funnel",
    "title": "The privacy funnel is a method for learning privacy-friendly {feature} of {datapoint} . \\\\ See also: , .",
    "color": "lightgreen"
  },
  {
    "id": 57,
    "label": "condition number",
    "title": "The condition number of a positive definite matrix is the ratio between the largest and the smallest of . The condition number is useful for the analysis of methods. The computational complexity of for crucially depends on the condition number of the matrix , with the of the . Thus, from a computational perspective, we prefer {feature} of {datapoint} such that has a condition number close to . \\\\ See also: , , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 58,
    "label": "classifier",
    "title": "A classifier is a (i.e., a ) used to predict a taking values from a finite . We might use the value itself as a for the . However, it is customary to use a that delivers a numeric quantity. The is then obtained by a simple thresholding step. For example, in a binary problem with {labelspace} , we might use a real-valued as a classifier. A can then be obtained via thresholding, {equation} {equ_def_threshold_bin_classifier_dict} =1 { for } ()\\!\\!0 { and } =-1 { otherwise.} {equation} We can characterize a classifier by its {decisionregion} , for every possible value . \\\\ See also: , , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 59,
    "label": "empirical risk",
    "title": "The empirical of a on a is the average incurred by when applied to the {datapoint} in . \\\\ See also: , , , , .",
    "color": "salmon"
  },
  {
    "id": 60,
    "label": "node degree",
    "title": "The degree of a node in an undirected is the number of its , i.e., . \\\\ See also: , .",
    "color": "orange"
  },
  {
    "id": 61,
    "label": "graph",
    "title": "A graph is a pair that consists of a node set and an edge set . In its most general form, a graph is specified by a that assigns each edge a pair of nodes . One important family of graphs is simple undirected graphs. A simple undirected graph is obtained by identifying each edge with two different nodes . Weighted graphs also specify numeric for each edge . \\\\ See also: , .",
    "color": "orange"
  },
  {
    "id": 62,
    "label": "uncertainty",
    "title": "Uncertainty refers to the degree of confidence\u2014or lack thereof\u2014associated with a quantity such as a , estimate, or observed . In , uncertainty arises from various sources, including noisy , limited training {sample}, or ambiguity in assumptions. theory offers a principled framework for representing and quantifying such uncertainty. \\\\ See also: , , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 63,
    "label": "upper confidence bound (UCB)",
    "title": "Consider an application that requires selecting, at each time step , an action from a finite set of alternatives . The utility of selecting action is quantified by a numeric signal . A widely used for this type of sequential decision-making problem is the setting . In this , the is viewed as the of an with unknown . Ideally, we would always choose the action with the largest expected , but these {mean} are unknown and must be estimated from observed . Simply choosing the action with the largest estimate can lead to suboptimal outcomes due to estimation . The UCB strategy addresses this by selecting actions not only based on their estimated {mean} but also by incorporating a term that reflects the in these estimates\u2014favoring actions with a high potential and high . Theoretical guarantees for the performance of UCB strategies, including logarithmic bounds, are established in . \\\\ See also: , , , , , , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 64,
    "label": "multi-armed bandit (MAB)",
    "title": "A MAB problem models a repeated decision-making scenario in which, at each time step , a learner must choose one out of several possible actions, often referred to as arms, from a finite set . Each arm yields a drawn from an unknown with . The learner\u2019s goal is to maximize the cumulative over time by strategically balancing exploration (i.e., gathering information about uncertain arms) and exploitation (i.e., selecting arms known to perform well). This balance is quantified by the notion of , which measures the performance gap between the learner's strategy and the optimal strategy that always selects the best arm. MAB problems form a foundational in , reinforcement learning, and sequential experimental design . \\\\ See also: , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 65,
    "label": "optimism in the face of uncertainty",
    "title": "methods learn according to some performance criterion . However, they usually cannot access directly but rely on an estimate (or approximation) of . As a case in point, -based methods use the average on a given (i.e., the ) as an estimate for the of a . Using a , one can construct a confidence interval for each choice for the . One simple construction is , , with being a measure of the (expected) deviation of from . We can also use other constructions for this interval as long as they ensure that with a sufficiently high . An optimist chooses the according to the most favorable\u2014yet still plausible\u2014value of the performance criterion. Two examples of methods that use such an optimistic construction of an are {ShalevMLBook} and methods for sequential decision making {Bubeck2012}. {figure}[H] {center} {tikzpicture}[x=3cm, y=1cm] (-1, 5) -- plot[domain=-2:1, samples=100] ({+1}, { + 1}) -- plot[domain=1:-2, samples=100] ({+1}, { - 0.5}) -- cycle; at (2, 4) {}; plot ({+1}, { -0.5}) node[right] {}; plot ({}, {}); (1, -0.5) -- (1, 1) node[midway, right] {}; {tikzpicture} { methods learn by using some estimate of for the ultimate performance criterion . Using a , one can use to construct confidence intervals which contain with a high probability. The best plausible performance measure for a specific choice of is .} {center} {figure} See also: , , , , , , , , , , , , .",
    "color": "violet"
  },
  {
    "id": 66,
    "label": "federated learning network (FL network)",
    "title": "An network is an undirected weighted whose nodes represent generators that aim to train a local (or personalized) . Each node in an network represents some capable of collecting a and, in turn, train a . methods learn a local , for each node , such that it incurs small on the {localdataset}. \\\\ See also: , , , , , , , , .",
    "color": "orange"
  },
  {
    "id": 67,
    "label": "norm",
    "title": "A norm is a that maps each (vector) element of a to a non-negative real number. This must be homogeneous and definite, and it must satisfy the triangle inequality . \\\\ See also: , .",
    "color": "khaki"
  },
  {
    "id": 68,
    "label": "dual norm",
    "title": "Every defined on an has an associated dual , which is denoted and defined as . The dual measures the largest possible inner product between and any vector in the unit ball of the original . For further details, see {BoydConvexBook}. \\\\ See also: , .",
    "color": "khaki"
  },
  {
    "id": 69,
    "label": "explanation",
    "title": "One approach to make methods transparent is to provide an explanation along with the delivered by an method. Explanations can take on many different forms. An explanation could be some natural text or some quantitative measure for the importance of individual {feature} of a . We can also use visual forms of explanations, such as intensity plots for image . \\\\ See also: , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 70,
    "label": "risk",
    "title": "Consider a used to predict the of a based on its {feature} . We measure the quality of a particular using a . If we interpret {datapoint} as the {realization} of {rv}, also the becomes the of an . The allows us to define the risk of a as the expected . Note that the risk of depends on both the specific choice for the and the of the {datapoint}. \\\\ See also: , , , , , , , , , , .",
    "color": "salmon"
  },
  {
    "id": 71,
    "label": "activation function",
    "title": "Each artificial neuron within an is assigned an activation that maps a weighted combination of the neuron inputs to a single output value . Note that each neuron is parametrized by the . \\\\ See also: , , .",
    "color": "salmon"
  },
  {
    "id": 72,
    "label": "distributed algorithm",
    "title": "A distributed is an designed for a special type of computer, i.e., a collection of interconnected computing devices (or nodes). These devices communicate and coordinate their local computations by exchanging messages over a network , . Unlike a classical , which is implemented on a single , a distributed is executed concurrently on multiple {device} with computational capabilities. Similar to a classical , a distributed can be modeled as a set of potential executions. However, each execution in the distributed setting involves both local computations and message-passing events. A generic execution might look as follows: \\[ {array}{l} {Node 1: } { input}_1, s_1^{(1)}, s_2^{(1)}, , s_{T_1}^{(1)}, { output}_1; \\\\ {Node 2: } { input}_2, s_1^{(2)}, s_2^{(2)}, , s_{T_2}^{(2)}, { output}_2; \\\\ \\\\ {Node N: } { input}_N, s_1^{(N)}, s_2^{(N)}, , s_{T_N}^{(N)}, { output}_N. {array} \\] Each starts from its own local input and performs a sequence of intermediate computations at discrete time instants . These computations may depend on both the previous local computations at the and the messages received from other {device}. One important application of distributed {algorithm} is in where a network of {device} collaboratively trains a personal for each . \\\\ See also: , , , .",
    "color": "orange"
  },
  {
    "id": 73,
    "label": "algorithm",
    "title": "An algorithm is a precise, step-by-step specification for how to produce an output from a given input within a finite number of computational steps . For example, an algorithm for training a explicitly describes how to transform a given into through a sequence of {gradstep}. To study algorithms rigorously, we can represent (or approximate) them by different mathematical structures . One approach is to represent an algorithm as a collection of possible executions. Each individual execution is a sequence of the form { input},s_1,s_2,,s_T,{ output}. This sequence starts from an input and progresses via intermediate steps until an output is delivered. Crucially, an algorithm encompasses more than just a mapping from input to output; it also includes intermediate computational steps . \\\\ See also: , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 74,
    "label": "stochastic algorithm",
    "title": "A uses a random mechanism during its execution. For example, uses a randomly selected subset of {datapoint} to compute an approximation for the of an . We can represent a by a process, i.e., The possible execution sequence is the possible outcomes of a random experiment , , . \\\\ See also: , , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 75,
    "label": "online learning",
    "title": "Some methods are designed to process in a sequential manner, updating their as new {datapoint} become available\u2014one at a time. A typical example is time series data, such as daily and temperatures recorded by a weather station. These values form a chronological sequence of observations. In online learning, the (or its ) is refined incrementally with each newly observed , without revisiting past . \\\\ See also: , , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 76,
    "label": "online algorithm",
    "title": "An online processes input incrementally, receiving {datapoint} sequentially and making decisions or producing outputs (or decisions) immediately without having access to the entire input in advance , . Unlike an offline , which has the entire input available from the start, an online must handle about future inputs and cannot revise past decisions. Similar to an offline , we also represent an online formally as a collection of possible executions. However, the execution sequence for an online has a distinct structure: { in}_{1},s_1,{ out}_{1},{ in}_{2},s_2,{ out}_{2},,{ in}_{T},s_T,{ out}_{T}. Each execution begins from an initial state (i.e., \\({in}_{1}\\)) and proceeds through alternating computational steps, outputs (or decisions), and inputs. Specifically, at step \\(\\), the performs a computational step \\(s_{}\\), generates an output \\({out}_{}\\), and then subsequently receives the next input () \\({in}_{+1}\\). A notable example of an online in is , which incrementally updates as new {datapoint} arrive. \\\\ See also: , , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 77,
    "label": "transparency",
    "title": "Transparency is a fundamental requirement for . In the context of methods, transparency is often used interchangeably with , . However, in the broader scope of systems, transparency extends beyond and includes providing information about the system\u2019s limitations, reliability, and intended use. In medical diagnosis systems, transparency requires disclosing the confidence level for the {prediction} delivered by a trained . In credit scoring, -based lending decisions should be accompanied by explanations of contributing factors, such as income level or credit history. These explanations allow humans (e.g., a loan applicant) to understand and contest automated decisions. Some methods inherently offer transparency. For example, provides a quantitative measure of reliability through the value . {decisiontree} are another example, as they allow human-readable decision rules . Transparency also requires a clear indication when a user is engaging with an system. For example, -powered chatbots should notify users that they are interacting with an automated system rather than a human. Furthermore, transparency encompasses comprehensive documentation detailing the purpose and design choices underlying the system. For instance, datasheets and system cards help practitioners understand the intended use cases and limitations of an system . \\\\ See also: , , , , , , , , .",
    "color": "khaki"
  },
  {
    "id": 78,
    "label": "sensitive attribute",
    "title": "revolves around learning a that allows us to predict the of a from its {feature}. In some applications, we must ensure that the output delivered by an system does not allow us to infer sensitive attributes of a . Which part of a is considered a sensitive attribute is a design choice that varies across different application domains. \\\\ See also: , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 79,
    "label": "stochastic block model (SBM)",
    "title": "The SBM is a probabilistic generative for an undirected with a given set of nodes . In its most basic variant, the SBM generates a by first randomly assigning each node to a index . A pair of different nodes in the is connected by an edge with that depends solely on the {label} . The presence of edges between different pairs of nodes is statistically independent. \\\\ See also: , , , , .",
    "color": "orange"
  },
  {
    "id": 80,
    "label": "deep net",
    "title": "A deep net is an with a (relatively) large number of hidden layers. Deep learning is an umbrella term for methods that use a deep net as their . \\\\ See also: , , .",
    "color": "khaki"
  },
  {
    "id": 81,
    "label": "baseline",
    "title": "Consider some method that produces a learned (or trained ) . We evaluate the quality of a trained by computing the average on a . But how can we assess whether the resulting performance is sufficiently good? How can we determine if the trained performs close to optimal and there is little point in investing more resources (for collection or computation) to improve it? To this end, it is useful to have a reference (or baseline) level against which we can compare the performance of the trained . Such a reference value might be obtained from human performance, e.g., the misclassification rate of dermatologists who diagnose cancer from visual inspection of skin . Another source for a baseline is an existing, but for some reason unsuitable, method. For example, the existing method might be computationally too expensive for the intended application. Nevertheless, its error can still serve as a baseline. Another, somewhat more principled, approach to constructing a baseline is via a . In many cases, given a , we can precisely determine the achievable among any hypotheses (not even required to belong to the ) . This achievable (referred to as the ) is the of the for the of a , given its {feature} . Note that, for a given choice of , the (if it exists) is completely determined by the {LC}. However, computing the and presents two main challenges: {enumerate}[label=)] The is unknown and needs to be estimated. Even if is known, it can be computationally too expensive to compute the exactly . {enumerate} A widely used is the for {datapoint} characterized by numeric {feature} and {label}. Here, for the , the is given by the posterior of the , given the {feature} , . The corresponding is given by the posterior (see Fig. {fig_post_baseline_dict}). {figure}[H] {center} {tikzpicture} (-1,0) -- (7,0) node[right] {}; plot ({}, {2*exp(-0.5*((-)^2))}); (,0) -- (,2.5); at ([yshift=-5pt] ,2.5) { }; (-1,1) -- (+1,1.0); at ([yshift=2pt] ,1.2) { }; in {0.5} { at (, 0) { }; } at (0.5,-0.2) { }; {tikzpicture} {center} {If the {feature} and the of a are drawn from a , we can achieve the (under ) by using the to predict the of a with {feature} . The corresponding is given by the posterior . We can use this quantity as a baseline for the average of a trained . {fig_post_baseline_dict}} {figure} See also: , , , , , , , , , , , , , , , , , , , , .",
    "color": "lightcoral"
  },
  {
    "id": 82,
    "label": "spectrogram",
    "title": "A spectrogram represents the time-frequency distribution of the energy of a time signal . Intuitively, it quantifies the amount of signal energy present within a specific time segment and frequency interval . Formally, the spectrogram of a signal is defined as the squared magnitude of its short-time Fourier transform (STFT) . Fig. {fig:spectrogram_dict} depicts a time signal along with its spectrogram. {figure}[H] {assets/spectrogram.png} {Left: A time signal consisting of two modulated Gaussian pulses. Right: An intensity plot of the spectrogram. {fig:spectrogram_dict}} {figure} The intensity plot of its spectrogram can serve as an image of a signal. A simple recipe for audio signal is to feed this signal image into {deepnet} originally developed for image and object detection . It is worth noting that, beyond the spectrogram, several alternative representations exist for the time-frequency distribution of signal energy , . \\\\ See also: , .",
    "color": "khaki"
  },
  {
    "id": 83,
    "label": "graph clustering",
    "title": "aims at {datapoint} that are represented as the nodes of a . The edges of represent pairwise similarities between {datapoint}. Sometimes we can quantify the extent of these similarities by an , . \\\\ See also: , , , .",
    "color": "orange"
  },
  {
    "id": 84,
    "label": "spectral clustering",
    "title": "Spectral is a particular instance of , i.e., it clusters {datapoint} represented as the nodes of a . Spectral uses the {eigenvector} of the to construct {featurevec} for each node (i.e., for each ) . We can feed these {featurevec} into -based methods, such as or via . Roughly speaking, the {featurevec} of nodes belonging to a well-connected subset (or ) of nodes in are located nearby in the (see Fig. {fig_lap_mtx_specclustering_dict}). {figure}[H] {center} {minipage}{0.4} {tikzpicture} {scope}[every node/.style={circle, fill=black, inner sep=0pt, minimum size=0.3cm}] (1) at (0,0) {}; (2) [below left=of 1, xshift=-0.2cm, yshift=-1cm] {}; (3) [below right=of 1, xshift=0.2cm, yshift=-1cm] {}; (4) [below=of 1, yshift=0.5cm] {}; {scope} (1) -- (2); (1) -- (3); at (1) {}; at (2) {}; at (3) {}; at (4) {}; {tikzpicture} {minipage} {5mm} {minipage}{0.4} {equation} {}\\!=\\! {pmatrix} 2 & -1 & -1 & 0 \\\\ -1 & 1 & 0 & 0 \\\\ -1 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 0 {pmatrix}\\!=\\!{V} { } {V}^{T} {equation} {minipage} {20mm}\\\\ {minipage}{0.4} {tikzpicture}[scale=3] (-0.2, 0) -- (1.2, 0) node[right] {}; (0, -0.2) -- (0, 1.2) node[above] {}; (0.577, 0) circle (0.03cm) node[above right] {}; (0.577, 0) circle (0.03cm); (0.577, 0) circle (0.03cm); (0, 1) circle (0.03cm) node[above right] {}; {tikzpicture} {minipage} {minipage}{0.4} {align} & {V} = ( ^{(1)},^{(2)},^{(3)},^{(4)} ) \\\\ & {v}^{(1)}\\!=\\!{1}{{3}} {pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 {pmatrix}, \\, {v}^{(2)}\\!=\\!{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 {pmatrix} {align} {minipage} {{fig_lap_mtx_specclustering_dict} { Top.} Left: An undirected with four nodes , each representing a . Right: The and its . { Bottom.} Left: A of {datapoint} using the {featurevec} . Right: Two {eigenvector} corresponding to the of the . } {center} {figure} See also: , , , , , , , , , , , , , , .",
    "color": "orange"
  },
  {
    "id": 85,
    "label": "flow-based clustering",
    "title": "Flow-based groups the nodes of an undirected by applying to node-wise {featurevec}. These {featurevec} are built from network flows between carefully selected sources and destination nodes . \\\\ See also: , , , .",
    "color": "khaki"
  },
  {
    "id": 86,
    "label": "estimation error",
    "title": "Consider {datapoint}, each with and . In some applications, we can model the relation between the and the of a as . Here, we use some true underlying and a noise term which summarizes any modeling or labeling errors. The estimation error incurred by an method that learns a , e.g., using , is defined as , for some . For a parametric , which consists of {map} determined by , we can define the estimation error as , . \\\\ See also: , , , , , , , , .",
    "color": "violet"
  },
  {
    "id": 87,
    "label": "degree of belonging",
    "title": "Degree of belonging is a number that indicates the extent to which a belongs to a {MLBasics}. The degree of belonging can be interpreted as a soft assignment. methods can encode the degree of belonging by a real number in the interval . is obtained as the extreme case when the degree of belonging only takes on values or . \\\\ See also: , , , .",
    "color": "lightgreen"
  },
  {
    "id": 88,
    "label": "mean squared estimation error (MSEE)",
    "title": "Consider an method that learns based on some . If we interpret the {datapoint} in as {realization} of an , we define the . Here, denotes the true of the of . The MSEE is defined as the of the squared Euclidean of the , . \\\\ See also: , , , , , , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 89,
    "label": "generalized total variation minimization (GTVMin)",
    "title": "GTVMin is an instance of using the of local as a . \\\\ See also: , , , .",
    "color": "lightgreen"
  },
  {
    "id": 90,
    "label": "regression",
    "title": "Regression problems revolve around the of a numeric solely from the {feature} of a {MLBasics}. \\\\ See also: , , , .",
    "color": "lightgreen"
  },
  {
    "id": 91,
    "label": "accuracy",
    "title": "Consider {datapoint} characterized by {feature} and a categorical which takes on values from a finite . The accuracy of a , when applied to the {datapoint} in a , is then defined as using the . \\\\ See also: , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 92,
    "label": "expert",
    "title": "aims to learn a that accurately predicts the of a based on its {feature}. We measure the error using some . Ideally, we want to find a that incurs minimal on any . We can make this informal goal precise via the and by using the as the for the (average) of a . An alternative approach to obtaining a is to use the learned by an existing method. We refer to this as an expert . minimization methods learn a that incurs a comparable to the best expert , . \\\\ See also: , , , , , , , , , , , .",
    "color": "lightcoral"
  },
  {
    "id": 93,
    "label": "networked federated learning (NFL)",
    "title": "NFL refers to methods that learn personalized {model} in a distributed fashion. These methods learn from {localdataset} that are related by an intrinsic network structure. \\\\ See also: , , .",
    "color": "salmon"
  },
  {
    "id": 94,
    "label": "regret",
    "title": "The regret of a relative to another , which serves as a , is the difference between the incurred by and the incurred by . The is also referred to as an . \\\\ See also: , , , .",
    "color": "lightcoral"
  },
  {
    "id": 95,
    "label": "strongly convex",
    "title": "A continuously real-valued is strongly with coefficient if ,{CvxAlgBertsekas}. \\\\ See also: , , .",
    "color": "salmon"
  },
  {
    "id": 96,
    "label": "differentiable",
    "title": "A real-valued is differentiable if it can, at any point, be approximated locally by a linear . The local linear approximation at the point is determined by the . \\\\ See also: , .",
    "color": "salmon"
  },
  {
    "id": 97,
    "label": "gradient",
    "title": "For a real-valued , if a vector exists such that , it is referred to as the gradient of at . If it exists, the gradient is unique and denoted or . \\\\ See also: .",
    "color": "salmon"
  },
  {
    "id": 98,
    "label": "subgradient",
    "title": "For a real-valued , a vector such that is referred to as a subgradient of at , . \\\\ See also: .",
    "color": "salmon"
  },
  {
    "id": 99,
    "label": "FedProx",
    "title": "FedProx refers to an iterative that alternates between separately training {localmodel} and combining the updated local . In contrast to , which uses to train {localmodel}, FedProx uses a for the training . \\\\ See also: , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 100,
    "label": "rectified linear unit (ReLU)",
    "title": "The ReLU is a popular choice for the of a neuron within an . It is defined as , with being the weighted input of the artificial neuron. \\\\ See also: , .",
    "color": "salmon"
  },
  {
    "id": 101,
    "label": "hypothesis",
    "title": "A hypothesis refers to a (or ) from the to the . Given a with {feature} , we use a hypothesis to estimate (or approximate) the using the . is all about learning (or finding) a hypothesis such that for any (having {feature} and ). \\\\ See also: , , , , , , , , .",
    "color": "violet"
  },
  {
    "id": 102,
    "label": "Vapnik\u2013Chervonenkis dimension (VC dimension)",
    "title": "The VC dimension of an infinite is a widely-used measure for its size. We refer to the literature (see ) for a precise definition of VC dimension as well as a discussion of its basic properties and use in . \\\\ See also: , .",
    "color": "violet"
  },
  {
    "id": 103,
    "label": "effective dimension",
    "title": "The effective dimension of an infinite is a measure of its size. Loosely speaking, the effective dimension is equal to the effective number of independent tunable . These {parameter} might be the coefficients used in a or the and bias terms of an . \\\\ See also: , , , , , .",
    "color": "violet"
  },
  {
    "id": 104,
    "label": "label space",
    "title": "Consider an application that involves {datapoint} characterized by {feature} and {label}. The space is constituted by all potential values that the of a can take on. methods, aiming at predicting numeric {label}, often use the space . Binary methods use a space that consists of two different elements, e.g., , , or . \\\\ See also: , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 105,
    "label": "prediction",
    "title": "A prediction is an estimate or approximation for some quantity of interest. revolves around learning or finding a that reads in the {feature} of a and delivers a prediction for its . \\\\ See also: , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 106,
    "label": "histogram",
    "title": "Consider a that consists of {datapoint} , each of them belonging to some cell with side length . We partition this cell evenly into smaller elementary cells with side length . The histogram of assigns each elementary cell to the corresponding fraction of {datapoint} in that fall into this elementary cell. A visual example of such a histogram is provided in Fig.~{fig:histogram}.\\\\ {figure}[H] {tikzpicture} {compat=1.18} {axis}[ ybar, ymin=0, ymax=6, bar width=22pt, width=10cm, height=6cm, xlabel={Value}, ylabel={Frequency}, ytick={1,2,3,4,5,6}, xtick={1,2,3,4,5}, xticklabels={{[0,1)}, {[1,2)}, {[2,3)}, {[3,4)}, {[4,5)}}, enlarge x limits=0.15, title={Histogram of Sample Data} ] +[fill=blue!40] coordinates {(1,2) (2,5) (3,4) (4,3) (5,1)}; {axis} {tikzpicture} {A histogram representing the frequency of {datapoint} falling within discrete value ranges (i.e., bins). Each bar height shows the count of {sample} in the corresponding interval.} {fig:histogram} {figure} See also: , , .",
    "color": "violet"
  },
  {
    "id": 107,
    "label": "bootstrap",
    "title": "For the analysis of methods, it is often useful to interpret a given set of {datapoint} as {realization} of {rv} with a common . In general, we do not know exactly, but we need to estimate it. The bootstrap uses the of as an estimator for the underlying . \\\\ See also: , , , , , , .",
    "color": "violet"
  },
  {
    "id": 108,
    "label": "feature space",
    "title": "The space of a given application or method is constituted by all potential values that the of a can take on. A widely used choice for the space is the , with the dimension being the number of individual {feature} of a . \\\\ See also: , , , , , .",
    "color": "violet"
  },
  {
    "id": 109,
    "label": "missing data",
    "title": "Consider a constituted by {datapoint} collected via some physical . Due to imperfections and failures, some of the or values of {datapoint} might be corrupted or simply missing. imputation aims at estimating these missing values . We can interpret imputation as an problem where the of a is the value of the corrupted . \\\\ See also: , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 110,
    "label": "feature",
    "title": "A feature of a is one of its properties that can be measured or computed easily without the need for human supervision. For example, if a is a digital image (e.g., stored as a {.jpeg} file), then we could use the red-green-blue intensities of its pixels as features. Domain-specific synonyms for the term feature are \"covariate,\" \"explanatory variable,\" \"independent variable,\" \"input (variable),\" \"predictor (variable),\" or \"regressor\" , , . \\\\ See also: .",
    "color": "lightgreen"
  },
  {
    "id": 111,
    "label": "feature vector",
    "title": "vector refers to a vector whose entries are individual {feature} . Many methods use vectors that belong to some finite-dimensional . For some methods, however, it can be more convenient to work with vectors that belong to an infinite-dimensional (e.g., see ). \\\\ See also: , , , , .",
    "color": "khaki"
  },
  {
    "id": 112,
    "label": "label",
    "title": "A higher-level fact or quantity of interest associated with a . For example, if the is an image, the label could indicate whether the image contains a cat or not. Synonyms for label, commonly used in specific domains, include \"response variable,\" \"output variable,\" and \"target\" , , . \\\\ See also: .",
    "color": "lightgreen"
  },
  {
    "id": 113,
    "label": "data",
    "title": "Data refers to objects that carry information. These objects can be either concrete physical objects (such as persons or animals) or abstract concepts (such as numbers). We often use representations (or approximations) of the original data that are more convenient for data processing. These approximations are based on different data {model}, with the relational data being one of the most widely used . \\\\ See also: .",
    "color": "lightblue"
  },
  {
    "id": 114,
    "label": "dataset",
    "title": "A dataset refers to a collection of {datapoint}. These {datapoint} carry information about some quantity of interest (or ) within an application. methods use datasets for training (e.g., via ) and . Note that our notion of a dataset is very flexible, as it allows for very different types of {datapoint}. Indeed, {datapoint} can be concrete physical objects (such as humans or animals) or abstract objects (such as numbers). As a case in point, Fig.\\ {fig_cows_dataset} depicts a dataset that consists of cows as {datapoint}. {figure}[H] {center} {fig:cowsintheswissalps} {assets/Cows_in_the_Swiss_Alps.jpg} {center} {{fig_cows_dataset}\u201cCows in the Swiss Alps\u201d by User:Huhu Uet is licensed under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/).} {figure} Quite often, an engineer does not have direct access to a dataset. Indeed, accessing the dataset in Fig.\\ {fig_cows_dataset} would require us to visit the cow herd in the Alps. Instead, we need to use an approximation (or representation) of the dataset which is more convenient to work with. Different mathematical {model} have been developed for the representation (or approximation) of datasets , , , . One of the most widely adopted data is the relational , which organizes as a table (or relation) , . A table consists of rows and columns: {itemize} Each row of the table represents a single . Each column of the table corresponds to a specific attribute of the . methods can use attributes as {feature} and {label} of the . {itemize} For example, Table {tab:cowdata} shows a representation of the dataset in Fig.\\ {fig_cows_dataset}. In the relational , the order of rows is irrelevant, and each attribute (i.e., column) must be precisely defined with a domain, which specifies the set of possible values. In applications, these attribute domains become the and the . {table}[H] {tabular}{lcccc} & & & & \\\\ Zenzi & 100 & 4 & 100 & 25 \\\\ Berta & 140 & 3 & 130 & 23 \\\\ Resi & 120 & 4 & 120 & 31 \\\\ {tabular} {A relation (or table) that represents the dataset in Fig.\\ {fig_cows_dataset}.} {tab:cowdata} {table} While the relational is useful for the study of many applications, it may be insufficient regarding the requirements for . Modern approaches like datasheets for datasets provide more comprehensive documentation, including details about the dataset\u2019s collection process, intended use, and other contextual information . \\\\ See also: , , , , , , , , , , .",
    "color": "violet"
  },
  {
    "id": 115,
    "label": "predictor",
    "title": "A predictor is a real-valued . Given a with {feature} , the value is used as a for the true numeric of the . \\\\ See also: , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 116,
    "label": "labeled datapoint",
    "title": "A whose is known or has been determined by some means which might require human labor. \\\\ See also: , .",
    "color": "lightgreen"
  },
  {
    "id": 117,
    "label": "random variable (RV)",
    "title": "An RV is a that maps from a to a value space , . The consists of elementary events and is equipped with a measure that assigns {probability} to subsets of . Different types of RVs include {itemize} {binary RVs}, which map each elementary event to an element of a binary set (e.g., or ); {real-valued RVs}, which take values in the real numbers ; {vector-valued RVs}, which map elementary events to the . {itemize} theory uses the concept of measurable spaces to rigorously define and study the properties of (large) collections of RVs . \\\\ See also: , , , .",
    "color": "lightblue"
  },
  {
    "id": 118,
    "label": "networked model",
    "title": "A networked over an assigns a (i.e., a ) to each node of the . \\\\ See also: , , , .",
    "color": "orange"
  },
  {
    "id": 119,
    "label": "batch",
    "title": "In the context of , a batch refers to a randomly chosen subset of the overall . We use the {datapoint} in this subset to estimate the of and, in turn, to update the . \\\\ See also: , , , , , .",
    "color": "salmon"
  },
  {
    "id": 120,
    "label": "networked data",
    "title": "Networked consists of {localdataset} that are related by some notion of pairwise similarity. We can represent networked using a whose nodes carry {localdataset} and edges encode pairwise similarities. One example of networked arises in applications where {localdataset} are generated by spatially distributed {device}. \\\\ See also: , , , , .",
    "color": "orange"
  },
  {
    "id": 121,
    "label": "training error",
    "title": "The average of a when predicting the {label} of the {datapoint} in a . We sometimes refer by training error also to minimal average which is achieved by a solution of . \\\\ See also: , , , , , .",
    "color": "violet"
  },
  {
    "id": 122,
    "label": "data point",
    "title": "A point is any object that conveys information . points might be students, radio signals, trees, forests, images, {rv}, real numbers, or proteins. We characterize points using two types of properties. One type of property is referred to as a . {feature} are properties of a point that can be measured or computed in an automated fashion. A different kind of property is referred to as a . The of a point represents some higher-level fact (or quantity of interest). In contrast to {feature}, determining the of a point typically requires human experts (or domain experts). Roughly speaking, aims to predict the of a point based solely on its {feature}. \\\\ See also: , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 123,
    "label": "validation error",
    "title": "Consider a which is obtained by some method, e.g., using on a . The average of on a , which is different from the , is referred to as the error. \\\\ See also: , , , , , , .",
    "color": "violet"
  },
  {
    "id": 124,
    "label": "validation",
    "title": "Consider a that has been learned via some method, e.g., by solving on a . Validation refers to the practice of evaluating the incurred by the on a set of {datapoint} that are not contained in the . \\\\ See also: , , , , , .",
    "color": "violet"
  },
  {
    "id": 125,
    "label": "quadratic function",
    "title": "A of the form f() = ^{T} {Q} {w} + {q}^{T} +a, with some matrix , vector , and scalar . \\\\ See also: .",
    "color": "salmon"
  },
  {
    "id": 126,
    "label": "validation set",
    "title": "A set of {datapoint} used to estimate the of a that has been learned by some method (e.g., solving ). The average of on the set is referred to as the and can be used to diagnose an method (see {MLBasics}). The comparison between and can inform directions for the improvement of the method (such as using a different ). \\\\ See also: , , , , , , , , , .",
    "color": "violet"
  },
  {
    "id": 127,
    "label": "test set",
    "title": "A set of {datapoint} that have been used neither to train a (e.g., via ) nor in a to choose between different {model}. \\\\ See also: , , , .",
    "color": "lightcoral"
  },
  {
    "id": 128,
    "label": "model selection",
    "title": "In , selection refers to the process of choosing between different candidate {model}. In its most basic form, selection amounts to: 1) training each candidate ; 2) computing the for each trained ; and 3) choosing the with the smallest {MLBasics}. \\\\ See also: , , .",
    "color": "violet"
  },
  {
    "id": 129,
    "label": "linear classifier",
    "title": "Consider {datapoint} characterized by numeric {feature} and a from some finite . A linear is characterized by having {decisionregion} that are separated by hyperplanes in {MLBasics}. \\\\ See also: , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 130,
    "label": "generalization",
    "title": "Generalization refers to the ability of a trained on a to make accurate {prediction} on new, unseen {datapoint}. This is a central goal of and , i.e., to learn patterns that extend beyond the . Most systems use to learn a by minimizing the average over a of {datapoint} , which is denoted . However, success on the does not guarantee success on unseen \u2014this discrepancy is the challenge of generalization. \\\\ To study generalization mathematically, we need to formalize the notion of ``unseen'' . A widely used approach is to assume a for generation, such as the . Here, we interpret {datapoint} as independent {rv} with an identical . This , which is assumed fixed but unknown, allows us to define the of a trained as the expected \\[ {}=_{ p()} \\{ (, ) \\}. \\] The difference between and is known as the . Tools from theory, such as {concentrationinequ} and uniform convergence, allow us to bound this gap under certain conditions .\\\\ Generalization without : theory is one way to study how well a generalizes beyond the , but it is not the only way. Another option is to use simple, deterministic changes to the {datapoint} in the . The basic idea is that a good should be robust, i.e., its should not change much if we slightly change the {feature} of a . \\\\[1mm] For example, an object detector trained on smartphone photos should still detect the object if a few random pixels are masked . Similarly, it should deliver the same result if we rotate the object in the image . {figure}[H] {tikzpicture}[scale=0.8] (3, 2) ellipse (6cm and 2cm); at (6, 3) {}; (1, 3) circle (4pt) node[below, xshift=0pt, yshift=0pt] {}; (5, 1) circle (4pt) node[below] {}; (1.6, 3) circle (3pt); (0.4, 3) circle (3pt); (1, 3) -- (1.6, 3); (1, 3) -- (0.4, 3); (5.6, 1) circle (3pt); (4.4, 1) circle (3pt); (5, 1) -- (5.6, 1); (5, 1) -- (4.4, 1); plot (, {- 1* + 5}); at (3, 2.5) [right] {}; {tikzpicture} {Two {datapoint} that are used as a to learn a via . We can evaluate outside either by an with some underlying or by perturbing the {datapoint}.} {fig:polynomial_fit_dict} {figure} See also: , , , , , , , , , , , , , , , , , , , .",
    "color": "salmon"
  },
  {
    "id": 131,
    "label": "generalized total variation (GTV)",
    "title": "The difference between the performance of a trained on the and its performance on other {datapoint} (such as those in a ). \\\\ See also: , , , , , , , , , , , , , , , , .",
    "color": "salmon"
  },
  {
    "id": 132,
    "label": "structural risk minimization (SRM)",
    "title": "SRM is an instance of , with which the can be expressed as a countable union of submodels such that . Each submodel permits the derivation of an approximate upper bound on the error incurred when applying to train . These individual bounds\u2014one for each submodel\u2014are then combined to form a used in the objective. These approximate upper bounds (one for each ) are then combined to construct a for {ShalevMLBook}. \\\\ See also: , , , , , .",
    "color": "salmon"
  },
  {
    "id": 133,
    "label": "algebraic connectivity",
    "title": "The algebraic connectivity of an undirected is the second-smallest of its . A is connected if and only if . \\\\ See also: , , .",
    "color": "orange"
  },
  {
    "id": 134,
    "label": "kernel",
    "title": "Consider a matrix with (or spectral decomposition), = _{=1}^{} {} ^{()} ( ^{()} )^{T}. Here, we use the ordered (in increasing fashion) {eigenvalue} {equation} {1} {}. {equation} The Courant\u2013Fischer\u2013Weyl min-max characterization {GolubVanLoanBook} represents the {eigenvalue} of as the solutions to certain {optproblem}. \\\\ See also: , , , .",
    "color": "orange"
  },
  {
    "id": 135,
    "label": "confusion matrix",
    "title": "Consider {datapoint}, which are characterized by {feature} and , having values from the finite . For a given , the confusion matrix is a matrix with rows representing the elements of . The columns of a confusion matrix correspond to the . The -th entry of the confusion matrix is the fraction of {datapoint} with and resulting in a . \\\\ See also: , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 136,
    "label": "outlier",
    "title": "Many methods are motivated by the , which interprets {datapoint} as {realization} of {rv} with a common . The is useful for applications where the statistical properties of the generation process are stationary (or time-invariant) . However, in some applications the consists of a majority of regular {datapoint} that conform with an as well as a small number of {datapoint} that have fundamentally different statistical properties compared to the regular {datapoint}. We refer to a that substantially deviates from the statistical properties of most {datapoint} as an outlier. Different methods for outlier detection use different measures for this deviation. Statistical learning theory studies fundamental limits on the ability to mitigate outliers reliably , . \\\\ See also: , , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 137,
    "label": "linear regression",
    "title": "Linear aims to learn a linear to predict a numeric based on the numeric {feature} of a . The quality of a linear is measured using the average incurred on a set of {labeled datapoint}, which we refer to as the . \\\\ See also: , , , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 138,
    "label": "ridge regression",
    "title": "Ridge learns the of a linear . The quality of a particular choice for the is measured by the sum of two components. The first component is the average incurred by on a set of {labeled datapoint} (i.e., the ). The second component is the scaled squared Euclidean with a . Adding to the average is equivalent to replacing original {datapoint} by the {realization} of (infinitely many) {rv} centered around these {datapoint} (see ). \\\\ See also: , , , , , , , , , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 139,
    "label": "expectation",
    "title": "Consider a numeric which we interpret as the of an with a . The expectation of is defined as the integral . Note that the expectation is only defined if this integral exists, i.e., if the is integrable , , . Fig. {fig_expect_discrete} illustrates the expectation of a scalar discrete which takes on values from a finite set only. {figure}[H] {center} {tikzpicture} {axis}[ ybar, y=5cm, x=2cm, bar width=0.6cm, xlabel={}, clip=false, ylabel={}, y label style={rotate=-90, anchor=west, xshift=-1cm}, xtick={1,2,3,4,5}, ymin=0, ymax=0.6, grid=both, major grid style={gray!20}, tick align=outside, axis line style={black!70}, ] +[ybar, fill=blue!50] coordinates { (1,0.1) (2,0.2) (3,0.4) (4,0.2) (5,0.1) }; at (axis cs:1,0.13) {}; at (axis cs:2,0.23) {}; at (axis cs:3,0.43) {}; at (axis cs:4,0.23) {}; at (axis cs:5,0.13) {}; at (axis cs:3.8,0.53) {}; {axis} {tikzpicture} {center} {-5mm} {The expectation of a discrete is obtained by summing up its possible values , weighted by the corresponding . {fig_expect_discrete}} {figure} See also: , , , , .",
    "color": "lightblue"
  },
  {
    "id": 140,
    "label": "logistic regression",
    "title": "Logistic learns a linear (or ) to predict a binary based on the numeric of a . The quality of a linear is measured by the average on some {labeled datapoint} (i.e., the ). \\\\ See also: , , , , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 141,
    "label": "logistic loss",
    "title": "Consider a characterized by the {feature} and a binary . We use a real-valued to predict the from the {feature} . The logistic incurred by this is defined as {equation} {equ_log_loss_gls_dict} {(,)}{} ( 1 + (- ())). {equation} {figure}[H] {center} {tikzpicture} {axis}[ axis lines=middle, xlabel={}, ylabel={}, xlabel style={at={(axis description cs:1.,0.3)}, anchor=north}, ylabel style={at={(axis description cs:0.5,1.1)}, anchor=center}, xmin=-3.5, xmax=3.5, ymin=-0.5, ymax=2.5, xtick={-3, -2, -1, 0, 1, 2, 3}, ytick={0, 1, 2}, domain=-3:3, samples=100, width=10cm, height=6cm, grid=both, major grid style={line width=.2pt, draw=gray!50}, minor grid style={line width=.1pt, draw=gray!20}, legend pos=south west ] [red, thick] {ln(1 + exp(-x))}; {axis} {tikzpicture} {The logistic incurred by the for a with .} {fig_logloss_dict} {center} {figure} Carefully note that the expression {equ_log_loss_gls_dict} for the logistic applies only for the and when using the thresholding rule {equ_def_threshold_bin_classifier_dict}. \\\\ See also: , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 142,
    "label": "hinge loss",
    "title": "Consider a characterized by a and a binary . The hinge incurred by a real-valued is defined as {equation} {equ_hinge_loss_gls_dict} {(,)}{} \\{ 0 , 1 - () \\}. {equation} {figure}[H] {center} {tikzpicture} {axis}[ axis lines=middle, xlabel={}, ylabel={}, xlabel style={at={(axis description cs:1.,0.3)}, anchor=north}, ylabel style={at={(axis description cs:0.5,1.1)}, anchor=center}, xmin=-3.5, xmax=3.5, ymin=-0.5, ymax=2.5, xtick={-3, -2, -1, 0, 1, 2, 3}, ytick={0, 1, 2}, domain=-3:3, samples=100, width=10cm, height=6cm, grid=both, major grid style={line width=.2pt, draw=gray!50}, minor grid style={line width=.1pt, draw=gray!20}, legend pos=south west ] {max(0, 1-x)}; {axis} {tikzpicture} {The hinge incurred by the for a with . A regularized variant of the hinge is used by the .} {fig_hingeloss_dict} {center} {figure} See also: , , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 143,
    "label": "independent and identically distributed assumption (i.i.d.\\ assumption)",
    "title": "The assumption interprets {datapoint} of a as the {realization} of {rv}. \\\\ See also: , , , , .",
    "color": "salmon"
  },
  {
    "id": 144,
    "label": "hypothesis space",
    "title": "Every practical method uses a space (or ) . The space of an method is a subset of all possible {map} from the to the . The design choice of the space should take into account available computational resources and . If the computational infrastructure allows for efficient matrix operations, and there is an (approximately) linear relation between a set of {feature} and a , a useful choice for the space might be the . \\\\ See also: , , , , , , , , , .",
    "color": "violet"
  },
  {
    "id": 145,
    "label": "artificial intelligence (AI)",
    "title": "AI refers to systems that behave rationally in the sense of maximizing a long-term . The -based approach to AI is to train a for predicting optimal actions. These {prediction} are computed from observations about the state of the environment. The choice of sets AI applications apart from more basic applications. AI systems rarely have access to a labeled that allows the average to be measured for any possible choice of . Instead, AI systems use observed signals to obtain a (point-wise) estimate for the incurred by the current choice of . \\\\ See also: , , , , , , .",
    "color": "khaki"
  },
  {
    "id": 146,
    "label": "reward",
    "title": "A reward refers to some observed (or measured) quantity that allows us to estimate the incurred by the (or decision) of a . For example, in an application to self-driving vehicles, could represent the current steering direction of a vehicle. We could construct a reward from the measurements of a collision sensor that indicate if the vehicle is moving towards an obstacle. We define a low reward for the steering direction if the vehicle moves dangerously towards an obstacle. \\\\ See also: , , , .",
    "color": "lightblue"
  },
  {
    "id": 147,
    "label": "soft clustering",
    "title": "Soft refers to the task of partitioning a given set of {datapoint} into (a few) overlapping {cluster}. Each is assigned to several different {cluster} with varying degrees of belonging. Soft methods determine the (or soft assignment) for each and each . A principled approach to soft is by interpreting {datapoint} as {realization} of a . We then obtain a natural choice for the as the conditional of a belonging to a specific mixture component. \\\\ See also: , , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 148,
    "label": "clustering",
    "title": "Clustering methods decompose a given set of {datapoint} into a few subsets, which are referred to as {cluster}. Each consists of {datapoint} that are more similar to each other than to {datapoint} outside the . Different clustering methods use different measures for the similarity between {datapoint} and different forms of representations. The clustering method uses the average vector of a (i.e., the ) as its representative. A popular method based on represents a by a . \\\\ See also: , , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 149,
    "label": "eigenvalue",
    "title": "We refer to a number as an eigenvalue of a square matrix if there is a non-zero vector such that .",
    "color": "orange"
  },
  {
    "id": 150,
    "label": "principal component analysis (PCA)",
    "title": "PCA determines a linear such that the new {feature} allow us to reconstruct the original {feature} with the reconstruction error . \\\\ See also: , , .",
    "color": "lightcoral"
  },
  {
    "id": 151,
    "label": "decision tree",
    "title": "A decision tree is a flow-chart-like representation of a . More formally, a decision tree is a directed containing a root node that reads in the of a . The root node then forwards the to one of its child nodes based on some elementary test on the {feature} . If the receiving child node is not a leaf node, i.e., it has itself child nodes, it represents another test. Based on the test result, the is forwarded to one of its descendants. This testing and forwarding of the is continued until the ends up in a leaf node without any children. {figure}[H] {minipage}{.45} {1}{ {tikzpicture} (A) {}; (B) {}; (C) {}; (D) {}; (E) {}; (A) -- (B) node[midway, left] {no}; (A) -- (C) node[midway, right] {yes}; (C) -- (D) node[midway, left] {no}; (C) -- (E) node[midway, right] {yes}; {tikzpicture} } {minipage} {15mm} {minipage}{.45} {15mm} {tikzpicture} (-2,2) rectangle (2,-2); {scope} (-0.5,0) circle (1cm); (0.5,0) circle (1cm); (-2,1.5) rectangle (2,-1.5); {scope} (-0.5,0) circle (1cm); (0.5,0) circle (1cm); (-0.5,0) circle [radius=0.025]; [below right, red] at (-0.5,0) {}; [below left, blue] at (-0.7,0) {}; [above left] at (-0.7,1) {}; [left] at (-0.4,0) {}; (0.5,0) circle [radius=0.025]; [right] at (0.6,0) {}; {tikzpicture} {minipage} {Left: A decision tree is a flow-chart-like representation of a piece-wise constant . Each piece is a . The depicted decision tree can be applied to numeric {featurevec}, i.e., . It is parametrized by the threshold and the vectors . Right: A decision tree partitions the into {decisionregion}. Each corresponds to a specific leaf node in the decision tree.} {fig_decision_tree} {figure} See also: , , , , , , , .",
    "color": "khaki"
  },
  {
    "id": 152,
    "label": "model inversion",
    "title": "A inversion is a form of on an system. An adversary seeks to infer {sensattr} of individual {datapoint} by exploiting partial access to a trained . This access typically consists of querying the for {prediction} on carefully chosen inputs. Basic inversion techniques have been demonstrated in the context of facial image , where images are reconstructed using the ( of) outputs combined with auxiliary information such as a person\u2019s name . {figure}[H] {center} {tikzpicture}[scale=1.5] (-0.5,0) -- (5.5,0) node[right] {face image }; (0,-0.2) -- (0,2.5) node[above] {name}; plot ({}, {2/(1 + exp(-3*( - 3)))}); {3} {}{2/(1 + exp(-3*( - 3)))} (,0) -- (,); (0,) -- (,); (,) circle (0.1); at (-0.1,) { ``Alexander Jung''}; at (,-0.25) {{assets/AlexanderJung.jpg}}; at (4,2.2) {trained }; {tikzpicture} {center} {figure} See also: , , , , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 153,
    "label": "Hilbert space",
    "title": "A Hilbert space is a complete inner product space . That is, it is a equipped with an inner product between pairs of vectors, and it satisfies the additional requirement of completeness, i.e., every Cauchy sequence of vectors converges to a limit within the space. A canonical example of a Hilbert space is the , for some dimension , consisting of vectors and the standard inner product . \\\\ See also: , .",
    "color": "khaki"
  },
  {
    "id": 154,
    "label": "sample",
    "title": "A finite sequence (or list) of {datapoint} that is obtained or interpreted as the of {rv} with a common . The length of the sequence is referred to as the . \\\\ See also: , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 155,
    "label": "artificial neural network (ANN)",
    "title": "An ANN is a graphical (signal-flow) representation of a that maps {feature} of a at its input to a for the corresponding at its output. The fundamental unit of an ANN is the artificial neuron, which applies an to its weighted inputs. The outputs of these neurons serve as inputs for other neurons, forming interconnected layers. \\\\ See also: , , , , , .",
    "color": "salmon"
  },
  {
    "id": 156,
    "label": "random forest",
    "title": "A random forest is a set of different {decisiontree}. Each of these {decisiontree} is obtained by fitting a perturbed copy of the original . \\\\ See also: , .",
    "color": "khaki"
  },
  {
    "id": 157,
    "label": "bagging (or bootstrap aggregation)",
    "title": "Bagging (or bootstrap aggregation) is a generic technique to improve (the of) a given method. The idea is to use the to generate perturbed copies of a given and then to learn a separate for each copy. We then predict the of a by combining or aggregating the individual {prediction} of each separate . For {map} delivering numeric values, this aggregation could be implemented by computing the average of individual {prediction}. \\\\ See also: , , , , , , , , .",
    "color": "violet"
  },
  {
    "id": 158,
    "label": "device",
    "title": "Any physical system that can be used to store and process . In the context of , we typically mean a computer that is able to read in {datapoint} from different sources and, in turn, to train an using these {datapoint}. \\\\ See also: , , , .",
    "color": "orange"
  },
  {
    "id": 159,
    "label": "law of large numbers",
    "title": "The law of large numbers refers to the convergence of the average of an increasing (large) number of {rv} to the of their common . Different instances of the law of large numbers are obtained by using different notions of convergence . \\\\ See also: , , , .",
    "color": "lightblue"
  },
  {
    "id": 160,
    "label": "$k$-fold cross-validation ($\\nrfolds$-fold CV)",
    "title": "-fold CV is a method for learning and validating a using a given . This method divides the evenly into subsets or folds and then executes repetitions of training (e.g., via ) and . Each repetition uses a different fold as the and the remaining folds as a . The final output is the average of the {valerr} obtained from the repetitions. \\\\ See also: , , , , , , , .",
    "color": "violet"
  },
  {
    "id": 161,
    "label": "Jacobi method",
    "title": "The Jacobi method is an for solving systems of linear equations (i.e., a linear system) of the form . Here, is a square matrix with non-zero main diagonal entries. The method constructs a sequence by updating each entry of according to \\[ x_i^{(+1)} = {1}{a_{ii}} ( b_i - _{j i} a_{ij} x_j^{()} ). \\] Carefully note that all entries are updated simultaneously. The above iteration converges to a solution, i.e., , under certain conditions on the matrix , e.g., being strictly diagonally dominant or symmetric positive definite , , . Jacobi-type methods are appealing for large linear systems due to their parallelizable structure . We can interpret the Jacobi method as a . Indeed, using the decomposition , with being the diagonal of , allows us to rewrite the linear equation as a fixed-point equation \\[ {x} = {^{-1}({b} - {x})}_{ }, \\] which leads to the iteration . \\\\ For example, for the linear equation \\[ {x} = {b}, {where} = {bmatrix} a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} {bmatrix}, {b} = {bmatrix} b_1 \\\\ b_2 \\\\ b_3 {bmatrix}, \\] the Jacobi method updates each component of \\( {x} \\) as follows: \\[ {aligned} x_1^{(k+1)} &= {1}{a_{11}} ( b_1 - a_{12} x_2^{(k)} - a_{13} x_3^{(k)} ), \\\\ x_2^{(k+1)} &= {1}{a_{22}} ( b_2 - a_{21} x_1^{(k)} - a_{23} x_3^{(k)} ), \\\\ x_3^{(k+1)} &= {1}{a_{33}} ( b_3 - a_{31} x_1^{(k)} - a_{32} x_2^{(k)} ). {aligned} \\] See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 162,
    "label": "R\\'enyi divergence",
    "title": "The R\\'enyi divergence measures the (dis)similarity between two {probdist} . \\\\ See also: .",
    "color": "orange"
  },
  {
    "id": 163,
    "label": "non-smooth",
    "title": "We refer to a as non-smooth if it is not . \\\\ See also: , .",
    "color": "salmon"
  }
]);
      const edges = new vis.DataSet([
  {
    "from": 1,
    "to": 138
  },
  {
    "from": 1,
    "to": 114
  },
  {
    "from": 1,
    "to": 112
  },
  {
    "from": 2,
    "to": 149
  },
  {
    "from": 3,
    "to": 8
  },
  {
    "from": 3,
    "to": 30
  },
  {
    "from": 3,
    "to": 25
  },
  {
    "from": 3,
    "to": 137
  },
  {
    "from": 3,
    "to": 150
  },
  {
    "from": 3,
    "to": 111
  },
  {
    "from": 3,
    "to": 9
  },
  {
    "from": 4,
    "to": 30
  },
  {
    "from": 4,
    "to": 3
  },
  {
    "from": 5,
    "to": 25
  },
  {
    "from": 5,
    "to": 62
  },
  {
    "from": 5,
    "to": 48
  },
  {
    "from": 5,
    "to": 117
  },
  {
    "from": 5,
    "to": 79
  },
  {
    "from": 6,
    "to": 25
  },
  {
    "from": 6,
    "to": 29
  },
  {
    "from": 6,
    "to": 62
  },
  {
    "from": 6,
    "to": 105
  },
  {
    "from": 6,
    "to": 48
  },
  {
    "from": 9,
    "to": 8
  },
  {
    "from": 11,
    "to": 73
  },
  {
    "from": 11,
    "to": 10
  },
  {
    "from": 12,
    "to": 10
  },
  {
    "from": 12,
    "to": 96
  },
  {
    "from": 12,
    "to": 8
  },
  {
    "from": 12,
    "to": 32
  },
  {
    "from": 12,
    "to": 33
  },
  {
    "from": 13,
    "to": 61
  },
  {
    "from": 13,
    "to": 48
  },
  {
    "from": 13,
    "to": 117
  },
  {
    "from": 14,
    "to": 25
  },
  {
    "from": 14,
    "to": 114
  },
  {
    "from": 14,
    "to": 158
  },
  {
    "from": 14,
    "to": 15
  },
  {
    "from": 14,
    "to": 78
  },
  {
    "from": 15,
    "to": 14
  },
  {
    "from": 15,
    "to": 25
  },
  {
    "from": 15,
    "to": 152
  },
  {
    "from": 15,
    "to": 78
  },
  {
    "from": 16,
    "to": 8
  },
  {
    "from": 16,
    "to": 61
  },
  {
    "from": 17,
    "to": 18
  },
  {
    "from": 19,
    "to": 120
  },
  {
    "from": 19,
    "to": 66
  },
  {
    "from": 19,
    "to": 101
  },
  {
    "from": 19,
    "to": 9
  },
  {
    "from": 19,
    "to": 35
  },
  {
    "from": 20,
    "to": 72
  },
  {
    "from": 21,
    "to": 73
  },
  {
    "from": 21,
    "to": 7
  },
  {
    "from": 22,
    "to": 72
  },
  {
    "from": 22,
    "to": 66
  },
  {
    "from": 22,
    "to": 31
  },
  {
    "from": 23,
    "to": 72
  },
  {
    "from": 23,
    "to": 66
  },
  {
    "from": 23,
    "to": 31
  },
  {
    "from": 24,
    "to": 103
  },
  {
    "from": 24,
    "to": 25
  },
  {
    "from": 24,
    "to": 137
  },
  {
    "from": 24,
    "to": 113
  },
  {
    "from": 24,
    "to": 110
  },
  {
    "from": 24,
    "to": 122
  },
  {
    "from": 24,
    "to": 70
  },
  {
    "from": 25,
    "to": 112
  },
  {
    "from": 25,
    "to": 122
  },
  {
    "from": 25,
    "to": 101
  },
  {
    "from": 25,
    "to": 144
  },
  {
    "from": 25,
    "to": 110
  },
  {
    "from": 26,
    "to": 25
  },
  {
    "from": 26,
    "to": 110
  },
  {
    "from": 26,
    "to": 9
  },
  {
    "from": 26,
    "to": 122
  },
  {
    "from": 26,
    "to": 108
  },
  {
    "from": 26,
    "to": 144
  },
  {
    "from": 26,
    "to": 150
  },
  {
    "from": 26,
    "to": 7
  },
  {
    "from": 26,
    "to": 114
  },
  {
    "from": 27,
    "to": 25
  },
  {
    "from": 27,
    "to": 9
  },
  {
    "from": 28,
    "to": 114
  },
  {
    "from": 28,
    "to": 158
  },
  {
    "from": 28,
    "to": 113
  },
  {
    "from": 28,
    "to": 46
  },
  {
    "from": 28,
    "to": 110
  },
  {
    "from": 28,
    "to": 122
  },
  {
    "from": 28,
    "to": 111
  },
  {
    "from": 28,
    "to": 112
  },
  {
    "from": 29,
    "to": 101
  },
  {
    "from": 29,
    "to": 111
  },
  {
    "from": 29,
    "to": 122
  },
  {
    "from": 29,
    "to": 105
  },
  {
    "from": 29,
    "to": 69
  },
  {
    "from": 29,
    "to": 112
  },
  {
    "from": 29,
    "to": 151
  },
  {
    "from": 29,
    "to": 80
  },
  {
    "from": 29,
    "to": 30
  },
  {
    "from": 30,
    "to": 111
  },
  {
    "from": 30,
    "to": 144
  },
  {
    "from": 30,
    "to": 105
  },
  {
    "from": 30,
    "to": 37
  },
  {
    "from": 30,
    "to": 122
  },
  {
    "from": 30,
    "to": 3
  },
  {
    "from": 30,
    "to": 110
  },
  {
    "from": 31,
    "to": 96
  },
  {
    "from": 31,
    "to": 8
  },
  {
    "from": 31,
    "to": 97
  },
  {
    "from": 31,
    "to": 52
  },
  {
    "from": 31,
    "to": 130
  },
  {
    "from": 31,
    "to": 33
  },
  {
    "from": 33,
    "to": 8
  },
  {
    "from": 33,
    "to": 130
  },
  {
    "from": 33,
    "to": 31
  },
  {
    "from": 34,
    "to": 8
  },
  {
    "from": 34,
    "to": 33
  },
  {
    "from": 35,
    "to": 61
  },
  {
    "from": 36,
    "to": 39
  },
  {
    "from": 36,
    "to": 117
  },
  {
    "from": 37,
    "to": 25
  },
  {
    "from": 37,
    "to": 48
  },
  {
    "from": 37,
    "to": 113
  },
  {
    "from": 38,
    "to": 25
  },
  {
    "from": 38,
    "to": 101
  },
  {
    "from": 38,
    "to": 7
  },
  {
    "from": 38,
    "to": 59
  },
  {
    "from": 38,
    "to": 70
  },
  {
    "from": 39,
    "to": 117
  },
  {
    "from": 39,
    "to": 49
  },
  {
    "from": 39,
    "to": 50
  },
  {
    "from": 39,
    "to": 25
  },
  {
    "from": 39,
    "to": 6
  },
  {
    "from": 39,
    "to": 40
  },
  {
    "from": 39,
    "to": 48
  },
  {
    "from": 39,
    "to": 17
  },
  {
    "from": 39,
    "to": 62
  },
  {
    "from": 40,
    "to": 49
  },
  {
    "from": 40,
    "to": 8
  },
  {
    "from": 40,
    "to": 62
  },
  {
    "from": 40,
    "to": 117
  },
  {
    "from": 40,
    "to": 154
  },
  {
    "from": 41,
    "to": 7
  },
  {
    "from": 42,
    "to": 31
  },
  {
    "from": 42,
    "to": 41
  },
  {
    "from": 43,
    "to": 25
  },
  {
    "from": 43,
    "to": 114
  },
  {
    "from": 43,
    "to": 47
  },
  {
    "from": 43,
    "to": 78
  },
  {
    "from": 43,
    "to": 122
  },
  {
    "from": 43,
    "to": 48
  },
  {
    "from": 43,
    "to": 117
  },
  {
    "from": 43,
    "to": 105
  },
  {
    "from": 44,
    "to": 25
  },
  {
    "from": 44,
    "to": 122
  },
  {
    "from": 44,
    "to": 105
  },
  {
    "from": 44,
    "to": 45
  },
  {
    "from": 44,
    "to": 110
  },
  {
    "from": 44,
    "to": 14
  },
  {
    "from": 45,
    "to": 25
  },
  {
    "from": 45,
    "to": 114
  },
  {
    "from": 45,
    "to": 105
  },
  {
    "from": 45,
    "to": 122
  },
  {
    "from": 45,
    "to": 130
  },
  {
    "from": 45,
    "to": 70
  },
  {
    "from": 45,
    "to": 113
  },
  {
    "from": 45,
    "to": 154
  },
  {
    "from": 46,
    "to": 25
  },
  {
    "from": 46,
    "to": 114
  },
  {
    "from": 46,
    "to": 105
  },
  {
    "from": 46,
    "to": 122
  },
  {
    "from": 46,
    "to": 112
  },
  {
    "from": 46,
    "to": 78
  },
  {
    "from": 46,
    "to": 9
  },
  {
    "from": 46,
    "to": 110
  },
  {
    "from": 47,
    "to": 25
  },
  {
    "from": 47,
    "to": 114
  },
  {
    "from": 47,
    "to": 110
  },
  {
    "from": 47,
    "to": 122
  },
  {
    "from": 47,
    "to": 48
  },
  {
    "from": 47,
    "to": 113
  },
  {
    "from": 47,
    "to": 43
  },
  {
    "from": 47,
    "to": 105
  },
  {
    "from": 48,
    "to": 122
  },
  {
    "from": 48,
    "to": 117
  },
  {
    "from": 49,
    "to": 117
  },
  {
    "from": 49,
    "to": 139
  },
  {
    "from": 50,
    "to": 117
  },
  {
    "from": 50,
    "to": 139
  },
  {
    "from": 51,
    "to": 101
  },
  {
    "from": 51,
    "to": 8
  },
  {
    "from": 51,
    "to": 114
  },
  {
    "from": 51,
    "to": 122
  },
  {
    "from": 51,
    "to": 111
  },
  {
    "from": 51,
    "to": 53
  },
  {
    "from": 52,
    "to": 53
  },
  {
    "from": 53,
    "to": 66
  },
  {
    "from": 53,
    "to": 35
  },
  {
    "from": 54,
    "to": 25
  },
  {
    "from": 54,
    "to": 144
  },
  {
    "from": 54,
    "to": 114
  },
  {
    "from": 54,
    "to": 122
  },
  {
    "from": 54,
    "to": 117
  },
  {
    "from": 55,
    "to": 112
  },
  {
    "from": 55,
    "to": 122
  },
  {
    "from": 55,
    "to": 110
  },
  {
    "from": 56,
    "to": 110
  },
  {
    "from": 56,
    "to": 122
  },
  {
    "from": 57,
    "to": 149
  },
  {
    "from": 57,
    "to": 25
  },
  {
    "from": 57,
    "to": 137
  },
  {
    "from": 57,
    "to": 110
  },
  {
    "from": 57,
    "to": 122
  },
  {
    "from": 58,
    "to": 101
  },
  {
    "from": 58,
    "to": 9
  },
  {
    "from": 58,
    "to": 112
  },
  {
    "from": 58,
    "to": 104
  },
  {
    "from": 58,
    "to": 8
  },
  {
    "from": 58,
    "to": 105
  },
  {
    "from": 58,
    "to": 55
  },
  {
    "from": 59,
    "to": 70
  },
  {
    "from": 59,
    "to": 101
  },
  {
    "from": 59,
    "to": 114
  },
  {
    "from": 59,
    "to": 122
  },
  {
    "from": 60,
    "to": 61
  },
  {
    "from": 60,
    "to": 53
  },
  {
    "from": 61,
    "to": 9
  },
  {
    "from": 62,
    "to": 105
  },
  {
    "from": 62,
    "to": 122
  },
  {
    "from": 62,
    "to": 25
  },
  {
    "from": 62,
    "to": 113
  },
  {
    "from": 62,
    "to": 154
  },
  {
    "from": 63,
    "to": 25
  },
  {
    "from": 63,
    "to": 146
  },
  {
    "from": 63,
    "to": 48
  },
  {
    "from": 63,
    "to": 5
  },
  {
    "from": 63,
    "to": 64
  },
  {
    "from": 63,
    "to": 117
  },
  {
    "from": 63,
    "to": 49
  },
  {
    "from": 63,
    "to": 113
  },
  {
    "from": 63,
    "to": 62
  },
  {
    "from": 63,
    "to": 94
  },
  {
    "from": 64,
    "to": 5
  },
  {
    "from": 64,
    "to": 146
  },
  {
    "from": 64,
    "to": 49
  },
  {
    "from": 64,
    "to": 94
  },
  {
    "from": 64,
    "to": 75
  },
  {
    "from": 65,
    "to": 25
  },
  {
    "from": 65,
    "to": 114
  },
  {
    "from": 65,
    "to": 70
  },
  {
    "from": 65,
    "to": 101
  },
  {
    "from": 65,
    "to": 48
  },
  {
    "from": 65,
    "to": 132
  },
  {
    "from": 65,
    "to": 63
  },
  {
    "from": 66,
    "to": 61
  },
  {
    "from": 66,
    "to": 113
  },
  {
    "from": 66,
    "to": 158
  },
  {
    "from": 66,
    "to": 101
  },
  {
    "from": 67,
    "to": 8
  },
  {
    "from": 67,
    "to": 4
  },
  {
    "from": 68,
    "to": 67
  },
  {
    "from": 69,
    "to": 25
  },
  {
    "from": 69,
    "to": 105
  },
  {
    "from": 69,
    "to": 122
  },
  {
    "from": 69,
    "to": 55
  },
  {
    "from": 69,
    "to": 110
  },
  {
    "from": 70,
    "to": 101
  },
  {
    "from": 70,
    "to": 112
  },
  {
    "from": 70,
    "to": 122
  },
  {
    "from": 70,
    "to": 105
  },
  {
    "from": 70,
    "to": 117
  },
  {
    "from": 70,
    "to": 143
  },
  {
    "from": 70,
    "to": 110
  },
  {
    "from": 71,
    "to": 155
  },
  {
    "from": 71,
    "to": 8
  },
  {
    "from": 72,
    "to": 73
  },
  {
    "from": 72,
    "to": 158
  },
  {
    "from": 73,
    "to": 30
  },
  {
    "from": 73,
    "to": 31
  },
  {
    "from": 73,
    "to": 5
  },
  {
    "from": 74,
    "to": 5
  },
  {
    "from": 74,
    "to": 73
  },
  {
    "from": 74,
    "to": 97
  },
  {
    "from": 74,
    "to": 122
  },
  {
    "from": 74,
    "to": 11
  },
  {
    "from": 75,
    "to": 25
  },
  {
    "from": 75,
    "to": 113
  },
  {
    "from": 75,
    "to": 7
  },
  {
    "from": 75,
    "to": 17
  },
  {
    "from": 75,
    "to": 101
  },
  {
    "from": 75,
    "to": 122
  },
  {
    "from": 75,
    "to": 76
  },
  {
    "from": 76,
    "to": 73
  },
  {
    "from": 76,
    "to": 113
  },
  {
    "from": 76,
    "to": 62
  },
  {
    "from": 76,
    "to": 122
  },
  {
    "from": 76,
    "to": 25
  },
  {
    "from": 76,
    "to": 75
  },
  {
    "from": 77,
    "to": 25
  },
  {
    "from": 77,
    "to": 29
  },
  {
    "from": 77,
    "to": 145
  },
  {
    "from": 77,
    "to": 140
  },
  {
    "from": 77,
    "to": 55
  },
  {
    "from": 77,
    "to": 105
  },
  {
    "from": 77,
    "to": 151
  },
  {
    "from": 78,
    "to": 25
  },
  {
    "from": 78,
    "to": 101
  },
  {
    "from": 78,
    "to": 9
  },
  {
    "from": 78,
    "to": 112
  },
  {
    "from": 78,
    "to": 122
  },
  {
    "from": 78,
    "to": 110
  },
  {
    "from": 79,
    "to": 61
  },
  {
    "from": 79,
    "to": 112
  },
  {
    "from": 79,
    "to": 35
  },
  {
    "from": 80,
    "to": 155
  },
  {
    "from": 80,
    "to": 25
  },
  {
    "from": 81,
    "to": 25
  },
  {
    "from": 81,
    "to": 101
  },
  {
    "from": 81,
    "to": 127
  },
  {
    "from": 81,
    "to": 113
  },
  {
    "from": 81,
    "to": 48
  },
  {
    "from": 81,
    "to": 7
  },
  {
    "from": 81,
    "to": 70
  },
  {
    "from": 81,
    "to": 144
  },
  {
    "from": 81,
    "to": 112
  },
  {
    "from": 81,
    "to": 122
  },
  {
    "from": 81,
    "to": 49
  },
  {
    "from": 81,
    "to": 50
  },
  {
    "from": 81,
    "to": 110
  },
  {
    "from": 82,
    "to": 55
  },
  {
    "from": 82,
    "to": 80
  },
  {
    "from": 83,
    "to": 61
  },
  {
    "from": 83,
    "to": 148
  },
  {
    "from": 83,
    "to": 122
  },
  {
    "from": 84,
    "to": 148
  },
  {
    "from": 84,
    "to": 83
  },
  {
    "from": 84,
    "to": 61
  },
  {
    "from": 84,
    "to": 122
  },
  {
    "from": 84,
    "to": 147
  },
  {
    "from": 84,
    "to": 149
  },
  {
    "from": 84,
    "to": 111
  },
  {
    "from": 84,
    "to": 7
  },
  {
    "from": 84,
    "to": 35
  },
  {
    "from": 85,
    "to": 148
  },
  {
    "from": 85,
    "to": 61
  },
  {
    "from": 85,
    "to": 111
  },
  {
    "from": 86,
    "to": 111
  },
  {
    "from": 86,
    "to": 112
  },
  {
    "from": 86,
    "to": 122
  },
  {
    "from": 86,
    "to": 101
  },
  {
    "from": 86,
    "to": 25
  },
  {
    "from": 86,
    "to": 144
  },
  {
    "from": 86,
    "to": 9
  },
  {
    "from": 87,
    "to": 122
  },
  {
    "from": 87,
    "to": 147
  },
  {
    "from": 88,
    "to": 25
  },
  {
    "from": 88,
    "to": 114
  },
  {
    "from": 88,
    "to": 117
  },
  {
    "from": 88,
    "to": 86
  },
  {
    "from": 88,
    "to": 139
  },
  {
    "from": 88,
    "to": 67
  },
  {
    "from": 88,
    "to": 122
  },
  {
    "from": 88,
    "to": 49
  },
  {
    "from": 90,
    "to": 105
  },
  {
    "from": 90,
    "to": 112
  },
  {
    "from": 90,
    "to": 122
  },
  {
    "from": 90,
    "to": 110
  },
  {
    "from": 91,
    "to": 112
  },
  {
    "from": 91,
    "to": 104
  },
  {
    "from": 91,
    "to": 101
  },
  {
    "from": 91,
    "to": 114
  },
  {
    "from": 91,
    "to": 122
  },
  {
    "from": 91,
    "to": 110
  },
  {
    "from": 92,
    "to": 25
  },
  {
    "from": 92,
    "to": 101
  },
  {
    "from": 92,
    "to": 112
  },
  {
    "from": 92,
    "to": 122
  },
  {
    "from": 92,
    "to": 105
  },
  {
    "from": 92,
    "to": 143
  },
  {
    "from": 92,
    "to": 81
  },
  {
    "from": 92,
    "to": 94
  },
  {
    "from": 92,
    "to": 110
  },
  {
    "from": 94,
    "to": 101
  },
  {
    "from": 94,
    "to": 81
  },
  {
    "from": 94,
    "to": 92
  },
  {
    "from": 95,
    "to": 96
  },
  {
    "from": 95,
    "to": 8
  },
  {
    "from": 96,
    "to": 8
  },
  {
    "from": 96,
    "to": 97
  },
  {
    "from": 97,
    "to": 8
  },
  {
    "from": 98,
    "to": 8
  },
  {
    "from": 99,
    "to": 73
  },
  {
    "from": 99,
    "to": 21
  },
  {
    "from": 99,
    "to": 33
  },
  {
    "from": 100,
    "to": 71
  },
  {
    "from": 100,
    "to": 155
  },
  {
    "from": 101,
    "to": 9
  },
  {
    "from": 101,
    "to": 8
  },
  {
    "from": 101,
    "to": 108
  },
  {
    "from": 101,
    "to": 104
  },
  {
    "from": 101,
    "to": 122
  },
  {
    "from": 101,
    "to": 112
  },
  {
    "from": 101,
    "to": 105
  },
  {
    "from": 101,
    "to": 25
  },
  {
    "from": 101,
    "to": 110
  },
  {
    "from": 102,
    "to": 144
  },
  {
    "from": 102,
    "to": 25
  },
  {
    "from": 103,
    "to": 144
  },
  {
    "from": 103,
    "to": 3
  },
  {
    "from": 103,
    "to": 155
  },
  {
    "from": 103,
    "to": 54
  },
  {
    "from": 104,
    "to": 25
  },
  {
    "from": 104,
    "to": 112
  },
  {
    "from": 104,
    "to": 122
  },
  {
    "from": 104,
    "to": 90
  },
  {
    "from": 104,
    "to": 55
  },
  {
    "from": 104,
    "to": 110
  },
  {
    "from": 105,
    "to": 25
  },
  {
    "from": 105,
    "to": 101
  },
  {
    "from": 105,
    "to": 9
  },
  {
    "from": 105,
    "to": 122
  },
  {
    "from": 105,
    "to": 112
  },
  {
    "from": 105,
    "to": 110
  },
  {
    "from": 106,
    "to": 114
  },
  {
    "from": 106,
    "to": 122
  },
  {
    "from": 106,
    "to": 154
  },
  {
    "from": 107,
    "to": 25
  },
  {
    "from": 107,
    "to": 106
  },
  {
    "from": 107,
    "to": 122
  },
  {
    "from": 107,
    "to": 117
  },
  {
    "from": 108,
    "to": 110
  },
  {
    "from": 108,
    "to": 25
  },
  {
    "from": 108,
    "to": 111
  },
  {
    "from": 108,
    "to": 122
  },
  {
    "from": 109,
    "to": 114
  },
  {
    "from": 109,
    "to": 158
  },
  {
    "from": 109,
    "to": 110
  },
  {
    "from": 109,
    "to": 112
  },
  {
    "from": 109,
    "to": 113
  },
  {
    "from": 109,
    "to": 25
  },
  {
    "from": 109,
    "to": 122
  },
  {
    "from": 110,
    "to": 122
  },
  {
    "from": 110,
    "to": 115
  },
  {
    "from": 111,
    "to": 110
  },
  {
    "from": 111,
    "to": 25
  },
  {
    "from": 111,
    "to": 4
  },
  {
    "from": 112,
    "to": 122
  },
  {
    "from": 114,
    "to": 112
  },
  {
    "from": 114,
    "to": 25
  },
  {
    "from": 114,
    "to": 124
  },
  {
    "from": 114,
    "to": 113
  },
  {
    "from": 114,
    "to": 122
  },
  {
    "from": 114,
    "to": 108
  },
  {
    "from": 114,
    "to": 104
  },
  {
    "from": 114,
    "to": 110
  },
  {
    "from": 115,
    "to": 101
  },
  {
    "from": 115,
    "to": 9
  },
  {
    "from": 115,
    "to": 122
  },
  {
    "from": 115,
    "to": 105
  },
  {
    "from": 115,
    "to": 112
  },
  {
    "from": 115,
    "to": 110
  },
  {
    "from": 116,
    "to": 122
  },
  {
    "from": 116,
    "to": 112
  },
  {
    "from": 117,
    "to": 8
  },
  {
    "from": 117,
    "to": 9
  },
  {
    "from": 118,
    "to": 66
  },
  {
    "from": 118,
    "to": 144
  },
  {
    "from": 119,
    "to": 97
  },
  {
    "from": 119,
    "to": 121
  },
  {
    "from": 119,
    "to": 122
  },
  {
    "from": 120,
    "to": 113
  },
  {
    "from": 120,
    "to": 61
  },
  {
    "from": 120,
    "to": 158
  },
  {
    "from": 121,
    "to": 101
  },
  {
    "from": 121,
    "to": 112
  },
  {
    "from": 121,
    "to": 122
  },
  {
    "from": 122,
    "to": 113
  },
  {
    "from": 122,
    "to": 110
  },
  {
    "from": 122,
    "to": 112
  },
  {
    "from": 122,
    "to": 25
  },
  {
    "from": 122,
    "to": 117
  },
  {
    "from": 123,
    "to": 101
  },
  {
    "from": 123,
    "to": 25
  },
  {
    "from": 123,
    "to": 126
  },
  {
    "from": 123,
    "to": 124
  },
  {
    "from": 124,
    "to": 101
  },
  {
    "from": 124,
    "to": 25
  },
  {
    "from": 124,
    "to": 122
  },
  {
    "from": 125,
    "to": 8
  },
  {
    "from": 126,
    "to": 70
  },
  {
    "from": 126,
    "to": 101
  },
  {
    "from": 126,
    "to": 25
  },
  {
    "from": 126,
    "to": 124
  },
  {
    "from": 126,
    "to": 123
  },
  {
    "from": 126,
    "to": 121
  },
  {
    "from": 126,
    "to": 144
  },
  {
    "from": 126,
    "to": 122
  },
  {
    "from": 127,
    "to": 126
  },
  {
    "from": 127,
    "to": 122
  },
  {
    "from": 128,
    "to": 25
  },
  {
    "from": 128,
    "to": 123
  },
  {
    "from": 129,
    "to": 112
  },
  {
    "from": 129,
    "to": 104
  },
  {
    "from": 129,
    "to": 58
  },
  {
    "from": 129,
    "to": 122
  },
  {
    "from": 129,
    "to": 110
  },
  {
    "from": 130,
    "to": 25
  },
  {
    "from": 130,
    "to": 145
  },
  {
    "from": 130,
    "to": 101
  },
  {
    "from": 130,
    "to": 113
  },
  {
    "from": 130,
    "to": 48
  },
  {
    "from": 130,
    "to": 143
  },
  {
    "from": 130,
    "to": 70
  },
  {
    "from": 130,
    "to": 59
  },
  {
    "from": 130,
    "to": 131
  },
  {
    "from": 130,
    "to": 105
  },
  {
    "from": 130,
    "to": 122
  },
  {
    "from": 130,
    "to": 117
  },
  {
    "from": 130,
    "to": 110
  },
  {
    "from": 130,
    "to": 19
  },
  {
    "from": 131,
    "to": 126
  },
  {
    "from": 131,
    "to": 122
  },
  {
    "from": 131,
    "to": 101
  },
  {
    "from": 131,
    "to": 151
  },
  {
    "from": 131,
    "to": 130
  },
  {
    "from": 131,
    "to": 59
  },
  {
    "from": 131,
    "to": 97
  },
  {
    "from": 131,
    "to": 31
  },
  {
    "from": 132,
    "to": 130
  },
  {
    "from": 132,
    "to": 70
  },
  {
    "from": 133,
    "to": 61
  },
  {
    "from": 133,
    "to": 149
  },
  {
    "from": 133,
    "to": 35
  },
  {
    "from": 134,
    "to": 149
  },
  {
    "from": 134,
    "to": 10
  },
  {
    "from": 135,
    "to": 112
  },
  {
    "from": 135,
    "to": 104
  },
  {
    "from": 135,
    "to": 101
  },
  {
    "from": 135,
    "to": 105
  },
  {
    "from": 135,
    "to": 122
  },
  {
    "from": 135,
    "to": 110
  },
  {
    "from": 136,
    "to": 25
  },
  {
    "from": 136,
    "to": 143
  },
  {
    "from": 136,
    "to": 113
  },
  {
    "from": 136,
    "to": 122
  },
  {
    "from": 136,
    "to": 117
  },
  {
    "from": 137,
    "to": 90
  },
  {
    "from": 137,
    "to": 101
  },
  {
    "from": 137,
    "to": 9
  },
  {
    "from": 137,
    "to": 112
  },
  {
    "from": 137,
    "to": 122
  },
  {
    "from": 137,
    "to": 110
  },
  {
    "from": 137,
    "to": 116
  },
  {
    "from": 138,
    "to": 90
  },
  {
    "from": 138,
    "to": 101
  },
  {
    "from": 138,
    "to": 9
  },
  {
    "from": 138,
    "to": 67
  },
  {
    "from": 138,
    "to": 116
  },
  {
    "from": 138,
    "to": 122
  },
  {
    "from": 138,
    "to": 117
  },
  {
    "from": 139,
    "to": 111
  },
  {
    "from": 139,
    "to": 117
  },
  {
    "from": 139,
    "to": 112
  },
  {
    "from": 140,
    "to": 90
  },
  {
    "from": 140,
    "to": 101
  },
  {
    "from": 140,
    "to": 9
  },
  {
    "from": 140,
    "to": 58
  },
  {
    "from": 140,
    "to": 112
  },
  {
    "from": 140,
    "to": 111
  },
  {
    "from": 140,
    "to": 122
  },
  {
    "from": 140,
    "to": 141
  },
  {
    "from": 140,
    "to": 116
  },
  {
    "from": 141,
    "to": 122
  },
  {
    "from": 141,
    "to": 112
  },
  {
    "from": 141,
    "to": 101
  },
  {
    "from": 141,
    "to": 105
  },
  {
    "from": 141,
    "to": 104
  },
  {
    "from": 141,
    "to": 110
  },
  {
    "from": 142,
    "to": 122
  },
  {
    "from": 142,
    "to": 111
  },
  {
    "from": 142,
    "to": 112
  },
  {
    "from": 142,
    "to": 101
  },
  {
    "from": 142,
    "to": 9
  },
  {
    "from": 142,
    "to": 105
  },
  {
    "from": 143,
    "to": 114
  },
  {
    "from": 143,
    "to": 122
  },
  {
    "from": 143,
    "to": 117
  },
  {
    "from": 144,
    "to": 25
  },
  {
    "from": 144,
    "to": 101
  },
  {
    "from": 144,
    "to": 108
  },
  {
    "from": 144,
    "to": 104
  },
  {
    "from": 144,
    "to": 37
  },
  {
    "from": 144,
    "to": 112
  },
  {
    "from": 144,
    "to": 30
  },
  {
    "from": 144,
    "to": 9
  },
  {
    "from": 144,
    "to": 110
  },
  {
    "from": 145,
    "to": 146
  },
  {
    "from": 145,
    "to": 25
  },
  {
    "from": 145,
    "to": 105
  },
  {
    "from": 146,
    "to": 105
  },
  {
    "from": 146,
    "to": 101
  },
  {
    "from": 146,
    "to": 25
  },
  {
    "from": 147,
    "to": 148
  },
  {
    "from": 147,
    "to": 122
  },
  {
    "from": 147,
    "to": 87
  },
  {
    "from": 148,
    "to": 110
  },
  {
    "from": 148,
    "to": 49
  },
  {
    "from": 148,
    "to": 147
  },
  {
    "from": 148,
    "to": 122
  },
  {
    "from": 150,
    "to": 7
  },
  {
    "from": 150,
    "to": 110
  },
  {
    "from": 151,
    "to": 101
  },
  {
    "from": 151,
    "to": 9
  },
  {
    "from": 151,
    "to": 61
  },
  {
    "from": 151,
    "to": 111
  },
  {
    "from": 151,
    "to": 122
  },
  {
    "from": 151,
    "to": 108
  },
  {
    "from": 151,
    "to": 110
  },
  {
    "from": 152,
    "to": 15
  },
  {
    "from": 152,
    "to": 25
  },
  {
    "from": 152,
    "to": 55
  },
  {
    "from": 152,
    "to": 97
  },
  {
    "from": 152,
    "to": 78
  },
  {
    "from": 152,
    "to": 122
  },
  {
    "from": 152,
    "to": 105
  },
  {
    "from": 152,
    "to": 46
  },
  {
    "from": 153,
    "to": 4
  },
  {
    "from": 154,
    "to": 122
  },
  {
    "from": 154,
    "to": 117
  },
  {
    "from": 155,
    "to": 8
  },
  {
    "from": 155,
    "to": 122
  },
  {
    "from": 155,
    "to": 105
  },
  {
    "from": 155,
    "to": 112
  },
  {
    "from": 155,
    "to": 71
  },
  {
    "from": 155,
    "to": 110
  },
  {
    "from": 156,
    "to": 114
  },
  {
    "from": 156,
    "to": 151
  },
  {
    "from": 157,
    "to": 44
  },
  {
    "from": 157,
    "to": 25
  },
  {
    "from": 157,
    "to": 107
  },
  {
    "from": 157,
    "to": 114
  },
  {
    "from": 157,
    "to": 101
  },
  {
    "from": 157,
    "to": 112
  },
  {
    "from": 157,
    "to": 122
  },
  {
    "from": 157,
    "to": 105
  },
  {
    "from": 157,
    "to": 9
  },
  {
    "from": 158,
    "to": 113
  },
  {
    "from": 158,
    "to": 25
  },
  {
    "from": 158,
    "to": 122
  },
  {
    "from": 158,
    "to": 49
  },
  {
    "from": 159,
    "to": 49
  },
  {
    "from": 159,
    "to": 117
  },
  {
    "from": 160,
    "to": 101
  },
  {
    "from": 160,
    "to": 114
  },
  {
    "from": 160,
    "to": 124
  },
  {
    "from": 160,
    "to": 126
  },
  {
    "from": 160,
    "to": 123
  },
  {
    "from": 161,
    "to": 73
  },
  {
    "from": 161,
    "to": 12
  },
  {
    "from": 161,
    "to": 11
  },
  {
    "from": 163,
    "to": 8
  }
]);
      const container = document.getElementById("mynetwork");
      const data = { nodes: nodes, edges: edges };
      const options = {
        nodes: {
          shape: "dot",
          size: 20,
          font: { size: 14, color: "#000" }
        },
        edges: {
          arrows: "to",
          color: "gray",
          smooth: true
        },
        physics: {
          enabled: true,
          solver: "forceAtlas2Based",
          stabilization: {
            enabled: true,
            iterations: 200,
            fit: true
          }
        },
        interaction: {
          navigationButtons: true,
          keyboard: true,
          zoomView: true,
          dragView: true
        }
      };
      const network = new vis.Network(container, data, options);
    </script>
  </body>
</html>
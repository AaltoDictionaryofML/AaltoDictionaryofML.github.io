<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Glossary Network</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css" rel="stylesheet" />
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js"></script>
    <script type="text/javascript" src="https://unpkg.com/vis-network/standalone/umd/vis-network.min.js"></script>
    <style>
      #mynetwork {
        width: 100%;
        height: 1000px;
        background-color: #ffffff;
        border: 1px solid lightgray;
      }
    </style>
  </head>
  <body>
    <center><h1>The Aalto Dictionary of Machine Learning</h1></center>
    <div id="mynetwork"></div>
    <script type="text/javascript">
      const nodes = new vis.DataSet([
  {
    "id": 1,
    "label": "pseudoinverse",
    "title": "The Moore\u2013Penrose pseudoinverse of a generalizes the notion of an . The pseudoinverse arises naturally within when applied to a with arbitrary {label} and a {hastie01statisticallearning}. The learned by are given by \\[ {}^{()} = (\\,^{T} + )^{-1} ^ , > 0. \\] We can then define the pseudoinverse via the limit {benisrael2003generalized} \\[ _{ 0^+} {}^{()} = ^+ . \\] \\\\ See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 2,
    "label": "inverse matrix",
    "title": "An inverse is defined for a square that is of full rank, meaning its columns are linearly independent. In this case, is said to be invertible, and its inverse satisfies \\[ ^{-1} = ^{-1} = . \\] A square is invertible if and only if its is non-zero. Inverse {matrix} are fundamental in solving systems of linear equations and in the closed-form solution of , . The concept of an inverse can be extended to {matrix} that are not square or not full-rank. One may define a ``left inverse'' satisfying or a ``right inverse'' satisfying . For general rectangular or singular {matrix}, the Moore\u2013Penrose provides a unified concept of a generalized inverse . {figure}[H] {tikzpicture}[x=2cm,y=2cm] {scope} (0,0) -- (1,0) node[below right] {}; (0,0) -- (0,1) node[above left] {}; {scope} {scope}[shift={(2.0,0)}] (A) at (1.5,0.5); (B) at (-0.2,1.2); (0,0) -- (A) node[pos=0.5, below right] {}; (0,0) -- (B) node[above right] {}; {scope} {scope}[shift={(4.9,0)}] (0,0) -- (1,0) node[pos=0.5, below] {}; (0,0) -- (0,1) node[above] {}; {scope} (1.2,0.4) to node[above] {} (1.8,0.4); (3.8,0.4) to node[below] {} (4.4,0.4); {tikzpicture} {A represents a linear transformation of . The inverse represents the inverse transformation. {fig_matrix_inverse_dict}} {figure} See also: , , , .",
    "color": "lightblue"
  },
  {
    "id": 3,
    "label": "matrix",
    "title": "A matrix of size is a two-dimensional array of numbers, which is denoted = {bmatrix} A_{1,1} & A_{1,2} & & A_{1,} \\\\ A_{2,1} & A_{2,2} & & A_{2,} \\\\ & & & \\\\ A_{,1} & A_{,2} & & A_{,} {bmatrix} {R}^{ }. Here, denotes the matrix entry in the -th row and the -th column. Matrices are useful representations of various mathematical objects , including the following: {itemize} Systems of linear equations: We can use a matrix to represent a system of linear equations {pmatrix} A_{1,1} & A_{1,2} \\\\ A_{2,1} & A_{2,2} {pmatrix} {pmatrix} w_1 \\\\ w_2 {pmatrix} ={pmatrix} y_1 \\\\ y_2 {pmatrix} { compactly as } = . One important example of systems of linear equations are the optimality condition for the within . {linearmap}: Consider a -dimensional and a -dimensional . If we fix a basis for and a basis for , each matrix naturally defines a such that ^{()} _{=1}^{} A_{,} ^{()}. {dataset}: We can use a matrix to represent a . Each row corresponds to a single , and each column corresponds to a specific or of a . {itemize} {figure}[H] {center} {tikzpicture}[x=2cm] {scope} (0,0) -- (1,0) node[below] {}; (0,0) -- (0,1) node[above] {}; {scope} {scope}[shift={(3.2,0)}] (0,0) -- (1,0) node[below] {}; (0,0) -- (0,1) node[above] {}; (A) at (0.2,-1.0); (B) at (0.4,1.2); (0,0) -- (A) node[below,right] {}; (0,0) -- (B) node[right,xshift=1pt] {}; {scope} (1.6,0.5) to[bend left] node[midway, above] {} (2.7,0.5); {tikzpicture} {center} {A matrix defines a between two {vectorspace}.} {figure} See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 4,
    "label": "determinant",
    "title": "The determinant of a square is a of its columns , that is {itemize} Normalized: () = 1 Multi-linear: {align} (^{(1)},,+ ,,^{()} ) & = (^{(1)},,,,^{()} ) \\\\ & + (^{(1)},,,,^{()} ) {align} Anti-symmetric: (,^{()}, , ^{(')}, ) = - (,^{(')}, , ^{()}, ). {itemize} We can interpret a as a linear transformation on . The determinant characterizes how (the orientation of) volumes in are altered by this transformation , . In particular, preserves orientation, reverses orientation, and collapses volume entirely, indicating that is non-invertible. The determinant also satisfies , and if is diagonalizable with {eigenvalue} , then . For the special cases (2D) and (3D), the determinant can be interpreted as an oriented area or volume spanned by the column {vector} of . {figure}[H] {center} {tikzpicture}[x=2cm] {scope} (0,0) -- (1,0) node[below right] {}; (0,0) -- (0,1) node[above left] {}; {scope} {scope}[shift={(2.8,0)}] (A) at (1.5,0.5); (B) at (-0.2,1.2); (0,0) -- (A) node[below right] {}; (0,0) -- (B) node[above left] {}; (0,0) -- (A) -- () -- (B) -- cycle; (A) -- (); (B) -- (); at (0.8,0.6) { }; (0.4,0.0) arc[start angle=0, end angle=35, radius=0.6]; {scope} (1.3,0.5) -- (2.4,0.5) node[midway, above] {}; {tikzpicture} {center} {We can interpret a square as a linear transformation of into itself. The determinant characterizes how this transformation alters an oriented volume.} {figure} See also: , .",
    "color": "lightblue"
  },
  {
    "id": 5,
    "label": "linear map",
    "title": "A linear is a that satisfies additivity, i.e., , and homogeneity, i.e., , for all {vector} and scalars . In particular, . Any linear can be represented as a multiplication for some . The collection of real-valued linear {map} for a given dimension constitute a which is used in many methods. \\\\ See also: , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 6,
    "label": "vector",
    "title": "A vector is an element of a . In the context of , a particularly important example of a is the , where is the (finite) dimension of the space. A vector can be represented as a list or one-dimensional array of real numbers, i.e., with for . The value is the -th entry of the vector . It can also be useful to view a vector as a that maps each index to a value , i.e., . This perspective is particularly useful for the study of {kernelmethod}. {figure}[H] {minipage}[c]{0.48} 2, -1, 3, 0, -2, 1 {minipage}{} {5ex} { (a)} {minipage} {minipage} {minipage}{0.48} {tikzpicture} {axis}[ width=6.5cm, height=5cm, title={}, xlabel={index }, ylabel={}, ymin=-3.5, ymax=3.5, xmin=0.5, xmax=6.5, xtick={1,2,3,4,5,6}, ytick={-3,-2,-1,0,1,2,3}, axis x line=bottom, axis y line=left, grid=both, major grid style={dotted, gray!60}, enlargelimits=0.1 ] +[ycomb, thick, mark=*] coordinates { (1,2) (2,-1) (3,3) (4,0) (5,-2) (6,1) }; {axis} at (2,-2.5) {(b)}; {tikzpicture} {minipage} {Two equivalent views of a vector . (a) As a numeric array. (b) As a .} {fig:vector-function-dual_dict} {figure} See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 7,
    "label": "vector space",
    "title": "A space (also called linear space) is a collection of elements (called {vector}) closed under addition and scalar multiplication, i.e., {itemize} If , then . If and , then . In particular, . {itemize} The is a space. {linmodel} and {linearmap} operate within such spaces. \\\\ See also: , , , .",
    "color": "lightblue"
  },
  {
    "id": 8,
    "label": "stochastic",
    "title": "We refer to a method as stochastic if it involves a random component or is governed by probabilistic laws. methods use randomness to reduce computational complexity (e.g., see ) or to capture in {probmodel}. \\\\ See also: , , .",
    "color": "salmon"
  },
  {
    "id": 9,
    "label": "stochastic process",
    "title": "A process is a collection of {rv} defined on a common , indexed by some set . The index set typically represents time or space, allowing to represent random phenomena that evolve across time or spatial dimensions\u2014for example, sensor noise or financial time series. Stochastic processes are not limited to temporal or spatial settings. For instance, random {graph} such as the or the can also be viewed as stochastic processes. Here, the index set consists of node pairs that index {rv} whose values encode the presence or weight of an edge between two nodes. Moreover, processes naturally arise in the analysis of {stochalgorithm}, such as , which construct a sequence of {rv}. \\\\ See also: , , , , .",
    "color": "salmon"
  },
  {
    "id": 10,
    "label": "characteristic function",
    "title": "The characteristic of a real-valued is the {BillingsleyProbMeasure} _{x}(t) { \\,(j t x) } { with } j = {-1}. The characteristic uniquely determines the of . \\\\ See also: , .",
    "color": "salmon"
  },
  {
    "id": 11,
    "label": "entropy",
    "title": "Entropy quantifies the or unpredictability associated with an . For a discrete taking on values in a finite set with a mass , the entropy is defined as \\[ H(x) -_{i=1}^n p_i p_i. \\] Entropy is maximized when all outcomes are equally likely, and minimized (i.e., zero) when the outcome is deterministic. A of the concept of entropy for continuous {rv} is . \\\\ See also: , .",
    "color": "salmon"
  },
  {
    "id": 12,
    "label": "differential entropy",
    "title": "For a real-valued with a , the differential is defined as \\[ h() - p() p() \\, d. \\] Differential can be negative and lacks some properties of for discrete-valued {rv}, such as invariance under a change of variables . Among all {rv} with a given and , is maximized by . \\\\ See also: , .",
    "color": "salmon"
  },
  {
    "id": 13,
    "label": "minimum",
    "title": "Given a set of real numbers, the minimum is the smallest of those numbers. Note that for some sets, such as the set of negative real numbers, the minimum does not exist.",
    "color": "violet"
  },
  {
    "id": 14,
    "label": "function",
    "title": "A function between two sets and assigns each element exactly one element . We write this as , where is the domain and the co-domain of . That is, a function defines a unique output for every input .",
    "color": "lightblue"
  },
  {
    "id": 15,
    "label": "map",
    "title": "We use the term map as a synonym for . \\\\ See also: .",
    "color": "lightgreen"
  },
  {
    "id": 16,
    "label": "optimization problem",
    "title": "An optimization problem is a mathematical structure consisting of an defined over an optimization variable , together with a feasible set . The co-domain is assumed to be ordered, meaning that for any two elements , we can determine whether , , or . The goal of optimization is to find those values for which the objective is extremal\u2014i.e., minimal or maximal , , . \\\\ See also: .",
    "color": "lightblue"
  },
  {
    "id": 17,
    "label": "optimization method",
    "title": "An optimization method is an that reads in a representation of an and delivers an (approximate) solution as its output , , . \\\\ See also: , .",
    "color": "lightblue"
  },
  {
    "id": 18,
    "label": "fixed-point iteration",
    "title": "A fixed-point iteration is an iterative method for solving a given . It constructs a sequence by repeatedly applying an operator , i.e., {equation} {equ_def_fixed_point_dict} ^{(+1)} = ^{()} {, for } =0, 1, . {equation} The operator is chosen such that any of its fixed points is a solution to the given . For example, given a and , the fixed points of the operator coincide with the minimizers of . In general, for a given with solution , there are many different operators whose fixed points are . Clearly, we should use an operator in {equ_def_fixed_point_dict} that reduces the distance to a solution such that {equation} {{ ^{(+1)} - {}}{2}}_{{{equ_def_fixed_point_dict}}{=} { ^{()} - {}}{2}} { ^{()} - {}}{2}. {equation} Thus, we require to be at least non-expansive, i.e., the iteration {equ_def_fixed_point_dict} should not result in worse that have a larger distance to a solution . Furthermore, each iteration {equ_def_fixed_point_dict} should also make some progress, i.e., reduce the distance to a solution . This requirement can be made precise using the notion of a , . The operator is a if, for some , {equation} { \\!-\\! '}{2} {\\!-\\!'}{2} { holds for any } ,'. {equation} For a , the fixed-point iteration {equ_def_fixed_point_dict} generates a sequence that converges quite rapidly. In particular {RudinBookPrinciplesMatheAnalysis}, {equation} { ^{()} - {}}{2} ^{} { ^{(0)} - {}}{2}. {equation} Here, is the distance between the initialization and the solution . It turns out that a fixed-point iteration {equ_def_fixed_point_dict} with a firmly non-expansive operator is guaranteed to converge to a fixed-point of {Bauschke:2017}. Fig. {fig_examples_nonexp_dict} depicts examples of a firmly non-expansive operator, a non-expansive operator, and a . All these operators are defined on the one-dimensional space . Another example of a firmly non-expansive operator is the of a , . {darkgreen}{rgb}{0.0, 0.5, 0.0} {figure}[H] {center} {tikzpicture}[scale=1.5] (-2,0) -- (2,0) node[right] {}; (0,-2) -- (0,2) node[above] {}; at (2.1,2.2) {}; at (1.9,-1.5) {}; at (1.5,1.2) {}; (1,-2) -- (1,2); (-2,1) -- (2,1); (-2,-1) -- (2,-1); (-1,-2) -- (-1,2); at (1,0) {}; at (0,-1) {}; plot(,{0.5* + 1}); plot(,{-}); plot(,{-1}); plot(,{}); plot(,{1}); {tikzpicture} {center} {Example of a non-expansive operator , a firmly non-expansive operator , and a . {fig_examples_nonexp_dict}} {figure} See also: , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 19,
    "label": "Erd\\H{o}s-R\\'enyi graph (ER graph)",
    "title": "An ER is a for {graph} defined over a given node set . One way to define the ER is via the collection of binary {rv} , for each pair of different nodes . A specific of an ER contains an edge if and only if . The ER is parameterized by the number of nodes and the . \\\\ See also: , , , , , .",
    "color": "salmon"
  },
  {
    "id": 20,
    "label": "attack",
    "title": "An attack on an system refers to an intentional action\u2014either active or passive\u2014that compromises the system's integrity, availability, or confidentiality. Active attacks involve perturbing components such as {dataset} (via ) or communication links between {device} within an application. Passive attacks, such as {privattack}, aim to infer {sensattr} without modifying the system. Depending on their goal, we distinguish between {dosattack}, attacks, and {privattack}. \\\\ See also: , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 21,
    "label": "privacy attack",
    "title": "A privacy on an system aims to infer {sensattr} of individuals by exploiting partial access to a trained . One form of a privacy is .\\\\ See also: , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 22,
    "label": "epigraph",
    "title": "The epigraph of a real-valued is the set of points lying on or above its , i.e., \\[ {epi}(f) = \\{ ({x}, t) {R}^n {R} \\,|\\, f({x}) t \\}. \\] A is if and only if its epigraph is a set , . {figure}[H] {tikzpicture}[scale=1.0] {axis}[ axis lines = middle, xlabel = , ylabel = {}, xmin=-2, xmax=2, ymin=0, ymax=4.5, samples=100, domain=-1.5:1.5, thick, width=8cm, height=6cm, grid=none, axis on top, ] [blue, thick, domain=-1.5:1.5] {x^2} node [pos=0.85, anchor=south west, xshift=5pt] {}; [ name path=f, draw=none, ytick=, domain=-1.5:1.5, ] {x^2}; (axis cs:-1.5,4) -- (axis cs:1.5,4); [ blue!20, opacity=0.6, draw=none, ] fill between [ of=f and top, soft clip={domain=-1.5:1.5}, ]; at (axis cs:-1.0,2.3) {}; {axis} {tikzpicture} {Epigraph of the (i.e., shaded area).} {figure} See also: , .",
    "color": "lightblue"
  },
  {
    "id": 23,
    "label": "nullspace",
    "title": "The nullspace of a , denoted , is the set of all {vector} such that {n} = {0}. Consider a method that uses the to transform a of a into a new . The nullspace characterizes all directions in the original along which the transformation remains unchanged. In other words, adding any from the nullspace to a does not affect the transformed representation . This property can be exploited to enforce invariances in the {prediction} (computed from ). Fig.\\ {fig:nullspace-rotation-dict} illustrates one such invariance. It shows rotated versions of two handwritten digits, which approximately lie along one-dimensional curves in the original . These curves are aligned with a direction . To ensure that the trained is invariant to such rotations, we can choose the transformation such that . This ensures that , and hence the resulting , is approximately insensitive to rotation of the input image. {figure}[h] {assets/pythonsnacks/nullspace_0_1.png} {Rotated images of two handwritten digits. The rotations are approximately aligned along linear curves that are parallel to the .{fig:nullspace-rotation-dict}} {figure} See also: . \\\\ Python demo: {https://github.com/AaltoDictionaryofML/AaltoDictionaryofML.github.io/blob/main/assets/pythonsnacks/nullspace.py}{click me}",
    "color": "lightblue"
  },
  {
    "id": 24,
    "label": "maximum",
    "title": "The maximum of a set of real numbers is the greatest element in that set, if such an element exists. A set has a maximum if it is bounded above and attains its {RudinBookPrinciplesMatheAnalysis}. \\\\ See also: .",
    "color": "khaki"
  },
  {
    "id": 25,
    "label": "supremum (or least upper bound)",
    "title": "The supremum of a set of real numbers is the smallest number that is greater than or equal to every element in the set. More formally, a real number is the supremum of a set if: 1) is an upper bound of ; and 2) no number smaller than is an upper bound of . Every non-empty set of real numbers that is bounded above has a supremum, even if it does not contain its supremum as an element {RudinBookPrinciplesMatheAnalysis}.",
    "color": "khaki"
  },
  {
    "id": 26,
    "label": "discrepancy",
    "title": "Consider an application with represented by an . methods use a discrepancy measure to compare {map} from {localmodel} at nodes , connected by an edge in the . \\\\ See also: , , .",
    "color": "orange"
  },
  {
    "id": 27,
    "label": "FedRelax",
    "title": "An . \\\\ See also: , .",
    "color": "orange"
  },
  {
    "id": 28,
    "label": "FedAvg",
    "title": "FedAvg refers to a family of iterative {algorithm}. It uses a server-client setting and alternates between client-wise {localmodel} re-training, followed by the aggregation of updated at the server . The local update at client at time starts from the current provided by the server and typically amounts to executing few iterations of . After completing the local updates, they are aggregated by the server (e.g., by averaging them). Fig. {fig_single_iteration_fedavg_dict} illustrates the execution of a single iteration of FedAvg. {figure}[H] {center} {tikzpicture}[>=Stealth, node distance=1cm and 1.5cm, every node/.style={font=}] {server} = [circle, fill=black, minimum size=6pt, inner sep=0pt] {client} = [circle, draw=black, minimum size=6pt, inner sep=0pt] (label1) at (0,3.5) {broadcast}; (label2) {local update}; (label3) {aggregate}; (s1) at (label1 |- 0,2.5) {}; (c1l) at () {}; (c1r) at () {}; (dots1) at () {}; (s1) -- (c1l) node[midway,left] {}; (s1) -- (c1r) node[midway,right] {}; (s1) -- (dots1); (s2) at (label2 |- 0,2.5) {}; (c2l) at () {}; (c2r) at () {}; (dots2) at () {}; {}; {}; (s3) at (label3 |- 0,2.5) {}; {}; (c3l) at () {}; (c3r) at () {}; (dots3) at () {}; (c3l) -- (s3) node[midway,left] {}; (c3r) -- (s3) node[midway,right] {}; (dots3) -- (s3); {tikzpicture} {center} {Illustration of a single iteration of FedAvg, which consists of broadcasting by the server, performing local updates at clients, and aggregating the updates by the server. {fig_single_iteration_fedavg_dict}} {figure} See also: , , , .",
    "color": "orange"
  },
  {
    "id": 29,
    "label": "FedGD",
    "title": "An that can be implemented as message passing across an . \\\\ See also: , , , , .",
    "color": "orange"
  },
  {
    "id": 30,
    "label": "FedSGD",
    "title": "An that can be implemented as message passing across an . \\\\ See also: , , , , , .",
    "color": "orange"
  },
  {
    "id": 31,
    "label": "horizontal federated learning (HFL)",
    "title": "HFL uses {localdataset} constitut\\-ed by different {datapoint} but uses the same {feature} to characterize them . For example, weather forecasting uses a network of spatially distributed weather (observation) stations. Each weather station measures the same quantities, such as daily temperature, air pressure, and precipitation. However, different weather stations measure the characteristics or {feature} of different spatiotemporal regions. Each spatiotemporal region represents an individual , each characterized by the same {feature} (e.g., daily temperature or air pressure).\\\\ See also: , , .",
    "color": "orange"
  },
  {
    "id": 32,
    "label": "dimensionality reduction",
    "title": "Dimensionality reduction refers to methods that learn a transformation of a (typically large) set of raw {feature} into a smaller set of informative {feature} . Using a smaller set of {feature} is beneficial in several ways: {itemize} {Statistical benefit:} It typically reduces the risk of , as reducing the number of {feature} often reduces the of a . {Computational benefit:} Using fewer {feature} means less computation for the training of {model}. As a case in point, methods need to invert a whose size is determined by the number of {feature}. {Visualization:} Dimensionality reduction is also instrumental for visualization. For example, we can learn a transformation that delivers two {feature} which we can use, in turn, as the coordinates of a . Fig.\\ {fig:dimred-scatter_dict} depicts the of hand-written digits that are placed using transformed {feature}. Here, the {datapoint} are naturally represented by a large number of grayscale values (one value for each pixel). {itemize} {figure}[H] {tikzpicture}[scale=1] (-0.5,0) -- (5.5,0) node[right] {}; (0,-0.5) -- (0,4.5) node[above] {}; // in { 1.2/0.5/3, 0.8/2.0/8, 2.5/1.8/1, 3.8/3.5/6, 4.2/0.7/9, 2.8/3.0/7, 1.5/3.8/2 }{ at (,) {}; } {tikzpicture} {Example of dimensionality reduction: High-dimensional image data (e.g., high-resolution images of hand-written digits) embedded into 2D using learned {feature} and visualized in a .} {fig:dimred-scatter_dict} {figure} See also: , , , .",
    "color": "khaki"
  },
  {
    "id": 33,
    "label": "machine learning (ML)",
    "title": "ML aims to predict a from the {feature} of a . ML methods achieve this by learning a from a (or ) through the minimization of a , . One precise formulation of this principle is . Different ML methods are obtained from different design choices for {datapoint} (i.e., their {feature} and ), the , and the {MLBasics}. \\\\ See also: , , .",
    "color": "lightgreen"
  },
  {
    "id": 34,
    "label": "reinforcement learning (RL)",
    "title": "RL refers to a setting where we can only evaluate the usefulness of a single (i.e., a choice of {parameter}) at each time step . In particular, RL methods apply the current to the of the newly received . The usefulness of the resulting is quantified by a signal . {figure} {center} {tikzpicture}[scale=1] (-2, 0) -- (6, 0); at (6.3, 0) {}; plot (-3, {-0.2*()^2 + 2}); at (0-3, {-0.2*(0)^2 + 2}) {}; (1.5-3, {-0.2*(1.5)^2 + 2}) circle (2pt); at (1.5-3, -0.3) {}; (1.5-3, 0) -- (1.5-3, {-0.2*(1.5)^2 + 2}); plot (, {-0.15*( - 2)^2 + 3}); at (3, {-0.15*(3 - 2)^2 + 3}) {}; (2, {-0.15*(2 - 2)^2 + 3}) circle (2pt); at (2, -0.3) {}; (2, 0) -- (2, {-0.15*(3 - 2)^2 + 3}); plot (+2, {-0.1*( - 4)^2 + 1.5}); at (4.5+2, {-0.1*(4.5 - 4)^2 + 1.5}) {}; (3.5+2, {-0.1*(3.5 - 4)^2 + 1.5}) circle (2pt); at (3.5+2, -0.3) {}; (3.5+2, 0) -- (3.5+2, {-0.1*(3.5 - 4)^2 + 1.5}); {tikzpicture} {Three consecutive time steps with corresponding {lossfunc} . During time step , a RL method can evaluate the only for one specific , resulting in the signal .} {center} {figure} In general, the depends also on the previous {prediction} for . The goal of RL is to learn , for each time step , such that the (possibly discounted) cumulative is maximized , . \\\\ See also: , , .",
    "color": "lightgreen"
  },
  {
    "id": 35,
    "label": "feature learning",
    "title": "Consider an application with {datapoint} characterized by raw {feature} . learning refers to the task of learning a : ': ' that reads in the {feature} of a and delivers new {feature} from a new . Different learning methods are obtained for different design choices of , for a of potential {map} , and for a quantitative measure of the usefulness of a specific . For example, uses , with , and a \\{ : {R}^{} \\!\\! {R}^{'}\\!:\\!'\\!\\! { with some } \\!\\! {R}^{' \\! } \\}. measures the usefulness of a specific by the linear reconstruction error incurred on a such that _{ {R}^{ \\!\\!\\! '}} _{=1}^{} { ^{()} - ^{()}}{2}^{2}. \\\\ See also: , , , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 36,
    "label": "autoencoder",
    "title": "An autoencoder is an method that simultaneously learns an encoder and a decoder . It is an instance of using a computed from the reconstruction error . \\\\ See also: , , , .",
    "color": "lightgreen"
  },
  {
    "id": 37,
    "label": "vertical federated learning (VFL)",
    "title": "VFL refers to applications where {device} have access to different {feature} of the same set of {datapoint} . Formally, the underlying global is \\[ ^{({global})} \\{ (^{(1)}, ^{(1)}), , (^{()}, ^{()}) \\}. \\] We denote by , for , the complete {featurevec} for the {datapoint}. Each observes only a subset of {feature}, resulting in a with {featurevec} \\[ ^{(,)} = ( ^{()}_{_{1}}, , ^{()}_{_{}} )\\,^{T}. \\] Some of the {device} might also have access to the {label} , for , of the global . One potential application of VFL is to enable collaboration between different healthcare providers. Each provider collects distinct types of measurements\u2014such as blood values, electrocardiography, and lung X-rays\u2014for the same patients. Another application is a national social insurance system, where health records, financial indicators, consumer behavior, and mobility are collected by different institutions. VFL enables joint learning across these parties while allowing well-defined levels of . {figure}[H] {center} {tikzpicture}[every node/.style={anchor=base}] {0} {1.6} {3.2} {4.8} {6.4} {0} {-1.2} {-2.4} {-3.6} / in {1/1, 2/2, 4/} { {}{-1.2*(-1)} (x1) at (0,) {}; (x2) at (1.6,) {}; (dots) at (3.2,) {}; (x3) at (4.8,) {}; (y) at (6.4,) {}; } (-0.6,0.6) rectangle (6.9,-4.2); at (3.1,0.9) {}; (-0.9,0.9) rectangle (2.1,-4.0); at (0.25,1.0) {}; () rectangle (); at () {}; {tikzpicture} {center} {VFL uses {localdataset} that are derived from the {datapoint} of a common global . The {localdataset} differ in the choice of {feature} used to characterize the {datapoint}.{fig_vertical_FL_dict}} {figure} See also: , .",
    "color": "orange"
  },
  {
    "id": 38,
    "label": "interpretability",
    "title": "An method is interpretable for a human user if they can comprehend the decision process of the method. One approach to develop a precise definition of interpretability is via the concept of simulatability, i.e., the ability of a human to mentally simulate the behavior , , , , . The idea is as follows: If a human user understands an method, then they should be able to anticipate its {prediction} on a . We illustrate such a in Fig. {fig_aug_simulatability_dict}, which also depicts two learned {hypothesis} and . The method producing the is interpretable to a human user familiar with the concept of a . Since corresponds to a , the user can anticipate the {prediction} of on the . In contrast, the method delivering is not interpretable, because its behavior is no longer aligned with the user\u2019s {expectation}. {figure}[H] {center} {tikzpicture}[x=1.5cm, y=1cm] {0.4} {2.0} (0,0.5) -- (7.7,0.5) node[below, xshift=-1cm] {}; (0.5,0) -- (0.5,4.2) node[above] {}; plot ({},{ + }); plot ({},{ + -(-4)*0.5}); at (7.2, {7.2 + }) {}; at (7.2, {7.2 + - 0.5*(7.2 - 4)}) {}; /// in { 1.2/1.0/blue/6, 1.4/1.0/blue/6, 1.7/1.0/blue/6, 2.2/3.9/blue/12, 2.6/4.2/blue/12, 3.0/4.4/blue/12 }{ (pt) at (,); at (pt) {}; }, color=, thick] (, { + }) -- (pt); } /// in { 5.7/2.6/red/12, 5.9/2.6/red/12, 6.2/2.6/red/12 }{ (pt) at (,{ + }); at (pt) {}; } (4.2, 1.7) circle (0.1cm) node [black,xshift=0.2cm,anchor=west] { }; (4.2, 1.2) circle (0.1cm) node [black,xshift=0.2cm,anchor=west] { }; {tikzpicture} {We can assess the interpretability of trained {model} and by comparing their {prediction} to pseudo-{label} generated by a human user for . {fig_aug_simulatability_dict}} {center} {figure} The notion of interpretability is closely related to the notion of , as both aim to make methods more understandable for humans. In the context of Fig. {fig_aug_simulatability_dict}, interpretability of an method requires that the human user can anticipate its {prediction} on an arbitrary . This contrasts with , where the user is supported by external {explanation}\u2014such as saliency {map} or reference examples from the \u2014to understand the {prediction} of on a specific . \\\\ See also: , , , .",
    "color": "lightblue"
  },
  {
    "id": 39,
    "label": "multitask learning",
    "title": "Multitask learning aims to leverage relations between different {learningtask}. Consider two {learningtask} obtained from the same of webcam snapshots. The first task is to predict the presence of a human, while the second task is to predict the presence of a car. It might be useful to use the same structure for both tasks and only allow the of the final output layer to be different. \\\\ See also: , , , .",
    "color": "violet"
  },
  {
    "id": 40,
    "label": "learning task",
    "title": "Consider a consisting of multiple {datapoint} . For example, might represent a collection of images in an image database. A learning task is defined by specifying those properties (or attributes) of a that are used as its {feature} and {label}. Given a choice of and , a learning task leads to an instance of and can thus be represented by the associated for . Importantly, multiple distinct learning tasks can be constructed from the same by selecting different sets of {feature} and {label}. {figure}[H] {minipage}[t]{0.95} {assets/CowsAustria.jpg} {An image showing cows grazing in the Austrian countryside.} {5mm} {minipage} {5mm} {minipage}[t]{0.45} Task 1 (): {itemize} {feature}: The RGB values of all image pixels. : The number of cows depicted. {itemize} {minipage} {minipage}[t]{0.45} Task 2 (): {itemize} {feature}: The average green intensity of the image. : Should cows be moved to another location (yes/no)? {itemize} {minipage} {Two learning tasks constructed from a single image . These tasks differ in selection and choice of (i.e., the objective), but are both derived from the same .} {fig:learning_tasks_cows_dict} {figure} Such tasks are inherently related, and solving them jointly, e.g., using methods, is often more effective than treating them independently , , . \\\\ See also: , , , , , .",
    "color": "violet"
  },
  {
    "id": 41,
    "label": "explainability",
    "title": "We define the (subjective) explainability of an method as the level of simulatability of the {prediction} delivered by an system to a human user. Quantitative measures for the (subjective) explainability of a trained can be constructed by comparing its {prediction} with the {prediction} provided by a user on a , . Alternatively, we can use {probmodel} for and measure the explainability of a trained via the conditional (or differential) of its {prediction}, given the user's {prediction} , . \\\\ See also: , .",
    "color": "lightgreen"
  },
  {
    "id": 42,
    "label": "local interpretable model-agnostic explanations (LIME)",
    "title": "Consider a trained (or learned ) , which maps the of a to the . LIME is a technique for explaining the behavior of , locally around a with . The is given in the form of a local approximation of (see Fig. {fig_lime_dict}). This approximation can be obtained by an instance of with a carefully designed . In particular, the consists of {datapoint} with close to and the (pseudo-) . Note that we can use a different for the approximation from the original . For example, we can use a to locally approximate a . Another widely-used choice for is the . {figure}[H] {center} {tikzpicture} {axis}[ axis lines=middle, xlabel={}, ylabel={}, xtick=, ytick=, xmin=0, xmax=6, ymin=0, ymax=6, domain=0:6, samples=100, width=10cm, height=6cm, clip=false ] {2 + sin(deg(x))} node[pos=0.85, above right,yshift=3pt] {}; coordinates {(3,0) (3,6)}; {2 + sin(deg(3))} node[pos=0.9, above] {}; coordinates {(3, {2 + sin(deg(3))})}; at (axis cs:3,-0.3) {}; {axis} {tikzpicture} {center} {To explain a trained , around a given , we can use a local approximation . } {fig_lime_dict} {figure} See also: , , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 43,
    "label": "linear model",
    "title": "Consider an application involving {datapoint}, each represented by a numeric . A linear defines a consisting of all real-valued {linearmap} from to such that {equation} {equ_def_lin_model_hypspace_dict} {} \\{ : {R}^{} {R} () = ^{} { for some } {R}^{} \\}. {equation} Each value of defines a different , corresponding to the number of {feature} used to compute the . The choice of is often guided not only by (e.g., fewer features reduce computation) and (e.g., more features typically reduce and ), but also by . A linear using a small number of well-chosen {feature} is generally considered more interpretable , . The linear is attractive because it can typically be trained using scalable {optmethod} , . Moreover, linear {model} often permit rigorous statistical analysis, including fundamental limits on the achievable . They are also useful for analyzing more complex, non-linear {model} such as {ann}. For instance, a can be viewed as the composition of a \u2014implemented by the input and hidden layers\u2014and a linear in the output layer. Similarly, a can be interpreted as applying a one-hot encoded based on {decisionregion}, followed by a linear that assigns a to each region. More generally, any trained that is at some can be locally approximated by a . Figure~{fig_linapprox_dict} illustrates such a local linear approximation, defined by the . Note that the is only defined where is . To ensure in the context of , one may prefer {model} whose associated is Lipschitz continuous. A classic result in mathematical analysis\u2014Rademacher\u2019s Theorem\u2014states that if is Lipschitz continuous with some constant over an open set , then is almost everywhere in {heinonen2005lectures}. {figure}[H] {center} {tikzpicture}[x=0.5cm] {axis}[ hide axis, xmin=-3, xmax=6, ymin=0, ymax=6, domain=0:6, samples=100, width=10cm, height=6cm, clip=false ] {2 + sin(deg(x))} node[pos=0.5, above right, yshift=3pt] {}; {2 + sin(deg(6)) + cos(deg(6))*(x - 6)} node[pos=0.95, above right] {}; coordinates {(6, {2 + sin(deg(6))})}; coordinates {(6,0) (6,2.4)}; at (axis cs:6, -0.2) {}; {}{-1.5} {}{3} {}{2 + sin(deg())} {}{2 + sin(deg())} coordinates {(, ) (, )}; (axis cs:,) -- (axis cs:,0); (axis cs:,) -- (axis cs:,0); (axis cs:,) -- (axis cs:0,); (axis cs:,) -- (axis cs:0,); (axis cs:,-0.4) -- node[below] {} (axis cs:,-0.4); (axis cs:-2.4,) -- node[left] {} (axis cs:-2.4,); {axis} {-10mm} {tikzpicture} {-5mm} {center} { A trained that is at a point can be locally approximated by a . This local approximation is determined by the .} {fig_linapprox_dict} {figure} See also: , , , , .",
    "color": "lightblue"
  },
  {
    "id": 44,
    "label": "gradient step",
    "title": "Given a real-valued and a , the step updates by adding the scaled negative to obtain the new (see Fig. {fig_basic_GD_step_single_dict}) {equation} {equ_def_gd_basic_dict} {} - f(). {equation} Mathematically, the step is an operator that is paramet\\-rized by the and the . {figure}[H] {center} {tikzpicture}[scale=0.8] (-4,0) grid (4,4); plot (, {(1/4)*}); plot (, {2* - 4}); (4,4) -- node[right] {} (4,2); (4,4) -- node[above] {} (2,4); (4,2) -- node[below] {} (3,2) ; at (-4.1, 4.1) {}; (0pt,2pt) -- (0pt,-2pt) node[below] {}; (0pt,2pt) -- (0pt,-2pt) node[below] {}; (0pt,2pt) -- (0pt,-2pt) node[below] {}; {tikzpicture} {center} {The basic step {equ_def_gd_basic_dict} maps a given to the updated . It defines an operator .} {fig_basic_GD_step_single_dict} {figure} Note that the step {equ_def_gd_basic_dict} optimizes locally\u2014in a whose size is determined by the \u2014a linear approximation to the . A natural of {equ_def_gd_basic_dict} is to locally optimize the itself\u2014instead of its linear approximation\u2014such that {align} {equ_approx_gd_step_dict} {} = _{' {R}^{}} f(')\\!+\\!{1}{}{-'}{2}^2. {align} We intentionally use the same symbol for the in {equ_approx_gd_step_dict} as we used for the in {equ_def_gd_basic_dict}. The larger the we choose in {equ_approx_gd_step_dict}, the more progress the update will make towards reducing the value . Note that, much like the step {equ_def_gd_basic_dict}, the update {equ_approx_gd_step_dict} also defines an operator that is parameterized by the and the . For a , this operator is known as the of . \\\\ See also: , , , , , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 45,
    "label": "contraction operator",
    "title": "An operator is a contraction if, for some , {equation} { \\!-\\! '}{2} {\\!-\\!'}{2} { holds for any } ,' {R}^{}. {equation}",
    "color": "lightblue"
  },
  {
    "id": 46,
    "label": "proximal operator",
    "title": "Given a , we define its proximal operator as , {f()}{}{} _{' {R}^{}} { with } > 0. As illustrated in Fig. {fig_proxoperator_opt_dict}, evaluating the proximal operator amounts to minimizing a penalized variant of . The penalty term is the scaled squared Euclidean distance to a given (which is the input to the proximal operator). The proximal operator can be interpreted as a of the , which is defined for a . Indeed, taking a with at the current is the same as applying the proximal operator of the and using . {figure}[H] {center} {tikzpicture}[scale=0.8] plot (, {(1/4)*}) node[above right] {}; plot (, {2*( - 2)*( - 2)}) node[below right] {}; (0pt,2pt) -- (0pt,-2pt) node[below] {}; {tikzpicture} {center} {The proximal operator updates a by minimizing a penalized version of the . The penalty term is the scaled squared Euclidean distance between the optimization variable and the given . {fig_proxoperator_opt_dict}} {figure} See also: , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 47,
    "label": "proximable",
    "title": "A for which the can be computed efficiently is sometimes referred to as proximable or simple . \\\\ See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 48,
    "label": "connected graph",
    "title": "An undirected is connected if every non-empty subset has at least one edge connecting it to . \\\\ See also: .",
    "color": "orange"
  },
  {
    "id": 49,
    "label": "multivariate normal distribution",
    "title": "The multivariate normal distribution, which is denoted , is a fundamental for numerical {featurevec} of fixed dimension . It defines a family of {probdist} over -valued {rv} ~, , . Each distribution in this family is fully specified by its and . When the is invertible, the corresponding is characterized by the following : \\[p() = {1}{{(2)^{} \\,()}} . \\] Note that this is only defined when is invertible. More generally, any admits the following representation: \\[ = + \\] where is a and satisfies . This representation remains valid even when is singular, in which case is not full-rank~{Lapidoth2017}. The family of multivariate normal distributions is exceptional among {probmodel} for numerical quantities, at least for the following reasons. First, the family is closed under affine transformations, i.e., \\[ {N}(,) { implies } \\!+\\! {N}( +, \\,^{T} ). \\] Second, the maximizes the among all distributions with the same ~. \\\\ See also: , , , , .",
    "color": "salmon"
  },
  {
    "id": 50,
    "label": "standard normal vector",
    "title": "A standard normal is a random whose entries are {gaussrv} . It is a special case of a , . \\\\ See also: , , , , .",
    "color": "salmon"
  },
  {
    "id": 51,
    "label": "statistical aspects",
    "title": "By statistical aspects of an method, we refer to (properties of) the of its output under a for the fed into the method. \\\\ See also: , , , .",
    "color": "lightgreen"
  },
  {
    "id": 52,
    "label": "computational aspects",
    "title": "By computational aspects of an method, we mainly refer to the computational resources required for its implementation. For example, if an method uses iterative optimization techniques to solve , then its computational aspects include: 1) how many arithmetic operations are needed to implement a single iteration (i.e., a ); and 2) how many iterations are needed to obtain useful . One important example of an iterative optimization technique is . \\\\ See also: , , , , .",
    "color": "lightblue"
  },
  {
    "id": 53,
    "label": "$\\bf 0/1$ loss",
    "title": "The measures the quality of a that delivers a (e.g., via thresholding {equ_def_threshold_bin_classifier_dict}) for the of a with {feature} . It is equal to if the is correct, i.e., when . It is equal to if the is wrong, i.e., when . \\\\ See also: , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 54,
    "label": "probability",
    "title": "We assign a probability value, typically chosen in the interval , to each event that might occur in a random experiment , , , .",
    "color": "salmon"
  },
  {
    "id": 55,
    "label": "underfitting",
    "title": "Consider an method that uses to learn a with the on a given . Such a method is underfitting the if it is not able to learn a with a sufficiently small on the . If a method is underfitting, it will typically also not be able to learn a with a small . \\\\ See also: , , , , , , .",
    "color": "violet"
  },
  {
    "id": 56,
    "label": "overfitting",
    "title": "Consider an method that uses to learn a with the on a given . Such a method is overfitting the if it learns a with a small on the but a significantly larger outside the . \\\\ See also: , , , .",
    "color": "violet"
  },
  {
    "id": 57,
    "label": "general data protection regulation (GDPR)",
    "title": "The GDPR was enacted by the European Union (EU), effective from May 25, 2018 . It safeguards the privacy and rights of individuals in the EU. The GDPR has significant implications for how is collected, stored, and used in applications. Key provisions include the following: {itemize} : systems should only use the necessary amount of personal for their purpose. and : systems should enable their users to understand how the systems make decisions that impact the users. subject rights: Users should get an opportunity to access, rectify, and delete their personal , as well as to object to automated decision-making and profiling. Accountability: Organizations must ensure robust security and demonstrate compliance through documentation and regular audits. {itemize} See also: , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 58,
    "label": "Gaussian random variable (Gaussian RV)",
    "title": "A standard Gaussian is a real-valued with , , {equation} p(x) = {1}{{2}} \\,(-x^2/2). {equation} Given a standard Gaussian , we can construct a general Gaussian with and via . The of a Gaussian is referred to as normal distribution, denoted . \\\\ A Gaussian random with and can be constructed as , , \\[ {A} + { } \\] where is a of standard Gaussian {rv}, and is any satisfying . The of a Gaussian random is referred to as the , denoted . \\\\ We can interpret a Gaussian random as a indexed by the set . A {GaussProc} is a over an arbitray index set such that any restriction to a finite subset yields a Gaussian random . \\\\ Gaussian {rv} are widely used {probmodel} in the statistical analysis of methods. Their significance arises partly from the , which is a mathematically precise formulation of the following rule-of-thumb: The average of many independent {rv} (not necessarily Gaussian themselves) tends towards a Gaussian . \\\\ The is also distinct in that it represents maximum : Among all -valued {rv} with a given , the maximizes {coverthomas}. This makes {GaussProc} a natural choice for capturing (or lack of knowledge) in the absence of additional structural information. \\\\ See also: , , , , .",
    "color": "salmon"
  },
  {
    "id": 59,
    "label": "central limit theorem (CLT)",
    "title": "Consider a sequence of {rv} \\( ^{()} \\), for \\( = 1, 2, \\), each with zero and finite \\( ^2 > 0 \\). The CLT states that the normalized sum \\[ s^{()} {1}{{}} _{ = 1}^{} ^{()} \\] converges in distribution to a with zero and \\( ^2 \\) as \\( \\) {AsympVanderVaartBook}. One elegant way to derive the CLT is via the of the normalized sum \\( s^{()} \\). Let (with the imaginary unit ) be the common of each sum and , and let \\( ^{()}(t) \\) denote the of \\( s^{()} \\). Define an operator \\( {T} \\) acting on {characteristicfunc} such that \\[ ^{()}(t) = {T}(^{(-1)})(t) ( {t}{{}} ) ^{(-1)}( {{-1}}{{}} t ). \\] This captures the effect of recursively adding an and rescaling. Iteratively applying \\( {T} \\) leads to convergence of \\( ^{()}(t) \\) toward the fixed point \\[ ^*(t) = \\,(-t^2 ^2 / 2) \\] which is the of a with zero and \\( ^2 \\). {generalization} of the CLT allow for dependent or non-identically distributed {rv} {AsympVanderVaartBook}. {figure}[H] {tikzpicture} {axis}[ width=10cm, height=6cm, xlabel={}, ylabel={}, legend style={at={(0.97,0.97)}, anchor=north west}, domain=-3:3, ylabel style={ yshift=10pt }, samples=400, ymin=-0.2, ymax=1.1, axis lines=middle, clip=false, grid=both, ] {cos(x/sqrt(1) r)^1}; {} {cos(x/sqrt(2) r)^2}; {} {cos(x/sqrt(3) r)^3}; {} {exp(-x^2/2)}; {} at (axis cs:-0.08,1.05) {}; at (axis cs: 3.2,0.1) {}; {axis} {tikzpicture} {{characteristicfunc} of normalized sums of {rv} for compared to the Gaussian limit.} {figure} See also: , .",
    "color": "salmon"
  },
  {
    "id": 60,
    "label": "Gaussian process (GP)",
    "title": "A GP is a collection of {rv} indexed by input values from some input space such that, for any finite subset , the corresponding {rv} have a joint \\[ f ( ^{(1)}, , ^{()} ) {N}({}, {K}). \\] For a fixed input space , a GP is fully specified (or parameterized) by: 1) a ; and 2) a .\\\\ Example: We can interpret the temperature distribution across Finland (at a specific point in time) as the of a GP , where each input denotes a geographic location. Temperature observations from weather stations provide {sample} of at specific locations (see Fig. {fig_gp_FMI_dict}). A GP allows us to predict the temperature nearby weather stations and to quantify the of these {prediction}. {figure}[H] {center} {tikzpicture} {axis}[ axis equal, hide axis, scale=1.2, xmin=17, xmax=32, ymin=55, ymax=71, clip=true ] table [x=lon, y=lat, col sep=comma] {assets/finland_border.csv}; table [x=lon, y=lat, col sep=comma] {assets/fmi_stations_subset.csv}; (axis cs:19,59) -- (axis cs:25.5,59) node[anchor=west] {lon}; (axis cs:19,59) -- (axis cs:19,65.5) node[anchor=south] {lat}; {axis} {tikzpicture} {-15mm} {center} {For a given point in time, we can interpret the current temperature distribution over Finland as a of a GP indexed by geographic coordinates and sampled at weather stations. The weather sations are indicated by blue dots. {fig_gp_FMI_dict}} {figure} See also: , , .",
    "color": "salmon"
  },
  {
    "id": 61,
    "label": "trustworthy artificial intelligence (trustworthy AI)",
    "title": "Besides the and , a third main design aspect of methods is their trustworthiness . The EU has put forward seven key requirements (KRs) for trustworthy (that typically build on methods) : {enumerate}[label=)] KR1 - Human agency and oversight; KR2 - Technical and safety; KR3 - Privacy and governance; KR4 - ; KR5 - Diversity, non-discrimination and fairness; KR6 - Societal and environmental well-being; KR7 - Accountability. {enumerate} See also: , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 62,
    "label": "squared error loss",
    "title": "The squared error measures the error of a when predicting a numeric from the {feature} of a . It is defined as {equation} {(,)}{} ( - {()}_{=} )^{2}. {equation} \\\\ See also: , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 63,
    "label": "projection",
    "title": "Consider a subset of the -dimensional . We define the projection of a onto as {equation} {equ_def_proj_generic_dict} {}{} = _{' } { - '}{2}. {equation} In other words, is the in which is closest to . The projection is only well-defined for subsets for which the above exists . \\\\ See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 64,
    "label": "projected gradient descent (projected GD)",
    "title": "Consider an -based method that uses a parameterized with . Even if the of is , we cannot use basic , as it does not take into account contraints on the optimization variable (i.e., the ). Projected extends basic to handle constraints on the optimization variable (i.e., the ). A single iteration of projected consists of first taking a and then projecting the result back onto the . {figure}[H] {center} {tikzpicture}[scale=0.9] [right] at (-5.1,1.7) {} ; plot (, {(1/8)*}); [fill] (2.83,1) circle [radius=0.1] node[right] {}; (2.83,1) -- node[midway,above] {} (-1.5,1); (-1.5,1) --(-1.5,-1.5) node [below, left]{} ; (-1.5,-1.5) -- node[midway,above] {} (1,-1.5) ; [fill] (1,-1.5) circle [radius=0.1] node[below] {}; (1,-1.5) -- (3,-1.5) node[midway, above] {}; {tikzpicture} {-5mm} {center} {Projected augments a basic with a back onto the constraint set .} {fig_projected_GD_dict} {figure} See also: , , , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 65,
    "label": "differential privacy (DP)",
    "title": "Consider some method that reads in a (e.g., the used for ) and delivers some output . The output could be either the learned or the {prediction} for specific {datapoint}. DP is a precise measure of incurred by revealing the output. Roughly speaking, an method is differentially private if the of the output remains largely unchanged if the of one in the is changed. Note that DP builds on a for an method, i.e., we interpret its output as the of an . The randomness in the output can be ensured by intentionally adding the of an auxiliary (i.e., adding noise) to the output of the method. \\\\ See also: , , , .",
    "color": "lightgreen"
  },
  {
    "id": 66,
    "label": "robustness",
    "title": "Robustness is a key requirement for . It refers to the property of an system to maintain acceptable performance even when subjected to different forms of perturbations. These perturbations can be to the {feature} of a in order to manipulate the delivered by a trained . Robustness also includes the of -based methods against perturbations of the . Such perturbations can occur within {attack}. \\\\ See also: , , , , , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 67,
    "label": "stability",
    "title": "Stability is a desirable property of an method that maps a (e.g., a ) to an output . The output can be the learned or the delivered by the trained for a specific . Intuitively, is stable if small changes in the input lead to small changes in the output . Several formal notions of stability exist that enable bounds on the error or of the method (see {ShalevMLBook}). To build intuition, consider the three {dataset} depicted in Fig. {fig_three_data_stability_dict}, each of which is equally likely under the same -generating . Since the optimal are determined by this underlying , an accurate method should return the same (or very similar) output for all three {dataset}. In other words, any useful must be robust to variability in {realization} from the same , i.e., it must be stable. {figure}[H] {tikzpicture} {axis}[ axis lines=none, xlabel={}, ylabel={}, legend pos=north west, ymin=0, ymax=10, xtick={1,2,3,4,5}, grid style=dashed, every axis plot/.append style={very thick} ] +[only marks,mark=*] coordinates { (1,2) (2,4) (3,3) (4,5) (5,7) }; +[only marks,mark=square*] coordinates { (1,3) (2,2) (3,6) (4,4) (5,5) }; +[only marks,mark=triangle*] coordinates { (1,5) (2,7) (3,4) (4,6) (5,3) }; {axis} {tikzpicture} {Three {dataset} , , and , each sampled independently from the same -generating . A stable method should return similar outputs when trained on any of these {dataset}. {fig_three_data_stability_dict}} {figure} See also: , , , , , , , , , , , , .",
    "color": "salmon"
  },
  {
    "id": 68,
    "label": "privacy protection",
    "title": "Consider some method that reads in a and delivers some output . The output could be the learned or the obtained for a specific with {feature} . Many important applications involve {datapoint} representing humans. Each is characterized by {feature} , potentially a , and a (e.g., a recent medical diagnosis). Roughly speaking, privacy protection means that it should be impossible to infer, from the output , any of the {sensattr} of {datapoint} in . Mathematically, privacy protection requires non-invertibility of the . In general, just making non-invertible is typically insufficient for privacy protection. We need to make sufficiently non-invertible. \\\\ See also: , , , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 69,
    "label": "privacy leakage",
    "title": "Consider an application that processes a and delivers some output, such as the {prediction} obtained for new {datapoint}. Privacy leakage arises if the output carries information about a private (or sensitive) of a (which might be a human) of . Based on a for the generation, we can measure the privacy leakage via the between the output and the sensitive . Another quantitative measure of privacy leakage is . The relations between different measures of privacy leakage have been studied in the literature (see ). \\\\ See also: , , , .",
    "color": "lightgreen"
  },
  {
    "id": 70,
    "label": "probabilistic model",
    "title": "A probabilistic interprets {datapoint} as {realization} of {rv} with a joint . This joint typically involves {parameter} that have to be manually chosen or learned via statistical inference methods such as estimation . \\\\ See also: , , , , , , .",
    "color": "salmon"
  },
  {
    "id": 71,
    "label": "mean",
    "title": "The mean of an , which takes on values in a , is its . It is defined as the Lebesgue integral of with respect to the underlying (e.g., see or ), i.e., \\[ \\{\\} = _{{R}^{}} \\, {d}P(). \\] It is useful to think of the mean as the solution of the following minimization problem : \\[ \\{\\} = _{ {R}^{}} \\{{ - }{2}^{2} \\}. \\] We also use the term to refer to the average of a finite sequence . However, these two definitions are essentially the same. Indeed, we can use the sequence to construct a discrete , with the index being chosen uniformly at random from the set . The mean of is precisely the average . \\\\ See also: , , .",
    "color": "salmon"
  },
  {
    "id": 72,
    "label": "median",
    "title": "A median of a real-valued is any number such that and . {figure}[H] {center} {tikzpicture} {axis}[ axis lines=middle, xlabel={}, ylabel={}, ymin=0, ymax=1.1, xmin=-2, xmax=6, xtick=, ytick={0,1/2,1}, domain=-2:6, samples=200, width=10cm, height=6cm, smooth, enlargelimits=true, clip=false ] {1/(1 + exp(-(x - 1)))} node[pos=0.5, above, yshift=15pt] {}; (axis cs:1,0) -- (axis cs:1,0.5); (axis cs:-2,0.5) -- (axis cs:1,0.5); (axis cs:1,0.5) circle (2pt); {axis} {tikzpicture} {center} {A representation of a median.} {figure} We can define the median of a via a specific that is naturally associated with . In particular, this is constructed by , with the index being chosen uniformly at random from the set , i.e., for all . If the is integrable, a median of is the solution of the following : _{x' {R}} {|x - x'|}. Like the , the median of a can also be used to estimate {parameter} of an underlying . Compared to the , the median is more robust to {outlier}. For example, a median of a with more than one does not change even if we arbitrarily increase the largest element of . In contrast, the will increase arbitrarily. {figure}[H] {tikzpicture}[scale=0.7, y=0.5cm, x=0.5cm] {scope} / in { 1/2, 4/3, 7/4 } { (, 0) -- (, ); (, ) circle (2pt); (ptA) at (, ) {}; } (0.5, 3) -- (10.5, 3) node[right] {}; at (7.5, -4) {(a)}; {scope} {scope}[xshift=10cm] / in { 1/2, 4/3, 7/10 } { (, 0) -- (, ); (, ) circle (2pt); (ptB) at (, ) {}; } (0.5, 3) -- (10.5, 3) node[right] {}; at (ptB7) {outlier}; at (7.5, -4) {(b)}; {scope} {tikzpicture} {The median is robust against contamination. (a) Original . (b) Noisy including an .} {figure} See also: , , .",
    "color": "salmon"
  },
  {
    "id": 73,
    "label": "variance",
    "title": "The variance of a real-valued is defined as the of the squared difference between and its . We extend this definition to -valued {rv} as . \\\\ See also: , , .",
    "color": "salmon"
  },
  {
    "id": 74,
    "label": "nearest neighbor (NN)",
    "title": "NN methods learn a whose value is solely determined by the NNs within a given . Different methods use different {metric} for determining the NNs. If {datapoint} are characterized by numeric {featurevec}, we can use their Euclidean distances as the . \\\\ See also: , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 75,
    "label": "neighborhood",
    "title": "The neighborhood of a node is the subset of nodes constituted by the of . \\\\ See also: .",
    "color": "lightblue"
  },
  {
    "id": 76,
    "label": "neighbors",
    "title": "The neighbors of a node within an are those nodes that are connected (via an edge) to node . \\\\ See also: .",
    "color": "orange"
  },
  {
    "id": 77,
    "label": "bias",
    "title": "Consider an method using a parameterized . It learns the using the =\\{ {^{()}}{^{()}} \\}_{=1}^{}. To analyze the properties of the method, we typically interpret the {datapoint} as {realization} of {rv}, ^{()} = ^{({})}( ^{()} ) + {}^{()}, =1, , . We can then interpret the method as an estimator computed from (e.g., by solving ). The (squared) bias incurred by the estimate is then defined as . \\\\ See also: , , , , , , , , .",
    "color": "violet"
  },
  {
    "id": 78,
    "label": "classification",
    "title": "Classification is the task of determining a discrete-valued for a given , based solely on its {feature} . The belongs to a finite set, such as or , and represents the category to which the corresponding belongs. \\\\ See also: , , .",
    "color": "lightgreen"
  },
  {
    "id": 79,
    "label": "privacy funnel",
    "title": "The privacy funnel is a method for learning privacy-friendly {feature} of {datapoint} . \\\\ See also: , .",
    "color": "lightgreen"
  },
  {
    "id": 80,
    "label": "condition number",
    "title": "The condition number of a positive definite is the ratio between the largest and the smallest of . The condition number is useful for the analysis of methods. The computational complexity of for crucially depends on the condition number of the , with the of the . Thus, from a computational perspective, we prefer {feature} of {datapoint} such that has a condition number close to . \\\\ See also: , , , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 81,
    "label": "classifier",
    "title": "A classifier is a (i.e., a ) used to predict a taking on values from a finite . We might use the value itself as a for the . However, it is customary to use a that delivers a numeric quantity. The is then obtained by a simple thresholding step. For example, in a binary problem with a , we might use a real-valued as a classifier. A can then be obtained via thresholding, {equation} {equ_def_threshold_bin_classifier_dict} =1 { for } ()\\!\\!0 { and } =-1 { otherwise.} {equation} We can characterize a classifier by its {decisionregion} , for every possible value . \\\\ See also: , , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 82,
    "label": "empirical risk",
    "title": "The empirical of a on a is the average incurred by when applied to the {datapoint} in . \\\\ See also: , , , , .",
    "color": "violet"
  },
  {
    "id": 83,
    "label": "node degree",
    "title": "The degree of a node in an undirected is the number of its , i.e., . \\\\ See also: , .",
    "color": "orange"
  },
  {
    "id": 84,
    "label": "graph",
    "title": "A graph is a pair that consists of a node set and an edge set . In its most general form, a graph is specified by a that assigns each edge a pair of nodes . One important family of graphs is simple undirected graphs. A simple undirected graph is obtained by identifying each edge with two different nodes . Weighted graphs also specify numeric for each edge . \\\\ See also: , .",
    "color": "orange"
  },
  {
    "id": 85,
    "label": "uncertainty",
    "title": "In the context of , uncertainty refers to the presence of multiple plausible outcomes or {explanation} based on available . For example, the produced by a trained often reflects a range of possible values for the true of a given . The broader this range, the greater the associated uncertainty. theory allows us to represent, quantify, and reason about uncertainty in a mathematically rigorous manner. \\\\ See also: , , , .",
    "color": "salmon"
  },
  {
    "id": 86,
    "label": "upper confidence bound (UCB)",
    "title": "Consider an application that requires selecting, at each time step , an action from a finite set of alternatives . The utility of selecting action is quantified by a numeric signal . A widely used for this type of sequential decision-making problem is the setting . In this , the is viewed as the of an with unknown . Ideally, we would always choose the action with the largest expected , but these {mean} are unknown and must be estimated from observed . Simply choosing the action with the largest estimate can lead to suboptimal outcomes due to estimation . The UCB strategy addresses this by selecting actions not only based on their estimated {mean} but also by incorporating a term that reflects the in these estimates\u2014favoring actions with a high potential and high . Theoretical guarantees for the performance of UCB strategies, including logarithmic bounds, are established in . \\\\ See also: , , , , , , , , , , , .",
    "color": "salmon"
  },
  {
    "id": 87,
    "label": "multi-armed bandit (MAB)",
    "title": "A MAB problem is a precise mathematical formulation of a sequential decision-making task under uncertainty. At each discrete time step , a learner selects one of several possible actions\u2014called arms\u2014from a finite set . Pulling arm at time yields a which is drawn from an unknown . We obtain different classes of MAB problems by placing different restrictions on this . In the simplest setting, the does not depend on . Given a MAB problem, the goal is to construct methods that maximize the cumulative over time by strategically balancing exploration (i.e., gathering information about uncertain arms) and exploitation (i.e., selecting arms known to perform well). MAB problems form an important special case of problems . \\\\ See also: , .",
    "color": "salmon"
  },
  {
    "id": 88,
    "label": "optimism in the face of uncertainty",
    "title": "methods learn according to some performance criterion . However, they usually cannot access directly but rely on an estimate (or approximation) of . As a case in point, -based methods use the average on a given (i.e., the ) as an estimate for the of a . Using a , one can construct a confidence interval for each choice for the . One simple construction is , , with being a measure of the (expected) deviation of from . We can also use other constructions for this interval as long as they ensure that with a sufficiently high . An optimist chooses the according to the most favorable\u2014yet still plausible\u2014value of the performance criterion. Two examples of methods that use such an optimistic construction of an are {ShalevMLBook} and methods for sequential decision making {Bubeck2012}. {figure}[H] {center} {tikzpicture}[x=3cm, y=1cm] (-1, 5) -- plot[domain=-2:1, samples=100] ({+1}, { + 1}) -- plot[domain=1:-2, samples=100] ({+1}, { - 0.5}) -- cycle; at (2, 4) {}; plot ({+1}, { -0.5}) node[right] {}; plot ({}, {}); (1, -0.5) -- (1, 1) node[midway, right] {}; {tikzpicture} { methods learn by using some estimate of for the ultimate performance criterion . Using a , one can use to construct confidence intervals which contain with a high probability. The best plausible performance measure for a specific choice of is .} {center} {figure} See also: , , , , , , , , , , , , .",
    "color": "violet"
  },
  {
    "id": 89,
    "label": "federated learning network (FL network)",
    "title": "An network consists of an undirected weighted . The nodes of represent {device} that can access a and train a . The edges of represent communication links between {device} as well as statistical similarities between their {localdataset}. A principled approach to train the {localmodel} is . The solutions of are local that optimally balance the incurred on {localdataset} with their discrepancy across the edges of . \\\\ See also: , , , .",
    "color": "orange"
  },
  {
    "id": 90,
    "label": "norm",
    "title": "A norm is a that maps each () element of a to a non-negative real number. This must be homogeneous and definite, and it must satisfy the triangle inequality . \\\\ See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 91,
    "label": "dual norm",
    "title": "Every defined on a has an associated dual , which is denoted and defined as . The dual measures the largest possible inner product between and any in the unit ball of the original . For further details, see {BoydConvexBook}. \\\\ See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 92,
    "label": "geometric median (GM)",
    "title": "The GM of a set of input {vector} in is a point that minimizes the sum of distances to the {vector} such that {equation} {equ_geometric_median_dict} _{ {R}^{}} _{=1}^{} { - ^{()}}{2}. {equation} Fig. {opt_cond_GM_dict} illustrates a fundamental property of the GM: If does not coincide with any of the input {vector}, then the unit {vector} pointing from to each must sum to zero\u2014this is the zero- (optimality) condition of {equ_geometric_median_dict}. It turns out that the solution to {equ_geometric_median_dict} cannot be arbitrarily pulled away from trustworthy input {vector} as long as they are the majority {Lopuhaae1991}. {figure}[H] {center} {tikzpicture}[scale=2, thick, >=stealth] (w) at (3,0); (w) circle (1.2pt) node[below right] {}; (w2) at (0.5,0.3); (w3) at (0.7,0.7); (w2) circle (1pt) node[above left] {}; (w3) circle (1pt) node[above left] {}; at () {}; (w) -- (w2); (w) -- (w3); (w) -- () ; (w) -- () node[pos=0.9, right,yshift=7pt] {}; (w4) at (5,0.2); at (5,0.6) {}; (w4) circle (1pt) node[below left] {}; (w) -- () ; {tikzpicture} {{opt_cond_GM_dict} Consider a solution of {equ_geometric_median_dict} that does not coincide with any of the input {vector}. The optimality condition for {equ_geometric_median_dict} requires that the unit {vector} from to the input {vector} sum to zero.} {center} {figure} See also: , .",
    "color": "lightblue"
  },
  {
    "id": 93,
    "label": "explanation",
    "title": "One approach to enhance the of an method for its human user is to provide an explanation alongside the {prediction} delivered by the method. Explanations can take different forms. For instance, they may consist of human-readable text or quantitative indicators, such as importance scores for the individual {feature} of a given ~. Alternatively, explanations can be visual, for example, intensity {map} that highlight image regions that drive the . Fig.\\ {fig_explanation_dict} illustrates two types of explanations. The first is a local linear approximation of a non-linear trained around a specific , as used in the method . The second form of explanation depicted in the figure is a sparse set of {prediction} at selected {featurevec}, offering concrete reference points for the user. {figure}[H] {center} {tikzpicture}[x=0.5cm] {axis}[ hide axis, xmin=-3, xmax=6, ymin=0, ymax=6, domain=0:6, samples=100, width=10cm, height=6cm, clip=false ] {2 + sin(deg(x))} node[pos=0.9, above right, yshift=10pt] {}; {2 + sin(deg(1.5)) + cos(deg(1.5))*(x - 1.5)} node[pos=0.2, above] {}; coordinates {(1.5, {2 + sin(deg(1.5))})}; coordinates {(1.5,0) (1.5,2.4)}; at (axis cs:1.5, -0.2) {}; coordinates {(-1, {2 + sin(deg(-1))})}; coordinates {(-1,0) (-1,{2 + sin(deg(-1))})}; at (axis cs:-1, -0.2) {}; coordinates {(0, {2 + sin(deg(0))})}; coordinates {(0,0) (0,{2 + sin(deg(0))})}; at (axis cs:0, -0.2) {}; coordinates {(5, {2 + sin(deg(5))})}; coordinates {(5,0) (5,{2 + sin(deg(5))})}; at (axis cs:5, -0.2) {}; {axis} {tikzpicture} {center} {A trained can be explained locally at some point by a linear approximation . For a , this approximation is determined by the . Another form of explanation could be the values for . {fig_explanation_dict}} {figure} See also: , , , , .",
    "color": "lightblue"
  },
  {
    "id": 94,
    "label": "risk",
    "title": "Consider a used to predict the of a based on its {feature} . We measure the quality of a particular using a . If we interpret {datapoint} as the {realization} of {rv}, the also becomes the of an . The allows us to define the risk of a as the expected . Note that the risk of depends on both the specific choice for the and the of the {datapoint}. \\\\ See also: , , , , , , , , , , .",
    "color": "salmon"
  },
  {
    "id": 95,
    "label": "activation function",
    "title": "Each artificial neuron within an is assigned an activation that maps a weighted combination of the neuron inputs to a single output value . Note that each neuron is parameterized by the . \\\\ See also: , , .",
    "color": "violet"
  },
  {
    "id": 96,
    "label": "distributed algorithm",
    "title": "A distributed is an designed for a special type of computer, i.e., a collection of interconnected computing devices (or nodes). These devices communicate and coordinate their local computations by exchanging messages over a network , . Unlike a classical , which is implemented on a single , a distributed is executed concurrently on multiple {device} with computational capabilities. Similar to a classical , a distributed can be modeled as a set of potential executions. However, each execution in the distributed setting involves both local computations and message-passing events. A generic execution might look as follows: \\[ {array}{l} {Node 1: } { input}_1, s_1^{(1)}, s_2^{(1)}, , s_{T_1}^{(1)}, { output}_1; \\\\ {Node 2: } { input}_2, s_1^{(2)}, s_2^{(2)}, , s_{T_2}^{(2)}, { output}_2; \\\\ \\\\ {Node N: } { input}_N, s_1^{(N)}, s_2^{(N)}, , s_{T_N}^{(N)}, { output}_N. {array} \\] Each starts from its own local input and performs a sequence of intermediate computations at discrete time instants . These computations may depend on both the previous local computations at the and the messages received from other {device}. One important application of distributed {algorithm} is in where a network of {device} collaboratively trains a personal for each . \\\\ See also: , , , .",
    "color": "orange"
  },
  {
    "id": 97,
    "label": "algorithm",
    "title": "An algorithm is a precise, step-by-step specification for producing an output from a given input within a finite number of computational steps . For example, an algorithm to train a explicitly describes how to transform a given into through a sequence of {gradstep}. To study algorithms rigorously, we can represent (or approximate) them by different mathematical structures . One approach is to represent an algorithm as a collection of possible executions. Each individual execution is then a sequence of the form { input}, s_1, s_2, , s_T, { output}. This sequence starts from an input and progresses via intermediate steps until an output is delivered. Crucially, an algorithm encompasses more than just a mapping from input to output; it also includes intermediate computational steps . \\\\ See also: , , , , , .",
    "color": "orange"
  },
  {
    "id": 98,
    "label": "stochastic algorithm",
    "title": "A uses a random mechanism during its execution. For example, uses a randomly selected subset of {datapoint} to compute an approximation for the of an . We can represent a by a {stochproc}, i.e., the possible execution sequence is the possible outcomes of a random experiment , , . \\\\ See also: , , , , , , , , .",
    "color": "salmon"
  },
  {
    "id": 99,
    "label": "online learning",
    "title": "Some methods are designed to process in a sequential manner, updating their one at a time, as new {datapoint} become available. A typical example is time series , such as daily and temperatures recorded by an weather station. These values form a chronological sequence of observations. During each time step , online learning methods update (or refine) the current (or ) based on the newly observed . \\\\ See also: , .",
    "color": "orange"
  },
  {
    "id": 100,
    "label": "online algorithm",
    "title": "An online processes input incrementally, receiving {datapoint} sequentially and making decisions or producing outputs (or decisions) immediately without having access to the entire input in advance , . Unlike an offline , which has the entire input available from the start, an online must handle about future inputs and cannot revise past decisions. Similar to an offline , we represent an online formally as a collection of possible executions. However, the execution sequence for an online has a distinct structure as follows: { in}_{1}, s_1, { out}_{1}, { in}_{2}, s_2, { out}_{2}, , { in}_{T}, s_T, { out}_{T}. Each execution begins from an initial state (i.e., \\({in}_{1}\\)) and proceeds through alternating computational steps, outputs (or decisions), and inputs. Specifically, at step \\(\\), the performs a computational step \\(s_{}\\), generates an output \\({out}_{}\\), and then subsequently receives the next input () \\({in}_{+1}\\). A notable example of an online in is , which incrementally updates as new {datapoint} arrive. \\\\ See also: , , , , , , , .",
    "color": "orange"
  },
  {
    "id": 101,
    "label": "transparency",
    "title": "Transparency is a fundamental requirement for . In the context of methods, transparency is often used interchangeably with , . However, in the broader scope of systems, transparency extends beyond and includes providing information about the system\u2019s limitations, reliability, and intended use. In medical diagnosis systems, transparency requires disclosing the confidence level for the {prediction} delivered by a trained . In credit scoring, -based lending decisions should be accompanied by explanations of contributing factors, such as income level or credit history. These explanations allow humans (e.g., a loan applicant) to understand and contest automated decisions. Some methods inherently offer transparency. For example, provides a quantitative measure of reliability through the value . {decisiontree} are another example, as they allow human-readable decision rules . Transparency also requires a clear indication when a user is engaging with an system. For example, -powered chatbots should notify users that they are interacting with an automated system rather than a human. Furthermore, transparency encompasses comprehensive documentation detailing the purpose and design choices underlying the system. For instance, datasheets and system cards help practitioners understand the intended use cases and limitations of an system . \\\\ See also: , , , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 102,
    "label": "sensitive attribute",
    "title": "revolves around learning a that allows us to predict the of a from its {feature}. In some applications, we must ensure that the output delivered by an system does not allow us to infer sensitive attributes of a . Which part of a is considered a sensitive attribute is a design choice that varies across different application domains. \\\\ See also: , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 103,
    "label": "stochastic block model (SBM)",
    "title": "The SBM is a probabilistic generative for an undirected with a given set of nodes . In its most basic variant, the SBM generates a by first randomly assigning each node to a index . A pair of different nodes in the is connected by an edge with that depends solely on the {label} . The presence of edges between different pairs of nodes is statistically independent. \\\\ See also: , , , , .",
    "color": "orange"
  },
  {
    "id": 104,
    "label": "deep net",
    "title": "A deep net is an with a (relatively) large number of hidden layers. Deep learning is an umbrella term for methods that use a deep net as their . \\\\ See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 105,
    "label": "baseline",
    "title": "Consider some method that produces a learned (or trained ) . We evaluate the quality of a trained by computing the average on a . But how can we assess whether the resulting performance is sufficiently good? How can we determine if the trained performs close to optimal such that there is little point in investing more resources (for collection or computation) to improve it? To this end, it is useful to have a reference (or baseline) level against which we can compare the performance of the trained . Such a reference value might be obtained from human performance, e.g., the misclassification rate of dermatologists who diagnose cancer from visual inspection of skin . Another source for a baseline is an existing, but for some reason unsuitable, method. For example, the existing method might be computationally too expensive for the intended application. Nevertheless, its error can still serve as a baseline. Another, somewhat more principled, approach to constructing a baseline is via a . In many cases, given a , we can precisely determine the achievable among any hypotheses (not even required to belong to the ) . This achievable (referred to as the ) is the of the for the of a , given its {feature} . Note that, for a given choice of , the (if it exists) is completely determined by the {LC}. However, computing the and presents two main challenges: {enumerate}[label=)] The is unknown and must be estimated from observed . Even if were known, computing the exactly may be computationally infeasible . {enumerate} A widely used is the for {datapoint} characterized by numeric {feature} and {label}. Here, for the , the is given by the posterior of the , given the {feature} , . The corresponding is given by the posterior (see Fig. {fig_post_baseline_dict}). {figure}[H] {center} {tikzpicture} (-1,0) -- (7,0) node[right] {}; plot ({}, {2*exp(-0.5*((-)^2))}); (,0) -- (,2.5); at ([yshift=-5pt] ,2.5) { }; (-1,1) -- (+1,1.0); at ([yshift=2pt] ,1.2) { }; in {0.5} { at (, 0) { }; } at (0.5,-0.2) { }; {tikzpicture} {center} {If the {feature} and the of a are drawn from a , we can achieve the (under ) by using the to predict the of a with {feature} . The corresponding is given by the posterior . We can use this quantity as a baseline for the average of a trained . {fig_post_baseline_dict}} {figure} See also: , .",
    "color": "salmon"
  },
  {
    "id": 106,
    "label": "spectrogram",
    "title": "A spectrogram represents the time-frequency distribution of the energy of a time signal . Intuitively, it quantifies the amount of signal energy present within a specific time segment and frequency interval . Formally, the spectrogram of a signal is defined as the squared magnitude of its short-time Fourier transform (STFT) . Fig. {fig:spectrogram_dict} depicts a time signal along with its spectrogram. {figure}[H] {assets/spectrogram.png} {minipage}{} {3ex} { (a) {10em} (b)} {minipage} {(a) A time signal consisting of two modulated Gaussian pulses. (b) An intensity plot of the spectrogram. {fig:spectrogram_dict}} {figure} The intensity plot of its spectrogram can serve as an image of a signal. A simple recipe for audio signal is to feed this signal image into {deepnet} originally developed for image and object detection . It is worth noting that, beyond the spectrogram, several alternative representations exist for the time-frequency distribution of signal energy , . \\\\ See also: , .",
    "color": "lightblue"
  },
  {
    "id": 107,
    "label": "graph clustering",
    "title": "aims to cluster {datapoint} that are represented as the nodes of a . The edges of represent pairwise similarities between {datapoint}. We can sometimes quantify the extent of these similarities by an , . \\\\ See also: , , , .",
    "color": "orange"
  },
  {
    "id": 108,
    "label": "spectral clustering",
    "title": "Spectral is a particular instance of , i.e., it clusters {datapoint} represented as the nodes of a . Spectral uses the {eigenvector} of the to construct {featurevec} for each node (i.e., for each ) . We can feed these {featurevec} into -based methods, such as or via . Roughly speaking, the {featurevec} of nodes belonging to a well-connected subset (or ) of nodes in are located nearby in the (see Fig. {fig_lap_mtx_specclustering_dict}). {figure}[H] {center} {minipage}{0.4} {tikzpicture} {scope}[every node/.style={circle, fill=black, inner sep=0pt, minimum size=0.3cm}] (1) at (0,0) {}; (2) [below left=of 1, xshift=-0.2cm, yshift=-1cm] {}; (3) [below right=of 1, xshift=0.2cm, yshift=-1cm] {}; (4) [below=of 1, yshift=0.5cm] {}; {scope} (1) -- (2); (1) -- (3); at (1) {}; at (2) {}; at (3) {}; at (4) {}; at (0,-4) {(a)}; {tikzpicture} {minipage} {5mm} {minipage}{0.4} {equation} {}\\!=\\! {pmatrix} 2 & -1 & -1 & 0 \\\\ -1 & 1 & 0 & 0 \\\\ -1 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 0 {pmatrix}\\!=\\!{V} { } {V}\\,^{T} {equation} {minipage}{} {3ex} { (b)} {minipage} {minipage} {20mm}\\\\ {minipage}{0.4} {tikzpicture}[scale=3] (-0.2, 0) -- (1.2, 0) node[right] {}; (0, -0.2) -- (0, 1.2) node[above] {}; (0.577, 0) circle (0.03cm) node[above right] {}; (0.577, 0) circle (0.03cm); (0.577, 0) circle (0.03cm); (0, 1) circle (0.03cm) node[above right] {}; at (0.5,-0.5) {(c)}; {tikzpicture} {minipage} {minipage}{0.4} {align} & {V} = ( ^{(1)},^{(2)},^{(3)},^{(4)} ) \\\\ & {v}^{(1)}\\!=\\!{1}{{3}} {pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 {pmatrix}, \\, {v}^{(2)}\\!=\\!{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 {pmatrix} {align} {minipage}{} {3ex} { (d)} {minipage} {minipage} {{fig_lap_mtx_specclustering_dict} (a) An undirected with four nodes , each representing a . (b) The and its . (c) A of {datapoint} using the {featurevec} . (d) Two {eigenvector} corresponding to the of the . } {center} {figure} See also: , , , .",
    "color": "orange"
  },
  {
    "id": 109,
    "label": "flow-based clustering",
    "title": "Flow-based groups the nodes of an undirected by applying to node-wise {featurevec}. These {featurevec} are built from network flows between carefully selected sources and destination nodes . \\\\ See also: , , , .",
    "color": "orange"
  },
  {
    "id": 110,
    "label": "estimation error",
    "title": "Consider {datapoint}, each with and . In some applications, we can model the relation between the and the of a as . Here, we use some true underlying and a noise term which summarizes any modeling or labeling errors. The estimation error incurred by an method that learns a , e.g., using , is defined as , for some . For a parametric , which consists of {map} determined by , we can define the estimation error as , . \\\\ See also: , , , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 111,
    "label": "degree of belonging",
    "title": "Degree of belonging is a number that indicates the extent to which a belongs to a {MLBasics}. The degree of belonging can be interpreted as a soft assignment. methods can encode the degree of belonging with a real number in the interval . is obtained as the extreme case when the degree of belonging only takes on values or . \\\\ See also: , , , .",
    "color": "orange"
  },
  {
    "id": 112,
    "label": "mean squared estimation error (MSEE)",
    "title": "Consider an method that learns based on some . If we interpret the {datapoint} in as {realization} of an , we define the . Here, denotes the true of the of . The MSEE is defined as the of the squared Euclidean of the , . \\\\ See also: , , , .",
    "color": "salmon"
  },
  {
    "id": 113,
    "label": "generalized total variation minimization (GTVMin)",
    "title": "GTVMin is an instance of using the of local as a . \\\\ See also: , , .",
    "color": "orange"
  },
  {
    "id": 114,
    "label": "regression",
    "title": "Regression problems revolve around the of a numeric solely from the {feature} of a {MLBasics}. \\\\ See also: , , , .",
    "color": "lightgreen"
  },
  {
    "id": 115,
    "label": "accuracy",
    "title": "Consider {datapoint} characterized by {feature} and a categorical which takes on values from a finite . The accuracy of a , when applied to the {datapoint} in a , is then defined as using the . \\\\ See also: , , .",
    "color": "lightgreen"
  },
  {
    "id": 116,
    "label": "expert",
    "title": "aims to learn a that accurately predicts the of a based on its {feature}. We measure the error using some . Ideally, we want to find a that incurs minimal on any . We can make this informal goal precise via the and by using the as the for the (average) of a . An alternative approach to obtaining a is to use the learned by an existing method. We refer to this as an expert . minimization methods learn a that incurs a comparable to the best expert , . \\\\ See also: , , .",
    "color": "salmon"
  },
  {
    "id": 117,
    "label": "networked federated learning (NFL)",
    "title": "NFL refers to methods that learn personalized {model} in a distributed fashion. These methods learn from {localdataset} that are related by an intrinsic network structure. \\\\ See also: , , .",
    "color": "orange"
  },
  {
    "id": 118,
    "label": "regret",
    "title": "The regret of a relative to another , which serves as a , is the difference between the incurred by and the incurred by . The is also referred to as an . \\\\ See also: , , .",
    "color": "salmon"
  },
  {
    "id": 119,
    "label": "strongly convex",
    "title": "A continuously real-valued is strongly with coefficient if ,{CvxAlgBertsekas}. \\\\ See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 120,
    "label": "differentiable",
    "title": "A real-valued is differentiable if it can be approximated locally at any point by a linear . The local linear approximation at the point is determined by the . \\\\ See also: , .",
    "color": "lightblue"
  },
  {
    "id": 121,
    "label": "gradient",
    "title": "For a real-valued , if a exists such that , it is referred to as the gradient of at . If it exists, the gradient is unique and denoted or . \\\\ See also: , .",
    "color": "lightblue"
  },
  {
    "id": 122,
    "label": "subgradient",
    "title": "For a real-valued , a such that is referred to as a subgradient of at , . \\\\ See also: , .",
    "color": "lightblue"
  },
  {
    "id": 123,
    "label": "FedProx",
    "title": "FedProx refers to an iterative that alternates between separately training {localmodel} and combining the updated local . In contrast to , which uses to train {localmodel}, FedProx uses a for the training . \\\\ See also: , , , , , , .",
    "color": "orange"
  },
  {
    "id": 124,
    "label": "rectified linear unit (ReLU)",
    "title": "The ReLU is a popular choice for the of a neuron within an . It is defined as , with being the weighted input of the artificial neuron. \\\\ See also: , .",
    "color": "violet"
  },
  {
    "id": 125,
    "label": "hypothesis",
    "title": "A hypothesis refers to a (or ) from the to the . Given a with {feature} , we use a hypothesis to estimate (or approximate) the using the . is all about learning (or finding) a hypothesis such that for any (with {feature} and ). \\\\ See also: , , , .",
    "color": "lightgreen"
  },
  {
    "id": 126,
    "label": "Vapnik\u2013Chervonenkis dimension (VC dimension)",
    "title": "The VC dimension is a widely-used measure for the size of an infinite . We refer to the literature (see ) for a precise definition of VC dimension as well as a discussion of its basic properties and use in . \\\\ See also: , , .",
    "color": "lightgreen"
  },
  {
    "id": 127,
    "label": "effective dimension",
    "title": "The effective dimension of an infinite is a measure of its size. Loosely speaking, the effective dimension is equal to the effective number of independent tunable . These {parameter} might be the coefficients used in a or the and terms of an . \\\\ See also: , , .",
    "color": "violet"
  },
  {
    "id": 128,
    "label": "label space",
    "title": "Consider an application that involves {datapoint} characterized by {feature} and {label}. The space is constituted by all potential values that the of a can take on. methods, aiming to predict numeric {label}, often use the space . Binary methods use a space that consists of two different elements, e.g., , , or . \\\\ See also: , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 129,
    "label": "prediction",
    "title": "A prediction is an estimate or approximation for some quantity of interest. revolves around learning or finding a that reads in the {feature} of a and delivers a prediction for its . \\\\ See also: , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 130,
    "label": "histogram",
    "title": "Consider a that consists of {datapoint} , each of them belonging to some cell with side length . We partition this cell evenly into smaller elementary cells with side length . The histogram of assigns each elementary cell to the corresponding fraction of {datapoint} in that fall into this elementary cell. A visual example of such a histogram is provided in Fig. {fig:histogram_dict}.\\\\ {figure}[H] {tikzpicture} {compat=1.18} {axis}[ ybar, ymin=0, ymax=6, bar width=22pt, width=10cm, height=6cm, xlabel={Value}, ylabel={Frequency}, ytick={1,2,3,4,5,6}, xtick={1,2,3,4,5}, xticklabels={{[0,1)}, {[1,2)}, {[2,3)}, {[3,4)}, {[4,5)}}, enlarge x limits=0.15, title={Histogram of Sample Data} ] +[fill=blue!40] coordinates {(1,2) (2,5) (3,4) (4,3) (5,1)}; {axis} {tikzpicture} {A histogram representing the frequency of {datapoint} falling within discrete value ranges (i.e., bins). Each bar height shows the count of {sample} in the corresponding interval.} {fig:histogram_dict} {figure} See also: , , .",
    "color": "salmon"
  },
  {
    "id": 131,
    "label": "bootstrap",
    "title": "For the analysis of methods, it is often useful to interpret a given set of {datapoint} as {realization} of {rv} drawn from a common . In practice, the is unknown and must be estimated from . The bootstrap approach uses the of as an estimator for . \\\\ See also: , , , .",
    "color": "salmon"
  },
  {
    "id": 132,
    "label": "feature space",
    "title": "The space of a given application or method is constituted by all potential values that the of a can take on. {figure}[H] {tikzpicture}[scale=0.6] {scope}[xshift=0cm] (-0.5, 0) -- (3.5, 0) node[right] {}; / in {0.5/, 1.5/, 2.8/} (,0) circle (2pt) node[above] {}; at (1.5, -4.0) {}; {scope} {scope}[xshift=8cm] (0,0) circle (1.8); (0,0) circle (1.8); (0.8, 0.9) circle (2pt) node[anchor=south west] {}; (-1.2, 0.5) circle (2pt) node[anchor=south east] {}; (0.3, -1.4) circle (2pt) node[anchor=north west] {}; at (1.5, -4) {}; {scope} {scope}[xshift=14cm, yshift=0.3cm] (0,0) circle (2pt) node[anchor=north east] {}; (2,1.2) circle (2pt) node[anchor=south west] {}; (1,2.5) circle (2pt) node[anchor=south east] {}; (3,2.5) circle (2pt) node[anchor=south west] {}; (0,0) -- (2,1.2); (2,1.2) -- (1,2.5); (1,2.5) -- (3,2.5); at (1.5, -4.0) {}; {scope} {tikzpicture} {Three different spaces: a linear space , a bounded set , and a discrete space whose elements are nodes of an undirected .} {figure} For {datapoint} described by a fixed number of numerical {feature}, a common choice for the space is the . However, the mere presence of numeric {feature} does not imply that is the most appropriate representation of the space. Indeed, the numerical {feature} might be assigned to {datapoint} in a largely arbitrary or random manner, resulting in {datapoint} that are randomly scattered throughout without any meaningful geometric structure. methods try to learn a transformation of the original (potentially non-numeric) {feature} to ensure a more meaningful arrangement of {datapoint} in . \\\\ See also: , .",
    "color": "lightblue"
  },
  {
    "id": 133,
    "label": "missing data",
    "title": "Consider a constituted by {datapoint} collected via some physical . Due to imperfections and failures, some of the or values of {datapoint} might be corrupted or simply missing. imputation aims to estimate these missing values . We can interpret imputation as an problem where the of a is the value of the corrupted . \\\\ See also: , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 134,
    "label": "positive semi-definite (psd)",
    "title": "A (real-valued) symmetric is referred to as psd if for every . The property of being psd can be extended from {matrix} to (real-valued) symmetric {map} (with ) as follows: For any finite set of {featurevec} , the resulting with entries is psd . \\\\ See also: , , , , .",
    "color": "lightblue"
  },
  {
    "id": 135,
    "label": "feature",
    "title": "A feature of a is one of its properties that can be measured or computed easily without the need for human supervision. For example, if a is a digital image (e.g., stored as a {.jpeg} file), then we could use the red-green-blue intensities of its pixels as features. Domain-specific synonyms for the term feature are \"covariate,\" \"explanatory variable,\" \"independent variable,\" \"input (variable),\" \"predictor (variable),\" or \"regressor\" , , . \\\\ See also: .",
    "color": "lightgreen"
  },
  {
    "id": 136,
    "label": "feature vector",
    "title": "refers to a whose entries are individual {feature} . Many methods use {vector} that belong to some finite-dimensional . For some methods, however, it can be more convenient to work with {vector} that belong to an infinite-dimensional (e.g., see ). \\\\ See also: , , , , .",
    "color": "lightblue"
  },
  {
    "id": 137,
    "label": "label",
    "title": "A higher-level fact or quantity of interest associated with a . For example, if the is an image, the label could indicate whether the image contains a cat or not. Synonyms for label, commonly used in specific domains, include \"response variable,\" \"output variable,\" and \"target\" , , . \\\\ See also: .",
    "color": "lightgreen"
  },
  {
    "id": 138,
    "label": "data",
    "title": "Data refers to objects that carry information. These objects can be either concrete physical objects (such as persons or animals) or abstract concepts (such as numbers). We often use representations (or approximations) of the original data that are more convenient for data processing. These approximations use different mathematical structures such as relations that are used in relational databases , . \\\\ See also: , , .",
    "color": "lightgreen"
  },
  {
    "id": 139,
    "label": "dataset",
    "title": "A dataset refers to a collection of {datapoint}. These {datapoint} carry information about some quantity of interest (or ) within an application. methods use datasets for training (e.g., via ) and . Note that our notion of a dataset is very flexible, as it allows for very different types of {datapoint}. Indeed, {datapoint} can be concrete physical objects (such as humans or animals) or abstract objects (such as numbers). As a case in point, Fig. {fig_cows_dataset_dict} depicts a dataset that consists of cows as {datapoint}. {figure}[H] {center} {fig:cowsintheswissalps_dict} {assets/CowsAustria.jpg} {center} {{fig_cows_dataset_dict}A cow herd somewhere in the Alps.} {figure} Quite often, an engineer does not have direct access to a dataset. Indeed, accessing the dataset in Fig. {fig_cows_dataset_dict} would require us to visit the cow herd in the Alps. Instead, we need to use an approximation (or representation) of the dataset which is more convenient to work with. Different mathematical {model} have been developed for the representation (or approximation) of datasets , , , . One of the most widely adopted data is the relational , which organizes as a table (or relation) , . A table consists of rows and columns where {itemize} each row of the table represents a single ; each column of the table corresponds to a specific attribute of the . methods can use attributes as {feature} and {label} of the . {itemize} For example, Table {tab:cowdata_dict} shows a representation of the dataset in Fig. {fig_cows_dataset_dict}. In the relational , the order of rows is irrelevant, and each attribute (i.e., column) must be precisely defined with a domain, which specifies the set of possible values. In applications, these attribute domains become the and the . {table}[H] {tabular}{lcccc} & & & & \\\\ Zenzi & 100 & 4 & 100 & 25 \\\\ Berta & 140 & 3 & 130 & 23 \\\\ Resi & 120 & 4 & 120 & 31 \\\\ {tabular} {A relation (or table) that represents the dataset in Fig. {fig_cows_dataset_dict}.} {tab:cowdata_dict} {table} While the relational is useful for the study of many applications, it may be insufficient regarding the requirements for . Modern approaches like datasheets for datasets provide more comprehensive documentation, including details about the collection process, intended use, and other contextual information . \\\\ See also: , , , , , .",
    "color": "violet"
  },
  {
    "id": 140,
    "label": "predictor",
    "title": "A predictor is a real-valued . Given a with {feature} , the value is used as a for the true numeric of the . \\\\ See also: , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 141,
    "label": "labeled data point",
    "title": "A whose is known or has been determined by some means which might require human labor. \\\\ See also: , .",
    "color": "lightgreen"
  },
  {
    "id": 142,
    "label": "random variable (RV)",
    "title": "An RV is a that maps the outcomes of a random experiment to a value space , . Mathematically, a RV is a function that is defined on the of a . Different types of RVs include {itemize} {binary RVs}, which map each outcome to an element of a binary set (e.g., or ); {real-valued RVs}, which take on values in the real numbers ; {-valued RVs}, which map outcomes to the . {itemize} theory uses the concept of measurable spaces to rigorously define and study the properties of collections of RVs . \\\\ See also: , , , , .",
    "color": "salmon"
  },
  {
    "id": 143,
    "label": "probability space",
    "title": "A space is a mathematical structure that allows to reason about a random experiment, e.g., the observation of a physical phenomenon. Formally, a space is a triplet where {itemize} is a space containing all possible outcomes of a random experiment; is a -algebra, i.e., a collection of subsets of (called events) that satisfies certain closure properties under set operations; is a , i.e., a that assigns a to each event . This must satisfy and for any countable sequence of pairwise disjoint events in . {itemize} spaces are the foundation of {probmodel} which can be used to study the behaviour of methods , , . \\\\ See also: , , .",
    "color": "salmon"
  },
  {
    "id": 144,
    "label": "training set",
    "title": "A training set is a which consists of some {datapoint} used in to learn a . The average of on the training set is referred to as the . The comparison of the with the of allows us to diagnose the method and informs how to improve the (e.g., using a different or collecting more {datapoint}) {MLBasics}. \\\\ See also: , , , , , , , , .",
    "color": "violet"
  },
  {
    "id": 145,
    "label": "networked model",
    "title": "A networked over an assigns a (i.e., a ) to each node of the . \\\\ See also: , , , .",
    "color": "orange"
  },
  {
    "id": 146,
    "label": "batch",
    "title": "In the context of , a batch refers to a randomly chosen subset of the overall . We use the {datapoint} in this subset to estimate the of and, in turn, to update the . \\\\ See also: , , , , , .",
    "color": "violet"
  },
  {
    "id": 147,
    "label": "networked data",
    "title": "Networked consists of {localdataset} that are related by some notion of pairwise similarity. We can represent networked using a whose nodes carry {localdataset} and edges encode pairwise similarities. An example of networked can be found in applications where {localdataset} are generated by spatially distributed {device}. \\\\ See also: , , , , .",
    "color": "orange"
  },
  {
    "id": 148,
    "label": "training error",
    "title": "The average of a when predicting the {label} of the {datapoint} in a . We sometimes refer by training error also to minimal average that is achieved by a solution of . \\\\ See also: , , , , , .",
    "color": "violet"
  },
  {
    "id": 149,
    "label": "data point",
    "title": "A point is any object that conveys information~. Examples include students, radio signals, trees, images, {rv}, real numbers, or proteins. We describe points of the same type by two categories of properties: {enumerate}[label=)] {feature} are measurable or computable properties of a point. These attributes can be automatically extracted or computed using sensors, computers, or other collection systems. For a point that represents a patient, one could be the body weight. {label} are higher-level facts (or quantities of interest) associated with the point. Determining the {label} of a point usually requires human expertise or domain knowledge. For a point that represents a patient, a cancer diagnosis provided by a physician would serve as the . {enumerate} Fig.\\ {fig:datapoint_cowherd_dict} depicts an image as an example of a point along with its {feature} and {label}. Importantly, what constitutes a or a is not inherent to the point itself\u2014it is a design choice that depends on the specific applicaton. {figure}[H] {minipage}[t]{0.95} {assets/CowsAustria.jpg} {A single point.} {5mm} {minipage} {minipage}[t]{0.95} {feature}: {itemize} : Colour intensities of all image pixels. : Time-stamp of the image capture. : Spatial location of the image capture. {itemize} {label}: {itemize} : Number of cows depicted. : Number of wolves depicted. : Condition of the pasture (e.g., healthy, overgrazed). {itemize} {minipage} {Illustration of a point consisting of an image. We can use different properties of the image as {feature} and higher-level facts about the image as {label}.{fig:datapoint_cowherd_dict}} {figure} The distinction between {feature} and {label} is not always clear-cut. A property that is considered a in one setting (e.g., a cancer diagnosis) may be treated as a in another setting\u2014particularly if reliable automation (e.g., via image analysis) allows it to be computed without human intervention. broadly aims to predict the of a point based on its {feature}. \\\\ See also: , , , .",
    "color": "lightgreen"
  },
  {
    "id": 150,
    "label": "validation error",
    "title": "Consider a which is obtained by some method, e.g., using on a . The average of on a , which is different from the , is referred to as the error. \\\\ See also: , , , , , , .",
    "color": "violet"
  },
  {
    "id": 151,
    "label": "validation",
    "title": "Consider a that has been learned via some method, e.g., by solving on a . Validation refers to the practice of evaluating the incurred by the on a set of {datapoint} that are not contained in the . \\\\ See also: , , , , , .",
    "color": "violet"
  },
  {
    "id": 152,
    "label": "quadratic function",
    "title": "A of the form f() = \\,^{T} {Q} {w} + {q}\\,^{T} +a with some , , and scalar . \\\\ See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 153,
    "label": "validation set",
    "title": "A set of {datapoint} used to estimate the of a that has been learned by some method (e.g., solving ). The average of on the set is referred to as the and can be used to diagnose an method (see {MLBasics}). The comparison between and can inform directions for the improvement of the method (such as using a different ). \\\\ See also: , , , , , , , , , .",
    "color": "violet"
  },
  {
    "id": 154,
    "label": "test set",
    "title": "A set of {datapoint} that have been used neither to train a (e.g., via ) nor in a to choose between different {model}. \\\\ See also: , , , .",
    "color": "violet"
  },
  {
    "id": 155,
    "label": "model selection",
    "title": "In , selection refers to the process of choosing between different candidate {model}. In its most basic form, selection amounts to: 1) training each candidate ; 2) computing the for each trained ; and 3) choosing the with the smallest {MLBasics}. \\\\ See also: , , .",
    "color": "violet"
  },
  {
    "id": 156,
    "label": "linear classifier",
    "title": "Consider {datapoint} characterized by numeric {feature} and a from some finite . A linear is characterized by having {decisionregion} that are separated by hyperplanes in {MLBasics}. \\\\ See also: , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 157,
    "label": "empirical risk minimization (ERM)",
    "title": "ERM is the of finding a (from a ) with the average (or ) on a given (i.e., the ). Many methods are obtained from via specific design choices for the , , and {MLBasics}. \\\\ See also: , , , , , , , , .",
    "color": "violet"
  },
  {
    "id": 158,
    "label": "multi-label classification",
    "title": "Multi- problems and methods use {datapoint} that are characterized by several {label}. As an example, consider a representing a picture with two {label}. One indicates the presence of a human in this picture and another indicates the presence of a car. \\\\ See also: , , .",
    "color": "lightgreen"
  },
  {
    "id": 159,
    "label": "semi-supervised learning (SSL)",
    "title": "SSL methods use unlabeled {datapoint} to support the learning of a from {labeled datapoint} . This approach is particularly useful for applications that offer a large amount of unlabeled {datapoint}, but only a limited number of {labeled datapoint}. \\\\ See also: , , , .",
    "color": "lightgreen"
  },
  {
    "id": 160,
    "label": "objective function",
    "title": "An objective is a that assigns a numeric objective value to each choice of some variable that we want to optimize (see Fig. {fig_obj_func_dict}). In the context of , the optimization variable could be the of a . Common objective {function} include the (i.e., expected ) or the (i.e., average over a ). methods apply optimization techniques, such as , to find the choice with the optimal value (e.g., the or the ) of the objective . \\\\ {figure}[H] {center} {tikzpicture}[scale=1.0] (-0.5,0) -- (4.5,0) node[right] {}; (0,-0.5) -- (0,3.5); plot ({}, {0.5*(-2)^2 + 0.5}); at (3.5,2.8) {}; {tikzpicture} {center} {An objective maps each possible value of an optimization variable, such as the of an , to a value that measures the usefulness of .{fig_obj_func_dict}} {figure} See also: , , , , , , , , , , , , , .",
    "color": "violet"
  },
  {
    "id": 161,
    "label": "regularizer",
    "title": "A regularizer assigns each from a a quantitative measure conveying to what extent its errors might differ on {datapoint} on and outside a . uses the regularizer for linear {map} {MLBasics}. uses the regularizer for linear {map} {MLBasics}. \\\\ See also: , , , , , , , .",
    "color": "violet"
  },
  {
    "id": 162,
    "label": "regularization",
    "title": "A key challenge of modern applications is that they often use large {model}, which have an in the order of billions. Training a high-dimensional using basic -based methods is prone to , i.e., the learned performs well on the but poorly outside the . Regularization refers to modifications of a given instance of in order to avoid , i.e., to ensure that the learned does not perform much worse outside the . There are three routes for implementing regularization: {enumerate}[label=)] { pruning:} We prune the original to obtain a smaller . For a parametric , the pruning can be implemented via constraints on the (such as for the weight of in ). { penalization:} We modify the of by adding a penalty term to the . The penalty term estimates how much larger the expected (or ) is compared to the average on the . {:} We can enlarge the by adding perturbed copies of the original {datapoint} in . One example for such a perturbation is to add the of an to the of a . {enumerate} Fig. {fig_equiv_dataaug_penal_dict} illustrates the above three routes to regularization. These routes are closely related and sometimes fully equivalent. using {gaussrv} to perturb the {featurevec} in the of has the same effect as adding the penalty to the (which is nothing but ). The decision on which route to use for regularization can be based on the available computational infrastructure. For example, it might be much easier to implement than pruning. {figure}[H] {center} {tikzpicture}[scale = 1] (0,0.5) -- (7.7,0.5) node[right] { }; (0.5,0) -- (0.5,4.2) node[above] { }; plot ({},{0.4 + 2.0}) ; plot ({},{0.6 + 2.0}) ; (5, 4.5) ellipse [x radius=0.2cm, y radius=1cm]; at (5, 5.8) [text=black, font=] {}; at (6.7,4.5) {}; (l1) at (1.2, 2.48); (l2) at (1.4, 2.56); (l3) at (1.7, 2.68); (l4) at (2.2, 2.2*0.4+2.0); (l5) at (2.4, 2.4*0.4+2.0); (l6) at (2.7, 2.7*0.4+2.0); (l7) at (3.9, 3.9*0.4+2.0); (l8) at (4.2, 4.2*0.4+2.0); (l9) at (4.5, 4.5*0.4+2.0); (n1) at (1.2, 1.8); (n2) at (1.4, 1.8); (n3) at (1.7, 1.8); (n4) at (2.2, 3.8); (n5) at (2.4, 3.8); (n6) at (2.7, 3.8); (n7) at (3.9, 2.6); (n8) at (4.2, 2.6); (n9) at (4.5, 2.6); at (n1) [circle,draw,fill=red,minimum size=6pt,scale=0.6, name=c1] {}; at (n2) [circle,draw,fill=blue,minimum size=6pt, scale=0.6, name=c2] {}; at (n3) [circle,draw,fill=red,minimum size=6pt,scale=0.6, name=c3] {}; at (n4) [circle,draw,fill=red,minimum size=12pt, scale=0.6, name=c4] {}; at (n5) [circle,draw,fill=blue,minimum size=12pt,scale=0.6, name=c5] {}; at (n6) [circle,draw,fill=red,minimum size=12pt, scale=0.6, name=c6] {}; at (n7) [circle,draw,fill=red,minimum size=12pt,scale=0.6, name=c7] {}; at (n8) [circle,draw,fill=blue,minimum size=12pt, scale=0.6, name=c8] {}; at (n9) [circle,draw,fill=red,minimum size=12pt, scale=0.6, name=c9] {}; [<->] () -- () node [pos=0.4, below] {}; ; (l1) -- (c1); (l2) -- (c2); (l3) -- (c3); (l4) -- (c4); (l5) -- (c5); (l6) -- (c6); (l7) -- (c7); (l8) -- (c8); (l9) -- (c9); (6.2, 3.7) circle (0.1cm) node [black,xshift=2.3cm] {original }; (6.2, 3.2) circle (0.1cm) node [black,xshift=1.3cm] {augmented}; at (4.6,1.2) [minimum size=12pt, font={12}{0}, text=blue] {}; at (7.8,1.2) [minimum size=12pt, font={12}{0}, text=red] {}; {tikzpicture} {Three approaches to regularization: 1) ; 2) penalization; and 3) pruning (via constraints on ). {fig_equiv_dataaug_penal_dict} } {center} {figure} See also: , , , .",
    "color": "violet"
  },
  {
    "id": 163,
    "label": "regularized empirical risk minimization (RERM)",
    "title": "Basic learns a (or trains a ) based solely on the incurred on a . To make less prone to , we can implement by including a (scaled) in the learning objective. This leads to RERM such that {equation} {equ_def_rerm_dict} _{ } {}{} + {}. {equation} The controls the strength. For , we recover standard without . As increases, the learned is increasingly biased toward small values of . The component in the of {equ_def_rerm_dict} can be intuitively understood as a surrogate for the increased average that may occur when predicting {label} for {datapoint} outside the . This intuition can be made precise in various ways. For example, consider a trained using and the . In this setting, corresponds to the expected increase in caused by adding {gaussrv} to the {featurevec} in the {MLBasics}. A principled construction for the arises from approximate upper bounds on the error. The resulting RERM instance is known as {ShalevShwartz2009}. \\\\ See also: , , , , , , , , , , , , , , , , , , .",
    "color": "violet"
  },
  {
    "id": 164,
    "label": "generalization",
    "title": "Generalization refers to the ability of a trained on a to make accurate {prediction} on new, unseen {datapoint}. This is a central goal of and , i.e., to learn patterns that extend beyond the . Most systems use to learn a by minimizing the average over a of {datapoint} , which is denoted . However, success on the does not guarantee success on unseen \u2014this discrepancy is the challenge of generalization. \\\\ To study generalization mathematically, we need to formalize the notion of ``unseen'' . A widely used approach is to assume a for generation, such as the . Here, we interpret {datapoint} as independent {rv} with an identical . This , which is assumed fixed but unknown, allows us to define the of a trained as the expected \\[ {}=_{ p()} \\{ (, ) \\}. \\] The difference between and is known as the . Tools from theory, such as {concentrationinequ} and uniform convergence, allow us to bound this gap under certain conditions .\\\\ Generalization without : theory is one way to study how well a generalizes beyond the , but it is not the only way. Another option is to use simple, deterministic changes to the {datapoint} in the . The basic idea is that a good should be robust, i.e., its should not change much if we slightly change the {feature} of a . \\\\[1mm] For example, an object detector trained on smartphone photos should still detect the object if a few random pixels are masked . Similarly, it should deliver the same result if we rotate the object in the image . {figure}[H] {tikzpicture}[scale=0.8] (3, 2) ellipse (6cm and 2cm); at (6, 3) {}; (1, 3) circle (4pt) node[below, xshift=0pt, yshift=0pt] {}; (5, 1) circle (4pt) node[below] {}; (1.6, 3) circle (3pt); (0.4, 3) circle (3pt); (1, 3) -- (1.6, 3); (1, 3) -- (0.4, 3); (5.6, 1) circle (3pt); (4.4, 1) circle (3pt); (5, 1) -- (5.6, 1); (5, 1) -- (4.4, 1); plot (, {- 1* + 5}); at (3, 2.5) [right] {}; {tikzpicture} {Two {datapoint} that are used as a to learn a via . We can evaluate outside either by an with some underlying or by perturbing the {datapoint}.} {fig:polynomial_fit_dict} {figure} See also: , , , , .",
    "color": "salmon"
  },
  {
    "id": 165,
    "label": "generalization gap",
    "title": "gap is the difference between the performance of a trained on the and its performance on {datapoint} outside . We can make this notion precise by using a that allows us to compute the of a trained as the expected . However, the underlying this is typically unknown and needs to be somehow estimated. techniques use different constructions of a , which is different from the , to estimate the gap. \\\\ See also: , , , .",
    "color": "salmon"
  },
  {
    "id": 166,
    "label": "concentration inequality",
    "title": "An upper bound on the that an deviates more than a prescribed amount from its . \\\\ See also: , , .",
    "color": "salmon"
  },
  {
    "id": 167,
    "label": "boosting",
    "title": "Boosting is an iterative to learn an accurate (or strong learner) by sequentially combining less accurate {map} (referred to as weak learners) {hastie01statisticallearning}. For example, weak learners are shallow {decisiontree} which are combined to obtain a deep . Boosting can be understood as a of for using parametric {model} and {lossfunc} . Just as iteratively updates to reduce the , boosting iteratively combines (e.g., by summation) {map} to reduce the . A widely-used instance of the generic boosting idea is referred to as boosting, which uses {gradient} of the for combining the weak learners . {figure}[H] {center} {tikzpicture}[scale=1.2] (-0.5,0) -- (5.5,0) node[right] {}; (0,-0.5) -- (0,4.5) node[above] {}; plot ({},{(4 - 1.3* + 0.15*)}); / in {0.7/, 1.5/, 2.3/, 3.0/} { (, 0) -- (, {4 - 1.3* + 0.15*}); (, {4 - 1.3* + 0.15*}) circle (2pt); at (, -0.1) {}; } {tikzpicture} {center} {Boosting methods construct a sequence of {map} that are increasingly strong learners (i.e., incurring a smaller ).} {figure} See also: , , , , , , , , , , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 168,
    "label": "generalized total variation (GTV)",
    "title": "GTV is a measure of the variation of trained {localmodel} (or their ) assigned to the nodes of an undirected weighted with edges . Given a measure for the between {map} , the GTV is {equation} _{{}{'} } _{,'} {{}}{{'}}. {equation} Here, denotes the weight of the undirected edge . \\\\ See also: , , , , , .",
    "color": "orange"
  },
  {
    "id": 169,
    "label": "structural risk minimization (SRM)",
    "title": "SRM is an instance of , with which the can be expressed as a countable union of submodels such that . Each submodel permits the derivation of an approximate upper bound on the error incurred when applying to train . These individual bounds\u2014one for each submodel\u2014are then combined to form a used in the objective. These approximate upper bounds (one for each ) are then combined to construct a for {ShalevMLBook}. \\\\ See also: , , , , , .",
    "color": "violet"
  },
  {
    "id": 170,
    "label": "backdoor",
    "title": "A backdoor attack refers to the intentional manipulation of the training process underlying an method. This manipulation can be implemented by perturbing the (i.e., through ) or via the optimization used by an -based method. The goal of a backdoor attack is to nudge the learned towards specific {prediction} for a certain range of values. This range of values serves as a key (or trigger) to unlock a backdoor in the sense of delivering anomalous {prediction}. The key and the corresponding anomalous are only known to the attacker. \\\\ See also: , , .",
    "color": "lightgreen"
  },
  {
    "id": 171,
    "label": "clustering assumption",
    "title": "The assumption postulates that {datapoint} in a form a (small) number of groups or {cluster}. {datapoint} in the same are more similar to each other than those outside the . We obtain different methods by using different notions of similarity between {datapoint}. \\\\ See also: , , , .",
    "color": "orange"
  },
  {
    "id": 172,
    "label": "denial-of-service attack",
    "title": "A denial-of-service aims (e.g., via ) to steer the training of a such that it performs poorly for typical {datapoint}. \\\\ See also: , , , .",
    "color": "lightgreen"
  },
  {
    "id": 173,
    "label": "networked exponential families (nExpFam)",
    "title": "A collection of exponential families, each of them assigned to a node of an . The are coupled via the network structure by requiring them to have a small . \\\\ See also: , , .",
    "color": "orange"
  },
  {
    "id": 174,
    "label": "scatterplot",
    "title": "A visualization technique that depicts {datapoint} using markers in a two-dimensional plane. Fig. {fig_scatterplot_temp_FMI_dict} depicts an example of a scatterplot. {figure}[H] {center} {tikzpicture}[scale=1] {x=2cm,y=2cm,every path/.style={>=latex},node style/.style={circle,draw}} {axis}[axis x line=none, axis y line=none, ylabel near ticks, xlabel near ticks, enlarge y limits=true, xmin=-5, xmax=30, ymin=-5, ymax=30, width=6cm, height=6cm ] table [x=mintmp, y=maxtmp, col sep = semicolon] {assets/FMIData1.csv}; at (axis cs:26,2) [anchor=west] {}; at (axis cs:0,30) [anchor=west] {}; (axis cs:-5,0) -- (axis cs:30,0); (axis cs:0,-5) -- (axis cs:0,30); {axis} {tikzpicture} {-10mm} {center} {A scatterplot with circle markers, where the {datapoint} represent daily weather conditions in Finland. Each is characterized by its daytime temperature as the and its daytime temperature as the . The temperatures have been measured at the weather station Helsinki Kaisaniemi during 1.9.2024 - 28.10.2024.} {fig_scatterplot_temp_FMI_dict} {-3mm} {figure} A scatterplot can enable the visual inspection of {datapoint} that are naturally represented by {featurevec} in high-dimensional spaces. \\\\ See also: , , , , , , , .",
    "color": "khaki"
  },
  {
    "id": 175,
    "label": "step size",
    "title": "See .",
    "color": "lightblue"
  },
  {
    "id": 176,
    "label": "learning rate",
    "title": "Consider an iterative method for finding or learning a useful . Such an iterative method repeats similar computational (update) steps that adjust or modify the current to obtain an improved . One well-known example of such an iterative learning method is and its variants, and . A key of an iterative method is the learning rate. The learning rate controls the extent to which the current can be modified during a single iteration. A well-known example of such a is the used in {MLBasics}. \\\\ See also: , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 177,
    "label": "feature map",
    "title": "A refers to a : ', ' that transforms a of a into a new , where is typically different from . The transformed representation is often more useful than the original . For instance, the geometry of {datapoint} may become more linear in , allowing the application of a to . This idea is central to the design of {kernelmethod}~. Other benefits of using a include reducing and improving ~. A common use case is visualization, where a with two output dimensions allows the representation of {datapoint} in a 2D . Some methods employ trainable {map}, whose {parameter} are learned from . An example is the use of hidden layers in a , which act as successive {map} . A principled way to train a is through , using a that measures reconstruction quality, e.g., , where is a trainable that attempts to reconstruct from the transformed . \\\\ See also: , , , , .",
    "color": "lightblue"
  },
  {
    "id": 178,
    "label": "least absolute shrinkage and selection operator (Lasso)",
    "title": "The Lasso is an instance of . It learns the of a from a . Lasso is obtained from by adding the scaled - to the average incurred on the . \\\\ See also: , , , , , , .",
    "color": "violet"
  },
  {
    "id": 179,
    "label": "similarity graph",
    "title": "Some applications generate {datapoint} that are related by a domain-specific notion of similarity. These similarities can be represented conveniently using a similarity . The node represents the -th . Two nodes are connected by an undirected edge if the corresponding {datapoint} are similar. \\\\ See also: , , .",
    "color": "orange"
  },
  {
    "id": 180,
    "label": "Kullback-Leibler divergence (KL divergence)",
    "title": "The KL divergence is a quantitative measure of how different one is from another . \\\\ See also: .",
    "color": "salmon"
  },
  {
    "id": 181,
    "label": "Laplacian matrix",
    "title": "The structure of a , with nodes , can be analyzed using the properties of special {matrix} that are associated with . One such is the Laplacian , which is defined for an undirected and weighted , . It is defined element-wise as (see Fig. {fig_lap_mtx_dict}) {equation} {}{}{'} {cases} - _{,'}, & { for } ', {}{'}\\!\\!; \\\\ _{'' } _{,''}, & { for } = '; \\\\ 0, & { else.} {cases} {equation} Here, denotes the of an edge . {figure}[H] {center} {minipage}{0.45} {tikzpicture} {scope}[every node/.style={circle, draw, minimum size=1cm}] (1) at (0,0) {1}; (2) [below left=of 1] {2}; (3) [below right=of 1] {3}; (1) -- (2); (1) -- (3); {scope} at (0,-3) {(a)}; {tikzpicture} {minipage} {-15mm} {minipage}{0.45} {equation} {} = {pmatrix} 2 & -1& -1 \\\\ -1& 1 & 0 \\\\ -1 & 0 & 1 {pmatrix} {equation} {minipage}{} {3.5ex} { (b)} {minipage} {minipage} {{fig_lap_mtx_dict} (a) Some undirected with three nodes . (b) The Laplacian of .} {center} {figure} See also: , , .",
    "color": "orange"
  },
  {
    "id": 182,
    "label": "algebraic connectivity",
    "title": "The algebraic connectivity of an undirected is the second-smallest of its . A is connected if and only if . \\\\ See also: , , .",
    "color": "orange"
  },
  {
    "id": 183,
    "label": "Courant\u2013Fischer\u2013Weyl min-max characterization",
    "title": "Consider a with (or spectral decomposition), i.e., = _{=1}^{} {} ^{()} ( ^{()} )\\,^{T}. Here, we use the ordered (in ascending order) {eigenvalue} {equation} {1} {}. {equation} The Courant\u2013Fischer\u2013Weyl min-max characterization {GolubVanLoanBook} represents the {eigenvalue} of as the solutions to certain {optproblem}. \\\\ See also: , , , , .",
    "color": "lightblue"
  },
  {
    "id": 184,
    "label": "kernel",
    "title": "Consider a set of {datapoint}, each represented by a , where denotes the . A (real-valued) kernel is a that assigns to every pair of {featurevec} a real number . This value is typically interpreted as a similarity measure between and . The defining property of a kernel is that it is symmetric, i.e., , and that for any finite set of {featurevec} , the {equation} {K} = {pmatrix} {_1}{_1} & {_1}{_2} & & {_1}{_n} \\\\ {_2}{_1} & {_2}{_2} & & {_2}{_n} \\\\ & & & \\\\ {_n}{_1} & {_n}{_2} & & {_n}{_n} {pmatrix} {R}^{n n} {equation} is . A kernel naturally defines a transformation of a into a . The maps an input to the value . We can view the as a new that belongs to a which is typically different from . This new has a particular mathematical structure, i.e., it is a reproducing kernel (RKHS)~, . Since belongs to a RKHS, which is a , we can interpret it as a generalized . Note that a finite-length can be viewed as a that assigns a real value to each index . \\\\ See also: , , , .",
    "color": "lightblue"
  },
  {
    "id": 185,
    "label": "kernel method",
    "title": "A method is an method that uses a to map the original (i.e., raw) of a to a new (transformed) , . The motivation for transforming the {featurevec} is that, by using a suitable , the {datapoint} have a \"more pleasant\" geometry in the transformed . For example, in a binary problem, using transformed {featurevec} might allow us to use {linmodel}, even if the {datapoint} are not linearly separable in the original (see Fig. {fig_linsep_kernel_dict}). {figure}[H] {center} {tikzpicture}[auto,scale=0.6] [thick] (-6,2) circle (0.1cm) node[anchor=west] {{0mm}}; [thick] (-8,1.6) circle (0.1cm) node[anchor=west] {{0mm}}; [thick] (-7.4,-1.7) circle (0.1cm) node[anchor=west] {{0mm}}; [thick] (-6,-1.9) circle (0.1cm) node[anchor=west] {{0mm}}; [thick] (-6.5,0.0) rectangle ++(0.1cm,0.1cm) node[anchor=west,above] {{0mm}}; [thick] (4,0) circle (0.1cm) node[anchor=north] {{0mm}}; [thick] (5,0) circle (0.1cm) node[anchor=north] {{0mm}}; [thick] (6,0) circle (0.1cm) node[anchor=north] {{0mm}}; [thick] (7,0) circle (0.1cm) node[anchor=north] {{0mm}}; [thick] (2,0) rectangle ++(0.1cm,0.1cm) node[anchor=west,above] {{0mm}}; (-3,0) to node[midway,above] {} (1,0); {tikzpicture} {center} { Five {datapoint} characterized by {featurevec} and {label} , for . With these {featurevec}, there is no way to separate the two classes by a straight line (representing the of a ). In contrast, the transformed {featurevec} allow us to separate the {datapoint} using a . {fig_linsep_kernel_dict}} {figure} See also: , , , .",
    "color": "lightblue"
  },
  {
    "id": 186,
    "label": "confusion matrix",
    "title": "Consider {datapoint} characterized by {feature} and corresponding {label} . The {label} take on values in a finite . For a given , the confusion is a where each row corresponds to a different value of the true and each column to a different value of the . The -th entry of the confusion represents the fraction of {datapoint} with a true that are predicted as . The main diagonal of the confusion contains the fractions of correctly classified {datapoint} (i.e., those for which ). The off-diagonal entries contain the fractions of {datapoint} that are misclassified by . \\\\ See also: , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 187,
    "label": "feature matrix",
    "title": "Consider a with {datapoint} with {featurevec} . It is convenient to collect the individual {featurevec} into a of size . \\\\ See also: , , , , .",
    "color": "lightblue"
  },
  {
    "id": 188,
    "label": "density-based spatial clustering of applications with noise (DBSCAN)",
    "title": "DBSCAN refers to a for {datapoint} that are characterized by numeric {featurevec}. Like and via , DBSCAN also uses the Euclidean distances between {featurevec} to determine the {cluster}. However, in contrast to and , DBSCAN uses a different notion of similarity between {datapoint}. DBSCAN considers two {datapoint} as similar if they are connected via a sequence (i.e., path) of nearby intermediate {datapoint}. Thus, DBSCAN might consider two {datapoint} as similar (and therefore belonging to the same cluster) even if their {featurevec} have a large Euclidean distance. \\\\ See also: , , , , .",
    "color": "orange"
  },
  {
    "id": 189,
    "label": "federated learning (FL)",
    "title": "FL is an umbrella term for methods that train {model} in a collaborative fashion using decentralized and computation. \\\\ See also: , , .",
    "color": "orange"
  },
  {
    "id": 190,
    "label": "clustered federated learning (CFL)",
    "title": "CFL trains {localmodel} for the {device} in a application by using a , i.e., the {device} of an form {cluster}. Two {device} in the same generate {localdataset} with similar statistical properties. CFL pools the {localdataset} of {device} in the same to obtain a for a -specific . clusters {device} implicitly by enforcing approximate similarity of across well-connected nodes of the .\\\\ See also: , , , , .",
    "color": "orange"
  },
  {
    "id": 191,
    "label": "independent and identically distributed (i.i.d.)",
    "title": "A collection of {rv} is said to be independent and identically distributed (i.i.d.) if each follows the same , and the {rv} are mutually independent: For any collection of {event} , we have \\[ { ^{(1)} {A}_1, , ^{()} {A}_{}} = _{=1}^{} { ^{()} {A}_}. \\] \\\\ See also: , , , .",
    "color": "salmon"
  },
  {
    "id": 192,
    "label": "preimage",
    "title": "Consider a between two sets. The preimage of a subset is the set of all inputs that are mapped into by , i.e., \\[ f^{-1}({B}) \\{ u {U} f(u) {B} \\}. \\] The preimage is well-defined even if the is not invertible .",
    "color": "salmon"
  },
  {
    "id": 193,
    "label": "measurable",
    "title": "Consider a random experiment, such as recording the air temperature at an weather station. The corresponding consists of all possible outcomes (e.g., all possible temperature values in degree Celsius). In many applications, we are not interested in the exact outcome , but only whether it belongs to a subset (e.g., \u201cis the temperature below zero degrees?\u201d). We call such a subset measurable if it is possible to decide, for any outcome , whether or not. {figure} {tikzpicture} (0,0) -- (8.5,0) node[right] {temperature [C]}; / in {0/-20, 1/-10, 2/0, 3/10, 4/20, 5/30, 6/40, 7/50, 8/60} { (,0.1) -- (,-0.1); at (,-0.1) {}; } (0,0.3) rectangle (2,0.6); at (1,0.6) {C}; (5.5,0.3) rectangle (7.5,0.6); at (6,0.6) {C C}; {10mm} {tikzpicture} {10mm} {A constituted by all possible temperature values that may be experienced at an station. Two measurable subsets of temperature values, denoted and , are highlighted. For any actual temperature value , it is possible to determine whether and whether . } {figure} In principle, measurable sets could be chosen freely (e.g., depending on the resolution of the measuring equipment). However, it is often useful to impose certain completeness requirements on the collection of measurable sets. For example, the itself should be measurable, and the union of two measurable sets should also be measurable. These completeness requirements can be formalized via the concept of -algebra (or -field) . A measurable space is a pair that consists of an arbitrary set and a collection of measurable subsets of that form a -algebra. \\\\ See also: , .",
    "color": "salmon"
  },
  {
    "id": 194,
    "label": "event",
    "title": "Consider a , defined on some , which takes values in a space . An event is a subset of such that the is well-defined. In other words, the preimage of an event belongs to the -algebra of . \\\\ See also: , , , .",
    "color": "salmon"
  },
  {
    "id": 195,
    "label": "outlier",
    "title": "Many methods are motivated by the , which interprets {datapoint} as {realization} of {rv} with a common . The is useful for applications where the statistical properties of the generation process are stationary (or time-invariant) . However, in some applications the consists of a majority of regular {datapoint} that conform with the as well as a small number of {datapoint} that have fundamentally different statistical properties compared to the regular {datapoint}. We refer to a that substantially deviates from the statistical properties of most {datapoint} as an outlier. Different methods for outlier detection use different measures for this deviation. Statistical learning theory studies fundamental limits on the ability to mitigate outliers reliably , . \\\\ See also: , , , , , , , .",
    "color": "salmon"
  },
  {
    "id": 196,
    "label": "decision region",
    "title": "Consider a that delivers values from a finite set . For each value (i.e., category) , the determines a subset of values that result in the same output . We refer to this subset as a decision region of the . \\\\ See also: , , , .",
    "color": "lightgreen"
  },
  {
    "id": 197,
    "label": "decision boundary",
    "title": "Consider a that reads in a and delivers a value from a finite set . The decision boundary of is the set of {vector} that lie between different {decisionregion}. More precisely, a belongs to the decision boundary if and only if each , for any , contains at least two {vector} with different values. \\\\ See also: , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 198,
    "label": "Euclidean space",
    "title": "The Euclidean space of dimension consists of {vector} , with real-valued entries . Such a Euclidean space is equipped with a geometric structure defined by the inner product between any two {vector} . \\\\ See also: .",
    "color": "lightblue"
  },
  {
    "id": 199,
    "label": "explainable empirical risk minimization (EERM)",
    "title": "EERM is an in- stance of that adds a term to the average in the of . The term is chosen to favor {map} that are intrinsically explainable for a specific user. This user is characterized by their {prediction} provided for the {datapoint} in a . \\\\ See also: , , , , , , , , , .",
    "color": "violet"
  },
  {
    "id": 200,
    "label": "$k$-means",
    "title": "The -{mean} is a method which assigns each of a to precisely one of different {cluster}. The method alternates between updating the assignments (to the with the nearest ) and re-calculating the {mean} given the updated assignments {MLBasics}. \\\\ See also: , .",
    "color": "orange"
  },
  {
    "id": 201,
    "label": "explainable machine learning (XML)",
    "title": "XML methods aim to complement each with an of how the has been obtained. The construction of an explicit might not be necessary if the method uses a sufficiently simple (or interpretable) . \\\\ See also: , , , .",
    "color": "lightgreen"
  },
  {
    "id": 202,
    "label": "Finnish Meteorological Institute (FMI)",
    "title": "The FMI is a government agency responsible for gathering and reporting weather in Finland. \\\\ See also: .",
    "color": "khaki"
  },
  {
    "id": 203,
    "label": "sample mean",
    "title": "The for a given , with {featurevec} , is defined as = {1}{} _{=1}^{} ^{()}. \\\\ See also: , , , .",
    "color": "salmon"
  },
  {
    "id": 204,
    "label": "sample covariance matrix",
    "title": "The for a given set of {featurevec} is defined as { } = {1}{} _{=1}^{} (^{()}\\!-\\!{}) (^{()}\\!-\\!{})\\,^{T}. Here, we use the . \\\\ See also: , , , .",
    "color": "salmon"
  },
  {
    "id": 205,
    "label": "covariance matrix",
    "title": "The of an is defined as . \\\\ See also: , , .",
    "color": "salmon"
  },
  {
    "id": 206,
    "label": "high-dimensional regime",
    "title": "The high-dimensional regime of is characterized by the of the being larger than the , i.e., the number of (labeled) {datapoint} in the . For example, methods operate in the high-dimensional regime whenever the number of {feature} used to characterize {datapoint} exceeds the number of {datapoint} in the . Another example of methods that operate in the high-dimensional regime is large {ann}, which have far more tunable (and bias terms) than the total number of {datapoint} in the . High-dimensional statistics is a recent main thread of theory that studies the behavior of methods in the high-dimensional regime , . \\\\ See also: , , , .",
    "color": "violet"
  },
  {
    "id": 207,
    "label": "covariance",
    "title": "The covariance between two real-valued {rv} and , defined on a common , measures their linear dependence. It is defined as {x}{y} = \\{ (x - \\{ x\\} )(y - \\{y\\} )\\}. A positive covariance indicates that and tend to increase together, while a negative covariance suggests that one tends to increase as the other decreases. If , the {rv} are said to be uncorrelated, though not necessarily statistically independent. See Fig. {fig:covariance-examples_dict} for visual illustrations. {figure}[H] {tikzpicture} {scope}[shift={(0,0)}] {axis}[ width=4.5cm, height=4.5cm, title={}, xlabel={}, ylabel={}, xmin=-3, xmax=3, ymin=-3, ymax=3, xtick=, ytick=, axis lines=middle, enlargelimits ] +[only marks, mark=*, samples=50, domain=-2:2] ({x}, {-x + rand}); {axis} at (1.5,-1) {(a)}; {scope} {scope}[shift={(5.2cm,0)}] {axis}[ width=4.5cm, height=4.5cm, title={}, xlabel={}, ylabel={}, xmin=-3, xmax=3, ymin=-3, ymax=3, xtick=, ytick=, axis lines=middle, enlargelimits ] +[only marks, mark=*, samples=50, domain=-2:2] ({x}, {rand}); {axis} at (1.5,-1) {(b)}; {scope} {scope}[shift={(10.4cm,0)}] {axis}[ width=4.5cm, height=4.5cm, title={}, xlabel={}, ylabel={}, xmin=-3, xmax=3, ymin=-3, ymax=3, xtick=, ytick=, axis lines=middle, enlargelimits ] +[only marks, mark=*, samples=50, domain=-2:2] ({x}, {x + rand}); {axis} at (1.5,-1) {(c)}; {scope} {tikzpicture} {{scatterplot} illustrating {realization} from three different {probmodel} for two {rv} with different covariance values. (a) Negative. (b) Zero. (c) Positive.} {fig:covariance-examples_dict} {figure} See also: , .",
    "color": "salmon"
  },
  {
    "id": 208,
    "label": "Gaussian mixture model (GMM)",
    "title": "A GMM is a particular type of for a numeric (e.g., the {feature} of a ). Within a GMM, the is drawn from a randomly selected with . The index is an with {probability} . Note that a GMM is parameterized by the , the , and the for each . GMMs are widely used for , density estimation, and as a generative . \\\\ See also: , , .",
    "color": "salmon"
  },
  {
    "id": 209,
    "label": "maximum likelihood",
    "title": "Consider {datapoint} that are interpreted as the {realization} of {rv} with a common which depends on the . likelihood methods learn by maximizing the probability (density) of the observed . Thus, the likelihood estimator is a solution to the . \\\\ See also: , , .",
    "color": "salmon"
  },
  {
    "id": 210,
    "label": "expectation-maximization (EM)",
    "title": "Consider a for the {datapoint} generated in some application. The estimator for the is obtained by maximizing . However, the resulting might be computationally challenging. EM approximates the estimator by introducing a latent such that maximizing would be easier , , . Since we do not observe , we need to estimate it from the observed using a conditional . The resulting estimate is then used to compute a new estimate by solving . The crux is that the conditional depends on the , which we have updated based on . Thus, we have to re-calculate , which, in turn, results in a new choice for the . In practice, we repeat the computation of the conditional (i.e., the E-step) and the update of the (i.e., the M-step) until some is met. \\\\ See also: , , .",
    "color": "salmon"
  },
  {
    "id": 211,
    "label": "probabilistic principal component analysis (PPCA)",
    "title": "PPCA extends basic by using a for {datapoint}. The of PPCA reduces the task of dimensionality reduction to an estimation problem that can be solved using . \\\\ See also: , , .",
    "color": "salmon"
  },
  {
    "id": 212,
    "label": "polynomial regression",
    "title": "Polynomial is an instance of that learns a polynomial to predict a numeric based on the numeric {feature} of a . For {datapoint} characterized by a single numeric , polynomial uses the The quality of a polynomial is measured using the average incurred on a set of {labeled datapoint} (which we refer to as the ). \\\\ See also: , , .",
    "color": "lightgreen"
  },
  {
    "id": 213,
    "label": "linear regression",
    "title": "Linear aims to learn a linear to predict a numeric based on the numeric {feature} of a . The quality of a linear is measured using the average incurred on a set of {labeled datapoint}, which we refer to as the . \\\\ See also: , , , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 214,
    "label": "ridge regression",
    "title": "Ridge learns the of a linear . The quality of a particular choice for the is measured by the sum of two components. The first component is the average incurred by on a set of {labeled datapoint} (i.e., the ). The second component is the scaled squared Euclidean with a . Adding to the average is equivalent to replacing original {datapoint} by the {realization} of (infinitely many) {rv} centered around these {datapoint} (see ). \\\\ See also: , , , , , , , , , , , , , , .",
    "color": "violet"
  },
  {
    "id": 215,
    "label": "expectation",
    "title": "Consider a numeric which we interpret as the of an with a . The expectation of is defined as the integral . Note that the expectation is only defined if this integral exists, i.e., if the is integrable , , . Fig. {fig_expect_discrete_dict} illustrates the expectation of a scalar discrete that takes on values from a finite set only. {figure}[H] {center} {tikzpicture} {axis}[ ybar, y=5cm, x=2cm, bar width=0.6cm, xlabel={}, clip=false, ylabel={}, y label style={rotate=-90, anchor=west, xshift=-1cm}, xtick={1,2,3,4,5}, ymin=0, ymax=0.6, grid=both, major grid style={gray!20}, tick align=outside, axis line style={black!70}, ] +[ybar, fill=blue!50] coordinates { (1,0.1) (2,0.2) (3,0.4) (4,0.2) (5,0.1) }; at (axis cs:1,0.13) {}; at (axis cs:2,0.23) {}; at (axis cs:3,0.43) {}; at (axis cs:4,0.23) {}; at (axis cs:5,0.13) {}; at (axis cs:3.8,0.53) {}; {axis} {tikzpicture} {center} {-5mm} {The expectation of a discrete is obtained by summing its possible values , weighted by the corresponding . {fig_expect_discrete_dict}} {figure} See also: , , , , .",
    "color": "salmon"
  },
  {
    "id": 216,
    "label": "logistic regression",
    "title": "Logistic learns a linear (or ) to predict a binary based on the numeric of a . The quality of a linear is measured by the average on some {labeled datapoint} (i.e., the ). \\\\ See also: , , , , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 217,
    "label": "logistic loss",
    "title": "Consider a characterized by the {feature} and a binary . We use a real-valued to predict the from the {feature} . The logistic incurred by this is defined as {equation} {equ_log_loss_gls_dict} {(,)}{} \\, ( 1 + \\,(- ())). {equation} {figure}[H] {center} {tikzpicture} {axis}[ axis lines=middle, xlabel={}, ylabel={}, xlabel style={at={(axis description cs:1.,0.3)}, anchor=north}, ylabel style={at={(axis description cs:0.5,1.1)}, anchor=center}, xmin=-3.5, xmax=3.5, ymin=-0.5, ymax=2.5, xtick={-3, -2, -1, 0, 1, 2, 3}, ytick={0, 1, 2}, domain=-3:3, samples=100, width=10cm, height=6cm, grid=both, major grid style={line width=.2pt, draw=gray!50}, minor grid style={line width=.1pt, draw=gray!20}, legend pos=south west ] [red, thick] {ln(1 + exp(-x))}; {axis} {tikzpicture} {The logistic incurred by the for a with .} {fig_logloss_dict} {center} {figure} Note that the expression {equ_log_loss_gls_dict} for the logistic applies only for the and when using the thresholding rule {equ_def_threshold_bin_classifier_dict}. \\\\ See also: , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 218,
    "label": "hinge loss",
    "title": "Consider a characterized by a and a binary . The hinge incurred by a real-valued is defined as {equation} {equ_hinge_loss_gls_dict} {(,)}{} \\{ 0 , 1 - () \\}. {equation} {figure}[H] {center} {tikzpicture} {axis}[ axis lines=middle, xlabel={}, ylabel={}, xlabel style={at={(axis description cs:1.,0.3)}, anchor=north}, ylabel style={at={(axis description cs:0.5,1.1)}, anchor=center}, xmin=-3.5, xmax=3.5, ymin=-0.5, ymax=2.5, xtick={-3, -2, -1, 0, 1, 2, 3}, ytick={0, 1, 2}, domain=-3:3, samples=100, width=10cm, height=6cm, grid=both, major grid style={line width=.2pt, draw=gray!50}, minor grid style={line width=.1pt, draw=gray!20}, legend pos=south west ] {max(0, 1-x)}; {axis} {tikzpicture} {The hinge incurred by the for a with . A regularized variant of the hinge is used by the .} {fig_hingeloss_dict} {center} {figure} See also: , , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 219,
    "label": "independent and identically distributed assumption (i.i.d.\\ assumption)",
    "title": "The assumption interprets {datapoint} of a as the {realization} of {rv}. \\\\ See also: , , , , .",
    "color": "salmon"
  },
  {
    "id": 220,
    "label": "hypothesis space",
    "title": "Every practical method uses a space (or ) . The space of an method is a subset of all possible {map} from the to the . The design choice of the space should take into account available computational resources and . If the computational infrastructure allows for efficient operations, and there is an (approximately) linear relation between a set of {feature} and a , the might be a useful choice for the space. \\\\ See also: , , , , , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 221,
    "label": "model parameters",
    "title": "{parameter} are quantities that are used to select a specific from a . We can think of a list of {parameter} as a unique identifier for a , similar to how a social security number identifies a person in Finland. \\\\ See also: , , , .",
    "color": "orange"
  },
  {
    "id": 222,
    "label": "artificial intelligence (AI)",
    "title": "AI refers to systems that behave rationally in the sense of maximizing a long-term . The -based approach to AI is to train a to predict optimal actions. These {prediction} are computed from observations about the state of the environment. The choice of sets AI applications apart from more basic applications. AI systems rarely have access to a labeled that allows the average to be measured for any possible choice of . Instead, AI systems use observed signals to estimate the incurred by the current choice of . \\\\ See also: , .",
    "color": "lightgreen"
  },
  {
    "id": 223,
    "label": "Markov decision process (MDP)",
    "title": "A MDP is a mathematical structure that can be used to study applications. An MDP formalizes how signals depend on the {prediction} (and corresponding actions) made by a method.Formally, an MDP is a specific type of defined by: {itemize} a state space , an action space (each action corresponds to a specific made by the method), a transition function specifying the over the next state , given the current state and action , a function that assigns a numerical to each state-action pair. {itemize} The defining property of an MDP is the Markov property: the next state and only depend on the current state and action , not on the entire history of interaction.",
    "color": "salmon"
  },
  {
    "id": 224,
    "label": "hard clustering",
    "title": "Hard refers to the task of partitioning a given set of {datapoint} into (a few) non-overlapping {cluster}. The most widely used hard method is . \\\\ See also: , , , .",
    "color": "orange"
  },
  {
    "id": 225,
    "label": "soft clustering",
    "title": "Soft refers to the task of partitioning a given set of {datapoint} into (a few) overlapping {cluster}. Each is assigned to several different {cluster} with varying {dob}. Soft methods determine the (or soft assignment) for each and each . A principled approach to soft is by interpreting {datapoint} as {realization} of a . We then obtain a natural choice for the as the conditional of a belonging to a specific mixture component. \\\\ See also: , , , , , , , .",
    "color": "orange"
  },
  {
    "id": 226,
    "label": "Kronecker product",
    "title": "The Kronecker product of two {matrix} and is a block denoted and defined as , \\[ = {bmatrix} a_{11} & & a_{1n} \\\\ & & \\\\ a_{m1} & & a_{mn} {bmatrix} {R}^{mp nq}. \\] The Kronecker product is a special case of the tensor product for {matrix} and is widely used in multivariate statistics, linear algebra, and structured {model}. It satisfies the identity for {vector} and of compatible dimensions. \\\\ See also: , , , .",
    "color": "lightblue"
  },
  {
    "id": 227,
    "label": "clustering",
    "title": "Clustering methods decompose a given set of {datapoint} into a few subsets, which are referred to as {cluster}. Each consists of {datapoint} that are more similar to each other than to {datapoint} outside the . Different clustering methods use different measures for the similarity between {datapoint} and different forms of representations. The clustering method uses the average of a (i.e., the ) as its representative. A popular method based on represents a by a . \\\\ See also: , , , .",
    "color": "orange"
  },
  {
    "id": 228,
    "label": "cluster",
    "title": "A cluster is a subset of {datapoint} that are more similar to each other than to the {datapoint} outside the cluster. The quantitative measure of similarity between {datapoint} is a design choice. If {datapoint} are characterized by Euclidean {featurevec} , we can define the similarity between two {datapoint} via the Euclidean distance between their {featurevec}. An example of such clusters is shown in Fig. {fig:clusters_dict}.\\\\ {figure}[H] {tikzpicture} {compat=1.18} {axis}[ width=10cm, height=8cm, xlabel={}, ylabel={}, title={Clusters of Data Points}, xmin=0, xmax=10, ymin=0, ymax=10, axis lines=left, legend style={at={(0.5,-0.25)}, anchor=north, legend columns=3} ] coordinates { (1,1) (2,1.2) (1.8,2) (2.2,1.5) (1.5,2.5) }; coordinates { (7,8) (8,7.5) (7.5,8.5) (8.2,7.8) (7.7,7) }; coordinates { (5,3) (5.5,3.2) (5.2,2.8) (4.8,3.5) (5.1,3.1) }; {Cluster 1, Cluster 2, Cluster 3} {axis} {tikzpicture} {Illustration of three clusters in a two-dimensional . Each cluster groups {datapoint} that are more similar to each other than to those in other clusters, based on the Euclidean distance.} {fig:clusters_dict} {figure} See also: , , .",
    "color": "orange"
  },
  {
    "id": 229,
    "label": "Huber loss",
    "title": "The Huber unifies the and the . \\\\ See also: , , .",
    "color": "lightgreen"
  },
  {
    "id": 230,
    "label": "support vector machine (SVM)",
    "title": "The SVM is a binary meth\\-od that learns a linear . Thus, like and , it is also an instance of for the . However, the SVM uses a different from the one used in those methods. As illustrated in Fig. {fig_svm_gls_dict}, it aims to maximally separate {datapoint} from the two different classes in the (i.e., margin principle). Maximizing this separation is equivalent to minimizing a regularized variant of the {equ_hinge_loss_gls_dict} , , . {figure}[H] {center} {tikzpicture}[auto,scale=0.8] [thick] (1,2) circle (0.1cm)node[anchor=west] {{0mm}}; [thick] (0,1.6) circle (0.1cm)node[anchor=west] {{0mm}}; [thick] (0,3) circle (0.1cm)node[anchor=west] {{0mm}}; [thick] (2,1) circle (0.1cm)node[anchor=east,above] {{0mm}}; (B) at (-2,0) {support }; (B) to (1.9,1) ; [|<->|,thick] (2.05,0.95) -- (2.75,0.25)node[pos=0.5] {} ; [thick] (1,-1.5) -- (4,1.5) node [right] {} ; [thick] (3,-1.9) rectangle ++(0.1cm,0.1cm) node[anchor=west,above] {{0mm}}; [thick] (4,.-1) rectangle ++(0.1cm,0.1cm) node[anchor=west,above] {{0mm}}; {tikzpicture} {The SVM learns a (or ) with minimal average soft-margin . Minimizing this is equivalent to maximizing the margin between the of and each class of the .} {fig_svm_gls_dict} {center} {figure} The above basic variant of SVM is only useful if the {datapoint} from different categories can be (approximately) linearly separated. For an application where the categories are not derived from a . \\\\ See also: , , , , , , , , , , , , , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 231,
    "label": "eigenvalue",
    "title": "We refer to a number as an eigenvalue of a square if there is a non-zero such that . \\\\ See also: , .",
    "color": "lightblue"
  },
  {
    "id": 232,
    "label": "eigenvector",
    "title": "An eigenvector of a is a non-zero such that with some . \\\\ See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 233,
    "label": "eigenvalue decomposition (EVD)",
    "title": "The EVD for a square is a factorization of the form = {V} { } {V}^{-1}. The columns of the are the {eigenvector} of the . The diagonal contains the {eigenvalue} corresponding to the {eigenvector} . Note that the above decomposition exists only if the is diagonalizable. \\\\ See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 234,
    "label": "singular value decomposition (SVD)",
    "title": "The SVD for a is a factorization of the form = {V} { } {U}\\,^{T} with orthonormal {matrix} and . The is only non-zero along the main diagonal, whose entries are non-negative and referred to as singular values. \\\\ See also: .",
    "color": "lightblue"
  },
  {
    "id": 235,
    "label": "total variation",
    "title": "See .",
    "color": "orange"
  },
  {
    "id": 236,
    "label": "convex clustering",
    "title": "Consider a . learns {vector} by minimizing _{=1}^{} {^{()} - ^{()}}{2}^2 + _{,' } {^{()} - ^{(')}}{p}. Here, denotes the - (for ). It turns out that many of the optimal {vector} coincide. A then consists of those {datapoint} with identical , . \\\\ See also: , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 237,
    "label": "gradient-based methods",
    "title": "-based methods are iterative techniques for finding the (or ) of a of the . These methods construct a sequence of approximations to an optimal choice for that results in a (or ) value of the . As their name indicates, -based methods use the {gradient} of the evaluated during previous iterations to construct new, (hopefully) improved . One important example of a -based method is . \\\\ See also: , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 238,
    "label": "subgradient descent",
    "title": "descent is a of that does not require differentiability of the to be minimized. This is obtained by replacing the concept of a with that of a . Similar to {gradient}, {subgradient} allow us to construct local approximations of an . The might be the viewed as a of the that select a . \\\\ See also: , , , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 239,
    "label": "stochastic gradient descent (SGD)",
    "title": "SGD is obtained from by replacing the of the with a approximation. A main application of SGD is to train a parameterized via on a that is either very large or not readily available (e.g., when {datapoint} are stored in a database distributed all over the planet). To evaluate the of the (as a of the ), we need to compute a sum over all {datapoint} in the . We obtain a approximation to the by replacing the sum with a sum over a randomly chosen subset (see Fig. {fig_sgd_approx_dict}). We often refer to these randomly chosen {datapoint} as a . The size is an important of SGD. SGD with is referred to as mini- SGD . {figure}[H] {tikzpicture}[scale=1.5, >=stealth] plot (, {(-1.5)^2 + 1}); at (0.5, 2) {}; plot (, {(-2)^2 + 0.5}); at (3.3, 1.5) {}; {tikzpicture} {SGD for approximates the by replacing the sum over all {datapoint} in the (indexed by ) with a sum over a randomly chosen subset .{fig_sgd_approx_dict}} {figure} See also: , , , , , , , , , , , , .",
    "color": "violet"
  },
  {
    "id": 240,
    "label": "online gradient descent (online GD)",
    "title": "Consider an method that learns from some . The learning process uses {datapoint} that arrive at consecutive time-instants . Let us interpret the {datapoint} as copies of an . The of a can then (under mild conditions) be obtained as the limit . We might use this limit as the for learning the . Unfortunately, this limit can only be evaluated if we wait infinitely long in order to collect all {datapoint}. Some applications require methods that learn online, i.e., as soon as a new arrives at time , we update the current . Note that the new contributes the component to the . As its name suggests, online updates via a (projected) such that {equation} {equ_def_ogd_dict} ^{(+1)} {}{^{()} - _{} _{} {^{()}}{}}. {equation} Note that {equ_def_ogd_dict} is a for the current component of the . The update {equ_def_ogd_dict} ignores all the previous components , for . It might therefore happen that, compared to , the updated increase the retrospective average . However, for a suitably chosen , online can be shown to be optimal in practically relevant settings. By optimal, we mean that the delivered by online after observing {datapoint} are at least as good as those delivered by any other learning method , . {figure}[H] {center} {tikzpicture}[x=1.5cm,scale=1.5, every node/.style={font=}] (0.5, 0) -- (5.5, 0) node[below] {}; in {1, 2, 3, 4, 5} { (, 0.1) -- (, -0.1) node[below] {}; } / in {1/2.5, 2/1.8, 3/2.3, 4/1.5, 5/2.0} { (, ) circle (2pt) node[above right] {}; } / in {1/1.0, 2/1.6, 3/1.8, 4/2.2, 5/1.9} { (, ) circle (2pt) node[below left] {}; } // in {1/2.5/1.0, 2/1.8/1.6, 3/2.3/2.0, 4/1.5/1.8, 5/2.0/1.9} { (, ) -- (, ); } {tikzpicture} {center} {An instance of online that updates the using the arriving at time . This instance uses the .} {figure} See also: , , , , , , , , , , , , , .",
    "color": "orange"
  },
  {
    "id": 241,
    "label": "principal component analysis (PCA)",
    "title": "PCA determines a linear such that the new {feature} allow us to reconstruct the original {feature} with the reconstruction error . \\\\ See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 242,
    "label": "loss",
    "title": "methods use a to measure the error incurred by applying a specific to a specific . With a slight abuse of notation, we use the term loss for both the itself and the specific value , for a and . \\\\ See also: , , , .",
    "color": "lightgreen"
  },
  {
    "id": 243,
    "label": "loss function",
    "title": "A is a : {R}_{+}: ( (,), ) {(,)}{}. It assigns a non-negative real number (i.e., the ) to a pair that consists of a , with {feature} and , and a . The value quantifies the discrepancy between the true and the . Lower (closer to zero) values indicate a smaller discrepancy between and . Fig. {fig_loss_function_gls_dict} depicts a for a given , with {feature} and , as a of the . {figure}[H] {center} {tikzpicture}[scale = 0.7] {axis} [axis x line=center, axis y line=center, xlabel={}, xlabel style={below right}, ylabel style={above right}, xtick=, ytick=, xmin=-4, xscale = 1.4, xmax=4, ymin=-0.5, ymax=2.5 ] [red, thick] {ln(1 + exp(-x))}; {axis} [above,centered,xshift=-5pt] at (1,5) {}; [above] at (10,1) { }; [right] at (4,6) {}; {tikzpicture} {center} {-7mm} {Some for a fixed , with and , and a varying . methods try to find (or learn) a that incurs minimal .} {fig_loss_function_gls_dict} {figure} See also: , , , , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 244,
    "label": "decision tree",
    "title": "A decision tree is a flow-chart-like representation of a . More formally, a decision tree is a directed containing a root node that reads in the of a . The root node then forwards the to one of its child nodes based on some elementary test on the {feature} . If the receiving child node is not a leaf node, i.e., it has child nodes itself, it represents another test. Based on the test result, the is forwarded to one of its descendants. This testing and forwarding of the is continued until the ends up in a leaf node without any children. {figure}[H] {minipage}{.45} {1}{ {tikzpicture} (A) {}; (B) {}; (C) {}; (D) {}; (E) {}; (A) -- (B) node[midway, left] {no}; (A) -- (C) node[midway, right] {yes}; (C) -- (D) node[midway, left] {no}; (C) -- (E) node[midway, right] {yes}; at (0.7,-4.5) { (a)}; {tikzpicture} } {minipage} {15mm} {minipage}{.45} {15mm} {tikzpicture} (-2,2) rectangle (2,-2); {scope} (-0.5,0) circle (1cm); (0.5,0) circle (1cm); (-2,1.5) rectangle (2,-1.5); {scope} (-0.5,0) circle (1cm); (0.5,0) circle (1cm); (-0.5,0) circle [radius=0.025]; [below right, red] at (-0.5,0) {}; [below left, blue] at (-0.7,0) {}; [above left] at (-0.7,1) {}; [left] at (-0.4,0) {}; (0.5,0) circle [radius=0.025]; [right] at (0.6,0) {}; at (0,-3.5) { (b)}; {tikzpicture} {minipage} {(a) A decision tree is a flow-chart-like representation of a piece-wise constant . Each piece is a . The depicted decision tree can be applied to numeric {featurevec}, i.e., . It is parameterized by the threshold and the {vector} . (b) A decision tree partitions the into {decisionregion}. Each corresponds to a specific leaf node in the decision tree.} {fig_decision_tree_dict} {figure} See also: .",
    "color": "lightblue"
  },
  {
    "id": 245,
    "label": "application programming interface (API)",
    "title": "An API is a formal mechanism that allows software components to interact in a structured and modular way . In the context of , APIs are commonly used to provide access to a trained . Users\u2014whether humans or machines\u2014can submit the of a and receive a corresponding . Suppose a trained is defined as . Through an API, a user can input and receive the output without knowledge of the detailed structure of the or its training. In practice, the is typically deployed on a server connected to the internet. Clients send requests containing values to the server, which responds with the computed . APIs promote modularity in system design, i.e., one team can develop and train the , while another team handles integration and user interaction. Publishing a trained via an API also offers practical advantages, including the following: {itemize} The server can centralize computational resources which are required to compute {prediction}. The internal structure of the remains hidden\u2014which is useful for protecting intellectual property or trade secrets. {itemize} However, APIs are not without . Techniques such as can potentially reconstruct a from its {prediction} using carefully selected {featurevec}. \\\\ See also: , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 246,
    "label": "model inversion",
    "title": "A inversion is a form of on an system. An adversary seeks to infer {sensattr} of individual {datapoint} by exploiting partial access to a trained . This access typically consists of querying the for {prediction} using carefully chosen inputs. Basic inversion techniques have been demonstrated in the context of facial image , where images are reconstructed using the ( of) outputs combined with auxiliary information such as a person\u2019s name (see Fig. {fig_model_inv_dict}). {figure}[H] {center} {tikzpicture}[scale=1.5] (-0.5,0) -- (5.5,0) node[right] {face image }; (0,-0.2) -- (0,2.5) node[above] {name}; plot ({}, {2/(1 + exp(-3*( - 3)))}); {3} {}{2/(1 + exp(-3*( - 3)))} (,0) -- (,); (0,) -- (,); (,) circle (0.1); at (-0.1,) { ``Alexander Jung''}; at (,-0.25) {{assets/AlexanderJung.jpg}}; at (4,2.2) {trained }; {tikzpicture} {center} {Model inversion techniques implemented in the context of facial image classification. {fig_model_inv_dict}} {figure} See also: , , , , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 247,
    "label": "Hilbert space",
    "title": "A Hilbert space is a complete inner product space . That is, it is a equipped with an inner product between pairs of {vector}, and it satisfies the additional requirement of completeness, i.e., every Cauchy sequence of {vector} converges to a limit within the space. A canonical example of a Hilbert space is the , for some dimension , consisting of {vector} and the standard inner product . \\\\ See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 248,
    "label": "sample",
    "title": "A finite sequence (or list) of {datapoint} that is obtained or interpreted as the of {rv} with a common . The length of the sequence is referred to as the . \\\\ See also: , , , , , .",
    "color": "salmon"
  },
  {
    "id": 249,
    "label": "sample size",
    "title": "The number of individual {datapoint} contained in a . \\\\ See also: , .",
    "color": "violet"
  },
  {
    "id": 250,
    "label": "artificial neural network (ANN)",
    "title": "An ANN is a graphical (signal-flow) representation of a that maps {feature} of a at its input to a for the corresponding at its output. The fundamental unit of an ANN is the artificial neuron, which applies an to its weighted inputs. The outputs of these neurons serve as inputs for other neurons, forming interconnected layers. \\\\ See also: , , , , , .",
    "color": "violet"
  },
  {
    "id": 251,
    "label": "random forest",
    "title": "A random forest is a set of different {decisiontree}. Each of these {decisiontree} is obtained by fitting a perturbed copy of the original . \\\\ See also: , .",
    "color": "violet"
  },
  {
    "id": 252,
    "label": "bagging (or bootstrap aggregation)",
    "title": "Bagging (or bootstrap aggregation) is a generic technique to improve (the of) a given method. The idea is to use the to generate perturbed copies of a given and to learn a separate for each copy. We then predict the of a by combining or aggregating the individual {prediction} of each separate . For {map} delivering numeric values, this aggregation could be implemented by computing the average of individual {prediction}. \\\\ See also: , .",
    "color": "lightgreen"
  },
  {
    "id": 253,
    "label": "gradient descent (GD)",
    "title": "Gradient descent (GD) is an iterative method for finding the of a . GD generates a sequence of estimates that (ideally) converge to a of . At each iteration , GD refines the current estimate by taking a step in the direction of steepest descent of a local linear approximation. This direction is given by the negative of the at the current estimate . The resulting update rule is given by {equation} {equ_def_GD_step_dict} ^{(\\!+\\!1)} = ^{()} - f(^{()}), {equation} where is a suitably small . For a suitably choosen , the update typically reduces the function value, i.e., . Fig.\\ {fig_basic_GD_step_dict} illustrates a single GD step. {figure}[H] {center} {tikzpicture}[scale=0.9] (-4,0) grid (4,4); plot (, {(1/4)*}); plot (, {2* - 4}); (4,4) -- node[right] {} (4,2); (4,4) -- node[above] {} (1,4); (4,2) -- node[below] {} (3,2) ; (-4.25,0) -- (4.25,0) node[right] {}; (0,-2pt) -- (0,4.25) node[above] {}; (0pt,2pt) -- (0pt,-2pt) node[below] {}; (0pt,2pt) -- (0pt,-2pt) node[below] {}; (0pt,2pt) -- (0pt,-2pt) node[below] {}; / in {1/1, 2/2, 3/3, 4/4} (2pt,0pt) -- (-2pt,0pt) node[left] {}; {tikzpicture} {center} {A single {equ_def_GD_step_dict} towards the minimizer of .} {fig_basic_GD_step_dict} {figure} See also: , , , , .",
    "color": "lightblue"
  },
  {
    "id": 254,
    "label": "absolute error loss",
    "title": "Consider a with {feature} and numeric . The absolute error incurred by a is defined as , i.e., the absolute difference between the and the true . \\\\ See also: , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 255,
    "label": "device",
    "title": "Any physical system that can be used to store and process . In the context of , we typically mean a computer that is able to read in {datapoint} from different sources and, in turn, train an using these {datapoint}. \\\\ See also: , , , .",
    "color": "orange"
  },
  {
    "id": 256,
    "label": "large language model (LLM)",
    "title": "LLM is an umbrella term for methods that process and generate human-like text. These methods typically use {deepnet} with billions (or even trillions) of {parameter}. A widely used choice for the network architecture is referred to as Transformers . The training of LLMs is often based on the task of predicting a few words that are intentionally removed from a large text corpus. Thus, we can construct {labeled datapoint} simply by selecting some words from a given text as {label} and the remaining words as {feature} of {datapoint}. This construction requires very little human supervision and allows for generating sufficiently large {trainset} for LLMs. \\\\ See also: , .",
    "color": "lightgreen"
  },
  {
    "id": 257,
    "label": "Huber regression",
    "title": "Huber refers to -based methods that use the as a measure of the error. Two important special cases of Huber are and . Tuning the threshold of the allows the user to trade the of the against the computational benefits of the . \\\\ See also: , , , .",
    "color": "lightgreen"
  },
  {
    "id": 258,
    "label": "least absolute deviation regression",
    "title": "Least absolute deviation regression is an instance of using the . It is a special case of . \\\\ See also: , , .",
    "color": "lightgreen"
  },
  {
    "id": 259,
    "label": "metric",
    "title": "In its most general form, a metric is a quantitative measure used to compare or evaluate objects. In mathematics, a metric measures the distance between two points and must follow specific rules, i.e., the distance is always non-negative, zero only if the points are the same, symmetric, and it satisfies the triangle inequality . In , a metric is a quantitative measure of how well a performs. Examples include , precision, and the average on a , . A is used to train {model}, while a metric is used to compare trained {model}. \\\\ See also: , , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 260,
    "label": "Bayes risk",
    "title": "Consider a with a joint for the {feature} and of a . The Bayes is the possible that can be achieved by any . Any that achieves the Bayes is referred to as a . \\\\ See also: , , .",
    "color": "salmon"
  },
  {
    "id": 261,
    "label": "Bayes estimator",
    "title": "Consider a with a joint over the {feature} and the of a . For a given , we refer to a as a Bayes estimator if its is the achievable . Note that whether a qualifies as a Bayes estimator depends on the underlying and the choice for the . \\\\ See also: , , .",
    "color": "salmon"
  },
  {
    "id": 262,
    "label": "weights",
    "title": "Consider a parameterized . We use the term weights for numeric that are used to scale {feature} or their transformations in order to compute . A uses weights to compute the linear combination . Weights are also used in {ann} to form linear combinations of {feature} or the outputs of neurons in hidden layers. {figure}[H] {center} {tikzpicture}[neuron/.style={circle, draw, minimum size=1cm}, thick, >=stealth] (h1) at (0, 2) {}; (h2) at (0, 0) {}; (h3) at (0, -2) {}; (outpoint); at ([xshift=0.2cm]outpoint) {}; (h1) -- node[above] {} (outpoint); (h2) -- node[above] {} (outpoint); (h3) -- node[below] {} (outpoint); {tikzpicture} {center} {A section of an that contains a hidden layer with outputs (or activations) ,, and . These outputs are combined linearly to compute that can be used either as output of the or as input to another layer.{fig_weights_dict}} {figure} See also: , , , , .",
    "color": "violet"
  },
  {
    "id": 263,
    "label": "probability distribution",
    "title": "To analyze methods, it can be useful to interpret {datapoint} as {realization} of an . The typical properties of such {datapoint} are then governed by the distribution of this . The distribution of a binary is fully specified by the {probability} and . The distribution of a real-valued might be specified by a such that . In the most general case, a distribution is defined by a measure , . \\\\ See also: , , , , .",
    "color": "salmon"
  },
  {
    "id": 264,
    "label": "probability density function (pdf)",
    "title": "The pdf of a real-valued is a particular representation of its . If the pdf exists, it can be used to compute the that takes on a value from a measurable set via {BertsekasProb}. If the pdf of a -valued exists, it allows us to compute the of belonging to a measurable region via {BertsekasProb}. \\\\ See also: , , , .",
    "color": "salmon"
  },
  {
    "id": 265,
    "label": "parameter",
    "title": "The parameter of an is a tunable (i.e., learnable or adjustable) quantity that allows us to choose between different {map}. For example, the consists of all {map} with a particular choice for the parameters . Another example of a parameter is the assigned to a connection between two neurons of an . \\\\ See also: , , , , , , .",
    "color": "violet"
  },
  {
    "id": 266,
    "label": "law of large numbers",
    "title": "The law of large numbers refers to the convergence of the average of an increasing (large) number of {rv} to the of their common . Different instances of the law of large numbers are obtained by using different notions of convergence . \\\\ See also: , , , .",
    "color": "salmon"
  },
  {
    "id": 267,
    "label": "stopping criterion",
    "title": "Many methods use iterative {algorithm} that construct a sequence of in order to minimize the . For example, iteratively update the {parameter} of a parametric , such as a or a . Given a finite amount of computational resources, we need to stop updating the {parameter} after a finite number of iterations. A stopping criterion is any well-defined condition for deciding when to stop updating. \\\\ See also: , .",
    "color": "lightblue"
  },
  {
    "id": 268,
    "label": "$k$-fold cross-validation ($\\nrfolds$-fold CV)",
    "title": "-fold CV is a method for learning and validating a using a given . This method divides the evenly into subsets or folds and then executes repetitions of training (e.g., via ) and . Each repetition uses a different fold as the and the remaining folds as a . The final output is the average of the {valerr} obtained from the repetitions. \\\\ See also: , , , , .",
    "color": "violet"
  },
  {
    "id": 269,
    "label": "Jacobi method",
    "title": "The Jacobi method is an for solving systems of linear equations (i.e., a linear system) of the form . Here, is a square with non-zero main diagonal entries. The method constructs a sequence by updating each entry of according to \\[ x_i^{(+1)} = {1}{a_{ii}} ( b_i - _{j i} a_{ij} x_j^{()} ). \\] Note that all entries are updated simultaneously. The above iteration converges to a solution, i.e., , under certain conditions on the , e.g., being strictly diagonally dominant or symmetric positive definite , , . Jacobi-type methods are appealing for large linear systems due to their parallelizable structure . We can interpret the Jacobi method as a . Indeed, using the decomposition , with being the diagonal of , allows us to rewrite the linear equation as a fixed-point equation \\[ {x} = {^{-1}({b} - {x})}_{ } \\] which leads to the iteration . \\\\ As an example, for the linear equation \\[ = {bmatrix} a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} {bmatrix}, {b} = {bmatrix} b_1 \\\\ b_2 \\\\ b_3 {bmatrix} \\] the Jacobi method updates each component of \\( {x} \\) as follows: \\[ {aligned} x_1^{(k+1)} &= {1}{a_{11}} ( b_1 - a_{12} x_2^{(k)} - a_{13} x_3^{(k)} ); \\\\ x_2^{(k+1)} &= {1}{a_{22}} ( b_2 - a_{21} x_1^{(k)} - a_{23} x_3^{(k)} ); \\\\ x_3^{(k+1)} &= {1}{a_{33}} ( b_3 - a_{31} x_1^{(k)} - a_{32} x_2^{(k)} ). {aligned} \\] See also: , , , .",
    "color": "lightblue"
  },
  {
    "id": 270,
    "label": "R\\'enyi divergence",
    "title": "The R\\'enyi divergence measures the (dis)similarity between two {probdist} . \\\\ See also: .",
    "color": "salmon"
  },
  {
    "id": 271,
    "label": "non-smooth",
    "title": "We refer to a as non-smooth if it is not . \\\\ See also: , .",
    "color": "lightblue"
  },
  {
    "id": 272,
    "label": "convex",
    "title": "A subset of the is referred to as convex if it contains the line segment between any two points in that set. A is convex if its is a convex set . We illustrate one example of a convex set and a convex in Fig. {fig_convex_set_function_dict}. {figure}[H] {center} {tikzpicture} (-3,0) ellipse (2 and 1.2); (-3,0) ellipse (2 and 1.2); (-3.7,0.2) circle (2pt) node[left] {}; (-2.3,-0.5) circle (2pt) node[right] {}; (-3.7,0.2) -- (-2.3,-0.5); at (-1.2,-1.0) {}; at (-3,-2.4) {(a)}; {scope}[shift={(5,-1)}] plot ({}, {0.5*}); plot[domain=-1.5:1.5, smooth] ({}, {0.5*}) -- (2, {0.5*2*2}) -- (-2, {0.5*2*2}) -- cycle; at (0,-0.4) {}; at (0,-1.4) {(b)}; {scope} {tikzpicture} {-8mm} {center} {(a) A convex set . (b) A convex .{fig_convex_set_function_dict}} {figure} See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 273,
    "label": "smooth",
    "title": "A real-valued is smooth if it is and its is continuous at all , . A smooth is referred to as -smooth if the is Lipschitz continuous with Lipschitz constant , i.e., \\| f() - f(') \\| \\| - ' \\| {, for any } ,' {R}^{}. The constant quantifies the smoothness of the : the smaller the , the smoother is. {optproblem} with a smooth can be solved effectively by . Indeed, approximate the locally around a current choice using its . This approximation works well if the does not change too rapidly. We can make this informal claim precise by studying the effect of a single with (see Fig. {fig_gd_smooth_dict}). {figure}[H] {center} {tikzpicture}[scale=0.8, x=0.6cm,y=0.05cm] {0.5} plot ({}, {^2}); (w) at (,{}); (wkplus1) at (4+,{(4+)^2}); (wk) at (8+,{(8+)^2}); (wk) -- +(-2, -{4*(8 + )} ) -- +(1, {2*(8 + )}); (w) -- +(-1, {-{2*}} ) -- +(1, {+{2*}}) node[below] {}; (wk) circle (2pt) node[above left] {} node[below right, xshift=-15pt,yshift=-15pt] {} ; (w) circle (2pt) node[above right] {} ; (wkplus1) circle (2pt) node[below right] {}; (wk) -- () ; (wkplus1) -- () ; () -- () node[midway, right] {}; {tikzpicture} {center} {Consider an that is -smooth. Taking a , with , decreases the objective by at least , , . Note that the becomes larger for smaller . Thus, for smoother {objfunc} (i.e., those with smaller ), we can take larger steps. {fig_gd_smooth_dict}} {figure} See also: , , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 274,
    "label": "parameter space",
    "title": "The space of an is the set of all feasible choices for the (see Fig. {fig_param_space_dict}). Many important methods use a that is parameterized by {vector} of the . Two widely used examples of parameterized {model} are {linmodel} and {deepnet}. The space is then often a subset , e.g., all {vector} with a smaller than one. {figure}[H] {center} {tikzpicture} (paramspace) {}; { space }; (theta1) at () {}; {}; (theta2) at () {}; {}; (plotcloud) {}; { }; (plot1start) at () {}; (plot1start) .. controls ++(0.8, 1) and ++(-0.8, -0.8) .. () node[anchor=west] {}; (plot2start) at () {}; (plot2start) .. controls ++(0.8, 0.5) and ++(-0.8, -0.8) .. () node[anchor=west] {}; (theta1) to (); (theta2) to (plot2start); {tikzpicture} {center} {The space of an consists of all feasible choices for the . Each choice for the selects a . {fig_param_space_dict}} {figure} See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 275,
    "label": "data normalization",
    "title": "normalization refers to transformations applied to the {featurevec} of {datapoint} to improve the method's or . For example, in with using a fixed , convergence depends on controlling the of {featurevec} in the . A common approach is to normalize {featurevec} such that their does not exceed one {MLBasics}. \\\\ See also: , , , , , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 276,
    "label": "data augmentation",
    "title": "augmentation methods add synthetic {datapoint} to an existing set of {datapoint}. These synthetic {datapoint} are obtained by perturbations (e.g., adding noise to physical measurements) or transformations (e.g., rotations of images) of the original {datapoint}. These perturbations and transformations are such that the resulting synthetic {datapoint} should still have the same . As a case in point, a rotated cat image is still a cat image even if their {featurevec} (obtained by stacking pixel color intensities) are very different (see Fig. {fig_symmetry_dataaug_dict}). augmentation can be an efficient form of . {figure}[H] {center} {tikzpicture} {}{0.5} {}{2} plot[smooth, tension=1] coordinates {(0,0) (2,1) (4,0) (6,-1) (8,0)}; at (0,0) {}; plot[smooth, tension=1] coordinates {(0 + ,0 + ) (2 + ,1 + ) (4 + ,0 + ) (6 + ,-1 + ) (8 + ,0 + )}; at (8 + ,0 + ) {}; (2,1) circle (2pt) node[above] {}; (6,-1) circle (2pt) node[above] {}; (2,1) to[out=240, in=240] node[midway, below] {} (6,-1); {tikzpicture} {-11mm} {center} { augmentation exploits intrinsic symmetries of {datapoint} in some . We can represent a symmetry by an operator , parameterized by some number . For example, might represent the effect of rotating a cat image by degrees. A with must have the same as a with .{fig_symmetry_dataaug_dict}} {figure} See also: , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 277,
    "label": "local dataset",
    "title": "The concept of a local is in between the concept of a and a . A local consists of several individual {datapoint} characterized by {feature} and {label}. In contrast to a single used in basic methods, a local is also related to other local {dataset} via different notions of similarity. These similarities might arise from {probmodel} or communication infrastructure and are encoded in the edges of an . \\\\ See also: , , , , , , .",
    "color": "orange"
  },
  {
    "id": 278,
    "label": "local model",
    "title": "Consider a collection of {device} that are represented as nodes of an . A local is a assigned to a node . Different nodes might be assigned different {hypospace}, i.e., in general for different nodes . \\\\ See also: , , , .",
    "color": "orange"
  },
  {
    "id": 279,
    "label": "mutual information (MI)",
    "title": "The MI between two {rv} , defined on the same is given by {}{} \\{ {p (,)}{p()p()} \\}. It is a measure of how well we can estimate based solely on . A large value of indicates that can be well predicted solely from . This could be obtained by a learned by an -based method. \\\\ See also: , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 280,
    "label": "zero-gradient condition",
    "title": "Consider the unconstrained with a and . A necessary and sufficient condition for a to solve this problem is that the is the zero such that f ( {} ) = {0} f ( {} ) = _{ {R}^{}} f(). \\\\ See also: , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 281,
    "label": "edge weight",
    "title": "Each edge of an is assigned a non-negative edge weight . A zero edge weight indicates the absence of an edge between nodes . \\\\ See also: .",
    "color": "orange"
  },
  {
    "id": 282,
    "label": "data minimization principle",
    "title": "European protection regulation includes a minimization principle. This principle requires a controller to limit the collection of personal information to what is directly relevant and necessary to accomplish a specified purpose. The should be retained only for as long as necessary to fulfill that purpose {GDPR2016}, . \\\\ See also: .",
    "color": "lightgreen"
  }
]);
      const edges = new vis.DataSet([
  {
    "from": 1,
    "to": 3
  },
  {
    "from": 1,
    "to": 2
  },
  {
    "from": 1,
    "to": 214
  },
  {
    "from": 1,
    "to": 139
  },
  {
    "from": 1,
    "to": 187
  },
  {
    "from": 1,
    "to": 221
  },
  {
    "from": 1,
    "to": 137
  },
  {
    "from": 2,
    "to": 3
  },
  {
    "from": 2,
    "to": 4
  },
  {
    "from": 2,
    "to": 213
  },
  {
    "from": 2,
    "to": 1
  },
  {
    "from": 3,
    "to": 221
  },
  {
    "from": 3,
    "to": 213
  },
  {
    "from": 3,
    "to": 7
  },
  {
    "from": 3,
    "to": 5
  },
  {
    "from": 3,
    "to": 139
  },
  {
    "from": 3,
    "to": 149
  },
  {
    "from": 3,
    "to": 135
  },
  {
    "from": 3,
    "to": 137
  },
  {
    "from": 3,
    "to": 43
  },
  {
    "from": 4,
    "to": 3
  },
  {
    "from": 4,
    "to": 14
  },
  {
    "from": 4,
    "to": 231
  },
  {
    "from": 4,
    "to": 2
  },
  {
    "from": 4,
    "to": 6
  },
  {
    "from": 5,
    "to": 15
  },
  {
    "from": 5,
    "to": 14
  },
  {
    "from": 5,
    "to": 3
  },
  {
    "from": 5,
    "to": 43
  },
  {
    "from": 5,
    "to": 33
  },
  {
    "from": 5,
    "to": 6
  },
  {
    "from": 6,
    "to": 7
  },
  {
    "from": 6,
    "to": 33
  },
  {
    "from": 6,
    "to": 198
  },
  {
    "from": 6,
    "to": 14
  },
  {
    "from": 6,
    "to": 15
  },
  {
    "from": 6,
    "to": 5
  },
  {
    "from": 6,
    "to": 185
  },
  {
    "from": 7,
    "to": 6
  },
  {
    "from": 7,
    "to": 198
  },
  {
    "from": 7,
    "to": 43
  },
  {
    "from": 7,
    "to": 5
  },
  {
    "from": 8,
    "to": 33
  },
  {
    "from": 8,
    "to": 239
  },
  {
    "from": 8,
    "to": 85
  },
  {
    "from": 8,
    "to": 70
  },
  {
    "from": 9,
    "to": 8
  },
  {
    "from": 9,
    "to": 143
  },
  {
    "from": 9,
    "to": 19
  },
  {
    "from": 9,
    "to": 103
  },
  {
    "from": 9,
    "to": 239
  },
  {
    "from": 9,
    "to": 142
  },
  {
    "from": 9,
    "to": 85
  },
  {
    "from": 9,
    "to": 70
  },
  {
    "from": 9,
    "to": 84
  },
  {
    "from": 9,
    "to": 98
  },
  {
    "from": 10,
    "to": 14
  },
  {
    "from": 10,
    "to": 142
  },
  {
    "from": 10,
    "to": 263
  },
  {
    "from": 11,
    "to": 85
  },
  {
    "from": 11,
    "to": 142
  },
  {
    "from": 11,
    "to": 54
  },
  {
    "from": 11,
    "to": 14
  },
  {
    "from": 11,
    "to": 164
  },
  {
    "from": 11,
    "to": 12
  },
  {
    "from": 11,
    "to": 70
  },
  {
    "from": 12,
    "to": 142
  },
  {
    "from": 12,
    "to": 264
  },
  {
    "from": 12,
    "to": 11
  },
  {
    "from": 12,
    "to": 71
  },
  {
    "from": 12,
    "to": 205
  },
  {
    "from": 12,
    "to": 85
  },
  {
    "from": 12,
    "to": 70
  },
  {
    "from": 15,
    "to": 14
  },
  {
    "from": 16,
    "to": 160
  },
  {
    "from": 17,
    "to": 97
  },
  {
    "from": 17,
    "to": 16
  },
  {
    "from": 18,
    "to": 16
  },
  {
    "from": 18,
    "to": 120
  },
  {
    "from": 18,
    "to": 272
  },
  {
    "from": 18,
    "to": 14
  },
  {
    "from": 18,
    "to": 221
  },
  {
    "from": 18,
    "to": 45
  },
  {
    "from": 18,
    "to": 46
  },
  {
    "from": 19,
    "to": 84
  },
  {
    "from": 19,
    "to": 70
  },
  {
    "from": 19,
    "to": 191
  },
  {
    "from": 19,
    "to": 54
  },
  {
    "from": 19,
    "to": 142
  },
  {
    "from": 20,
    "to": 33
  },
  {
    "from": 20,
    "to": 170
  },
  {
    "from": 20,
    "to": 21
  },
  {
    "from": 20,
    "to": 102
  },
  {
    "from": 20,
    "to": 172
  },
  {
    "from": 20,
    "to": 139
  },
  {
    "from": 20,
    "to": 255
  },
  {
    "from": 21,
    "to": 20
  },
  {
    "from": 21,
    "to": 33
  },
  {
    "from": 21,
    "to": 246
  },
  {
    "from": 21,
    "to": 102
  },
  {
    "from": 21,
    "to": 61
  },
  {
    "from": 21,
    "to": 57
  },
  {
    "from": 22,
    "to": 14
  },
  {
    "from": 22,
    "to": 84
  },
  {
    "from": 22,
    "to": 272
  },
  {
    "from": 23,
    "to": 3
  },
  {
    "from": 23,
    "to": 35
  },
  {
    "from": 23,
    "to": 136
  },
  {
    "from": 23,
    "to": 149
  },
  {
    "from": 23,
    "to": 132
  },
  {
    "from": 23,
    "to": 6
  },
  {
    "from": 23,
    "to": 129
  },
  {
    "from": 24,
    "to": 25
  },
  {
    "from": 26,
    "to": 189
  },
  {
    "from": 26,
    "to": 147
  },
  {
    "from": 26,
    "to": 89
  },
  {
    "from": 26,
    "to": 125
  },
  {
    "from": 26,
    "to": 278
  },
  {
    "from": 26,
    "to": 15
  },
  {
    "from": 26,
    "to": 48
  },
  {
    "from": 27,
    "to": 189
  },
  {
    "from": 27,
    "to": 96
  },
  {
    "from": 28,
    "to": 189
  },
  {
    "from": 28,
    "to": 221
  },
  {
    "from": 28,
    "to": 239
  },
  {
    "from": 28,
    "to": 97
  },
  {
    "from": 28,
    "to": 278
  },
  {
    "from": 28,
    "to": 13
  },
  {
    "from": 29,
    "to": 189
  },
  {
    "from": 29,
    "to": 96
  },
  {
    "from": 29,
    "to": 89
  },
  {
    "from": 29,
    "to": 44
  },
  {
    "from": 29,
    "to": 237
  },
  {
    "from": 30,
    "to": 189
  },
  {
    "from": 30,
    "to": 96
  },
  {
    "from": 30,
    "to": 89
  },
  {
    "from": 30,
    "to": 44
  },
  {
    "from": 30,
    "to": 237
  },
  {
    "from": 30,
    "to": 239
  },
  {
    "from": 31,
    "to": 149
  },
  {
    "from": 31,
    "to": 159
  },
  {
    "from": 31,
    "to": 189
  },
  {
    "from": 31,
    "to": 37
  },
  {
    "from": 31,
    "to": 135
  },
  {
    "from": 31,
    "to": 277
  },
  {
    "from": 32,
    "to": 56
  },
  {
    "from": 32,
    "to": 127
  },
  {
    "from": 32,
    "to": 33
  },
  {
    "from": 32,
    "to": 213
  },
  {
    "from": 32,
    "to": 3
  },
  {
    "from": 32,
    "to": 138
  },
  {
    "from": 32,
    "to": 174
  },
  {
    "from": 32,
    "to": 94
  },
  {
    "from": 32,
    "to": 135
  },
  {
    "from": 32,
    "to": 149
  },
  {
    "from": 33,
    "to": 137
  },
  {
    "from": 33,
    "to": 149
  },
  {
    "from": 33,
    "to": 125
  },
  {
    "from": 33,
    "to": 220
  },
  {
    "from": 33,
    "to": 243
  },
  {
    "from": 33,
    "to": 157
  },
  {
    "from": 33,
    "to": 138
  },
  {
    "from": 33,
    "to": 242
  },
  {
    "from": 33,
    "to": 135
  },
  {
    "from": 34,
    "to": 99
  },
  {
    "from": 34,
    "to": 125
  },
  {
    "from": 34,
    "to": 136
  },
  {
    "from": 34,
    "to": 149
  },
  {
    "from": 34,
    "to": 129
  },
  {
    "from": 34,
    "to": 243
  },
  {
    "from": 34,
    "to": 33
  },
  {
    "from": 34,
    "to": 265
  },
  {
    "from": 35,
    "to": 33
  },
  {
    "from": 35,
    "to": 135
  },
  {
    "from": 35,
    "to": 15
  },
  {
    "from": 35,
    "to": 149
  },
  {
    "from": 35,
    "to": 132
  },
  {
    "from": 35,
    "to": 220
  },
  {
    "from": 35,
    "to": 241
  },
  {
    "from": 35,
    "to": 13
  },
  {
    "from": 35,
    "to": 139
  },
  {
    "from": 36,
    "to": 33
  },
  {
    "from": 36,
    "to": 15
  },
  {
    "from": 36,
    "to": 157
  },
  {
    "from": 36,
    "to": 242
  },
  {
    "from": 37,
    "to": 189
  },
  {
    "from": 37,
    "to": 139
  },
  {
    "from": 37,
    "to": 255
  },
  {
    "from": 37,
    "to": 277
  },
  {
    "from": 37,
    "to": 138
  },
  {
    "from": 37,
    "to": 68
  },
  {
    "from": 37,
    "to": 135
  },
  {
    "from": 37,
    "to": 136
  },
  {
    "from": 37,
    "to": 137
  },
  {
    "from": 37,
    "to": 149
  },
  {
    "from": 38,
    "to": 33
  },
  {
    "from": 38,
    "to": 154
  },
  {
    "from": 38,
    "to": 125
  },
  {
    "from": 38,
    "to": 5
  },
  {
    "from": 38,
    "to": 144
  },
  {
    "from": 38,
    "to": 41
  },
  {
    "from": 38,
    "to": 61
  },
  {
    "from": 38,
    "to": 162
  },
  {
    "from": 38,
    "to": 42
  },
  {
    "from": 38,
    "to": 15
  },
  {
    "from": 38,
    "to": 93
  },
  {
    "from": 38,
    "to": 129
  },
  {
    "from": 38,
    "to": 137
  },
  {
    "from": 38,
    "to": 215
  },
  {
    "from": 39,
    "to": 139
  },
  {
    "from": 39,
    "to": 104
  },
  {
    "from": 39,
    "to": 262
  },
  {
    "from": 39,
    "to": 40
  },
  {
    "from": 40,
    "to": 139
  },
  {
    "from": 40,
    "to": 149
  },
  {
    "from": 40,
    "to": 243
  },
  {
    "from": 40,
    "to": 157
  },
  {
    "from": 40,
    "to": 160
  },
  {
    "from": 40,
    "to": 114
  },
  {
    "from": 40,
    "to": 137
  },
  {
    "from": 40,
    "to": 78
  },
  {
    "from": 40,
    "to": 135
  },
  {
    "from": 40,
    "to": 39
  },
  {
    "from": 40,
    "to": 128
  },
  {
    "from": 41,
    "to": 33
  },
  {
    "from": 41,
    "to": 154
  },
  {
    "from": 41,
    "to": 138
  },
  {
    "from": 41,
    "to": 11
  },
  {
    "from": 41,
    "to": 61
  },
  {
    "from": 41,
    "to": 162
  },
  {
    "from": 41,
    "to": 70
  },
  {
    "from": 41,
    "to": 129
  },
  {
    "from": 42,
    "to": 125
  },
  {
    "from": 42,
    "to": 136
  },
  {
    "from": 42,
    "to": 149
  },
  {
    "from": 42,
    "to": 129
  },
  {
    "from": 42,
    "to": 93
  },
  {
    "from": 42,
    "to": 157
  },
  {
    "from": 42,
    "to": 144
  },
  {
    "from": 42,
    "to": 137
  },
  {
    "from": 42,
    "to": 244
  },
  {
    "from": 42,
    "to": 104
  },
  {
    "from": 42,
    "to": 43
  },
  {
    "from": 43,
    "to": 33
  },
  {
    "from": 43,
    "to": 136
  },
  {
    "from": 43,
    "to": 220
  },
  {
    "from": 43,
    "to": 129
  },
  {
    "from": 43,
    "to": 52
  },
  {
    "from": 43,
    "to": 51
  },
  {
    "from": 43,
    "to": 77
  },
  {
    "from": 43,
    "to": 94
  },
  {
    "from": 43,
    "to": 38
  },
  {
    "from": 43,
    "to": 272
  },
  {
    "from": 43,
    "to": 13
  },
  {
    "from": 43,
    "to": 104
  },
  {
    "from": 43,
    "to": 177
  },
  {
    "from": 43,
    "to": 244
  },
  {
    "from": 43,
    "to": 120
  },
  {
    "from": 43,
    "to": 5
  },
  {
    "from": 43,
    "to": 121
  },
  {
    "from": 43,
    "to": 66
  },
  {
    "from": 43,
    "to": 61
  },
  {
    "from": 43,
    "to": 15
  },
  {
    "from": 43,
    "to": 42
  },
  {
    "from": 43,
    "to": 17
  },
  {
    "from": 43,
    "to": 135
  },
  {
    "from": 43,
    "to": 149
  },
  {
    "from": 43,
    "to": 196
  },
  {
    "from": 43,
    "to": 250
  },
  {
    "from": 44,
    "to": 120
  },
  {
    "from": 44,
    "to": 14
  },
  {
    "from": 44,
    "to": 6
  },
  {
    "from": 44,
    "to": 121
  },
  {
    "from": 44,
    "to": 175
  },
  {
    "from": 44,
    "to": 75
  },
  {
    "from": 44,
    "to": 164
  },
  {
    "from": 44,
    "to": 265
  },
  {
    "from": 44,
    "to": 176
  },
  {
    "from": 44,
    "to": 272
  },
  {
    "from": 44,
    "to": 46
  },
  {
    "from": 46,
    "to": 272
  },
  {
    "from": 46,
    "to": 14
  },
  {
    "from": 46,
    "to": 6
  },
  {
    "from": 46,
    "to": 164
  },
  {
    "from": 46,
    "to": 44
  },
  {
    "from": 46,
    "to": 273
  },
  {
    "from": 46,
    "to": 175
  },
  {
    "from": 47,
    "to": 272
  },
  {
    "from": 47,
    "to": 14
  },
  {
    "from": 47,
    "to": 46
  },
  {
    "from": 48,
    "to": 84
  },
  {
    "from": 49,
    "to": 70
  },
  {
    "from": 49,
    "to": 6
  },
  {
    "from": 49,
    "to": 71
  },
  {
    "from": 49,
    "to": 205
  },
  {
    "from": 49,
    "to": 263
  },
  {
    "from": 49,
    "to": 264
  },
  {
    "from": 49,
    "to": 142
  },
  {
    "from": 49,
    "to": 50
  },
  {
    "from": 49,
    "to": 12
  },
  {
    "from": 49,
    "to": 58
  },
  {
    "from": 49,
    "to": 136
  },
  {
    "from": 50,
    "to": 6
  },
  {
    "from": 50,
    "to": 191
  },
  {
    "from": 50,
    "to": 49
  },
  {
    "from": 50,
    "to": 58
  },
  {
    "from": 50,
    "to": 142
  },
  {
    "from": 51,
    "to": 33
  },
  {
    "from": 51,
    "to": 263
  },
  {
    "from": 51,
    "to": 70
  },
  {
    "from": 51,
    "to": 138
  },
  {
    "from": 52,
    "to": 33
  },
  {
    "from": 52,
    "to": 157
  },
  {
    "from": 52,
    "to": 44
  },
  {
    "from": 52,
    "to": 221
  },
  {
    "from": 52,
    "to": 253
  },
  {
    "from": 53,
    "to": 242
  },
  {
    "from": 53,
    "to": 81
  },
  {
    "from": 53,
    "to": 129
  },
  {
    "from": 53,
    "to": 137
  },
  {
    "from": 53,
    "to": 149
  },
  {
    "from": 53,
    "to": 135
  },
  {
    "from": 54,
    "to": 194
  },
  {
    "from": 55,
    "to": 33
  },
  {
    "from": 55,
    "to": 157
  },
  {
    "from": 55,
    "to": 125
  },
  {
    "from": 55,
    "to": 13
  },
  {
    "from": 55,
    "to": 82
  },
  {
    "from": 55,
    "to": 144
  },
  {
    "from": 55,
    "to": 94
  },
  {
    "from": 56,
    "to": 33
  },
  {
    "from": 56,
    "to": 157
  },
  {
    "from": 56,
    "to": 125
  },
  {
    "from": 56,
    "to": 13
  },
  {
    "from": 56,
    "to": 82
  },
  {
    "from": 56,
    "to": 144
  },
  {
    "from": 56,
    "to": 242
  },
  {
    "from": 56,
    "to": 164
  },
  {
    "from": 56,
    "to": 151
  },
  {
    "from": 56,
    "to": 165
  },
  {
    "from": 57,
    "to": 138
  },
  {
    "from": 57,
    "to": 33
  },
  {
    "from": 57,
    "to": 282
  },
  {
    "from": 57,
    "to": 101
  },
  {
    "from": 57,
    "to": 41
  },
  {
    "from": 58,
    "to": 142
  },
  {
    "from": 58,
    "to": 264
  },
  {
    "from": 58,
    "to": 71
  },
  {
    "from": 58,
    "to": 73
  },
  {
    "from": 58,
    "to": 263
  },
  {
    "from": 58,
    "to": 6
  },
  {
    "from": 58,
    "to": 205
  },
  {
    "from": 58,
    "to": 191
  },
  {
    "from": 58,
    "to": 3
  },
  {
    "from": 58,
    "to": 49
  },
  {
    "from": 58,
    "to": 9
  },
  {
    "from": 58,
    "to": 33
  },
  {
    "from": 58,
    "to": 59
  },
  {
    "from": 58,
    "to": 85
  },
  {
    "from": 58,
    "to": 12
  },
  {
    "from": 58,
    "to": 60
  },
  {
    "from": 58,
    "to": 70
  },
  {
    "from": 58,
    "to": 24
  },
  {
    "from": 59,
    "to": 191
  },
  {
    "from": 59,
    "to": 71
  },
  {
    "from": 59,
    "to": 73
  },
  {
    "from": 59,
    "to": 58
  },
  {
    "from": 59,
    "to": 10
  },
  {
    "from": 59,
    "to": 18
  },
  {
    "from": 59,
    "to": 142
  },
  {
    "from": 59,
    "to": 164
  },
  {
    "from": 60,
    "to": 49
  },
  {
    "from": 60,
    "to": 71
  },
  {
    "from": 60,
    "to": 14
  },
  {
    "from": 60,
    "to": 207
  },
  {
    "from": 60,
    "to": 202
  },
  {
    "from": 60,
    "to": 85
  },
  {
    "from": 60,
    "to": 58
  },
  {
    "from": 60,
    "to": 129
  },
  {
    "from": 60,
    "to": 142
  },
  {
    "from": 60,
    "to": 248
  },
  {
    "from": 61,
    "to": 52
  },
  {
    "from": 61,
    "to": 51
  },
  {
    "from": 61,
    "to": 33
  },
  {
    "from": 61,
    "to": 222
  },
  {
    "from": 61,
    "to": 66
  },
  {
    "from": 61,
    "to": 138
  },
  {
    "from": 61,
    "to": 101
  },
  {
    "from": 61,
    "to": 137
  },
  {
    "from": 62,
    "to": 242
  },
  {
    "from": 62,
    "to": 129
  },
  {
    "from": 62,
    "to": 125
  },
  {
    "from": 62,
    "to": 137
  },
  {
    "from": 62,
    "to": 149
  },
  {
    "from": 62,
    "to": 135
  },
  {
    "from": 63,
    "to": 198
  },
  {
    "from": 63,
    "to": 6
  },
  {
    "from": 63,
    "to": 13
  },
  {
    "from": 64,
    "to": 157
  },
  {
    "from": 64,
    "to": 274
  },
  {
    "from": 64,
    "to": 160
  },
  {
    "from": 64,
    "to": 273
  },
  {
    "from": 64,
    "to": 253
  },
  {
    "from": 64,
    "to": 221
  },
  {
    "from": 64,
    "to": 44
  },
  {
    "from": 64,
    "to": 63
  },
  {
    "from": 65,
    "to": 33
  },
  {
    "from": 65,
    "to": 139
  },
  {
    "from": 65,
    "to": 144
  },
  {
    "from": 65,
    "to": 157
  },
  {
    "from": 65,
    "to": 221
  },
  {
    "from": 65,
    "to": 69
  },
  {
    "from": 65,
    "to": 263
  },
  {
    "from": 65,
    "to": 102
  },
  {
    "from": 65,
    "to": 149
  },
  {
    "from": 65,
    "to": 70
  },
  {
    "from": 65,
    "to": 142
  },
  {
    "from": 65,
    "to": 21
  },
  {
    "from": 65,
    "to": 79
  },
  {
    "from": 65,
    "to": 129
  },
  {
    "from": 66,
    "to": 61
  },
  {
    "from": 66,
    "to": 33
  },
  {
    "from": 66,
    "to": 149
  },
  {
    "from": 66,
    "to": 129
  },
  {
    "from": 66,
    "to": 67
  },
  {
    "from": 66,
    "to": 157
  },
  {
    "from": 66,
    "to": 144
  },
  {
    "from": 66,
    "to": 135
  },
  {
    "from": 66,
    "to": 20
  },
  {
    "from": 67,
    "to": 33
  },
  {
    "from": 67,
    "to": 139
  },
  {
    "from": 67,
    "to": 144
  },
  {
    "from": 67,
    "to": 221
  },
  {
    "from": 67,
    "to": 129
  },
  {
    "from": 67,
    "to": 149
  },
  {
    "from": 67,
    "to": 164
  },
  {
    "from": 67,
    "to": 94
  },
  {
    "from": 67,
    "to": 138
  },
  {
    "from": 67,
    "to": 263
  },
  {
    "from": 67,
    "to": 248
  },
  {
    "from": 68,
    "to": 33
  },
  {
    "from": 68,
    "to": 139
  },
  {
    "from": 68,
    "to": 221
  },
  {
    "from": 68,
    "to": 129
  },
  {
    "from": 68,
    "to": 149
  },
  {
    "from": 68,
    "to": 137
  },
  {
    "from": 68,
    "to": 102
  },
  {
    "from": 68,
    "to": 15
  },
  {
    "from": 68,
    "to": 135
  },
  {
    "from": 69,
    "to": 33
  },
  {
    "from": 69,
    "to": 139
  },
  {
    "from": 69,
    "to": 135
  },
  {
    "from": 69,
    "to": 149
  },
  {
    "from": 69,
    "to": 70
  },
  {
    "from": 69,
    "to": 138
  },
  {
    "from": 69,
    "to": 279
  },
  {
    "from": 69,
    "to": 65
  },
  {
    "from": 69,
    "to": 21
  },
  {
    "from": 69,
    "to": 57
  },
  {
    "from": 69,
    "to": 129
  },
  {
    "from": 70,
    "to": 263
  },
  {
    "from": 70,
    "to": 209
  },
  {
    "from": 70,
    "to": 149
  },
  {
    "from": 70,
    "to": 142
  },
  {
    "from": 70,
    "to": 265
  },
  {
    "from": 71,
    "to": 142
  },
  {
    "from": 71,
    "to": 198
  },
  {
    "from": 71,
    "to": 215
  },
  {
    "from": 71,
    "to": 263
  },
  {
    "from": 71,
    "to": 94
  },
  {
    "from": 72,
    "to": 142
  },
  {
    "from": 72,
    "to": 139
  },
  {
    "from": 72,
    "to": 16
  },
  {
    "from": 72,
    "to": 71
  },
  {
    "from": 72,
    "to": 70
  },
  {
    "from": 72,
    "to": 149
  },
  {
    "from": 72,
    "to": 195
  },
  {
    "from": 72,
    "to": 66
  },
  {
    "from": 72,
    "to": 265
  },
  {
    "from": 72,
    "to": 273
  },
  {
    "from": 73,
    "to": 142
  },
  {
    "from": 73,
    "to": 215
  },
  {
    "from": 73,
    "to": 6
  },
  {
    "from": 74,
    "to": 125
  },
  {
    "from": 74,
    "to": 14
  },
  {
    "from": 74,
    "to": 139
  },
  {
    "from": 74,
    "to": 259
  },
  {
    "from": 74,
    "to": 149
  },
  {
    "from": 74,
    "to": 136
  },
  {
    "from": 74,
    "to": 76
  },
  {
    "from": 75,
    "to": 76
  },
  {
    "from": 76,
    "to": 89
  },
  {
    "from": 76,
    "to": 48
  },
  {
    "from": 77,
    "to": 33
  },
  {
    "from": 77,
    "to": 220
  },
  {
    "from": 77,
    "to": 221
  },
  {
    "from": 77,
    "to": 139
  },
  {
    "from": 77,
    "to": 191
  },
  {
    "from": 77,
    "to": 157
  },
  {
    "from": 77,
    "to": 149
  },
  {
    "from": 77,
    "to": 142
  },
  {
    "from": 78,
    "to": 137
  },
  {
    "from": 78,
    "to": 149
  },
  {
    "from": 78,
    "to": 135
  },
  {
    "from": 79,
    "to": 135
  },
  {
    "from": 79,
    "to": 149
  },
  {
    "from": 80,
    "to": 3
  },
  {
    "from": 80,
    "to": 231
  },
  {
    "from": 80,
    "to": 33
  },
  {
    "from": 80,
    "to": 237
  },
  {
    "from": 80,
    "to": 213
  },
  {
    "from": 80,
    "to": 187
  },
  {
    "from": 80,
    "to": 144
  },
  {
    "from": 80,
    "to": 135
  },
  {
    "from": 80,
    "to": 149
  },
  {
    "from": 81,
    "to": 125
  },
  {
    "from": 81,
    "to": 15
  },
  {
    "from": 81,
    "to": 137
  },
  {
    "from": 81,
    "to": 128
  },
  {
    "from": 81,
    "to": 14
  },
  {
    "from": 81,
    "to": 129
  },
  {
    "from": 81,
    "to": 78
  },
  {
    "from": 81,
    "to": 196
  },
  {
    "from": 82,
    "to": 94
  },
  {
    "from": 82,
    "to": 125
  },
  {
    "from": 82,
    "to": 139
  },
  {
    "from": 82,
    "to": 242
  },
  {
    "from": 82,
    "to": 149
  },
  {
    "from": 83,
    "to": 84
  },
  {
    "from": 83,
    "to": 76
  },
  {
    "from": 84,
    "to": 15
  },
  {
    "from": 84,
    "to": 262
  },
  {
    "from": 85,
    "to": 33
  },
  {
    "from": 85,
    "to": 138
  },
  {
    "from": 85,
    "to": 129
  },
  {
    "from": 85,
    "to": 137
  },
  {
    "from": 85,
    "to": 149
  },
  {
    "from": 85,
    "to": 54
  },
  {
    "from": 85,
    "to": 70
  },
  {
    "from": 85,
    "to": 94
  },
  {
    "from": 85,
    "to": 11
  },
  {
    "from": 85,
    "to": 73
  },
  {
    "from": 85,
    "to": 93
  },
  {
    "from": 86,
    "to": 33
  },
  {
    "from": 86,
    "to": 70
  },
  {
    "from": 86,
    "to": 8
  },
  {
    "from": 86,
    "to": 87
  },
  {
    "from": 86,
    "to": 142
  },
  {
    "from": 86,
    "to": 71
  },
  {
    "from": 86,
    "to": 138
  },
  {
    "from": 86,
    "to": 85
  },
  {
    "from": 86,
    "to": 118
  },
  {
    "from": 87,
    "to": 263
  },
  {
    "from": 87,
    "to": 33
  },
  {
    "from": 87,
    "to": 34
  },
  {
    "from": 87,
    "to": 118
  },
  {
    "from": 87,
    "to": 85
  },
  {
    "from": 88,
    "to": 33
  },
  {
    "from": 88,
    "to": 221
  },
  {
    "from": 88,
    "to": 157
  },
  {
    "from": 88,
    "to": 242
  },
  {
    "from": 88,
    "to": 139
  },
  {
    "from": 88,
    "to": 144
  },
  {
    "from": 88,
    "to": 94
  },
  {
    "from": 88,
    "to": 125
  },
  {
    "from": 88,
    "to": 70
  },
  {
    "from": 88,
    "to": 54
  },
  {
    "from": 88,
    "to": 160
  },
  {
    "from": 88,
    "to": 169
  },
  {
    "from": 88,
    "to": 86
  },
  {
    "from": 89,
    "to": 189
  },
  {
    "from": 89,
    "to": 84
  },
  {
    "from": 89,
    "to": 277
  },
  {
    "from": 89,
    "to": 278
  },
  {
    "from": 89,
    "to": 113
  },
  {
    "from": 89,
    "to": 221
  },
  {
    "from": 89,
    "to": 242
  },
  {
    "from": 89,
    "to": 255
  },
  {
    "from": 89,
    "to": 26
  },
  {
    "from": 90,
    "to": 14
  },
  {
    "from": 90,
    "to": 6
  },
  {
    "from": 90,
    "to": 7
  },
  {
    "from": 91,
    "to": 90
  },
  {
    "from": 91,
    "to": 198
  },
  {
    "from": 91,
    "to": 6
  },
  {
    "from": 92,
    "to": 122
  },
  {
    "from": 92,
    "to": 6
  },
  {
    "from": 93,
    "to": 101
  },
  {
    "from": 93,
    "to": 33
  },
  {
    "from": 93,
    "to": 135
  },
  {
    "from": 93,
    "to": 149
  },
  {
    "from": 93,
    "to": 129
  },
  {
    "from": 93,
    "to": 136
  },
  {
    "from": 93,
    "to": 42
  },
  {
    "from": 93,
    "to": 120
  },
  {
    "from": 93,
    "to": 121
  },
  {
    "from": 93,
    "to": 14
  },
  {
    "from": 93,
    "to": 78
  },
  {
    "from": 93,
    "to": 15
  },
  {
    "from": 94,
    "to": 125
  },
  {
    "from": 94,
    "to": 137
  },
  {
    "from": 94,
    "to": 149
  },
  {
    "from": 94,
    "to": 129
  },
  {
    "from": 94,
    "to": 243
  },
  {
    "from": 94,
    "to": 191
  },
  {
    "from": 94,
    "to": 142
  },
  {
    "from": 94,
    "to": 219
  },
  {
    "from": 94,
    "to": 242
  },
  {
    "from": 94,
    "to": 263
  },
  {
    "from": 94,
    "to": 135
  },
  {
    "from": 95,
    "to": 250
  },
  {
    "from": 95,
    "to": 14
  },
  {
    "from": 95,
    "to": 262
  },
  {
    "from": 96,
    "to": 97
  },
  {
    "from": 96,
    "to": 255
  },
  {
    "from": 96,
    "to": 189
  },
  {
    "from": 97,
    "to": 43
  },
  {
    "from": 97,
    "to": 144
  },
  {
    "from": 97,
    "to": 221
  },
  {
    "from": 97,
    "to": 44
  },
  {
    "from": 97,
    "to": 8
  },
  {
    "from": 98,
    "to": 8
  },
  {
    "from": 98,
    "to": 97
  },
  {
    "from": 98,
    "to": 239
  },
  {
    "from": 98,
    "to": 121
  },
  {
    "from": 98,
    "to": 160
  },
  {
    "from": 98,
    "to": 149
  },
  {
    "from": 98,
    "to": 9
  },
  {
    "from": 98,
    "to": 17
  },
  {
    "from": 98,
    "to": 237
  },
  {
    "from": 99,
    "to": 33
  },
  {
    "from": 99,
    "to": 138
  },
  {
    "from": 99,
    "to": 221
  },
  {
    "from": 99,
    "to": 13
  },
  {
    "from": 99,
    "to": 24
  },
  {
    "from": 99,
    "to": 202
  },
  {
    "from": 99,
    "to": 125
  },
  {
    "from": 99,
    "to": 149
  },
  {
    "from": 99,
    "to": 240
  },
  {
    "from": 99,
    "to": 100
  },
  {
    "from": 100,
    "to": 97
  },
  {
    "from": 100,
    "to": 138
  },
  {
    "from": 100,
    "to": 85
  },
  {
    "from": 100,
    "to": 149
  },
  {
    "from": 100,
    "to": 33
  },
  {
    "from": 100,
    "to": 240
  },
  {
    "from": 100,
    "to": 221
  },
  {
    "from": 100,
    "to": 99
  },
  {
    "from": 101,
    "to": 61
  },
  {
    "from": 101,
    "to": 33
  },
  {
    "from": 101,
    "to": 41
  },
  {
    "from": 101,
    "to": 222
  },
  {
    "from": 101,
    "to": 216
  },
  {
    "from": 101,
    "to": 78
  },
  {
    "from": 101,
    "to": 129
  },
  {
    "from": 101,
    "to": 244
  },
  {
    "from": 102,
    "to": 33
  },
  {
    "from": 102,
    "to": 125
  },
  {
    "from": 102,
    "to": 15
  },
  {
    "from": 102,
    "to": 137
  },
  {
    "from": 102,
    "to": 149
  },
  {
    "from": 102,
    "to": 135
  },
  {
    "from": 103,
    "to": 84
  },
  {
    "from": 103,
    "to": 228
  },
  {
    "from": 103,
    "to": 54
  },
  {
    "from": 103,
    "to": 137
  },
  {
    "from": 103,
    "to": 48
  },
  {
    "from": 104,
    "to": 250
  },
  {
    "from": 104,
    "to": 33
  },
  {
    "from": 105,
    "to": 33
  },
  {
    "from": 105,
    "to": 125
  },
  {
    "from": 105,
    "to": 242
  },
  {
    "from": 105,
    "to": 154
  },
  {
    "from": 105,
    "to": 138
  },
  {
    "from": 105,
    "to": 70
  },
  {
    "from": 105,
    "to": 13
  },
  {
    "from": 105,
    "to": 94
  },
  {
    "from": 105,
    "to": 220
  },
  {
    "from": 105,
    "to": 260
  },
  {
    "from": 105,
    "to": 261
  },
  {
    "from": 105,
    "to": 137
  },
  {
    "from": 105,
    "to": 149
  },
  {
    "from": 105,
    "to": 243
  },
  {
    "from": 105,
    "to": 263
  },
  {
    "from": 105,
    "to": 49
  },
  {
    "from": 105,
    "to": 62
  },
  {
    "from": 105,
    "to": 71
  },
  {
    "from": 105,
    "to": 73
  },
  {
    "from": 105,
    "to": 135
  },
  {
    "from": 106,
    "to": 78
  },
  {
    "from": 106,
    "to": 104
  },
  {
    "from": 107,
    "to": 84
  },
  {
    "from": 107,
    "to": 227
  },
  {
    "from": 107,
    "to": 281
  },
  {
    "from": 107,
    "to": 149
  },
  {
    "from": 107,
    "to": 228
  },
  {
    "from": 108,
    "to": 227
  },
  {
    "from": 108,
    "to": 107
  },
  {
    "from": 108,
    "to": 84
  },
  {
    "from": 108,
    "to": 181
  },
  {
    "from": 108,
    "to": 149
  },
  {
    "from": 108,
    "to": 198
  },
  {
    "from": 108,
    "to": 200
  },
  {
    "from": 108,
    "to": 225
  },
  {
    "from": 108,
    "to": 208
  },
  {
    "from": 108,
    "to": 228
  },
  {
    "from": 108,
    "to": 233
  },
  {
    "from": 108,
    "to": 174
  },
  {
    "from": 108,
    "to": 231
  },
  {
    "from": 108,
    "to": 13
  },
  {
    "from": 108,
    "to": 48
  },
  {
    "from": 108,
    "to": 136
  },
  {
    "from": 108,
    "to": 232
  },
  {
    "from": 109,
    "to": 227
  },
  {
    "from": 109,
    "to": 84
  },
  {
    "from": 109,
    "to": 200
  },
  {
    "from": 109,
    "to": 136
  },
  {
    "from": 110,
    "to": 136
  },
  {
    "from": 110,
    "to": 137
  },
  {
    "from": 110,
    "to": 149
  },
  {
    "from": 110,
    "to": 125
  },
  {
    "from": 110,
    "to": 33
  },
  {
    "from": 110,
    "to": 157
  },
  {
    "from": 110,
    "to": 220
  },
  {
    "from": 110,
    "to": 221
  },
  {
    "from": 110,
    "to": 15
  },
  {
    "from": 111,
    "to": 149
  },
  {
    "from": 111,
    "to": 228
  },
  {
    "from": 111,
    "to": 225
  },
  {
    "from": 111,
    "to": 224
  },
  {
    "from": 112,
    "to": 33
  },
  {
    "from": 112,
    "to": 221
  },
  {
    "from": 112,
    "to": 139
  },
  {
    "from": 112,
    "to": 191
  },
  {
    "from": 112,
    "to": 142
  },
  {
    "from": 112,
    "to": 110
  },
  {
    "from": 112,
    "to": 263
  },
  {
    "from": 112,
    "to": 215
  },
  {
    "from": 112,
    "to": 90
  },
  {
    "from": 112,
    "to": 70
  },
  {
    "from": 112,
    "to": 62
  },
  {
    "from": 112,
    "to": 149
  },
  {
    "from": 113,
    "to": 163
  },
  {
    "from": 113,
    "to": 168
  },
  {
    "from": 113,
    "to": 221
  },
  {
    "from": 113,
    "to": 161
  },
  {
    "from": 114,
    "to": 129
  },
  {
    "from": 114,
    "to": 137
  },
  {
    "from": 114,
    "to": 149
  },
  {
    "from": 114,
    "to": 135
  },
  {
    "from": 115,
    "to": 137
  },
  {
    "from": 115,
    "to": 128
  },
  {
    "from": 115,
    "to": 125
  },
  {
    "from": 115,
    "to": 139
  },
  {
    "from": 115,
    "to": 53
  },
  {
    "from": 115,
    "to": 242
  },
  {
    "from": 115,
    "to": 259
  },
  {
    "from": 115,
    "to": 135
  },
  {
    "from": 115,
    "to": 149
  },
  {
    "from": 116,
    "to": 33
  },
  {
    "from": 116,
    "to": 125
  },
  {
    "from": 116,
    "to": 137
  },
  {
    "from": 116,
    "to": 149
  },
  {
    "from": 116,
    "to": 129
  },
  {
    "from": 116,
    "to": 243
  },
  {
    "from": 116,
    "to": 242
  },
  {
    "from": 116,
    "to": 219
  },
  {
    "from": 116,
    "to": 260
  },
  {
    "from": 116,
    "to": 105
  },
  {
    "from": 116,
    "to": 118
  },
  {
    "from": 116,
    "to": 135
  },
  {
    "from": 117,
    "to": 277
  },
  {
    "from": 117,
    "to": 189
  },
  {
    "from": 118,
    "to": 125
  },
  {
    "from": 118,
    "to": 105
  },
  {
    "from": 118,
    "to": 242
  },
  {
    "from": 118,
    "to": 116
  },
  {
    "from": 119,
    "to": 120
  },
  {
    "from": 119,
    "to": 14
  },
  {
    "from": 119,
    "to": 272
  },
  {
    "from": 120,
    "to": 14
  },
  {
    "from": 120,
    "to": 121
  },
  {
    "from": 121,
    "to": 14
  },
  {
    "from": 121,
    "to": 6
  },
  {
    "from": 122,
    "to": 14
  },
  {
    "from": 122,
    "to": 6
  },
  {
    "from": 123,
    "to": 189
  },
  {
    "from": 123,
    "to": 97
  },
  {
    "from": 123,
    "to": 221
  },
  {
    "from": 123,
    "to": 28
  },
  {
    "from": 123,
    "to": 239
  },
  {
    "from": 123,
    "to": 46
  },
  {
    "from": 123,
    "to": 278
  },
  {
    "from": 124,
    "to": 95
  },
  {
    "from": 124,
    "to": 250
  },
  {
    "from": 125,
    "to": 15
  },
  {
    "from": 125,
    "to": 14
  },
  {
    "from": 125,
    "to": 132
  },
  {
    "from": 125,
    "to": 128
  },
  {
    "from": 125,
    "to": 149
  },
  {
    "from": 125,
    "to": 137
  },
  {
    "from": 125,
    "to": 129
  },
  {
    "from": 125,
    "to": 33
  },
  {
    "from": 125,
    "to": 135
  },
  {
    "from": 126,
    "to": 220
  },
  {
    "from": 126,
    "to": 33
  },
  {
    "from": 126,
    "to": 127
  },
  {
    "from": 127,
    "to": 220
  },
  {
    "from": 127,
    "to": 221
  },
  {
    "from": 127,
    "to": 5
  },
  {
    "from": 127,
    "to": 262
  },
  {
    "from": 127,
    "to": 77
  },
  {
    "from": 127,
    "to": 250
  },
  {
    "from": 127,
    "to": 265
  },
  {
    "from": 128,
    "to": 33
  },
  {
    "from": 128,
    "to": 137
  },
  {
    "from": 128,
    "to": 149
  },
  {
    "from": 128,
    "to": 114
  },
  {
    "from": 128,
    "to": 78
  },
  {
    "from": 128,
    "to": 135
  },
  {
    "from": 129,
    "to": 33
  },
  {
    "from": 129,
    "to": 125
  },
  {
    "from": 129,
    "to": 15
  },
  {
    "from": 129,
    "to": 149
  },
  {
    "from": 129,
    "to": 137
  },
  {
    "from": 129,
    "to": 135
  },
  {
    "from": 130,
    "to": 139
  },
  {
    "from": 130,
    "to": 149
  },
  {
    "from": 130,
    "to": 248
  },
  {
    "from": 131,
    "to": 33
  },
  {
    "from": 131,
    "to": 191
  },
  {
    "from": 131,
    "to": 263
  },
  {
    "from": 131,
    "to": 130
  },
  {
    "from": 131,
    "to": 142
  },
  {
    "from": 131,
    "to": 149
  },
  {
    "from": 132,
    "to": 135
  },
  {
    "from": 132,
    "to": 33
  },
  {
    "from": 132,
    "to": 136
  },
  {
    "from": 132,
    "to": 149
  },
  {
    "from": 132,
    "to": 272
  },
  {
    "from": 132,
    "to": 84
  },
  {
    "from": 132,
    "to": 198
  },
  {
    "from": 132,
    "to": 35
  },
  {
    "from": 133,
    "to": 139
  },
  {
    "from": 133,
    "to": 255
  },
  {
    "from": 133,
    "to": 135
  },
  {
    "from": 133,
    "to": 137
  },
  {
    "from": 133,
    "to": 138
  },
  {
    "from": 133,
    "to": 33
  },
  {
    "from": 133,
    "to": 149
  },
  {
    "from": 134,
    "to": 3
  },
  {
    "from": 134,
    "to": 6
  },
  {
    "from": 134,
    "to": 184
  },
  {
    "from": 134,
    "to": 15
  },
  {
    "from": 134,
    "to": 136
  },
  {
    "from": 135,
    "to": 149
  },
  {
    "from": 135,
    "to": 140
  },
  {
    "from": 136,
    "to": 135
  },
  {
    "from": 136,
    "to": 6
  },
  {
    "from": 136,
    "to": 33
  },
  {
    "from": 136,
    "to": 198
  },
  {
    "from": 136,
    "to": 7
  },
  {
    "from": 136,
    "to": 185
  },
  {
    "from": 137,
    "to": 149
  },
  {
    "from": 138,
    "to": 139
  },
  {
    "from": 138,
    "to": 149
  },
  {
    "from": 139,
    "to": 137
  },
  {
    "from": 139,
    "to": 33
  },
  {
    "from": 139,
    "to": 157
  },
  {
    "from": 139,
    "to": 151
  },
  {
    "from": 139,
    "to": 138
  },
  {
    "from": 139,
    "to": 149
  },
  {
    "from": 139,
    "to": 132
  },
  {
    "from": 139,
    "to": 128
  },
  {
    "from": 139,
    "to": 61
  },
  {
    "from": 139,
    "to": 135
  },
  {
    "from": 140,
    "to": 125
  },
  {
    "from": 140,
    "to": 15
  },
  {
    "from": 140,
    "to": 149
  },
  {
    "from": 140,
    "to": 129
  },
  {
    "from": 140,
    "to": 137
  },
  {
    "from": 140,
    "to": 135
  },
  {
    "from": 141,
    "to": 149
  },
  {
    "from": 141,
    "to": 137
  },
  {
    "from": 142,
    "to": 14
  },
  {
    "from": 142,
    "to": 143
  },
  {
    "from": 142,
    "to": 6
  },
  {
    "from": 142,
    "to": 198
  },
  {
    "from": 142,
    "to": 54
  },
  {
    "from": 142,
    "to": 15
  },
  {
    "from": 142,
    "to": 193
  },
  {
    "from": 143,
    "to": 54
  },
  {
    "from": 143,
    "to": 248
  },
  {
    "from": 143,
    "to": 263
  },
  {
    "from": 143,
    "to": 14
  },
  {
    "from": 143,
    "to": 33
  },
  {
    "from": 143,
    "to": 142
  },
  {
    "from": 143,
    "to": 85
  },
  {
    "from": 143,
    "to": 70
  },
  {
    "from": 143,
    "to": 194
  },
  {
    "from": 144,
    "to": 139
  },
  {
    "from": 144,
    "to": 157
  },
  {
    "from": 144,
    "to": 125
  },
  {
    "from": 144,
    "to": 242
  },
  {
    "from": 144,
    "to": 148
  },
  {
    "from": 144,
    "to": 150
  },
  {
    "from": 144,
    "to": 33
  },
  {
    "from": 144,
    "to": 220
  },
  {
    "from": 144,
    "to": 149
  },
  {
    "from": 145,
    "to": 89
  },
  {
    "from": 145,
    "to": 278
  },
  {
    "from": 145,
    "to": 220
  },
  {
    "from": 146,
    "to": 239
  },
  {
    "from": 146,
    "to": 144
  },
  {
    "from": 146,
    "to": 121
  },
  {
    "from": 146,
    "to": 148
  },
  {
    "from": 146,
    "to": 221
  },
  {
    "from": 146,
    "to": 149
  },
  {
    "from": 147,
    "to": 138
  },
  {
    "from": 147,
    "to": 84
  },
  {
    "from": 147,
    "to": 189
  },
  {
    "from": 147,
    "to": 277
  },
  {
    "from": 147,
    "to": 255
  },
  {
    "from": 148,
    "to": 242
  },
  {
    "from": 148,
    "to": 125
  },
  {
    "from": 148,
    "to": 144
  },
  {
    "from": 148,
    "to": 157
  },
  {
    "from": 148,
    "to": 137
  },
  {
    "from": 148,
    "to": 149
  },
  {
    "from": 149,
    "to": 138
  },
  {
    "from": 149,
    "to": 135
  },
  {
    "from": 149,
    "to": 137
  },
  {
    "from": 149,
    "to": 33
  },
  {
    "from": 149,
    "to": 139
  },
  {
    "from": 149,
    "to": 142
  },
  {
    "from": 149,
    "to": 193
  },
  {
    "from": 150,
    "to": 125
  },
  {
    "from": 150,
    "to": 33
  },
  {
    "from": 150,
    "to": 157
  },
  {
    "from": 150,
    "to": 144
  },
  {
    "from": 150,
    "to": 242
  },
  {
    "from": 150,
    "to": 153
  },
  {
    "from": 150,
    "to": 151
  },
  {
    "from": 151,
    "to": 125
  },
  {
    "from": 151,
    "to": 33
  },
  {
    "from": 151,
    "to": 157
  },
  {
    "from": 151,
    "to": 144
  },
  {
    "from": 151,
    "to": 242
  },
  {
    "from": 151,
    "to": 149
  },
  {
    "from": 152,
    "to": 14
  },
  {
    "from": 152,
    "to": 3
  },
  {
    "from": 152,
    "to": 6
  },
  {
    "from": 153,
    "to": 94
  },
  {
    "from": 153,
    "to": 125
  },
  {
    "from": 153,
    "to": 33
  },
  {
    "from": 153,
    "to": 157
  },
  {
    "from": 153,
    "to": 242
  },
  {
    "from": 153,
    "to": 151
  },
  {
    "from": 153,
    "to": 150
  },
  {
    "from": 153,
    "to": 148
  },
  {
    "from": 153,
    "to": 220
  },
  {
    "from": 153,
    "to": 149
  },
  {
    "from": 154,
    "to": 157
  },
  {
    "from": 154,
    "to": 153
  },
  {
    "from": 154,
    "to": 149
  },
  {
    "from": 155,
    "to": 33
  },
  {
    "from": 155,
    "to": 150
  },
  {
    "from": 156,
    "to": 137
  },
  {
    "from": 156,
    "to": 128
  },
  {
    "from": 156,
    "to": 81
  },
  {
    "from": 156,
    "to": 149
  },
  {
    "from": 156,
    "to": 135
  },
  {
    "from": 156,
    "to": 196
  },
  {
    "from": 157,
    "to": 16
  },
  {
    "from": 157,
    "to": 125
  },
  {
    "from": 157,
    "to": 13
  },
  {
    "from": 157,
    "to": 242
  },
  {
    "from": 157,
    "to": 82
  },
  {
    "from": 157,
    "to": 139
  },
  {
    "from": 157,
    "to": 144
  },
  {
    "from": 157,
    "to": 33
  },
  {
    "from": 158,
    "to": 137
  },
  {
    "from": 158,
    "to": 78
  },
  {
    "from": 158,
    "to": 149
  },
  {
    "from": 159,
    "to": 125
  },
  {
    "from": 159,
    "to": 33
  },
  {
    "from": 159,
    "to": 149
  },
  {
    "from": 159,
    "to": 141
  },
  {
    "from": 160,
    "to": 14
  },
  {
    "from": 160,
    "to": 15
  },
  {
    "from": 160,
    "to": 33
  },
  {
    "from": 160,
    "to": 221
  },
  {
    "from": 160,
    "to": 125
  },
  {
    "from": 160,
    "to": 94
  },
  {
    "from": 160,
    "to": 242
  },
  {
    "from": 160,
    "to": 82
  },
  {
    "from": 160,
    "to": 144
  },
  {
    "from": 160,
    "to": 237
  },
  {
    "from": 160,
    "to": 13
  },
  {
    "from": 160,
    "to": 24
  },
  {
    "from": 160,
    "to": 243
  },
  {
    "from": 161,
    "to": 125
  },
  {
    "from": 161,
    "to": 220
  },
  {
    "from": 161,
    "to": 129
  },
  {
    "from": 161,
    "to": 144
  },
  {
    "from": 161,
    "to": 214
  },
  {
    "from": 161,
    "to": 178
  },
  {
    "from": 161,
    "to": 149
  },
  {
    "from": 161,
    "to": 15
  },
  {
    "from": 162,
    "to": 33
  },
  {
    "from": 162,
    "to": 127
  },
  {
    "from": 162,
    "to": 157
  },
  {
    "from": 162,
    "to": 56
  },
  {
    "from": 162,
    "to": 125
  },
  {
    "from": 162,
    "to": 144
  },
  {
    "from": 162,
    "to": 221
  },
  {
    "from": 162,
    "to": 135
  },
  {
    "from": 162,
    "to": 213
  },
  {
    "from": 162,
    "to": 242
  },
  {
    "from": 162,
    "to": 160
  },
  {
    "from": 162,
    "to": 148
  },
  {
    "from": 162,
    "to": 94
  },
  {
    "from": 162,
    "to": 276
  },
  {
    "from": 162,
    "to": 142
  },
  {
    "from": 162,
    "to": 136
  },
  {
    "from": 162,
    "to": 149
  },
  {
    "from": 162,
    "to": 214
  },
  {
    "from": 162,
    "to": 137
  },
  {
    "from": 162,
    "to": 151
  },
  {
    "from": 162,
    "to": 155
  },
  {
    "from": 162,
    "to": 13
  },
  {
    "from": 162,
    "to": 58
  },
  {
    "from": 163,
    "to": 157
  },
  {
    "from": 163,
    "to": 125
  },
  {
    "from": 163,
    "to": 82
  },
  {
    "from": 163,
    "to": 144
  },
  {
    "from": 163,
    "to": 56
  },
  {
    "from": 163,
    "to": 162
  },
  {
    "from": 163,
    "to": 161
  },
  {
    "from": 163,
    "to": 265
  },
  {
    "from": 163,
    "to": 160
  },
  {
    "from": 163,
    "to": 242
  },
  {
    "from": 163,
    "to": 43
  },
  {
    "from": 163,
    "to": 62
  },
  {
    "from": 163,
    "to": 164
  },
  {
    "from": 163,
    "to": 169
  },
  {
    "from": 163,
    "to": 137
  },
  {
    "from": 163,
    "to": 149
  },
  {
    "from": 163,
    "to": 58
  },
  {
    "from": 163,
    "to": 136
  },
  {
    "from": 164,
    "to": 144
  },
  {
    "from": 164,
    "to": 33
  },
  {
    "from": 164,
    "to": 222
  },
  {
    "from": 164,
    "to": 157
  },
  {
    "from": 164,
    "to": 125
  },
  {
    "from": 164,
    "to": 242
  },
  {
    "from": 164,
    "to": 138
  },
  {
    "from": 164,
    "to": 70
  },
  {
    "from": 164,
    "to": 219
  },
  {
    "from": 164,
    "to": 263
  },
  {
    "from": 164,
    "to": 94
  },
  {
    "from": 164,
    "to": 82
  },
  {
    "from": 164,
    "to": 165
  },
  {
    "from": 164,
    "to": 54
  },
  {
    "from": 164,
    "to": 129
  },
  {
    "from": 164,
    "to": 149
  },
  {
    "from": 164,
    "to": 56
  },
  {
    "from": 164,
    "to": 151
  },
  {
    "from": 164,
    "to": 26
  },
  {
    "from": 164,
    "to": 135
  },
  {
    "from": 164,
    "to": 142
  },
  {
    "from": 164,
    "to": 166
  },
  {
    "from": 165,
    "to": 164
  },
  {
    "from": 165,
    "to": 144
  },
  {
    "from": 165,
    "to": 70
  },
  {
    "from": 165,
    "to": 94
  },
  {
    "from": 165,
    "to": 242
  },
  {
    "from": 165,
    "to": 263
  },
  {
    "from": 165,
    "to": 215
  },
  {
    "from": 165,
    "to": 151
  },
  {
    "from": 165,
    "to": 153
  },
  {
    "from": 165,
    "to": 157
  },
  {
    "from": 165,
    "to": 243
  },
  {
    "from": 165,
    "to": 149
  },
  {
    "from": 166,
    "to": 54
  },
  {
    "from": 166,
    "to": 142
  },
  {
    "from": 166,
    "to": 215
  },
  {
    "from": 167,
    "to": 17
  },
  {
    "from": 167,
    "to": 125
  },
  {
    "from": 167,
    "to": 15
  },
  {
    "from": 167,
    "to": 244
  },
  {
    "from": 167,
    "to": 164
  },
  {
    "from": 167,
    "to": 237
  },
  {
    "from": 167,
    "to": 157
  },
  {
    "from": 167,
    "to": 273
  },
  {
    "from": 167,
    "to": 253
  },
  {
    "from": 167,
    "to": 221
  },
  {
    "from": 167,
    "to": 82
  },
  {
    "from": 167,
    "to": 121
  },
  {
    "from": 167,
    "to": 243
  },
  {
    "from": 167,
    "to": 242
  },
  {
    "from": 167,
    "to": 44
  },
  {
    "from": 168,
    "to": 221
  },
  {
    "from": 168,
    "to": 84
  },
  {
    "from": 168,
    "to": 26
  },
  {
    "from": 168,
    "to": 125
  },
  {
    "from": 168,
    "to": 278
  },
  {
    "from": 168,
    "to": 15
  },
  {
    "from": 169,
    "to": 163
  },
  {
    "from": 169,
    "to": 164
  },
  {
    "from": 169,
    "to": 157
  },
  {
    "from": 169,
    "to": 161
  },
  {
    "from": 169,
    "to": 94
  },
  {
    "from": 170,
    "to": 33
  },
  {
    "from": 170,
    "to": 144
  },
  {
    "from": 170,
    "to": 97
  },
  {
    "from": 170,
    "to": 157
  },
  {
    "from": 170,
    "to": 125
  },
  {
    "from": 170,
    "to": 135
  },
  {
    "from": 170,
    "to": 129
  },
  {
    "from": 170,
    "to": 20
  },
  {
    "from": 170,
    "to": 61
  },
  {
    "from": 171,
    "to": 227
  },
  {
    "from": 171,
    "to": 139
  },
  {
    "from": 171,
    "to": 228
  },
  {
    "from": 171,
    "to": 149
  },
  {
    "from": 172,
    "to": 20
  },
  {
    "from": 172,
    "to": 149
  },
  {
    "from": 173,
    "to": 89
  },
  {
    "from": 173,
    "to": 221
  },
  {
    "from": 173,
    "to": 168
  },
  {
    "from": 174,
    "to": 149
  },
  {
    "from": 174,
    "to": 13
  },
  {
    "from": 174,
    "to": 135
  },
  {
    "from": 174,
    "to": 24
  },
  {
    "from": 174,
    "to": 137
  },
  {
    "from": 174,
    "to": 202
  },
  {
    "from": 174,
    "to": 136
  },
  {
    "from": 174,
    "to": 32
  },
  {
    "from": 175,
    "to": 176
  },
  {
    "from": 176,
    "to": 33
  },
  {
    "from": 176,
    "to": 125
  },
  {
    "from": 176,
    "to": 253
  },
  {
    "from": 176,
    "to": 239
  },
  {
    "from": 176,
    "to": 64
  },
  {
    "from": 176,
    "to": 265
  },
  {
    "from": 176,
    "to": 175
  },
  {
    "from": 177,
    "to": 135
  },
  {
    "from": 177,
    "to": 15
  },
  {
    "from": 177,
    "to": 14
  },
  {
    "from": 177,
    "to": 136
  },
  {
    "from": 177,
    "to": 149
  },
  {
    "from": 177,
    "to": 43
  },
  {
    "from": 177,
    "to": 56
  },
  {
    "from": 177,
    "to": 38
  },
  {
    "from": 177,
    "to": 138
  },
  {
    "from": 177,
    "to": 174
  },
  {
    "from": 177,
    "to": 33
  },
  {
    "from": 177,
    "to": 104
  },
  {
    "from": 177,
    "to": 157
  },
  {
    "from": 177,
    "to": 243
  },
  {
    "from": 177,
    "to": 185
  },
  {
    "from": 177,
    "to": 35
  },
  {
    "from": 177,
    "to": 241
  },
  {
    "from": 177,
    "to": 265
  },
  {
    "from": 178,
    "to": 169
  },
  {
    "from": 178,
    "to": 262
  },
  {
    "from": 178,
    "to": 5
  },
  {
    "from": 178,
    "to": 144
  },
  {
    "from": 178,
    "to": 213
  },
  {
    "from": 178,
    "to": 90
  },
  {
    "from": 178,
    "to": 62
  },
  {
    "from": 179,
    "to": 33
  },
  {
    "from": 179,
    "to": 84
  },
  {
    "from": 179,
    "to": 149
  },
  {
    "from": 179,
    "to": 48
  },
  {
    "from": 180,
    "to": 263
  },
  {
    "from": 181,
    "to": 84
  },
  {
    "from": 181,
    "to": 3
  },
  {
    "from": 181,
    "to": 281
  },
  {
    "from": 181,
    "to": 13
  },
  {
    "from": 182,
    "to": 84
  },
  {
    "from": 182,
    "to": 231
  },
  {
    "from": 182,
    "to": 181
  },
  {
    "from": 182,
    "to": 48
  },
  {
    "from": 183,
    "to": 134
  },
  {
    "from": 183,
    "to": 3
  },
  {
    "from": 183,
    "to": 233
  },
  {
    "from": 183,
    "to": 231
  },
  {
    "from": 183,
    "to": 16
  },
  {
    "from": 184,
    "to": 136
  },
  {
    "from": 184,
    "to": 132
  },
  {
    "from": 184,
    "to": 14
  },
  {
    "from": 184,
    "to": 3
  },
  {
    "from": 184,
    "to": 134
  },
  {
    "from": 184,
    "to": 247
  },
  {
    "from": 184,
    "to": 7
  },
  {
    "from": 184,
    "to": 185
  },
  {
    "from": 184,
    "to": 149
  },
  {
    "from": 185,
    "to": 184
  },
  {
    "from": 185,
    "to": 33
  },
  {
    "from": 185,
    "to": 136
  },
  {
    "from": 185,
    "to": 149
  },
  {
    "from": 185,
    "to": 132
  },
  {
    "from": 185,
    "to": 78
  },
  {
    "from": 185,
    "to": 197
  },
  {
    "from": 185,
    "to": 156
  },
  {
    "from": 185,
    "to": 15
  },
  {
    "from": 185,
    "to": 43
  },
  {
    "from": 185,
    "to": 137
  },
  {
    "from": 186,
    "to": 128
  },
  {
    "from": 186,
    "to": 125
  },
  {
    "from": 186,
    "to": 3
  },
  {
    "from": 186,
    "to": 137
  },
  {
    "from": 186,
    "to": 129
  },
  {
    "from": 186,
    "to": 78
  },
  {
    "from": 186,
    "to": 135
  },
  {
    "from": 186,
    "to": 149
  },
  {
    "from": 187,
    "to": 139
  },
  {
    "from": 187,
    "to": 135
  },
  {
    "from": 187,
    "to": 3
  },
  {
    "from": 187,
    "to": 149
  },
  {
    "from": 187,
    "to": 136
  },
  {
    "from": 188,
    "to": 227
  },
  {
    "from": 188,
    "to": 97
  },
  {
    "from": 188,
    "to": 200
  },
  {
    "from": 188,
    "to": 225
  },
  {
    "from": 188,
    "to": 208
  },
  {
    "from": 188,
    "to": 228
  },
  {
    "from": 188,
    "to": 84
  },
  {
    "from": 188,
    "to": 48
  },
  {
    "from": 188,
    "to": 136
  },
  {
    "from": 188,
    "to": 149
  },
  {
    "from": 189,
    "to": 33
  },
  {
    "from": 189,
    "to": 138
  },
  {
    "from": 190,
    "to": 189
  },
  {
    "from": 190,
    "to": 171
  },
  {
    "from": 190,
    "to": 89
  },
  {
    "from": 190,
    "to": 228
  },
  {
    "from": 190,
    "to": 144
  },
  {
    "from": 190,
    "to": 113
  },
  {
    "from": 190,
    "to": 221
  },
  {
    "from": 190,
    "to": 107
  },
  {
    "from": 190,
    "to": 48
  },
  {
    "from": 190,
    "to": 255
  },
  {
    "from": 190,
    "to": 277
  },
  {
    "from": 190,
    "to": 278
  },
  {
    "from": 191,
    "to": 263
  },
  {
    "from": 191,
    "to": 149
  },
  {
    "from": 191,
    "to": 219
  },
  {
    "from": 191,
    "to": 142
  },
  {
    "from": 191,
    "to": 70
  },
  {
    "from": 191,
    "to": 194
  },
  {
    "from": 192,
    "to": 14
  },
  {
    "from": 193,
    "to": 202
  },
  {
    "from": 193,
    "to": 33
  },
  {
    "from": 193,
    "to": 54
  },
  {
    "from": 194,
    "to": 142
  },
  {
    "from": 194,
    "to": 143
  },
  {
    "from": 194,
    "to": 193
  },
  {
    "from": 194,
    "to": 54
  },
  {
    "from": 194,
    "to": 149
  },
  {
    "from": 194,
    "to": 219
  },
  {
    "from": 194,
    "to": 70
  },
  {
    "from": 194,
    "to": 192
  },
  {
    "from": 195,
    "to": 33
  },
  {
    "from": 195,
    "to": 219
  },
  {
    "from": 195,
    "to": 191
  },
  {
    "from": 195,
    "to": 263
  },
  {
    "from": 195,
    "to": 138
  },
  {
    "from": 195,
    "to": 149
  },
  {
    "from": 195,
    "to": 142
  },
  {
    "from": 196,
    "to": 125
  },
  {
    "from": 196,
    "to": 15
  },
  {
    "from": 196,
    "to": 137
  },
  {
    "from": 196,
    "to": 135
  },
  {
    "from": 197,
    "to": 125
  },
  {
    "from": 197,
    "to": 15
  },
  {
    "from": 197,
    "to": 136
  },
  {
    "from": 197,
    "to": 6
  },
  {
    "from": 197,
    "to": 75
  },
  {
    "from": 197,
    "to": 14
  },
  {
    "from": 197,
    "to": 196
  },
  {
    "from": 198,
    "to": 6
  },
  {
    "from": 199,
    "to": 169
  },
  {
    "from": 199,
    "to": 162
  },
  {
    "from": 199,
    "to": 242
  },
  {
    "from": 199,
    "to": 160
  },
  {
    "from": 199,
    "to": 157
  },
  {
    "from": 199,
    "to": 125
  },
  {
    "from": 199,
    "to": 144
  },
  {
    "from": 199,
    "to": 15
  },
  {
    "from": 199,
    "to": 129
  },
  {
    "from": 199,
    "to": 149
  },
  {
    "from": 200,
    "to": 97
  },
  {
    "from": 200,
    "to": 224
  },
  {
    "from": 200,
    "to": 149
  },
  {
    "from": 200,
    "to": 139
  },
  {
    "from": 200,
    "to": 228
  },
  {
    "from": 200,
    "to": 71
  },
  {
    "from": 201,
    "to": 129
  },
  {
    "from": 201,
    "to": 93
  },
  {
    "from": 201,
    "to": 33
  },
  {
    "from": 202,
    "to": 138
  },
  {
    "from": 203,
    "to": 248
  },
  {
    "from": 203,
    "to": 71
  },
  {
    "from": 203,
    "to": 139
  },
  {
    "from": 203,
    "to": 136
  },
  {
    "from": 204,
    "to": 248
  },
  {
    "from": 204,
    "to": 205
  },
  {
    "from": 204,
    "to": 203
  },
  {
    "from": 204,
    "to": 136
  },
  {
    "from": 205,
    "to": 207
  },
  {
    "from": 205,
    "to": 3
  },
  {
    "from": 205,
    "to": 142
  },
  {
    "from": 206,
    "to": 157
  },
  {
    "from": 206,
    "to": 127
  },
  {
    "from": 206,
    "to": 249
  },
  {
    "from": 206,
    "to": 144
  },
  {
    "from": 206,
    "to": 213
  },
  {
    "from": 206,
    "to": 33
  },
  {
    "from": 206,
    "to": 262
  },
  {
    "from": 206,
    "to": 54
  },
  {
    "from": 206,
    "to": 56
  },
  {
    "from": 206,
    "to": 162
  },
  {
    "from": 206,
    "to": 77
  },
  {
    "from": 206,
    "to": 135
  },
  {
    "from": 206,
    "to": 149
  },
  {
    "from": 206,
    "to": 250
  },
  {
    "from": 207,
    "to": 143
  },
  {
    "from": 207,
    "to": 70
  },
  {
    "from": 207,
    "to": 215
  },
  {
    "from": 207,
    "to": 142
  },
  {
    "from": 207,
    "to": 174
  },
  {
    "from": 208,
    "to": 70
  },
  {
    "from": 208,
    "to": 6
  },
  {
    "from": 208,
    "to": 149
  },
  {
    "from": 208,
    "to": 49
  },
  {
    "from": 208,
    "to": 142
  },
  {
    "from": 208,
    "to": 54
  },
  {
    "from": 208,
    "to": 71
  },
  {
    "from": 208,
    "to": 205
  },
  {
    "from": 208,
    "to": 227
  },
  {
    "from": 208,
    "to": 135
  },
  {
    "from": 209,
    "to": 191
  },
  {
    "from": 209,
    "to": 263
  },
  {
    "from": 209,
    "to": 221
  },
  {
    "from": 209,
    "to": 24
  },
  {
    "from": 209,
    "to": 139
  },
  {
    "from": 209,
    "to": 16
  },
  {
    "from": 209,
    "to": 70
  },
  {
    "from": 209,
    "to": 54
  },
  {
    "from": 209,
    "to": 142
  },
  {
    "from": 209,
    "to": 149
  },
  {
    "from": 210,
    "to": 70
  },
  {
    "from": 210,
    "to": 33
  },
  {
    "from": 210,
    "to": 209
  },
  {
    "from": 210,
    "to": 221
  },
  {
    "from": 210,
    "to": 16
  },
  {
    "from": 210,
    "to": 142
  },
  {
    "from": 210,
    "to": 139
  },
  {
    "from": 210,
    "to": 215
  },
  {
    "from": 210,
    "to": 267
  },
  {
    "from": 210,
    "to": 149
  },
  {
    "from": 211,
    "to": 241
  },
  {
    "from": 211,
    "to": 70
  },
  {
    "from": 211,
    "to": 210
  },
  {
    "from": 211,
    "to": 149
  },
  {
    "from": 212,
    "to": 114
  },
  {
    "from": 212,
    "to": 157
  },
  {
    "from": 212,
    "to": 125
  },
  {
    "from": 212,
    "to": 15
  },
  {
    "from": 212,
    "to": 137
  },
  {
    "from": 212,
    "to": 149
  },
  {
    "from": 212,
    "to": 135
  },
  {
    "from": 212,
    "to": 220
  },
  {
    "from": 212,
    "to": 62
  },
  {
    "from": 212,
    "to": 144
  },
  {
    "from": 212,
    "to": 141
  },
  {
    "from": 213,
    "to": 114
  },
  {
    "from": 213,
    "to": 125
  },
  {
    "from": 213,
    "to": 15
  },
  {
    "from": 213,
    "to": 137
  },
  {
    "from": 213,
    "to": 149
  },
  {
    "from": 213,
    "to": 62
  },
  {
    "from": 213,
    "to": 144
  },
  {
    "from": 213,
    "to": 135
  },
  {
    "from": 213,
    "to": 141
  },
  {
    "from": 214,
    "to": 114
  },
  {
    "from": 214,
    "to": 262
  },
  {
    "from": 214,
    "to": 125
  },
  {
    "from": 214,
    "to": 15
  },
  {
    "from": 214,
    "to": 221
  },
  {
    "from": 214,
    "to": 62
  },
  {
    "from": 214,
    "to": 144
  },
  {
    "from": 214,
    "to": 90
  },
  {
    "from": 214,
    "to": 162
  },
  {
    "from": 214,
    "to": 265
  },
  {
    "from": 214,
    "to": 191
  },
  {
    "from": 214,
    "to": 141
  },
  {
    "from": 214,
    "to": 149
  },
  {
    "from": 214,
    "to": 142
  },
  {
    "from": 215,
    "to": 136
  },
  {
    "from": 215,
    "to": 142
  },
  {
    "from": 215,
    "to": 263
  },
  {
    "from": 215,
    "to": 54
  },
  {
    "from": 215,
    "to": 137
  },
  {
    "from": 216,
    "to": 114
  },
  {
    "from": 216,
    "to": 125
  },
  {
    "from": 216,
    "to": 15
  },
  {
    "from": 216,
    "to": 81
  },
  {
    "from": 216,
    "to": 137
  },
  {
    "from": 216,
    "to": 136
  },
  {
    "from": 216,
    "to": 149
  },
  {
    "from": 216,
    "to": 217
  },
  {
    "from": 216,
    "to": 144
  },
  {
    "from": 216,
    "to": 141
  },
  {
    "from": 217,
    "to": 149
  },
  {
    "from": 217,
    "to": 137
  },
  {
    "from": 217,
    "to": 125
  },
  {
    "from": 217,
    "to": 242
  },
  {
    "from": 217,
    "to": 129
  },
  {
    "from": 217,
    "to": 128
  },
  {
    "from": 217,
    "to": 135
  },
  {
    "from": 218,
    "to": 149
  },
  {
    "from": 218,
    "to": 136
  },
  {
    "from": 218,
    "to": 137
  },
  {
    "from": 218,
    "to": 242
  },
  {
    "from": 218,
    "to": 125
  },
  {
    "from": 218,
    "to": 15
  },
  {
    "from": 218,
    "to": 129
  },
  {
    "from": 218,
    "to": 230
  },
  {
    "from": 219,
    "to": 191
  },
  {
    "from": 219,
    "to": 139
  },
  {
    "from": 219,
    "to": 149
  },
  {
    "from": 219,
    "to": 142
  },
  {
    "from": 220,
    "to": 33
  },
  {
    "from": 220,
    "to": 125
  },
  {
    "from": 220,
    "to": 132
  },
  {
    "from": 220,
    "to": 128
  },
  {
    "from": 220,
    "to": 51
  },
  {
    "from": 220,
    "to": 3
  },
  {
    "from": 220,
    "to": 137
  },
  {
    "from": 220,
    "to": 43
  },
  {
    "from": 220,
    "to": 15
  },
  {
    "from": 220,
    "to": 135
  },
  {
    "from": 221,
    "to": 125
  },
  {
    "from": 221,
    "to": 15
  },
  {
    "from": 221,
    "to": 265
  },
  {
    "from": 222,
    "to": 33
  },
  {
    "from": 222,
    "to": 243
  },
  {
    "from": 222,
    "to": 144
  },
  {
    "from": 222,
    "to": 242
  },
  {
    "from": 222,
    "to": 221
  },
  {
    "from": 222,
    "to": 34
  },
  {
    "from": 222,
    "to": 129
  },
  {
    "from": 223,
    "to": 34
  },
  {
    "from": 223,
    "to": 9
  },
  {
    "from": 223,
    "to": 129
  },
  {
    "from": 223,
    "to": 263
  },
  {
    "from": 223,
    "to": 14
  },
  {
    "from": 224,
    "to": 227
  },
  {
    "from": 224,
    "to": 200
  },
  {
    "from": 224,
    "to": 149
  },
  {
    "from": 224,
    "to": 228
  },
  {
    "from": 225,
    "to": 227
  },
  {
    "from": 225,
    "to": 149
  },
  {
    "from": 225,
    "to": 111
  },
  {
    "from": 225,
    "to": 228
  },
  {
    "from": 225,
    "to": 191
  },
  {
    "from": 225,
    "to": 208
  },
  {
    "from": 225,
    "to": 54
  },
  {
    "from": 226,
    "to": 3
  },
  {
    "from": 226,
    "to": 33
  },
  {
    "from": 226,
    "to": 6
  },
  {
    "from": 227,
    "to": 228
  },
  {
    "from": 227,
    "to": 200
  },
  {
    "from": 227,
    "to": 136
  },
  {
    "from": 227,
    "to": 71
  },
  {
    "from": 227,
    "to": 225
  },
  {
    "from": 227,
    "to": 208
  },
  {
    "from": 227,
    "to": 49
  },
  {
    "from": 227,
    "to": 149
  },
  {
    "from": 228,
    "to": 132
  },
  {
    "from": 228,
    "to": 149
  },
  {
    "from": 228,
    "to": 136
  },
  {
    "from": 229,
    "to": 242
  },
  {
    "from": 229,
    "to": 62
  },
  {
    "from": 229,
    "to": 254
  },
  {
    "from": 230,
    "to": 78
  },
  {
    "from": 230,
    "to": 125
  },
  {
    "from": 230,
    "to": 15
  },
  {
    "from": 230,
    "to": 213
  },
  {
    "from": 230,
    "to": 216
  },
  {
    "from": 230,
    "to": 157
  },
  {
    "from": 230,
    "to": 43
  },
  {
    "from": 230,
    "to": 243
  },
  {
    "from": 230,
    "to": 132
  },
  {
    "from": 230,
    "to": 24
  },
  {
    "from": 230,
    "to": 218
  },
  {
    "from": 230,
    "to": 6
  },
  {
    "from": 230,
    "to": 81
  },
  {
    "from": 230,
    "to": 242
  },
  {
    "from": 230,
    "to": 197
  },
  {
    "from": 230,
    "to": 144
  },
  {
    "from": 230,
    "to": 33
  },
  {
    "from": 230,
    "to": 184
  },
  {
    "from": 230,
    "to": 149
  },
  {
    "from": 231,
    "to": 3
  },
  {
    "from": 231,
    "to": 6
  },
  {
    "from": 232,
    "to": 3
  },
  {
    "from": 232,
    "to": 6
  },
  {
    "from": 232,
    "to": 231
  },
  {
    "from": 233,
    "to": 3
  },
  {
    "from": 233,
    "to": 232
  },
  {
    "from": 233,
    "to": 231
  },
  {
    "from": 234,
    "to": 3
  },
  {
    "from": 235,
    "to": 168
  },
  {
    "from": 236,
    "to": 139
  },
  {
    "from": 236,
    "to": 272
  },
  {
    "from": 236,
    "to": 227
  },
  {
    "from": 236,
    "to": 90
  },
  {
    "from": 236,
    "to": 228
  },
  {
    "from": 236,
    "to": 6
  },
  {
    "from": 236,
    "to": 149
  },
  {
    "from": 237,
    "to": 121
  },
  {
    "from": 237,
    "to": 13
  },
  {
    "from": 237,
    "to": 24
  },
  {
    "from": 237,
    "to": 120
  },
  {
    "from": 237,
    "to": 160
  },
  {
    "from": 237,
    "to": 221
  },
  {
    "from": 237,
    "to": 253
  },
  {
    "from": 238,
    "to": 122
  },
  {
    "from": 238,
    "to": 164
  },
  {
    "from": 238,
    "to": 253
  },
  {
    "from": 238,
    "to": 14
  },
  {
    "from": 238,
    "to": 121
  },
  {
    "from": 238,
    "to": 160
  },
  {
    "from": 238,
    "to": 82
  },
  {
    "from": 238,
    "to": 221
  },
  {
    "from": 238,
    "to": 125
  },
  {
    "from": 239,
    "to": 253
  },
  {
    "from": 239,
    "to": 121
  },
  {
    "from": 239,
    "to": 160
  },
  {
    "from": 239,
    "to": 8
  },
  {
    "from": 239,
    "to": 157
  },
  {
    "from": 239,
    "to": 144
  },
  {
    "from": 239,
    "to": 82
  },
  {
    "from": 239,
    "to": 14
  },
  {
    "from": 239,
    "to": 221
  },
  {
    "from": 239,
    "to": 146
  },
  {
    "from": 239,
    "to": 265
  },
  {
    "from": 239,
    "to": 149
  },
  {
    "from": 240,
    "to": 33
  },
  {
    "from": 240,
    "to": 221
  },
  {
    "from": 240,
    "to": 274
  },
  {
    "from": 240,
    "to": 191
  },
  {
    "from": 240,
    "to": 142
  },
  {
    "from": 240,
    "to": 94
  },
  {
    "from": 240,
    "to": 125
  },
  {
    "from": 240,
    "to": 160
  },
  {
    "from": 240,
    "to": 149
  },
  {
    "from": 240,
    "to": 253
  },
  {
    "from": 240,
    "to": 44
  },
  {
    "from": 240,
    "to": 242
  },
  {
    "from": 240,
    "to": 176
  },
  {
    "from": 240,
    "to": 62
  },
  {
    "from": 240,
    "to": 71
  },
  {
    "from": 241,
    "to": 177
  },
  {
    "from": 241,
    "to": 13
  },
  {
    "from": 241,
    "to": 135
  },
  {
    "from": 242,
    "to": 33
  },
  {
    "from": 242,
    "to": 243
  },
  {
    "from": 242,
    "to": 125
  },
  {
    "from": 242,
    "to": 149
  },
  {
    "from": 243,
    "to": 242
  },
  {
    "from": 243,
    "to": 14
  },
  {
    "from": 243,
    "to": 15
  },
  {
    "from": 243,
    "to": 149
  },
  {
    "from": 243,
    "to": 137
  },
  {
    "from": 243,
    "to": 125
  },
  {
    "from": 243,
    "to": 129
  },
  {
    "from": 243,
    "to": 136
  },
  {
    "from": 243,
    "to": 33
  },
  {
    "from": 243,
    "to": 135
  },
  {
    "from": 243,
    "to": 26
  },
  {
    "from": 244,
    "to": 125
  },
  {
    "from": 244,
    "to": 15
  },
  {
    "from": 244,
    "to": 84
  },
  {
    "from": 244,
    "to": 136
  },
  {
    "from": 244,
    "to": 149
  },
  {
    "from": 244,
    "to": 196
  },
  {
    "from": 244,
    "to": 132
  },
  {
    "from": 244,
    "to": 6
  },
  {
    "from": 244,
    "to": 135
  },
  {
    "from": 245,
    "to": 33
  },
  {
    "from": 245,
    "to": 136
  },
  {
    "from": 245,
    "to": 149
  },
  {
    "from": 245,
    "to": 129
  },
  {
    "from": 245,
    "to": 135
  },
  {
    "from": 245,
    "to": 94
  },
  {
    "from": 245,
    "to": 246
  },
  {
    "from": 245,
    "to": 48
  },
  {
    "from": 246,
    "to": 21
  },
  {
    "from": 246,
    "to": 33
  },
  {
    "from": 246,
    "to": 78
  },
  {
    "from": 246,
    "to": 121
  },
  {
    "from": 246,
    "to": 102
  },
  {
    "from": 246,
    "to": 149
  },
  {
    "from": 246,
    "to": 129
  },
  {
    "from": 246,
    "to": 61
  },
  {
    "from": 246,
    "to": 68
  },
  {
    "from": 247,
    "to": 7
  },
  {
    "from": 247,
    "to": 198
  },
  {
    "from": 247,
    "to": 6
  },
  {
    "from": 248,
    "to": 191
  },
  {
    "from": 248,
    "to": 263
  },
  {
    "from": 248,
    "to": 249
  },
  {
    "from": 248,
    "to": 149
  },
  {
    "from": 248,
    "to": 142
  },
  {
    "from": 249,
    "to": 139
  },
  {
    "from": 249,
    "to": 149
  },
  {
    "from": 250,
    "to": 14
  },
  {
    "from": 250,
    "to": 149
  },
  {
    "from": 250,
    "to": 129
  },
  {
    "from": 250,
    "to": 137
  },
  {
    "from": 250,
    "to": 95
  },
  {
    "from": 250,
    "to": 135
  },
  {
    "from": 251,
    "to": 139
  },
  {
    "from": 251,
    "to": 244
  },
  {
    "from": 252,
    "to": 66
  },
  {
    "from": 252,
    "to": 33
  },
  {
    "from": 252,
    "to": 131
  },
  {
    "from": 252,
    "to": 139
  },
  {
    "from": 252,
    "to": 125
  },
  {
    "from": 252,
    "to": 137
  },
  {
    "from": 252,
    "to": 149
  },
  {
    "from": 252,
    "to": 15
  },
  {
    "from": 252,
    "to": 129
  },
  {
    "from": 253,
    "to": 13
  },
  {
    "from": 253,
    "to": 120
  },
  {
    "from": 253,
    "to": 14
  },
  {
    "from": 253,
    "to": 121
  },
  {
    "from": 253,
    "to": 175
  },
  {
    "from": 253,
    "to": 44
  },
  {
    "from": 254,
    "to": 149
  },
  {
    "from": 254,
    "to": 137
  },
  {
    "from": 254,
    "to": 242
  },
  {
    "from": 254,
    "to": 125
  },
  {
    "from": 254,
    "to": 129
  },
  {
    "from": 254,
    "to": 135
  },
  {
    "from": 255,
    "to": 138
  },
  {
    "from": 255,
    "to": 33
  },
  {
    "from": 255,
    "to": 149
  },
  {
    "from": 255,
    "to": 71
  },
  {
    "from": 256,
    "to": 33
  },
  {
    "from": 256,
    "to": 104
  },
  {
    "from": 256,
    "to": 141
  },
  {
    "from": 256,
    "to": 135
  },
  {
    "from": 256,
    "to": 137
  },
  {
    "from": 256,
    "to": 144
  },
  {
    "from": 256,
    "to": 149
  },
  {
    "from": 256,
    "to": 265
  },
  {
    "from": 257,
    "to": 114
  },
  {
    "from": 257,
    "to": 157
  },
  {
    "from": 257,
    "to": 229
  },
  {
    "from": 257,
    "to": 129
  },
  {
    "from": 257,
    "to": 258
  },
  {
    "from": 257,
    "to": 213
  },
  {
    "from": 257,
    "to": 265
  },
  {
    "from": 257,
    "to": 66
  },
  {
    "from": 257,
    "to": 254
  },
  {
    "from": 257,
    "to": 273
  },
  {
    "from": 257,
    "to": 62
  },
  {
    "from": 258,
    "to": 157
  },
  {
    "from": 258,
    "to": 254
  },
  {
    "from": 258,
    "to": 257
  },
  {
    "from": 258,
    "to": 114
  },
  {
    "from": 259,
    "to": 33
  },
  {
    "from": 259,
    "to": 115
  },
  {
    "from": 259,
    "to": 53
  },
  {
    "from": 259,
    "to": 154
  },
  {
    "from": 259,
    "to": 243
  },
  {
    "from": 259,
    "to": 242
  },
  {
    "from": 259,
    "to": 155
  },
  {
    "from": 260,
    "to": 70
  },
  {
    "from": 260,
    "to": 263
  },
  {
    "from": 260,
    "to": 137
  },
  {
    "from": 260,
    "to": 149
  },
  {
    "from": 260,
    "to": 94
  },
  {
    "from": 260,
    "to": 13
  },
  {
    "from": 260,
    "to": 125
  },
  {
    "from": 260,
    "to": 261
  },
  {
    "from": 260,
    "to": 135
  },
  {
    "from": 261,
    "to": 70
  },
  {
    "from": 261,
    "to": 263
  },
  {
    "from": 261,
    "to": 137
  },
  {
    "from": 261,
    "to": 149
  },
  {
    "from": 261,
    "to": 243
  },
  {
    "from": 261,
    "to": 125
  },
  {
    "from": 261,
    "to": 94
  },
  {
    "from": 261,
    "to": 13
  },
  {
    "from": 261,
    "to": 135
  },
  {
    "from": 262,
    "to": 220
  },
  {
    "from": 262,
    "to": 221
  },
  {
    "from": 262,
    "to": 43
  },
  {
    "from": 262,
    "to": 250
  },
  {
    "from": 262,
    "to": 135
  },
  {
    "from": 262,
    "to": 13
  },
  {
    "from": 263,
    "to": 33
  },
  {
    "from": 263,
    "to": 191
  },
  {
    "from": 263,
    "to": 142
  },
  {
    "from": 263,
    "to": 54
  },
  {
    "from": 263,
    "to": 264
  },
  {
    "from": 263,
    "to": 149
  },
  {
    "from": 264,
    "to": 142
  },
  {
    "from": 264,
    "to": 263
  },
  {
    "from": 264,
    "to": 54
  },
  {
    "from": 264,
    "to": 6
  },
  {
    "from": 264,
    "to": 193
  },
  {
    "from": 265,
    "to": 33
  },
  {
    "from": 265,
    "to": 125
  },
  {
    "from": 265,
    "to": 43
  },
  {
    "from": 265,
    "to": 262
  },
  {
    "from": 265,
    "to": 250
  },
  {
    "from": 265,
    "to": 15
  },
  {
    "from": 266,
    "to": 191
  },
  {
    "from": 266,
    "to": 71
  },
  {
    "from": 266,
    "to": 263
  },
  {
    "from": 266,
    "to": 142
  },
  {
    "from": 267,
    "to": 33
  },
  {
    "from": 267,
    "to": 221
  },
  {
    "from": 267,
    "to": 148
  },
  {
    "from": 267,
    "to": 237
  },
  {
    "from": 267,
    "to": 43
  },
  {
    "from": 267,
    "to": 104
  },
  {
    "from": 267,
    "to": 97
  },
  {
    "from": 267,
    "to": 265
  },
  {
    "from": 268,
    "to": 125
  },
  {
    "from": 268,
    "to": 139
  },
  {
    "from": 268,
    "to": 157
  },
  {
    "from": 268,
    "to": 151
  },
  {
    "from": 268,
    "to": 153
  },
  {
    "from": 268,
    "to": 144
  },
  {
    "from": 268,
    "to": 150
  },
  {
    "from": 269,
    "to": 97
  },
  {
    "from": 269,
    "to": 3
  },
  {
    "from": 269,
    "to": 18
  },
  {
    "from": 269,
    "to": 17
  },
  {
    "from": 270,
    "to": 263
  },
  {
    "from": 271,
    "to": 14
  },
  {
    "from": 271,
    "to": 273
  },
  {
    "from": 272,
    "to": 198
  },
  {
    "from": 272,
    "to": 14
  },
  {
    "from": 272,
    "to": 22
  },
  {
    "from": 272,
    "to": 273
  },
  {
    "from": 273,
    "to": 14
  },
  {
    "from": 273,
    "to": 120
  },
  {
    "from": 273,
    "to": 121
  },
  {
    "from": 273,
    "to": 160
  },
  {
    "from": 273,
    "to": 237
  },
  {
    "from": 273,
    "to": 44
  },
  {
    "from": 273,
    "to": 175
  },
  {
    "from": 273,
    "to": 16
  },
  {
    "from": 274,
    "to": 265
  },
  {
    "from": 274,
    "to": 33
  },
  {
    "from": 274,
    "to": 221
  },
  {
    "from": 274,
    "to": 198
  },
  {
    "from": 274,
    "to": 90
  },
  {
    "from": 274,
    "to": 125
  },
  {
    "from": 274,
    "to": 15
  },
  {
    "from": 274,
    "to": 6
  },
  {
    "from": 274,
    "to": 43
  },
  {
    "from": 274,
    "to": 104
  },
  {
    "from": 275,
    "to": 138
  },
  {
    "from": 275,
    "to": 33
  },
  {
    "from": 275,
    "to": 51
  },
  {
    "from": 275,
    "to": 52
  },
  {
    "from": 275,
    "to": 213
  },
  {
    "from": 275,
    "to": 237
  },
  {
    "from": 275,
    "to": 176
  },
  {
    "from": 275,
    "to": 90
  },
  {
    "from": 275,
    "to": 144
  },
  {
    "from": 275,
    "to": 136
  },
  {
    "from": 275,
    "to": 149
  },
  {
    "from": 276,
    "to": 138
  },
  {
    "from": 276,
    "to": 137
  },
  {
    "from": 276,
    "to": 162
  },
  {
    "from": 276,
    "to": 132
  },
  {
    "from": 276,
    "to": 149
  },
  {
    "from": 276,
    "to": 136
  },
  {
    "from": 276,
    "to": 273
  },
  {
    "from": 277,
    "to": 139
  },
  {
    "from": 277,
    "to": 149
  },
  {
    "from": 277,
    "to": 33
  },
  {
    "from": 277,
    "to": 89
  },
  {
    "from": 277,
    "to": 135
  },
  {
    "from": 277,
    "to": 137
  },
  {
    "from": 277,
    "to": 70
  },
  {
    "from": 278,
    "to": 89
  },
  {
    "from": 278,
    "to": 220
  },
  {
    "from": 278,
    "to": 255
  },
  {
    "from": 279,
    "to": 143
  },
  {
    "from": 279,
    "to": 129
  },
  {
    "from": 279,
    "to": 125
  },
  {
    "from": 279,
    "to": 157
  },
  {
    "from": 279,
    "to": 33
  },
  {
    "from": 279,
    "to": 142
  },
  {
    "from": 280,
    "to": 16
  },
  {
    "from": 280,
    "to": 273
  },
  {
    "from": 280,
    "to": 272
  },
  {
    "from": 280,
    "to": 160
  },
  {
    "from": 280,
    "to": 6
  },
  {
    "from": 280,
    "to": 121
  },
  {
    "from": 281,
    "to": 89
  },
  {
    "from": 282,
    "to": 138
  }
]);
      const container = document.getElementById("mynetwork");
      const data = { nodes: nodes, edges: edges };
      const options = {
        nodes: {
          shape: "dot",
          size: 20,
          font: { size: 14, color: "#000" }
        },
        edges: {
          arrows: "to",
          color: "gray",
          smooth: true
        },
        physics: {
          enabled: true,
          solver: "forceAtlas2Based",
          stabilization: {
            enabled: true,
            iterations: 200,
            fit: true
          }
        },
        interaction: {
          navigationButtons: true,
          keyboard: true,
          zoomView: true,
          dragView: true
        }
      };
      const network = new vis.Network(container, data, options);
    </script>
  </body>
</html>
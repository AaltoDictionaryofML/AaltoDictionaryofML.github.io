<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Glossary Network</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css" rel="stylesheet" />
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js"></script>
    <script type="text/javascript" src="https://unpkg.com/vis-network/standalone/umd/vis-network.min.js"></script>
    <style>
      #mynetwork {
        width: 100%;
        height: 1000px;
        background-color: #ffffff;
        border: 1px solid lightgray;
      }
    </style>
  </head>
  <body>
    <center><h1>The Aalto Dictionary of Machine Learning</h1></center>
    <div id="mynetwork"></div>
    <script type="text/javascript">
      const nodes = new vis.DataSet([
  {
    "id": 1,
    "label": "pseudoinverse",
    "title": "The Moore\u2013Penrose pseudoinverse of a generalizes the notion of an . The pseudoinverse arises naturally in for a with and {hastie01statisticallearning}. The {modelparam} learned by are given by \\[ {}^{()} = (^{T} + )^{-1} ^{T} , > 0. \\] We can then define the pseudoinverse via the limit {benisrael2003generalized} \\[ _{ 0^+} {}^{()} = ^+ . \\] \\\\ See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 2,
    "label": "random experiment",
    "title": "A random experiment is a physical (or abstract) process that produces an outcome from a set of possibilities. This set of all possible outcomes is referred to as the of the experiment. The key characteristic of a random experiment is that its outcome is unpredictable (or uncertain). Any measurement or observation of the outcome is a , i.e., a of the outcome . theory uses a as a mathematical structure for the study of random experiments. A key conceptual property of a random experiment is that it can be repeated under identical conditions. Strictly speaking, repeating a random experiment a given number of times defines a new random experiment. The outcomes of this new experiment are length- sequences of outcomes from the original experiment (see Fig. {fig_randomexperiment_dict}). While the outcome of a single experiment is uncertain, the long-run behaviour of the outcomes of repeated experiments tends to become increasingly predictable. This informal claim can be made precise via fundamental results of theory, such as the and the . {figure}[H] {center} {tikzpicture}[>=Stealth, node distance=1.5cm and 2cm, every node/.style={font=}] (experiment) [draw, rectangle, rounded corners, minimum width=2.6cm, align=center] {random\\}; (omega) [right=of experiment] {}; (rightpad) at (); (experiment) -- (omega); (sequence) [below=of experiment, yshift=-0.5cm] {}; (sequence1) [below=of sequence, yshift=-0.5cm] {}; (experiment.south) -- node[midway, right, xshift=3pt] {repeat times} (sequence.north); (sequence.south) -- node[midway, right, xshift=3pt] {{rv}} (sequence1.north); (experiment.south) -- (sequence.north) coordinate[pos=0.6] (repeatpoint); {}; {tikzpicture} {center} {A random experiment produces an outcome from a set of possibilities (i.e., a ) . Repeating the experiment times yields another random experiment, whose outcomes are sequences . One example of a random experiment arising in many applications is the gathering of a . {fig_randomexperiment_dict}} {figure} Examples for random experiments arising in applications include the following: {itemize} collection: The {datapoint} collected in -based methods can be interpreted as {rv}, i.e., as {function} of the outcome of a random experiment. uses a random experiment at each iteration to select a subset of the . methods use random experiments to perturb the outputs of an method to ensure . {itemize} See also: , , , .",
    "color": "salmon"
  },
  {
    "id": 3,
    "label": "dimension",
    "title": "The dimension of a is the cardinality of any of . Strictly speaking, this definition applies only to finite-dimensional {vectorspace}, i.e., those that possess a finite . {figure}[H] {tikzpicture}[scale=1] (O) at (0,0); (O) -- (1.8,0) node[below right] {}; (O) -- (0,1.6) node[above left] {}; (0,0) -- (1.2,1.2) node[above right] {}; (0,0) -- (-1.2,1.2) node[above left] {}; (O) -- (2.0,0.6) node[above right] {}; (O) -- (0.4,1.8) node[left] {}; {tikzpicture} {Three {basis}, , for the .} {figure} For such spaces, all {basis} have the same cardinality, which is the dimension of the space {Axler2025}. \\\\ See also: , .",
    "color": "lightcoral"
  },
  {
    "id": 4,
    "label": "linearly independent",
    "title": "A subset of a is linearly independent if there is no non-trivial linear combination of these {vector} that equals the zero . In other words, _{=1}^{} _{} ^{()} = {0} { implies } _{1} = _{2} = = _{k} = 0. \\\\ See also: , , , .",
    "color": "lightcoral"
  },
  {
    "id": 5,
    "label": "basis",
    "title": "A basis of a is a set of {vector} such that any vector can be expressed as a linear combination of the basis {vector}, i.e., = _{=1}^{} _{} ^{()} { for some } _{1}, \\,, \\,_{} {R}.",
    "color": "lightcoral"
  },
  {
    "id": 6,
    "label": "wide matrix",
    "title": "A is referred to as wide if it has more columns than rows, i.e., .\\\\ See also: .",
    "color": "lightblue"
  },
  {
    "id": 7,
    "label": "tall matrix",
    "title": "A is referred to as tall if it has more rows than columns, i.e., when .\\\\ See also: .",
    "color": "lightblue"
  },
  {
    "id": 8,
    "label": "rank-deficient",
    "title": "A is if it is not full-rank, i.e., when .\\\\ See also: .",
    "color": "lightblue"
  },
  {
    "id": 9,
    "label": "inverse matrix",
    "title": "An inverse is defined for a square that is of full rank, meaning its columns are linearly independent. In this case, is said to be invertible, and its inverse satisfies \\[ ^{-1} = ^{-1} = . \\] A square is invertible if and only if its is non-zero. Inverse {matrix} are fundamental in solving systems of linear equations and in the closed-form solution of , . The concept of an inverse can be extended to {matrix} that are not square or does not have full . One may define a ``left inverse'' satisfying or a ``right inverse'' satisfying . For general rectangular or singular {matrix}, the Moore\u2013Penrose provides a unified concept of a generalized inverse . {figure}[H] {tikzpicture}[x=2cm,y=2cm] {scope} (0,0) -- (1,0) node[below right] {}; (0,0) -- (0,1) node[above left] {}; {scope} {scope}[shift={(2.0,0)}] (A) at (1.5,0.5); (B) at (-0.2,1.2); (0,0) -- (A) node[pos=0.5, below right] {}; (0,0) -- (B) node[above right] {}; {scope} {scope}[shift={(4.9,0)}] (0,0) -- (1,0) node[pos=0.5, below] {}; (0,0) -- (0,1) node[above] {}; {scope} (1.2,0.4) to node[above] {} (1.8,0.4); (3.8,0.4) to node[below] {} (4.4,0.4); {tikzpicture} {A represents a linear transformation of . The inverse represents the inverse transformation. {fig_matrix_inverse_dict}} {figure} See also: , , , .",
    "color": "lightblue"
  },
  {
    "id": 10,
    "label": "matrix",
    "title": "A matrix of size is a 2-D array of numbers, which is denoted by = {pmatrix} A_{1,1} & A_{1,2} & & A_{1,} \\\\ A_{2,1} & A_{2,2} & & A_{2,} \\\\ & & & \\\\ A_{,1} & A_{,2} & & A_{,} {pmatrix} {R}^{ }. Here, denotes the matrix entry in the th row and the th column. Matrices are useful representations of various mathematical objects , including the following: {itemize} Systems of linear equations: We can use a matrix to represent a system of linear equations {pmatrix} A_{1,1} & A_{1,2} \\\\ A_{2,1} & A_{2,2} {pmatrix} {pmatrix} w_1 \\\\ w_2 {pmatrix} ={pmatrix} y_1 \\\\ y_2 {pmatrix} { compactly as } = . One important example of systems of linear equations is the optimality condition for the {modelparam} within . {linearmap}: Consider a -dimensional and a -dimensional . If we fix a basis for and a basis for , each matrix naturally defines a (see Fig. {fig_matrix_dict}) such that ^{()} _{=1}^{} A_{,} ^{()}. {dataset}: We can use a matrix to represent a . Each row corresponds to a single , and each column corresponds to a specific or of a . {itemize} {figure}[H] {center} {tikzpicture}[x=2cm] {scope} (0,0) -- (1,0) node[below] {}; (0,0) -- (0,1) node[above] {}; {scope} {scope}[shift={(3.2,0)}] (0,0) -- (1,0) node[below] {}; (0,0) -- (0,1) node[above] {}; (A) at (0.2,-1.0); (B) at (0.4,1.2); (0,0) -- (A) node[below,right] {}; (0,0) -- (B) node[right,xshift=1pt] {}; {scope} (1.6,0.5) to[bend left] node[midway, above] {} (2.7,0.5); {tikzpicture} {center} {A matrix defines a between two {vectorspace}. {fig_matrix_dict}} {figure} See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 11,
    "label": "stratification",
    "title": "The process of splitting a into subsets, so called {stratum}, according to some key attribute is called stratification . The goal is to ensure that a method performs well for each defined by these attributes. For example, in a medical , we may want to stratify a patient by age groups to ensure that an model performs well across all age groups. {figure}[H] {tikzpicture}[font=,x=1.7cm] {6.0} {0.5} {1} {0.8} {0.9} at (0.5*, +0.2) {}; at ({0.5*}, {-(+)++0.2}) {train/val split without }; at (0.5*, {-2*(+)++0.2}) {with }; {}{0} {}{-(+)} {}{-2*(+)} {}{} {}{} {}{(1-)*} (0,) rectangle ++(,); (,) -- ++(0,); at (0.5*, +0.5*) {}; at ({+0.5*}, {+0.5*}) {}; (0,) rectangle ++(,); {}{} {}{(1-)*} (,) rectangle ++(,); (,) -- ++(0,); (0,) rectangle ++(,); {}{} (,) -- ++(0,); {}{} {}{(1-)*} (,) rectangle ++(,); {}{ + } {}{(1-)*} (,) rectangle ++(,); {tikzpicture} {Stratification ensures that both the and the (shaded grey) have similar distributions of a binary key attribute .} {figure} When splitting a into a and a , stratification ensures that both sets have similar distributions of the key attribute. Without stratification, using a small may underrepresent or even completely miss {datapoint} with a rare attribute, leading to misleading performance estimates.\\\\ See also: , , .",
    "color": "lightgreen"
  },
  {
    "id": 12,
    "label": "stratum",
    "title": "A stratum is a subset of {datapoint} that all possess a shared property (which could be a or a ). For example, in a weather , all measurements from the same weather station form one stratum.\\\\[0.5em] {center} \\\\ { time,station,value,units\\\\ 2023-06-01 12:00,Helsinki,18.2,degC\\\\ 2023-06-01 13:00,Helsinki,18.5,degC\\\\ 2023-06-01 14:00,Helsinki,19.0,degC\\\\ 2023-06-01 12:00,Oulu,12.1,degC\\\\ 2023-06-01 13:00,Oulu,12.4,degC\\\\ 2023-06-01 14:00,Oulu,12.7,degC\\\\ 2023-06-01 12:00,Tampere,15.3,degC\\\\ 2023-06-01 13:00,Tampere,15.6,degC\\\\ 2023-06-01 14:00,Tampere,16.0,degC } {center} Here, the rows for each station ({Helsinki}, {Oulu}, {Tampere}) represent different strata.\\\\ See also: , , .",
    "color": "lightgreen"
  },
  {
    "id": 13,
    "label": "determinant",
    "title": "The determinant of a square is a of its columns , i.e., it satisfies the following properties : {itemize} Normalized: \\,() = 1 Multilinear: {align} (^{(1)},\\,,\\,+ ,\\,,\\,^{()} ) & = (^{(1)},\\,,\\,,\\,,\\,^{()} ) \\\\ & + (^{(1)},\\,,\\,,\\,,\\,^{()} ) {align} Antisymmetric: (,\\,^{()}, \\,, \\,^{(')},\\, ) = - (,\\,^{(')}, \\,, \\,^{()},\\, ). {itemize} We can interpret a as a linear transformation on . The determinant characterizes how volumes in (and their orientation) are altered by this transformation (see Fig. {fig_det_dict}) , . In particular, preserves orientation, reverses orientation, and collapses volume entirely, indicating that is non-invertible. The determinant also satisfies , and if is diagonalizable with {eigenvalue} , then . For the special cases (i.e., two-dimensional or 2-D) and (i.e., three-dimensional or 3-D), the determinant can be interpreted as an oriented area or volume spanned by the column {vector} of . {figure}[H] {center} {tikzpicture}[x=2cm] {scope} (0,0) -- (1,0) node[below right] {}; (0,0) -- (0,1) node[above left] {}; {scope} {scope}[shift={(2.8,0)}] (A) at (1.5,0.5); (B) at (-0.2,1.2); (0,0) -- (A) node[below right] {}; (0,0) -- (B) node[above left] {}; (0,0) -- (A) -- () -- (B) -- cycle; (A) -- (); (B) -- (); at (0.8,0.6) { }; (0.4,0.0) arc[start angle=0, end angle=35, radius=0.6]; {scope} (1.3,0.5) -- (2.4,0.5) node[midway, above] {}; {tikzpicture} {center} {We can interpret a square as a linear transformation of into itself. The determinant characterizes how this transformation alters an oriented volume. {fig_det_dict}} {figure} See also: , .",
    "color": "lightblue"
  },
  {
    "id": 14,
    "label": "Hessian",
    "title": "Consider a for which the second-order partial derivatives exist at . Then, the Hessian of at is defined as the of second-order partial derivatives of at , ^{2} f(') \\;=\\; {pmatrix} {^{2} f}{ _{1}^{2}} & {^{2} f}{ _{1}\\, _{2}} & & {^{2} f}{ _{1}\\, _{}} \\\\ {^{2} f}{ _{2}\\, _{1}} & {^{2} f}{ _{2}^{2}} & & {^{2} f}{ _{2}\\, _{}} \\\\ & & & \\\\[1.2ex] {^{2} f}{ _{}\\, _{1}} & {^{2} f}{ _{}\\, _{2}} & & {^{2} f}{ _{}^{2}} {pmatrix}. If the second-order partial derivatives are in a around , then the Hessian is a symmetric , i.e., for all . If additionally is , then the Hessian is a . {figure}[H] {center} {tikzpicture}[x=0.5cm] {axis}[ hide axis, xmin=3, xmax=6, ymin=0, ymax=6, domain=0:6, samples=100, width=10cm, height=6cm, clip=false ] {2 + sin(deg(x))} node[pos=0.5, above right, yshift=3pt] {}; {2 + sin(deg(6)) + cos(deg(6))*(x - 6)} node[pos=0, below left] {}; {2 + sin(deg(6)) + cos(deg(6))*(x - 6) - 0.5*sin(deg(6))*(x - 6)^2} node[pos=0, below left, yshift=10pt] {}; coordinates {(6, {2 + sin(deg(6))})}; coordinates {(6,0) (6,2.4)}; at (axis cs:6, -0.2) {}; {}{-1.5} {}{3} {}{2 + sin(deg())} {}{2 + sin(deg())} {axis} {-10mm} {tikzpicture} {-5mm} {center} { A that is sufficiently at a point can be locally approximated by a which allows for a more accurate approximation compared to a linear . {fig_quadapprox_hessian_dict}} {figure} The Hessian can be used to compute a q() = (1/2) (- ')^{T} {^2 f(')}_{{Hessian}} (- ') + (- ')^{T} { f(')}_{{gradient}} + f(') that approximates locally around . \\\\ See also: , , , .",
    "color": "violet"
  },
  {
    "id": 15,
    "label": "continuous",
    "title": "A is continuous at a point if for every there is a such that for all with , it holds that . In other words, we can make arbitrarily close to by choosing sufficiently close to . If is continuous at every point , then is said to be continuous on . The notion of a continuous can be naturally extended to {function} between general spaces .\\\\ See also: , .",
    "color": "violet"
  },
  {
    "id": 16,
    "label": "linear map",
    "title": "A linear is a that satisfies additivity, i.e., , and homogeneity, i.e., , for all {vector} and scalars . In particular, . Any linear can be represented as a multiplication for some . The collection of real-valued linear {map} for a given dimension constitute a , which is used in many methods. \\\\ See also: , , , , , .",
    "color": "orange"
  },
  {
    "id": 17,
    "label": "vector",
    "title": "A vector is an element of a . In the context of , a particularly important example of a is the , where is the (finite) dimension of the space. A vector can be represented as a list or one-dimensional (1-D) array of real numbers, i.e., with for . The value is the th entry of the vector . It can also be useful to view a vector as a that maps each index to a value , i.e., . This perspective is particularly useful for the study of {kernelmethod}. See Fig. {fig:vector-function-dual_dict} for the two views of a vector. {figure}[H] {minipage}[c]{0.48} 2, --1, 3, 0, --2, 1 {minipage}{} {5ex} { (a)} {minipage} {minipage} {minipage}{0.48} {tikzpicture} {axis}[ width=6.5cm, height=5cm, title={}, xlabel={index }, ylabel={}, ymin=-3.5, ymax=3.5, xmin=0.5, xmax=6.5, xtick={1,2,3,4,5,6}, ytick={-3,-2,-1,0,1,2,3}, axis x line=bottom, axis y line=left, grid=both, major grid style={dotted, gray!60}, enlargelimits=0.1 ] +[ycomb, thick, mark=*] coordinates { (1,2) (2,-1) (3,3) (4,0) (5,-2) (6,1) }; {axis} at (2,-2.5) {(b)}; {tikzpicture} {minipage} {Two equivalent views of a vector . (a) As a numeric array. (b) As a .} {fig:vector-function-dual_dict} {figure} See also: , , .",
    "color": "lightcoral"
  },
  {
    "id": 18,
    "label": "vector space",
    "title": "A space (also called linear space) is a collection of elements, called {vector}, along with the following two operations (see also Fig. {fig:vector-ops_dict}): 1) addition (denoted by ) of two {vector} ; and 2) multiplication (denoted by ) of a with a scalar that belongs to some number field (with a typical choice for this field being ). The defining property of a space is that it is closed under two specific operations. First, if , then . Second, if and , then . {figure}[H] {tikzpicture}[>=Stealth, scale=1.2] (O) at (0,0); (V) at (2,1.5); (W) at (1,3); (VplusW) at (3,4.5); (HalfV) at (1,0.75); (O) -- (V) node[pos=1, right] {}; (O) -- (W) node[pos=1, left] {}; (O) -- (VplusW) node[pos=0.99, above right] {}; (V) -- (VplusW); (W) -- (VplusW); (O) -- (HalfV) node[midway, right] {}; (O) circle (2pt) node[below left] {}; (V) circle (2pt); (W) circle (2pt); (VplusW) circle (2pt); (HalfV) circle (2pt); {tikzpicture} {A space is a collection of {vector} such that scaling and adding them always yields another in .} {fig:vector-ops_dict} {figure} A common example of a space is the , which is widely used in to represent {dataset}. We can also use to represent, either exactly or approximately, the used by an method. Another example of a space, which is naturally associated with every , is the collection of all real-valued {rv} , . \\\\ See also: , , , .",
    "color": "lightcoral"
  },
  {
    "id": 19,
    "label": "stochastic",
    "title": "We refer to a method as stochastic if it involves a random component or is governed by probabilistic laws. methods use randomness to reduce computational complexity (e.g., see ) or to capture in {probmodel}. \\\\ See also: , , .",
    "color": "salmon"
  },
  {
    "id": 20,
    "label": "stochastic process",
    "title": "A process is a collection of {rv} defined on a common and indexed by some set , , . The index set typically represents time or space, allowing us to represent random phenomena that evolve across time or spatial dimensions\u2014for example, sensor noise or financial time series. processes are not limited to temporal or spatial settings. For instance, random {graph} such as the or the can also be viewed as processes. Here, the index set consists of node pairs that index {rv} whose values encode the presence or weight of an edge between two nodes. Moreover, processes naturally arise in the analysis of {stochalgorithm}, such as , which construct a sequence of {rv}. \\\\ See also: , , , , .",
    "color": "salmon"
  },
  {
    "id": 21,
    "label": "characteristic function",
    "title": "The characteristic of a real-valued is the {BillingsleyProbMeasure} _{x}(t) { \\,(j t x) } { with } j = {-1}. The characteristic uniquely determines the of . \\\\ See also: , .",
    "color": "salmon"
  },
  {
    "id": 22,
    "label": "entropy",
    "title": "Entropy quantifies the or unpredictability associated with a . For a discrete taking on values in a finite set with a mass , the entropy is defined as \\[ H(x) -_{i=1}^n p_i p_i. \\] Entropy is maximized when all outcomes are equally likely, and minimized (i.e., zero) when the outcome is deterministic. A of the concept of entropy for continuous {rv} is . \\\\ See also: , .",
    "color": "salmon"
  },
  {
    "id": 23,
    "label": "differential entropy",
    "title": "For a real-valued with a , the differential is defined as \\[ h() - p() p() \\, d. \\] Differential can be negative and lacks some properties of for discrete-valued {rv}, such as invariance under a change of variables . Among all {rv} with a given and , is maximized by . \\\\ See also: , .",
    "color": "salmon"
  },
  {
    "id": 24,
    "label": "minimum",
    "title": "Given a set of real numbers, the minimum is the smallest of those numbers. Note that for some sets, such as the set of negative real numbers, the minimum does not exist.",
    "color": "lightblue"
  },
  {
    "id": 25,
    "label": "co-domain",
    "title": "The co-domain of a is the set into which maps elements of its domain . \\\\ See also: , , .",
    "color": "orange"
  },
  {
    "id": 26,
    "label": "domain",
    "title": "The domain of a is the set from which takes its inputs. \\\\ See also: , , .",
    "color": "orange"
  },
  {
    "id": 27,
    "label": "function",
    "title": "A function between two sets and assigns each element exactly one element . We write this as f: {U} {V}: u f(u) where is the domain and the co-domain of . That is, a function defines a unique output for every input (see Fig. {fig_function_dict}). {figure}[H] {tikzpicture}[>=stealth, node distance=1.2cm and 2.5cm] {dot/.style={circle, fill=black, inner sep=1.2pt}} (A) [dot, label=left:] {}; (B) [dot, below=of A, label=left:] {}; (C) [dot, below=of B, label=left:] {}; (1) [dot, right=4cm of A, label=right:] {}; (2) [dot, below=of 1, label=right:] {}; (3) [dot, below=of 2, label=right:] {}; {}; {}; (A) -- (2); (B) -- (1); (C) -- (2); {tikzpicture} {A function \\( f {U} {V} \\) mapping each element of the domain to exactly one element of the co-domain . {fig_function_dict}} {figure}",
    "color": "orange"
  },
  {
    "id": 28,
    "label": "map",
    "title": "We use the term map as a synonym for . \\\\ See also: .",
    "color": "orange"
  },
  {
    "id": 29,
    "label": "attention",
    "title": "Some applications involve {datapoint} composed of smaller units, referred to as {token}. For example, a sentence consists of words, an image of pixel patches, and a network of nodes. In general, the {token} that constitute a single are not independent of one another. Instead, each of a depends (or pays attention) to specific other {token}. {probmodel} provide a principled framework for representing and analyzing such dependencies . Attention mechanisms use a more direct approach without explicit reference to a . The idea is to represent the relationship between two tokens and using a parameterized , where the {parameter} are learned via a variant of . Practical attention mechanisms differ in their precise choice of attention as well as in the precise variant used to learn the {parameter} . One widely used family of attention mechanisms defines the parameters in terms of two {vector} associated with each token , i.e., a query and a key . For a given token with query , and another token with key , the quantity quantifies the extent to which token attends to (or depends on) token (see Fig. {fig_attention_dict}). {figure}[H] {tikzpicture}[>=stealth, node distance=0.2cm and 0.2cm, every node/.style={inner sep=2pt, font=}, baseline] (w1) [draw, fill=gray!10, rounded corners] {All}; (w2) [draw, fill=gray!10, right=of w1, rounded corners] {human}; (w3) [draw, fill=gray!10, right=of w2, rounded corners] {beings}; (w4) [draw, fill=gray!10, right=of w3, rounded corners] {are}; (w5) [draw, fill=gray!10, right=of w4, rounded corners] {born}; (w6) [draw, fill=gray!10, right=of w5, rounded corners] {free}; (w7) [draw, fill=gray!10, right=of w6, rounded corners] {and}; (w8) [draw, fill=blue!20, right=of w7, rounded corners] {equal}; (labeli) { \\\\ }; (labelii) { \\\\ }; (eqTop) [above=1.8cm of w8] {}; (w8.north) .. controls +(up:1.0cm) and +(up:1.0cm) .. (w6.north); (w8.north) .. controls +(up:1.2cm) and +(up:1.0cm) .. (w5.north); (w8.north) .. controls +(up:1.8cm) and +(up:1.0cm) .. node[midway, text=black, above] {} (w1.north); {tikzpicture} {Attention mechanisms learn a parameterized to measure how much token attends to token . One widely used construction of uses query and key {vector}, denoted by and , assigned to each token . {fig_attention_dict}} {figure} See also: , , .",
    "color": "lightgreen"
  },
  {
    "id": 30,
    "label": "transformer",
    "title": "In the context of , the term transformer refers to an {ann} that uses some form of mechanism to capture dependencies among {token} . The mechansim is what sets transformers apart from previous {model} used for sequential such as {rnn}. A transformer often combines several {layer} that are combined via more traditional architectures. \\\\ See also: , .",
    "color": "lightgreen"
  },
  {
    "id": 31,
    "label": "recurrent neural network (RNN)",
    "title": "A RNN is a specific type of that is designed for processing that consists of a sequence of {token}. An RNN maintains an internal hidden state that is updated recurrently as new {token} are processed. This recurrent dependence allows information to propagate across time steps, making RNNs suitable for tasks such as speech recognition, language modeling, or time series prediction. However, their inherently sequential computation limits parallelization and is challenging for {gdmethod}. Variants like the long short-term memory (LSTM) and gated recurrent unit (GRU) mitigate these problems.",
    "color": "violet"
  },
  {
    "id": 32,
    "label": "large language model (LLM)",
    "title": "A LLM is an umbrella term for methods that use high-dimensional {model} (with billions of {modelparam}) trained on large collections of text . LLMs are used to analyze or generate sequences of {token} which constitute text . Many current LLMs use some variant of a that is trained via self-supervised learning: The training is based on the task of predicting a few words that are intentionally removed from a large text corpus. Thus, we can construct {labeled datapoint} simply by selecting some words from a given text as {label} and the remaining words as {feature} of {datapoint}. This construction requires very little human supervision and allows for generating sufficiently large {trainset} for LLMs.\\\\ See also: , , .",
    "color": "lightgreen"
  },
  {
    "id": 33,
    "label": "self-supervised learning",
    "title": "Self-supervised learning uses some of the {feature} of a as its . For example, if a consists of a sentence within a text document, we an use the last word of the sentence as the which is to be predicted from all the previous words (which form the {feature} of the ). A main application of self-supervised learning is in for the training of {llm} from large collections of text . \\\\ See also: , , .",
    "color": "lightgreen"
  },
  {
    "id": 34,
    "label": "optimization problem",
    "title": "An optimization problem is a mathematical structure consisting of an defined over an optimization variable , together with a feasible set . The co-domain is assumed to be ordered, meaning that for any two elements , we can determine whether , , or . The goal of optimization is to find those values for which the objective is extremal\u2014i.e., minimal or maximal , , . \\\\ See also: .",
    "color": "violet"
  },
  {
    "id": 35,
    "label": "optimization method",
    "title": "An optimization method is an that reads in a representation of an and delivers an (approximate) solution as its output , , . \\\\ See also: , .",
    "color": "violet"
  },
  {
    "id": 36,
    "label": "convex optimization",
    "title": "Convex optimization studies the formulation, properties, and efficient solution methods for {optproblem} . A (defined on the ) consists of a and a constraint set for the optimization variable . It can be written compactly as _{ {C}} f(). Alternatively, a can be expressed in terms of constraint {function} as {align} _{ {R}^{}} & f() \\\\ {s.t.} & g_{}() 0, =1,\\,,\\,k. {equ_def_convx_opt_constr_dict} {align} {figure} {tikzpicture}[>=stealth, scale=1.0] (-3,0) -- (5.2,0) node[below] {}; (0,-0.2) -- (0,4.2) node[left] {}; plot ({},{exp(-)}); (-1,3) -- (-1,{exp(1)}) -- plot[domain=-1:3] ({},{exp(-)}) -- (3,3) -- cycle; (0,1) circle (1.6pt) node[left] {}; plot ({},{(2/exp(1)) - (1/exp(1))*}); (1,{1/exp(1)}) circle (1.2pt); at (2.6,2.5) {}; [below,yshift=-3pt] at (0,-0.2) {}; {tikzpicture} {A {equ_def_convx_opt_constr_dict} can be represented by a set that consists of objective values and constraint values that are achievable, i.e., by some . The optimal value of the is the smallest for which . } {figure} The formulation {equ_def_convx_opt_constr_dict} lends, in turn, to the epigraph form of {BoydConvexBook} \\{ t {R} : ({0}, t) {A} \\}, with the set {align} {A} \\{ (,t) & {R}^{} {R} : f() t, \\, \\\\ & g_{}() c_{}, \\, = 1,,k, { for some } {R}^{} \\}. {align} It can be shown that, since are {function}, is a set {BoydConvexBook}. The set fully characterizes the ~{equ_def_convx_opt_constr_dict} and can be interpreted as the of the ~ restricted to the feasible region defined by the constraint {function} .\\\\ See also: , , .",
    "color": "orange"
  },
  {
    "id": 37,
    "label": "Newton's method",
    "title": "Newton's method is an iterative for finding local {minimum} or {maximum} of a . Like {gdmethod}, Newton's method also computes a new estimate by optimizing a local approximation of around the current estimate . In contrast to {gdmethod}, which use the to build a local linear approximation, Newton's method uses the to build a local quadratic approximation. In particular, starting from an initial estimate , Newton's method iteratively updates the estimate according to \\[ {+1}= {}- ( ^2 f({}) )^{-1} f( {} ) {, for } =0,\\,1,\\,. \\] Here, is the , and is the of the . Since using a as local approximation is more accurate than using a linear (which is a special case of a ), Newton's method tends to converge faster than {gdmethod} (see Fig. {fig_newtonmethod_dict}). However, this faster comes at the increased computational complexity of the iterations. Indeed, each iteration of Newton's method requires the inversion of the . {figure}[H] {tikzpicture}[samples=200,smooth] {scope} (-5,-2) rectangle (5,5); plot[domain=0:360] ({1.5*cos()*sqrt(20/(sin(2*)+2))},{1.5*sin()*sqrt(20/(sin(2*)+2))}); plot[domain=0:360] ({1.5*cos()*sqrt(16/(sin(2*)+2))},{1.5*sin()*sqrt(16/(sin(2*)+2))}); plot[domain=0:360] ({1.5*cos()*sqrt(12/(sin(2*)+2))},{1.5*sin()*sqrt(12/(sin(2*)+2))}); plot[domain=0:360] ({1.5*cos()*sqrt(8/(sin(2*)+2))},{1.5*sin()*sqrt(8/(sin(2*)+2))}); plot[domain=0:360] ({1.5*cos()*sqrt(4/(sin(2*)+2))},{1.5*sin()*sqrt(4/(sin(2*)+2))}); plot[domain=0:360] ({1.5*cos()*sqrt(1/(sin(2*)+2))},{1.5*sin()*sqrt(1/(sin(2*)+2))}); plot[domain=0:360] ({1.5*cos()*sqrt(0.0625/(sin(2*)+2))},{1.5*sin()*sqrt(0.0625/(sin(2*)+2))}); (-3.5,4.5) -- (-3.3,3.8); (-3.3,3.8) -- (-3,3.1); (-3,3.1) -- (-2.6,2.4); (-2.6,2.4) -- (-2.1,1.8); (-2.1,1.8) -- (-1.5,1.3); (-1.5,1.3) -- (-0.9,0.9); (-0.9,0.9) -- (-0.4,0.5); (-0.4,0.5) -- (-0.1,0.2); at (-3.5,4.5) {start}; at (0.5,3) {}; (-3.5,4.5) -- (-2,2.5); (-2,2.5) -- (-0.8,0.8); (-0.8,0.8) -- (0,0.1); at (1.5,4) {Newton's method}; at (0,0) [circle,fill,inner sep=1pt,label=below:] {}; {scope} {tikzpicture} {Comparison of (blue) and Newton's method (red) paths toward the of a . {fig_newtonmethod_dict}} {figure} See also: , , , .",
    "color": "violet"
  },
  {
    "id": 38,
    "label": "Hilbert space",
    "title": "A Hilbert space is a complete inner product space. Thus, is a equipped with an inner product . The inner product induces a norm via . Furthermore, is complete in the sense that every in converges to a limit that is also contained in . {figure}[H] {tikzpicture}[scale=3] (0,0) circle (1); {35} (0,0) -- (1,0) node[below right] {}; (0,0) -- ({cos()},{sin()}) node[above] {}; (P) at ({cos()},0); ({cos()},{sin()}) -- (P); (0,0) -- (P) node[pos=0.5,below] {}; () -- (P) -- (); at ({cos(-)},{sin(-)}) {}; {tikzpicture} {For two unit-norm vectors the inner product is the expantion coefficient for the projection of onto the subspace spanned by . The absolute value measures the norm of this projection.{fig_hilbertspace_dict}} {figure} One important example of a Hilbert space is the with the inner product \\\\ See also: .",
    "color": "lightcoral"
  },
  {
    "id": 39,
    "label": "non-expansive operator",
    "title": "An operator defined on a Hilbert space is called non-expansive if it does not increase distances. In other words, {equation} { - '}{2} { - '}{2} { for any } , ' . {equation} See also: , .",
    "color": "lightgreen"
  },
  {
    "id": 40,
    "label": "Erd\\H{o}s\u2013R\\'enyi graph (ER graph)",
    "title": "An ER {Erd{o}s\u2013R\\'enyi graph (ER graph)} is a for {graph} defined over a given node set . One way to define the ER is via the collection of binary {rv} , for each pair of different nodes . A specific of an ER contains an edge if and only if . The ER is parameterized by the number of nodes and the . \\\\ See also: , , , , , .",
    "color": "salmon"
  },
  {
    "id": 41,
    "label": "attack",
    "title": "An attack on an system refers to an intentional action\u2014either active or passive\u2014that compromises the system's integrity, availability, or confidentiality. Active attacks involve perturbing components such as {dataset} (via ) or communication links between {device} within an application. Passive attacks, such as {privattack}, aim to infer {sensattr} without modifying the system. Depending on their goal, we distinguish among {dosattack}, attacks, and {privattack}. \\\\ See also: , , , , .",
    "color": "khaki"
  },
  {
    "id": 42,
    "label": "privacy attack",
    "title": "A privacy on an system aims to infer {sensattr} of individuals by exploiting partial access to a trained . One form of a privacy is .\\\\ See also: , , , , .",
    "color": "khaki"
  },
  {
    "id": 43,
    "label": "epigraph",
    "title": "The epigraph of a real-valued is the set of points lying on or above its (see Fig. {fig_epigraph_dict}), i.e., \\[ {epi}(f) = \\{ ({x}, t) {R}^n {R} \\,|\\, f({x}) t \\}. \\] A is if and only if its epigraph is a set , . {figure}[H] {tikzpicture}[scale=1.0] {axis}[ axis lines = middle, xlabel = , ylabel = {}, xmin=-2, xmax=2, ymin=0, ymax=4.5, samples=100, domain=-1.5:1.5, thick, width=8cm, height=6cm, grid=none, axis on top, ] [blue, thick, domain=-1.5:1.5] {x^2} node [pos=0.85, anchor=south west, xshift=5pt] {}; [ name path=f, draw=none, ytick=, domain=-1.5:1.5, ] {x^2}; (axis cs:-1.5,4) -- (axis cs:1.5,4); [ blue!20, opacity=0.6, draw=none, ] fill between [ of=f and top, soft clip={domain=-1.5:1.5}, ]; at (axis cs:-1.0,2.3) {}; {axis} {tikzpicture} {Epigraph of the (i.e., the shaded area). {fig_epigraph_dict}} {figure} See also: , .",
    "color": "orange"
  },
  {
    "id": 44,
    "label": "nullspace",
    "title": "The nullspace of a , denoted by , is the set of all {vector} such that {n} = {0}. Consider a method that uses the to transform a of a into a new . The nullspace characterizes all directions in the original along which the transformation remains unchanged. In other words, adding any from the nullspace to a does not affect the transformed representation . This property can be exploited to enforce invariances in the {prediction} (computed from ). Fig.\\ {fig:nullspace-rotation-dict} illustrates one such invariance. It shows rotated versions of two handwritten digits, which approximately lie along 1-D curves in the original . These curves are aligned with a direction . To ensure that the trained is invariant to such rotations, we can choose the transformation such that . This ensures that , and hence the resulting , is approximately insensitive to rotations of the input image. {figure}[H] {assets/pythonsnacks/nullspace_0_1.png} { Rotated handwritings of two different digits. The rotations are approximately aligned along straight lines parallel to the . For a binary distinguishing between these digits, a natural choice is a linear with a whose nullspace contains , i.e., . {fig:nullspace-rotation-dict}} {figure} See also: , , . \\\\ Python demo: {https://github.com/AaltoDictionaryofML/AaltoDictionaryofML.github.io/blob/main/assets/pythonsnacks/nullspace.py}{click me}",
    "color": "lightblue"
  },
  {
    "id": 45,
    "label": "maximum",
    "title": "The maximum of a set of real numbers is the greatest element in that set, if such an element exists. A set has a maximum if it is bounded above and attains its {RudinBookPrinciplesMatheAnalysis}. \\\\ See also: .",
    "color": "violet"
  },
  {
    "id": 46,
    "label": "supremum (or least upper bound)",
    "title": "The supremum of a set of real numbers is the smallest number that is greater than or equal to every element in the set. More formally, a real number is the supremum of a set if: 1) is an upper bound of ; and 2) no number smaller than is an upper bound of . Every non-empty set of real numbers that is bounded above has a supremum, even if it does not contain its supremum as an element {RudinBookPrinciplesMatheAnalysis}.",
    "color": "violet"
  },
  {
    "id": 47,
    "label": "discrepancy",
    "title": "Consider an application with represented by an . methods use a discrepancy measure to compare {map} from {localmodel} at nodes , connected by an edge in the . \\\\ See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 48,
    "label": "federated relaxed (FedRelax)",
    "title": "An . \\\\ See also: , .",
    "color": "lightblue"
  },
  {
    "id": 49,
    "label": "federated averaging (FedAvg)",
    "title": "FedAvg refers to a family of iterative {algorithm}. It uses a server-client setting and alternates between clientwise {localmodel} retraining, followed by the aggregation of updated {modelparam} at the server . The local update at client at time starts from the current {modelparam} provided by the server and typically amounts to executing few iterations of . After completing the local updates, they are aggregated by the server (e.g., by averaging them). Fig. {fig_single_iteration_fedavg_dict} illustrates the execution of a single iteration of FedAvg. {figure}[H] {center} {tikzpicture}[>=Stealth, node distance=1cm and 1.5cm, every node/.style={font=}] {server} = [circle, fill=black, minimum size=6pt, inner sep=0pt] {client} = [circle, draw=black, minimum size=6pt, inner sep=0pt] (label1) at (0,3.5) {broadcast}; (label2) {local update}; (label3) {aggregate}; (s1) at (label1 |- 0,2.5) {}; (c1l) at () {}; (c1r) at () {}; (dots1) at () {}; (s1) -- (c1l) node[midway,left] {}; (s1) -- (c1r) node[midway,right] {}; (s1) -- (dots1); (s2) at (label2 |- 0,2.5) {}; (c2l) at () {}; (c2r) at () {}; (dots2) at () {}; {}; {}; (s3) at (label3 |- 0,2.5) {}; {}; (c3l) at () {}; (c3r) at () {}; (dots3) at () {}; (c3l) -- (s3) node[midway,left] {}; (c3r) -- (s3) node[midway,right] {}; (dots3) -- (s3); {tikzpicture} {center} {Illustration of a single iteration of FedAvg, which consists of broadcasting {modelparam} by the server, performing local updates at clients, and aggregating the updates by the server. {fig_single_iteration_fedavg_dict}} {figure} See also: , , , .",
    "color": "lightblue"
  },
  {
    "id": 50,
    "label": "federated gradient descent (FedGD)",
    "title": "An that can be implemented as message passing across an . \\\\ See also: , , , , .",
    "color": "lightblue"
  },
  {
    "id": 51,
    "label": "federated stochastic gradient descent (FedSGD)",
    "title": "An that can be implemented as message passing across an . \\\\ See also: , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 52,
    "label": "horizontal federated learning (HFL)",
    "title": "HFL uses {localdataset} constitut\\-ed by different {datapoint} but uses the same {feature} to characterize them . For example, weather forecasting uses a network of spatially distributed weather (observation) stations. Each weather station measures the same quantities, such as daily temperature, air pressure, and precipitation. However, different weather stations measure the characteristics or {feature} of different spatiotemporal regions. Each spatiotemporal region represents an individual , each characterized by the same {feature} (e.g., daily temperature or air pressure).\\\\ See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 53,
    "label": "dimensionality reduction",
    "title": "Dimensionality reduction refers to methods that learn a transformation of a (typically large) set of raw {feature} into a smaller set of informative {feature} . Using a smaller set of {feature} is beneficial in several ways: {itemize} {Statistical benefit:} It typically reduces the of , as reducing the number of {feature} often reduces the of a . {Computational benefit:} Using fewer {feature} means less computation for the training of {model}. As a case in point, methods need to invert a whose size is determined by the number of {feature}. {Visualization:} Dimensionality reduction is also instrumental for visualization. For example, we can learn a transformation that delivers two {feature} , which we can use, in turn, as the coordinates of a . Fig.\\ {fig:dimred-scatter_dict} depicts the of handwritten digits that are placed using transformed {feature}. Here, the {datapoint} are naturally represented by a large number of greyscale values (one value for each pixel). {itemize} {figure}[H] {tikzpicture}[scale=1] (-0.5,0) -- (5.5,0) node[right] {}; (0,-0.5) -- (0,4.5) node[above] {}; // in { 1.2/0.5/3, 0.8/2.0/8, 2.5/1.8/1, 3.8/3.5/6, 4.2/0.7/9, 2.8/3.0/7, 1.5/3.8/2 }{ at (,) {}; } {tikzpicture} {Example of dimensionality reduction: High-dimensional image data (e.g., high-resolution images of handwritten digits) embedded into 2-D using learned {feature} and visualized in a .} {fig:dimred-scatter_dict} {figure} See also: , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 54,
    "label": "diagnosis",
    "title": "Consider an -based method that resulted in a trained (or learned ) . We can diagnose the method by comparing the with the incurred by on the and the . {figure}[htbp] {center} {tikzpicture}[ycomb] plot coordinates{(0,3)}; [below] at (0,0) {} ; plot coordinates{(5,5)}; [below] at (5,0) {} ; (-1,2) -- (7,2) node[right,text width=5cm]{ \\\\ (e.g., , existing ML methods or human performance)}; {tikzpicture} {center} {We can diagnose a -based method by comparing its with its . Ideally both are on the same level as a .{fig_diagnosis_dict}} {figure} \\\\ See also: , , , .",
    "color": "lightgreen"
  },
  {
    "id": 55,
    "label": "machine learning (ML)",
    "title": "ML aims to predict a from the {feature} of a . ML methods achieve this by learning a from a (or ) through the minimization of a , . One precise formulation of this principle is . Different ML methods are obtained from different design choices for {datapoint} (i.e., their {feature} and ), the , and the {MLBasics}. \\\\ See also: , , .",
    "color": "lightgreen"
  },
  {
    "id": 56,
    "label": "reinforcement learning (RL)",
    "title": "RL refers to an setting where we can only evaluate the usefulness of a single (i.e., a choice of {modelparam}) at each time step . In particular, RL methods apply the current to the of the newly received . The usefulness of the resulting is quantified by a signal (see Fig. {fig_reinforcementlearning_dict}). {figure}[H] {center} {tikzpicture}[scale=1] (-2, 0) -- (6, 0); at (6.3, 0) {}; plot (-3, {-0.2*()^2 + 2}); at (0-3, {-0.2*(0)^2 + 2}) {}; (1.5-3, {-0.2*(1.5)^2 + 2}) circle (2pt); at (1.5-3, -0.3) {}; (1.5-3, 0) -- (1.5-3, {-0.2*(1.5)^2 + 2}); plot (, {-0.15*( - 2)^2 + 3}); at (3, {-0.15*(3 - 2)^2 + 3}) {}; (2, {-0.15*(2 - 2)^2 + 3}) circle (2pt); at (2, -0.3) {}; (2, 0) -- (2, {-0.15*(3 - 2)^2 + 3}); plot (+2, {-0.1*( - 4)^2 + 1.5}); at (4.5+2, {-0.1*(4.5 - 4)^2 + 1.5}) {}; (3.5+2, {-0.1*(3.5 - 4)^2 + 1.5}) circle (2pt); at (3.5+2, -0.3) {}; (3.5+2, 0) -- (3.5+2, {-0.1*(3.5 - 4)^2 + 1.5}); {tikzpicture} {Three consecutive time steps with corresponding {lossfunc} . During time step , an RL method can evaluate the only for one specific , resulting in the signal . {fig_reinforcementlearning_dict}} {center} {figure} In general, the depends also on the previous {prediction} for . The goal of RL is to learn , for each time step , such that the (possibly discounted) cumulative is maximized , . \\\\ See also: , , .",
    "color": "orange"
  },
  {
    "id": 57,
    "label": "feature learning",
    "title": "Consider an application with {datapoint} characterized by raw {feature} . learning refers to the task of learning a : ': ' that reads in the {feature} of a and delivers new {feature} from a new . Different learning methods are obtained for different design choices of , for a of potential {map} , and for a quantitative measure of the usefulness of a specific . For example, uses , with , and a \\{ : {R}^{} \\!\\! {R}^{'}\\!:\\!'\\!\\! { with some } \\!\\! {R}^{' \\! } \\}. measures the usefulness of a specific by the linear reconstruction error incurred on a such that _{ {R}^{ \\!\\!\\! '}} _{=1}^{} { ^{()} - ^{()}}{2}^{2}. \\\\ See also: , , , .",
    "color": "lightblue"
  },
  {
    "id": 58,
    "label": "autoencoder",
    "title": "An autoencoder is an method that simultaneously learns an encoder and a decoder . Different autoencoders use different {model} , e.g., {ann} with different architectures. The special case of an autoeencoder using (vector-valued) {linmodel} for results in . {figure}[H] {tikzpicture}[>=Latex, thick, node distance=1.6cm] (x) {}; (enc) {}; (z) {}; (dec) {}; (xhat) {}; (x) -- (enc); (enc) -- node[above] {} (z); (z) -- (dec); (dec) -- node[above] {} (xhat); {tikzpicture} {Autoencoder with encoder mapping and decoder mapping .} {figure} The training of the encoder and decoder can be implemented via using a that measures the deviation of the reconstructed from the original . \\\\ See also: , .",
    "color": "lightgreen"
  },
  {
    "id": 59,
    "label": "vertical federated learning (VFL)",
    "title": "VFL refers to applications where {device} have access to different {feature} of the same set of {datapoint} . Formally, the underlying global is \\[ ^{({global})} \\{ (^{(1)}, ^{(1)}), \\,, \\,(^{()}, ^{()}) \\}. \\] We denote by , for , the complete {featurevec} for the {datapoint}. Each observes only a subset of {feature}, resulting in a with {featurevec} \\[ ^{(,)} = ( ^{()}_{_{1}}, \\,, \\,^{()}_{_{}} )\\,^{T}. \\] Some of the {device} may also have access to the {label} , for , of the global (see Fig. {fig_vertical_FL_dict}). {figure}[H] {center} {tikzpicture}[every node/.style={anchor=base}] {0} {1.6} {3.2} {4.8} {6.4} {0} {-1.2} {-2.4} {-3.6} / in {1/1, 2/2, 4/} { {}{-1.2*(-1)} (x1) at (0,) {}; (x2) at (1.6,) {}; (dots) at (3.2,) {}; (x3) at (4.8,) {}; (y) at (6.4,) {}; } (-0.6,0.6) rectangle (6.9,-4.2); at (3.1,0.9) {}; (-0.9,0.9) rectangle (2.1,-4.0); at (0.25,1.0) {}; () rectangle (); at () {}; {tikzpicture} {center} {VFL uses {localdataset} that are derived from the {datapoint} of a common global . The {localdataset} differ in the choice of {feature} used to characterize the {datapoint}.{fig_vertical_FL_dict}} {figure} One potential application of VFL is to enable collaboration between different healthcare providers. Each provider collects distinct types of measurements\u2014such as blood values, electrocardiography, and lung X-rays\u2014for the same patients. Another application is a national social insurance system, where health records, financial indicators, consumer behavior, and mobility are collected by different institutions. VFL enables joint learning across these parties while allowing well-defined levels of . \\\\ See also: , .",
    "color": "lightblue"
  },
  {
    "id": 60,
    "label": "interpretability",
    "title": "An method is interpretable for a human user if they can comprehend the decision process of the method. One approach to develop a precise definition of interpretability is via the concept of simulatability, i.e., the ability of a human to mentally simulate the behavior , , , , . The idea is as follows: If a human user understands an method, then they should be able to anticipate its {prediction} on a . We illustrate such a in Fig. {fig_aug_simulatability_dict}, which also depicts two learned {hypothesis} and . The method producing the is interpretable to a human user familiar with the concept of a . Since corresponds to a , the user can anticipate the {prediction} of on the . In contrast, the method delivering is not interpretable, because its behavior is no longer aligned with the user\u2019s {expectation}. {figure}[H] {center} {tikzpicture}[x=1.5cm, y=1cm] {0.4} {2.0} (0,0.5) -- (7.7,0.5) node[below, xshift=-1cm] {}; (0.5,0) -- (0.5,4.2) node[above] {}; plot ({},{ + }); plot ({},{ + -(-4)*0.5}); at (7.2, {7.2 + }) {}; at (7.2, {7.2 + - 0.5*(7.2 - 4)}) {}; /// in { 1.2/1.0/blue/6, 1.4/1.0/blue/6, 1.7/1.0/blue/6, 2.2/3.9/blue/12, 2.6/4.2/blue/12, 3.0/4.4/blue/12 }{ (pt) at (,); at (pt) {}; }, color=, thick] (, { + }) -- (pt); } /// in { 5.7/2.6/red/12, 5.9/2.6/red/12, 6.2/2.6/red/12 }{ (pt) at (,{ + }); at (pt) {}; } (4.2, 1.7) circle (0.1cm) node [black,xshift=0.2cm,anchor=west] { }; (4.2, 1.2) circle (0.1cm) node [black,xshift=0.2cm,anchor=west] { }; {tikzpicture} {We can assess the interpretability of trained {model} and by comparing their {prediction} to pseudo-{label} generated by a human user for . {fig_aug_simulatability_dict}} {center} {figure} The notion of interpretability is closely related to the notion of , as both aim to make methods more understandable for humans. In the context of Fig. {fig_aug_simulatability_dict}, interpretability of an method requires that the human user can anticipate its {prediction} on an arbitrary . This contrasts with , where the user is supported by external {explanation}\u2014such as saliency {map} or reference examples from the \u2014to understand the {prediction} of on a specific . \\\\ See also: , , , .",
    "color": "orange"
  },
  {
    "id": 61,
    "label": "multitask learning",
    "title": "Multitask learning aims to leverage relations between different {learningtask}. Consider two {learningtask} obtained from the same of webcam snapshots. The first task is to predict the presence of a human, while the second task is to predict the presence of a car. It may be useful to use the same structure for both tasks and only allow the of the final output to be different. \\\\ See also: , , , , .",
    "color": "khaki"
  },
  {
    "id": 62,
    "label": "learning task",
    "title": "Consider a consisting of multiple {datapoint} . For example, can be a collection of images in an image database. A learning task is defined by specifying those properties (or attributes) of a that are used as its {feature} and {label}. Given a choice of and , a learning task leads to an instance of and can thus be represented by the associated for . Importantly, multiple distinct learning tasks can be constructed from the same by selecting different sets of {feature} and {label} (see Fig. {fig:learning_tasks_cows_dict}). {figure}[H] {minipage}[t]{0.95} {assets/CowsAustria.jpg} {An image showing cows grazing in the Austrian countryside.} {5mm} {minipage} {5mm} {minipage}[t]{0.45} Task 1 (): \\\\ {feature} are the RGB values of all image pixels, and the is the number of cows depicted. {minipage} {minipage}[t]{0.45} Task 2 (): \\\\ {feature} include the average green intensity of the image, and the indicates whether cows should be moved to another location (i.e., yes/no). {minipage} {Two learning tasks constructed from a single image . These tasks differ in selection and choice of (i.e., the objective), but are both derived from the same .} {fig:learning_tasks_cows_dict} {figure} Different learning tasks arising from the same underlying are often coupled. For example, when a is used to generate {datapoint}, statistical dependencies among different {label} induce dependencies among the corresponding learning tasks. In general, solving learning tasks jointly, e.g., using methods, tends to be more effective than solving them independently (thereby ignoring dependencies among learning tasks) , , . \\\\ See also: , .",
    "color": "khaki"
  },
  {
    "id": 63,
    "label": "explainability",
    "title": "We define the (subjective) explainability of an method as the level of simulatability of the {prediction} delivered by an system to a human user. Quantitative measures for the (subjective) explainability of a trained can be constructed by comparing its {prediction} with the {prediction} provided by a user on a , . Alternatively, we can use {probmodel} for and measure the explainability of a trained via the conditional (or differential) of its {prediction}, given the user's {prediction} , . \\\\ See also: , .",
    "color": "orange"
  },
  {
    "id": 64,
    "label": "local interpretable model-agnostic explanations (LIME)",
    "title": "Consider a trained (or learned ) , which maps the of a to the . LIME is a technique for explaining the behavior of , locally around a with . The is given in the form of a local approximation of (see Fig. {fig_lime_dict}). This approximation can be obtained by an instance of with a carefully designed . In particular, the consists of {datapoint} with {featurevec} centered around and the (pseudo-) . Note that we can use a different for the approximation from the original . For example, we can use a to locally approximate a . Another widely used choice for is the . {figure}[H] {center} {tikzpicture} {axis}[ axis lines=middle, xlabel={}, ylabel={}, xtick=, ytick=, xmin=0, xmax=6, ymin=0, ymax=6, domain=0:6, samples=100, width=10cm, height=6cm, clip=false ] {2 + sin(deg(x))} node[pos=0.85, above right,yshift=3pt] {}; coordinates {(3,0) (3,6)}; {2 + sin(deg(3))} node[pos=0.9, above] {}; coordinates {(3, {2 + sin(deg(3))})}; at (axis cs:3,-0.3) {}; {axis} {tikzpicture} {center} {To explain a trained , around a given , we can use a local approximation . } {fig_lime_dict} {figure} See also: , , , , , , , .",
    "color": "orange"
  },
  {
    "id": 65,
    "label": "linear model",
    "title": "Consider an application involving {datapoint}, each represented by a numeric . A linear defines a consisting of all real-valued {linearmap} from to such that {equation} {equ_def_lin_model_hypspace_dict} {} \\{ : {R}^{} {R} () = ^{} { for some } {R}^{} \\}. {equation} Each value of defines a different , corresponding to the number of {feature} used to compute the . The choice of is often guided not only by (e.g., fewer features reduce computation) and (e.g., more features typically reduce and ), but also by . A linear using a small number of well-chosen {feature} is generally considered more interpretable , . The linear is attractive because it can typically be trained using scalable {optmethod} , . Moreover, linear {model} often permit rigorous statistical analysis, including fundamental limits on the achievable . They are also useful for analyzing more complex nonlinear {model} such as {ann}. For instance, a can be viewed as the composition of a \u2014implemented by the input and hidden {layer}\u2014and a linear in the output . Similarly, a can be interpreted as applying a one-hot-encoded based on {decisionregion}, followed by a linear that assigns a to each region. More generally, any trained that is at some can be locally approximated by a . Fig.~{fig_linapprox_dict} illustrates such a local linear approximation, defined by the . Note that the is only defined where is . To ensure in the context of , one may prefer {model} whose associated is Lipschitz continuous. A classic result in mathematical analysis\u2014Rademacher\u2019s Theorem\u2014states that if is Lipschitz continuous with some constant over an open set , then is almost everywhere in {heinonen2005lectures}. {figure}[H] {center} {tikzpicture}[x=0.5cm] {axis}[ hide axis, xmin=-3, xmax=6, ymin=0, ymax=6, domain=0:6, samples=100, width=10cm, height=6cm, clip=false ] {2 + sin(deg(x))} node[pos=0.5, above right, yshift=3pt] {}; {2 + sin(deg(6)) + cos(deg(6))*(x - 6)} node[pos=0.95, above right] {}; coordinates {(6, {2 + sin(deg(6))})}; coordinates {(6,0) (6,2.4)}; at (axis cs:6, -0.2) {}; {}{-1.5} {}{3} {}{2 + sin(deg())} {}{2 + sin(deg())} coordinates {(, ) (, )}; (axis cs:,) -- (axis cs:,0); (axis cs:,) -- (axis cs:,0); (axis cs:,) -- (axis cs:0,); (axis cs:,) -- (axis cs:0,); (axis cs:,-0.4) -- node[below] {} (axis cs:,-0.4); (axis cs:-2.4,) -- node[left] {} (axis cs:-2.4,); {axis} {-10mm} {tikzpicture} {-5mm} {center} { A trained that is at a point can be locally approximated by a . This local approximation is determined by the .} {fig_linapprox_dict} {figure} See also: , , , , .",
    "color": "orange"
  },
  {
    "id": 66,
    "label": "gradient step",
    "title": "Given a real-valued and a , the step updates by adding the scaled negative to obtain the new (see Fig. {fig_basic_GD_step_single_dict}) {equation} {equ_def_gd_basic_dict} {} - f(). {equation} Mathematically, the step is an operator that is paramet\\-rized by the and the . {figure}[H] {center} {tikzpicture}[scale=0.8] (-4,0) grid (4,4); plot (, {(1/4)*}); plot (, {2* - 4}); (4,4) -- node[right] {} (4,2); (4,4) -- node[above] {} (2,4); (4,2) -- node[below] {} (3,2) ; at (-4.1, 4.1) {}; (0pt,2pt) -- (0pt,-2pt) node[below] {}; (0pt,2pt) -- (0pt,-2pt) node[below] {}; (0pt,2pt) -- (0pt,-2pt) node[below] {}; {tikzpicture} {center} {The basic step {equ_def_gd_basic_dict} maps a given to the updated . It defines an operator .} {fig_basic_GD_step_single_dict} {figure} Note that the step {equ_def_gd_basic_dict} optimizes locally\u2014in a whose size is determined by the \u2014a linear approximation to the . A natural of {equ_def_gd_basic_dict} is to locally optimize the itself\u2014instead of its linear approximation\u2014such that {align} {equ_approx_gd_step_dict} {} = _{' {R}^{}} f(')\\!+\\!{1}{}{-'}{2}^2. {align} We intentionally use the same symbol for the in {equ_approx_gd_step_dict} as we used for the in {equ_def_gd_basic_dict}. The larger the we choose in {equ_approx_gd_step_dict}, the more progress the update will make toward reducing the value . Note that, much like the step {equ_def_gd_basic_dict}, the update {equ_approx_gd_step_dict} also defines an operator that is parameterized by the and the . For a , this operator is known as the of . \\\\ See also: , , , , , , , , , , .",
    "color": "violet"
  },
  {
    "id": 67,
    "label": "contraction operator",
    "title": "An operator is a contraction if, for some , {equation} { \\!-\\! '}{2} {\\!-\\!'}{2} { holds for any } ,' {R}^{}. {equation}",
    "color": "lightgreen"
  },
  {
    "id": 68,
    "label": "proximal operator",
    "title": "Given a , we define its proximal operator as , {f()}{}{} _{' {R}^{}} { with } > 0. As illustrated in Fig. {fig_proxoperator_opt_dict}, evaluating the proximal operator amounts to minimizing a penalized variant of . The penalty term is the scaled squared Euclidean distance to a given (which is the input to the proximal operator). The proximal operator can be interpreted as a of the , which is defined for a . Indeed, taking a with at the current is the same as applying the proximal operator of the and using . {figure}[H] {center} {tikzpicture}[scale=0.8] plot (, {(1/4)*}) node[above right] {}; plot (, {2*( - 2)*( - 2)}) node[below right] {}; (0pt,2pt) -- (0pt,-2pt) node[below] {}; {tikzpicture} {center} {The proximal operator updates a by minimizing a penalized version of the . The penalty term is the scaled squared Euclidean distance between the optimization variable and the given . {fig_proxoperator_opt_dict}} {figure} See also: , , , , , , .",
    "color": "violet"
  },
  {
    "id": 69,
    "label": "proximable",
    "title": "A for which the can be computed efficiently is sometimes referred to as proximable or simple . \\\\ See also: , , .",
    "color": "orange"
  },
  {
    "id": 70,
    "label": "connected",
    "title": "An is connected if for every non-empty subset we can find at least one edge connecting a node in with some node in . {figure}[H] {tikzpicture} (A1) at (0, 1.5) {}; {not connected}; (B1) [below right=0.8cm and 0.5cm of A1] {}; (C1) [below left=0.8cm and 0.5cm of A1] {}; [line width=1 pt] (A1) -- (B1); {scope}[xshift=3.5cm] (A2) at (0, 1.5) {}; {connected}; (B2) [below right=0.8cm and 0.5cm of A2] {}; (C2) [below left=0.8cm and 0.5cm of A2] {}; [line width=1 pt] (A2) -- (B2); [line width=1 pt] (B2) -- (C2); {scope} {tikzpicture} {figure} See also: , .",
    "color": "lightblue"
  },
  {
    "id": 71,
    "label": "multivariate normal distribution",
    "title": "The multivariate normal distribution, which is denoted by , is a fundamental for numerical {featurevec} of fixed dimension . It defines a family of {probdist} over -valued {rv} ~, , . Each distribution in this family is fully specified by its and . When the is invertible, the corresponding is characterized by the following : \\[p() = {1}{{(2)^{} \\,()}} . \\] Note that this is only defined when is invertible. More generally, any admits the following representation: \\[ = + \\] where is a and satisfies . This representation remains valid even when is singular, in which case is not full rank~{Lapidoth2017}. The family of multivariate normal distributions is exceptional among {probmodel} for numerical quantities, at least for the following reasons. First, the family is closed under affine transformations, i.e., \\[ {N}(,) { implies } \\!+\\! {N}( +, \\,^{T} ). \\] Second, the maximizes the among all distributions with the same ~. \\\\ See also: , , , , .",
    "color": "salmon"
  },
  {
    "id": 72,
    "label": "standard normal vector",
    "title": "A standard normal is a random whose entries are {gaussrv} . It is a special case of a , . \\\\ See also: , , , , .",
    "color": "salmon"
  },
  {
    "id": 73,
    "label": "early stopping",
    "title": "Consider an -based method that uses an iterative (such as ) to learn {modelparam} via minimizing the on a given . Early stopping refers to terminating the iterations even if they still substantially decrease the on the . Instead of monitoring the (which is the on the ), early stopping monitors the incurred by the {modelparam} in each iteration. Early stopping can be interpreted as an implementation of via pruning. Indeed, terminating an iterative after a small number of iterations restricts the set of {modelparam} that can be reached from the initialization (see Fig.\\ {fig_early_stopping_dict}). {figure}[htbp] {tikzpicture}[>=Stealth, scale=2] (0,0) circle (0.6pt) node[above] { }; at (-0.4,0) { }; at (-1.2,0) { }; at (-2,0) { }; (0,0) ellipse (0.8 and 0.4); (0,0) ellipse (1.6 and 0.8); (0,0) -- (0.8,0.) node [pos=0.6,above] { step}; (0.0,0.0) -- (0,-0.8) node [pos=0.9,right] { steps}; {tikzpicture} {A for using a defines a nested of effective {hypospace} . The effective is determined by all {modelparam} that can be reached from the initialization within {gradstep}. {fig_early_stopping_dict}} {figure} \\\\ See also: , , .",
    "color": "lightgreen"
  },
  {
    "id": 74,
    "label": "statistical aspects",
    "title": "By statistical aspects of an method, we refer to (properties of) the of its output under a for the fed into the method. \\\\ See also: , , , .",
    "color": "orange"
  },
  {
    "id": 75,
    "label": "computational aspects",
    "title": "By computational aspects of an method, we mainly refer to the computational resources required for its implementation. For example, if an method uses iterative optimization techniques to solve , then its computational aspects include: 1) how many arithmetic operations are needed to implement a single iteration (i.e., a ); and 2) how many iterations are needed to obtain useful {modelparam}. One important example of an iterative optimization technique is . \\\\ See also: , , , {modelparam}, .",
    "color": "violet"
  },
  {
    "id": 76,
    "label": "$\\bf 0/1$ loss",
    "title": "The measures the quality of a that delivers a (e.g., via thresholding {equ_def_threshold_bin_classifier_dict}) for the of a with {feature} . It is equal to if the is correct, i.e., when . It is equal to if the is wrong, i.e., when . \\\\ See also: , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 77,
    "label": "probability",
    "title": "We assign a probability value, typically chosen in the interval , to each that can occur in a , , , . \\\\ See also: , .",
    "color": "salmon"
  },
  {
    "id": 78,
    "label": "underfitting",
    "title": "Consider an method that uses to learn a with the on a given . Such a method is underfitting the if it is not able to learn a with a sufficiently low on the . If a method is underfitting, it will typically also not be able to learn a with a low . \\\\ See also: , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 79,
    "label": "overfitting",
    "title": "Consider an method that uses to learn a with the on a given . Such a method is overfitting the if it learns a with a low on the but a significantly higher outside the . \\\\ See also: , , , .",
    "color": "lightgreen"
  },
  {
    "id": 80,
    "label": "general data protection regulation (GDPR)",
    "title": "The GDPR was enacted by the European Union (EU), effective from 25 May 2018 . It safeguards the privacy and rights of individuals in the EU. The GDPR has significant implications for how are collected, stored, and used in applications. Key provisions include the following: {itemize} : systems should only use the necessary amount of personal for their purpose. and : systems should enable their users to understand how the systems make decisions that impact the users. subject rights: Users should get an opportunity to access, rectify, and delete their personal , as well as to object to automated decision-making and profiling. Accountability: Organizations must ensure robust security and demonstrate compliance through documentation and regular audits. {itemize} See also: , , , , .",
    "color": "khaki"
  },
  {
    "id": 81,
    "label": "normal distribution",
    "title": "See .",
    "color": "salmon"
  },
  {
    "id": 82,
    "label": "Gaussian random variable (Gaussian RV)",
    "title": "A standard Gaussian is a real-valued with , , {equation} p(x) = {1}{{2}} \\,(-x^2/2). {equation} Given a standard , we can construct a general with and via . The of a is referred to as normal distribution, denoted by . \\\\ A random with and can be constructed as , , \\[ {A} + { } \\] where is a of standard Gaussian {rv}, and is any satisfying . The of a Gaussian random is referred to as the , denoted by . \\\\ We can interpret a random as a indexed by the set . A {GaussProc} is a over an arbitray index set such that any restriction to a finite subset yields a random . \\\\ {rv} are widely used {probmodel} in the statistical analysis of methods. Their significance arises partly from the which provides conditions under which the average of many independent {rv} (not necessarily themselves) tends toward a . \\\\ The is also distinct in that it represents . Among all -valued {rv} with a given , the maximizes {coverthomas}. This makes {GaussProc} a natural choice for capturing or the lack of (domain) knowledge. \\\\ See also: , , , , .",
    "color": "salmon"
  },
  {
    "id": 83,
    "label": "Gaussian",
    "title": "See .",
    "color": "salmon"
  },
  {
    "id": 84,
    "label": "Gaussian process (GP)",
    "title": "A GP is a collection of {rv} indexed by input values from some input space such that, for any finite subset , the corresponding {rv} have a joint \\[ f ( ^{(1)}, \\,, \\,^{()} ) {N}({}, {K}). \\] For a fixed input space , a GP is fully specified (or parameterized) by: 1) a ; and 2) a .\\\\ { Example.} We can interpret the temperature distribution across Finland (at a specific point in time) as the of a GP , where each input denotes a geographic location. Temperature observations from weather stations provide values at specific locations (see Fig. {fig_gp_FMI_dict}). A GP allows us to predict the temperature nearby weather stations and to quantify the of these {prediction}. {figure}[H] {center} {tikzpicture} {axis}[ axis equal, hide axis, scale=1.2, xmin=17, xmax=32, ymin=55, ymax=71, clip=true ] table [x=lon, y=lat, col sep=comma] {assets/finland_border.csv}; table [x=lon, y=lat, col sep=comma] {assets/fmi_stations_subset.csv}; (axis cs:19,59) -- (axis cs:25.5,59) node[anchor=west] {longitude (lon)}; (axis cs:19,59) -- (axis cs:19,65.5) node[anchor=south] {latitude (lat)}; {axis} {tikzpicture} {-15mm} {center} {For a given point in time, we can interpret the current temperature distribution over Finland as a of a GP indexed by geographic coordinates and sampled at weather stations. The weather stations are indicated by blue dots. {fig_gp_FMI_dict}} {figure} See also: , , .",
    "color": "salmon"
  },
  {
    "id": 85,
    "label": "trustworthy artificial intelligence (trustworthy AI)",
    "title": "Besides the and , a third main design aspect of methods is their trustworthiness . The EU has put forward seven key requirements (KRs) for trustworthy (which typically build on methods) : {enumerate}[label=)] KR1 \u2013 Human agency and oversight; KR2 \u2013 Technical and safety; KR3 \u2013 Privacy and governance; KR4 \u2013 ; KR5 \u2013 Diversity, non-discrimination and fairness; KR6 \u2013 Societal and environmental well-being; KR7 \u2013 Accountability. {enumerate} See also: , , , , , , .",
    "color": "orange"
  },
  {
    "id": 86,
    "label": "squared error loss",
    "title": "The squared error measures the error of a when predicting a numeric from the {feature} of a . It is defined as {equation} {(,)}{} ( - {()}_{=} )^{2}. {equation} \\\\ See also: , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 87,
    "label": "projection",
    "title": "Consider a subset of the -dimensional . We define the projection of a onto as {equation} {equ_def_proj_generic_dict} {}{} = _{' } { - '}{2}. {equation} In other words, is the in that is closest to . The projection is only well defined for subsets for which the above exists . \\\\ See also: , , .",
    "color": "lightcoral"
  },
  {
    "id": 88,
    "label": "projected gradient descent (projected GD)",
    "title": "Consider an -based method that uses a parameterized with . Even if the of is , we cannot use basic , as it does not take into account constraints on the optimization variable (i.e., the {modelparam}). Projected extends basic to address this issue. A single iteration of projected consists of first taking a and then projecting the result back onto the . See Fig. {fig_projected_GD_dict} for a visual illustration. {figure}[H] {center} {tikzpicture}[scale=0.9] [right] at (-5.1,1.7) {} ; plot (, {(1/8)*}); [fill] (2.83,1) circle [radius=0.1] node[right] {}; (2.83,1) -- node[midway,above] {} (-1.5,1); (-1.5,1) --(-1.5,-1.5) node [below, left]{} ; (-1.5,-1.5) -- node[midway,above] {} (1,-1.5) ; [fill] (1,-1.5) circle [radius=0.1] node[below] {}; (1,-1.5) -- (3,-1.5) node[midway, above] {}; {tikzpicture} {-5mm} {center} {Projected augments a basic with a back onto the constraint set .} {fig_projected_GD_dict} {figure} See also: , , , , , , {modelparam}, , .",
    "color": "lightgreen"
  },
  {
    "id": 89,
    "label": "differential privacy (DP)",
    "title": "Consider some method that reads in a (e.g., the used for ) and delivers some output . The output could be either the learned {modelparam} or the {prediction} for specific {datapoint}. DP is a precise measure of incurred by revealing the output. Roughly speaking, an method is differentially private if the of the output remains largely unchanged if the of one in the is changed. Note that DP builds on a for an method, i.e., we interpret its output as the of a . The randomness in the output can be ensured by intentionally adding the of an auxiliary (i.e., adding noise) to the output of the method. \\\\ See also: , , , .",
    "color": "khaki"
  },
  {
    "id": 90,
    "label": "robustness",
    "title": "Robustness is a key requirement for . It refers to the property of an system to maintain acceptable performance even when subjected to different forms of perturbations. These perturbations may affect the {feature} of a in order to manipulate the delivered by a trained . Robustness also includes the of -based methods against perturbations of the . Such perturbations can occur within {attack}. \\\\ See also: ,,, .",
    "color": "orange"
  },
  {
    "id": 91,
    "label": "stability",
    "title": "Mathematically, a method is a map from a given to an output . As a case in point, consider an -based method that maps a to the learned {modelparam} which achieve the minimum average on the . Instead of the learned {modelparam}, the output could also be the {prediction} obtained from the trained . Stability refers to the desirable property of that small changes in the input result in small changes in the output . The notion of stability is intimately related to the notion of . In particular, there are formal notions of stability that allow to bound the (see {ShalevMLBook}). To build intuition, consider the three {dataset} depicted in Fig. {fig_three_data_stability_dict}, each of which is equally likely under the same -generating . Since the optimal {modelparam} are determined by this underlying , an accurate method should return the same (or very similar) output for all three {dataset}. In other words, any useful must be robust to variability in {realization} from the same , i.e., it must be stable. {figure}[H] {tikzpicture} {axis}[ axis lines=none, xlabel={}, ylabel={}, legend pos=north west, ymin=0, ymax=10, xtick={1,2,3,4,5}, grid style=dashed, every axis plot/.append style={very thick} ] +[only marks,mark=*] coordinates { (1,2) (2,4) (3,3) (4,5) (5,7) }; +[only marks,mark=square*] coordinates { (1,3) (2,2) (3,6) (4,4) (5,5) }; +[only marks,mark=triangle*] coordinates { (1,5) (2,7) (3,4) (4,6) (5,3) }; {axis} {tikzpicture} {Three {dataset} , , and , each sampled independently from the same -generating . A stable method should return similar outputs when trained on any of these {dataset}. {fig_three_data_stability_dict}} {figure} See also: , .",
    "color": "orange"
  },
  {
    "id": 92,
    "label": "privacy protection",
    "title": "Consider some method that reads in a and delivers some output . The output could be the learned {modelparam} or the obtained for a specific with {feature} . Many important applications involve {datapoint} representing humans. Each is characterized by {feature} , potentially a , and a (e.g., a recent medical diagnosis). Roughly speaking, privacy protection means that it should be impossible to infer, from the output , any of the {sensattr} of {datapoint} in . Mathematically, privacy protection requires non-invertibility of the . In general, just making non-invertible is typically insufficient for privacy protection. We need to make sufficiently non-invertible. \\\\ See also: , , {modelparam}, , , , , , .",
    "color": "khaki"
  },
  {
    "id": 93,
    "label": "privacy leakage",
    "title": "Consider an application that processes a and delivers some output, such as the {prediction} obtained for new {datapoint}. Privacy leakage arises if the output carries information about a private (or sensitive) of a of (such as a human). Based on a for the generation, we can measure the privacy leakage via the between the output and the sensitive . Another quantitative measure of privacy leakage is . The relations between different measures of privacy leakage have been studied in the literature (see ). \\\\ See also: , , , .",
    "color": "khaki"
  },
  {
    "id": 94,
    "label": "probabilistic model",
    "title": "A probabilistic interprets the generation of {datapoint} as {rv} with a joint . This joint typically involves {parameter} that have to be manually chosen or learned via statistical inference methods such as estimation . \\\\ See also: , , , , , , .",
    "color": "salmon"
  },
  {
    "id": 95,
    "label": "mean",
    "title": "The mean of a , which takes on values in a , is its . It is defined as the Lebesgue integral of with respect to the underlying (e.g., see or ), i.e., \\[ \\{\\} = _{{R}^{}} \\, {d}P(). \\] We also use the term to refer to the average of a finite . However, these two definitions are essentially the same. Indeed, we can use a to construct a discrete on the . Here, the index is chosen uniformly at random, for all . The mean of is precisely the average . For a with finite second-order moment, i.e., is well-defined and fnite, the mean is characterized as the solution of the following minimization problem : \\[ \\{\\} = _{ {R}^{}} \\{{ - }{2}^{2} \\}. \\] For the , associated with a , this reduces to with on . \\\\ See also: , , , .",
    "color": "salmon"
  },
  {
    "id": 96,
    "label": "median",
    "title": "A median of a real-valued is any number such that and (see Fig. {fig_median1_dict}) . {figure}[H] {center} {tikzpicture} {axis}[ axis lines=middle, xlabel={}, ylabel={}, ymin=0, ymax=1.1, xmin=-2, xmax=6, xtick=, ytick={0,1/2,1}, domain=-2:6, samples=200, width=10cm, height=6cm, smooth, enlargelimits=true, clip=false ] {1/(1 + exp(-(x - 1)))} node[pos=0.5, above, yshift=15pt] {}; (axis cs:1,0) -- (axis cs:1,0.5); (axis cs:-2,0.5) -- (axis cs:1,0.5); (axis cs:1,0.5) circle (2pt); at (axis cs:1,0) {}; at (axis cs:6.3,0) {}; {axis} {tikzpicture} {center} {The median of a real-valued is any number that partitions into two rays with equal . {fig_median1_dict}} {figure} We can define the median of a via a specific that is naturally associated with . In particular, this is defined on the via . Here, the index is chosen uniformly at random, i.e., for all . If the is integrable, any median of solves the : _{x' {R}} {|x - x'|}. For a the above (constructed from a ), this is on using . Like the , the median of a can also be used to estimate {parameter} of an underlying . Compared with the , the median is more robust to {outlier}. For example, a median of a with more than one does not change even if we arbitrarily increase the largest element of (see Fig. {fig_median2_dict}). In contrast, the will increase arbitrarily. {figure}[H] {tikzpicture}[scale=0.7, y=0.5cm, x=0.5cm] {scope} / in { 1/2, 4/3, 7/4 } { (, 0) -- (, ); (, ) circle (2pt); (ptA) at (, ) {}; } (0.5, 3) -- (10.5, 3) node[right] {}; at (7.5, -4) {(a)}; {scope} {scope}[xshift=12cm] / in { 1/2, 4/3, 7/10 } { (, 0) -- (, ); (, ) circle (2pt); (ptB) at (, ) {}; } (0.5, 7.5) -- (10.5, 7.5) node[right] {}; (0.5, 3) -- (10.5, 3) node[right] {}; at (ptB7) {}; at (7.5, -4) {(b)}; {scope} {tikzpicture} {The median is robust against contamination. (a) Original . (b) Noisy including an . {fig_median2_dict}} {figure} See also: , , , .",
    "color": "salmon"
  },
  {
    "id": 97,
    "label": "variance",
    "title": "The variance of a real-valued is defined as the of the squared difference between and its . We extend this definition to -valued {rv} as . \\\\ See also: , , .",
    "color": "salmon"
  },
  {
    "id": 98,
    "label": "nearest neighbor (NN)",
    "title": "NN methods learn a whose value is solely determined by the NNs within a given . Different methods use different {metric} for determining the NNs. If {datapoint} are characterized by numeric {featurevec}, we can use their Euclidean distances as the . \\\\ See also: , .",
    "color": "lightblue"
  },
  {
    "id": 99,
    "label": "neighborhood",
    "title": "Consider some space with . The neighborhood of a point is the set of other points having a sufficiently small distance to . For example, the -neighborhood of is defined as \\{ ' : d(, ') \\}. If is an undirected , which is a special case of a space, the neighborhood of a node is the set of its {neighbor}. \\\\ See also: , .",
    "color": "violet"
  },
  {
    "id": 100,
    "label": "bias",
    "title": "Consider an method using a parameterized . It learns the {modelparam} using the =\\{ {^{()}}{^{()}} \\}_{=1}^{}. To analyze the properties of the method, we typically interpret the {datapoint} as {realization} of {rv}, ^{()} = ^{({})}( ^{()} ) + {}^{()}, =1, \\,, \\,. We can then interpret the method as an estimator computed from (e.g., by solving ). The (squared) bias incurred by the estimate is then defined as . \\\\ See also: , , , .",
    "color": "lightgreen"
  },
  {
    "id": 101,
    "label": "classification",
    "title": "Classification is the task of determining a discrete-valued for a given , based solely on its {feature} . The belongs to a finite set, such as or , and represents the category to which the corresponding belongs. \\\\ See also: , , .",
    "color": "orange"
  },
  {
    "id": 102,
    "label": "privacy funnel",
    "title": "The privacy funnel is a method for learning a {featuremap} that provides privacy-friendly {feature} of a {datapoint} . \\\\ See also: , , , .",
    "color": "khaki"
  },
  {
    "id": 103,
    "label": "condition number",
    "title": "The condition number of a positive definite is the ratio between the largest and the smallest of . The condition number is useful for the analysis of methods. The computational complexity of {gdmethod} for crucially depends on the condition number of the , with the of the . Thus, from a computational perspective, we prefer {feature} of {datapoint} such that has a condition number close to . \\\\ See also: , , , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 104,
    "label": "classifier",
    "title": "A classifier is a (i.e., a ) used to predict a taking on values from a finite . We might use the value itself as a for the . However, it is customary to use a that delivers a numeric quantity. The is then obtained by a simple thresholding step. For example, in a binary problem with a , we might use a real-valued as a classifier. A can then be obtained via thresholding, {equation} {equ_def_threshold_bin_classifier_dict} =1 { for } ()\\!\\!0 { and } =-1 { otherwise.} {equation} We can characterize a classifier by its {decisionregion} , for every possible value . \\\\ See also: , , .",
    "color": "orange"
  },
  {
    "id": 105,
    "label": "empirical risk",
    "title": "The empirical of a on a is the average incurred by when applied to the {datapoint} in . \\\\ See also: , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 106,
    "label": "node degree",
    "title": "The degree of a node in an undirected is the number of its {neighbor}, i.e., . \\\\ See also: , .",
    "color": "lightblue"
  },
  {
    "id": 107,
    "label": "token",
    "title": "A is a basic unit of information obtained by splitting a sequence of symbols, such as a text string, into smaller parts. In , tokens often correspond to words, subwords, or characters that form the {feature} of a . Tokenization transforms raw text (e.g., ``The cat sleeps'') into a sequence of tokens (e.g., [``The'', ``cat'', ``sleeps'']), which can then be mapped to numerical {featurevec}.",
    "color": "lightgreen"
  },
  {
    "id": 108,
    "label": "natural language processing (NLP)",
    "title": "NLP studies methods for the the analysis and generation of human language. Typical NLP tasks include text , machine translation, sentiment analysis, and question answering. Modern NLP systems represent language as sequences of {token} and train {model} that capture contextual dependencies, such as {attention}-based methods.\\\\ See also: , .",
    "color": "lightgreen"
  },
  {
    "id": 109,
    "label": "directed cycle",
    "title": "A directed cycle in a is a sequence of distinct nodes such that . In a directed cycle, following the direction of each edge eventually leads back to the starting node, creating a closed loop. {figure}[H] {tikzpicture}[>=Latex, node distance=1.4cm, thick] (a1) at (90:1.5); (a2) at (210:1.5); (a3) at (330:1.5); (a1) circle (2pt) node[above=3pt] {}; (a2) circle (2pt) node[below left=3pt] {}; (a3) circle (2pt) node[below right=3pt] {}; (a1) -- (a2); (a2) -- (a3); (a3) -- (a1); {}; {}; {}; {tikzpicture} {A directed cycle consisting of three nodes connected in a closed loop.} {figure} The presence of a directed cycle prevents a from being a . \\\\ See also: , .",
    "color": "violet"
  },
  {
    "id": 110,
    "label": "directed acyclic graph (DAG)",
    "title": "A DAG is a which contains no directed cycles. Formally, a DAG satisfies that for any sequence of distinct nodes , the presence of directed edges implies that . {figure}[H] {tikzpicture}[>=Latex, node distance=1.4cm, thick, every node/.style={circle, fill=black, inner sep=1.5pt}] (a1) {}; (a2) {}; (a3) {}; (a1) -- (a2); (a2) -- (a3); (b1) {}; (b2) {}; (b3) {}; (b1) -- (b2); (b2) -- (b3); (b3) to (b1); {tikzpicture} {Left: A DAG defined on three nodes . Right: Another on the same nodes that is not a DAG since it contains a directed cycle.} {figure} The absence of directed cycles allows for a topological ordering of nodes such that all edges point from earlier to later nodes in this order. Several {model}, such as {ann} or {decisiontree}, are naturally represented as DAGs. \\\\ See also: , , .",
    "color": "violet"
  },
  {
    "id": 111,
    "label": "directed graph",
    "title": "A directed contains edges that possess an orientiation (or direction). Mathematically, a directed consists of nodes and a set of directed edges. {figure}[H] {tikzpicture}[>=stealth, node distance=1.8cm] (i) {}; (ip) [right=of i] {}; (i) -- (ip); {tikzpicture} {The edges of a directed have an orientation (or direction). We can indicate the orientaton by an arrow head.} {figure} We can represent a directed edge from node to node by an ordered pair . Directed {graph} are widely used to model interconnected systems or networks, such as transportation systems, electronic circuits, and biological processes . \\\\ See also: .",
    "color": "violet"
  },
  {
    "id": 112,
    "label": "undirected graph",
    "title": "See . \\\\ See also: .",
    "color": "lightblue"
  },
  {
    "id": 113,
    "label": "simple function",
    "title": "A simple is a which takes on only finite many values. In other words, f() = _{=1}^{} _{} {I}^{(^{()})}(), with the indicator of a subset and arbitrary coefficents . The subsets in the above decomposition must be and form a partition of . \\\\ See also: ,.",
    "color": "orange"
  },
  {
    "id": 114,
    "label": "Lebesgue integral",
    "title": "The Lebesgue integral assigns each integrable a number which is referred to as the integral of . The integral of can be interpreted as the volume that is enclosed by the in the space . We can compute it be increasingly accurate approximations by {simplefunction} {RudinBook}. {figure}[H] {tikzpicture}[scale=1.5] (-0.2,0) -- (3.4,0) node[right] {}; plot(,{0.5+0.3*+0.1*}) node[right] {}; (0,0) -- plot[domain=0:3] (,{0.5+0.3*+0.1*}) -- (3,0) -- cycle; (0,0) rectangle (1,0.5); at (0.5,-0.2) {}; (1,0) rectangle (2,0.9); (2,0) rectangle (3,1.5); (0,0.5)--(1,0.5); (1,0.9)--(2,0.9); (2,1.5)--(3,1.5); at (0.8,0.5) {}; in {0,1,2,3} (,0) -- (,2.5); at (0,-0.05) {}; at (1,-0.05) {}; at (2,-0.05) {}; at (3,-0.05) {}; at (1.6,3.0) {}; [anchor=west] at (3.2,1.5) { }; {tikzpicture} {figure} It is useful to think of the Lebesgue integral as a that maps an integrable to the value of its integral, f _{} f() d. The precise defintion of this , including its which are the integrable {function}, is a corner-stone of measure theory {RudinBook}. \\\\ See also: .",
    "color": "orange"
  },
  {
    "id": 115,
    "label": "cumulative distribution function (cdf)",
    "title": "The cdf of a real-valued is {}{} { }. \\\\ See also: , , .",
    "color": "salmon"
  },
  {
    "id": 116,
    "label": "weighted graph",
    "title": "A whose edges are assigned numeric weights. Typically, these edge weights are non-negative real numbers. For example, if a graph represents a road network with nodes being intersections and edges representing road segments, the edge weight could represent the capacity (measured in maximum vehicles per hour) of the road segment . \\\\ See also: .",
    "color": "khaki"
  },
  {
    "id": 117,
    "label": "graph",
    "title": "A graph is a pair that consists of a node set and an edge set . In its most general form, a graph is specified by a that assigns each edge a pair of nodes . Unless specified otherwise, the term graph refers to an undirected graph. A simple undirected graph is obtained by identifying each edge with a set that contains two different nodes . Weighted graphs also specify numeric for each edge . \\\\ See also: , .",
    "color": "lightblue"
  },
  {
    "id": 118,
    "label": "uncertainty",
    "title": "In the context of , uncertainty refers to the presence of multiple plausible outcomes or {explanation} based on available . For example, the produced by a trained often reflects a range of possible values for the true of a given . The broader this range, the greater the associated uncertainty. theory allows us to represent, quantify, and reason about uncertainty in a mathematically rigorous manner. \\\\ See also: , , , .",
    "color": "salmon"
  },
  {
    "id": 119,
    "label": "upper confidence bound (UCB)",
    "title": "Consider an application that requires selecting, at each time step , an action from a finite set of alternatives . The utility of selecting action is quantified by a numeric signal . A widely used for this type of sequential decision-making problem is the setting . In this , the is viewed as the of a with unknown . Ideally, we would always choose the action with the largest expected , but these {mean} are unknown and must be estimated from observed . Simply choosing the action with the largest estimate can lead to suboptimal outcomes due to estimation . The UCB strategy addresses this by selecting actions not only based on their estimated {mean} but also by incorporating a term that reflects the in these estimates\u2014favoring actions with a high-potential and high . Theoretical guarantees for the performance of UCB strategies, including logarithmic bounds, are established in . \\\\ See also: , , , , .",
    "color": "salmon"
  },
  {
    "id": 120,
    "label": "multiarmed bandit (MAB)",
    "title": "An MAB problem is a precise formulation of a sequential decision-making task under . At each discrete time step , a learner selects one of several possible actions\u2014called arms\u2014from a finite set . Pulling arm at time yields a that is drawn from an unknown . We obtain different classes of MAB problems by placing different restrictions on this . In the simplest setting, the does not depend on . Given an MAB problem, the goal is to construct methods that maximize the cumulative over time by strategically balancing exploration (i.e., gathering information about uncertain arms) and exploitation (i.e., selecting arms known to perform well). MAB problems form an important special case of problems , . \\\\ See also: , .",
    "color": "salmon"
  },
  {
    "id": 121,
    "label": "optimism in the face of uncertainty",
    "title": "methods learn {modelparam} according to some performance criterion . However, they usually cannot access directly but rely on an estimate (or approximation) of . As a case in point, -based methods use the average on a given (i.e., the ) as an estimate for the of a . Using a , one can construct a confidence interval for each choice for the {modelparam}. One simple construction is , , with being a measure of the (expected) deviation of from . We can also use other constructions for this interval as long as they ensure that with a sufficiently high . An optimist chooses the {modelparam} according to the most favorable\u2014yet still plausible\u2014value of the performance criterion (see Fig. {fig_optimism_dict}). Two examples of methods that use such an optimistic construction of an are {ShalevMLBook} and methods for sequential decision making {Bubeck2012}. {figure}[H] {center} {tikzpicture}[x=3cm, y=1cm] (-1, 5) -- plot[domain=-2:1, samples=100] ({+1}, { + 1}) -- plot[domain=1:-2, samples=100] ({+1}, { - 0.5}) -- cycle; at (2, 4) {}; plot ({+1}, { -0.5}) node[right] {}; plot ({}, {}); (1, -0.5) -- (1, 1) node[midway, right] {}; {tikzpicture} { methods learn {modelparam} by using some estimate of for the ultimate performance criterion . Using a , one can use to construct confidence intervals , which contain with a high probability. The best plausible performance measure for a specific choice of {modelparam} is . {fig_optimism_dict}} {center} {figure} See also: , , , .",
    "color": "salmon"
  },
  {
    "id": 122,
    "label": "federated learning network (FL network)",
    "title": "An network consists of an undirected weighted . The nodes of represent {device} that can access a and train a . The edges of represent communication links between {device} as well as statistical similarities between their {localdataset}. A principled approach to train the {localmodel} is . The solutions of are local {modelparam} that optimally balance the incurred on {localdataset} with their across the edges of . \\\\ See also: , , , .",
    "color": "lightblue"
  },
  {
    "id": 123,
    "label": "norm",
    "title": "A norm is a that maps each () element of a to a nonnegative real number. This must be homogeneous and definite, and it must satisfy the triangle inequality . \\\\ See also: , , .",
    "color": "lightcoral"
  },
  {
    "id": 124,
    "label": "dual norm",
    "title": "Every defined on a has an associated dual , which is denoted by and defined as . The dual measures the largest possible inner product between and any in the unit ball of the original . For further details, see {BoydConvexBook}. \\\\ See also: , , .",
    "color": "lightcoral"
  },
  {
    "id": 125,
    "label": "geometric median (GM)",
    "title": "The GM of a set of input {vector} in is a point that minimizes the sum of distances to the {vector} such that {equation} {equ_geometric_median_dict} _{ {R}^{}} _{=1}^{} { - ^{()}}{2}. {equation} Fig. {opt_cond_GM_dict} illustrates a fundamental property of the GM: If does not coincide with any of the input {vector}, then the unit {vector} pointing from to each must sum to zero\u2014this is the zero- (optimality) condition for {equ_geometric_median_dict}. It turns out that the solution to {equ_geometric_median_dict} cannot be arbitrarily pulled away from trustworthy input {vector} as long as they are the majority {Lopuhaae1991}. {figure}[H] {center} {tikzpicture}[scale=2, thick, >=stealth] (w) at (3,0); (w) circle (1.2pt) node[below right] {}; (w2) at (0.5,0.3); (w3) at (0.7,0.7); (w2) circle (1pt) node[above left] {}; (w3) circle (1pt) node[above left] {}; at () {}; (w) -- (w2); (w) -- (w3); (w) -- () ; (w) -- () node[pos=0.9, right,yshift=7pt] {}; (w4) at (5,0.2); at (5,0.6) {}; (w4) circle (1pt) node[below left] {}; (w) -- () ; {tikzpicture} {{opt_cond_GM_dict} Consider a solution of {equ_geometric_median_dict} that does not coincide with any of the input {vector}. The optimality condition for {equ_geometric_median_dict} requires that the unit {vector} from to the input {vector} sum to zero.} {center} {figure} See also: , .",
    "color": "lightcoral"
  },
  {
    "id": 126,
    "label": "explanation",
    "title": "One approach to enhance the of an method for its human user is to provide an explanation alongside the {prediction} delivered by the method. Explanations can take different forms. For instance, they may consist of human-readable text or quantitative indicators, such as importance scores for the individual {feature} of a given ~. Alternatively, explanations can be visual\u2014for example, intensity {map} that highlight image regions that drive the . Fig.\\ {fig_explanation_dict} illustrates two types of explanations. The first is a local linear approximation of a nonlinear trained around a specific , as used in the method . The second form of explanation depicted in the figure is a sparse set of {prediction} at selected {featurevec}, offering concrete reference points for the user. {figure}[H] {center} {tikzpicture}[x=0.5cm] {axis}[ hide axis, xmin=-3, xmax=6, ymin=0, ymax=6, domain=0:6, samples=100, width=10cm, height=6cm, clip=false ] {2 + sin(deg(x))} node[pos=0.9, above right, yshift=10pt] {}; {2 + sin(deg(1.5)) + cos(deg(1.5))*(x - 1.5)} node[pos=0.2, above] {}; coordinates {(1.5, {2 + sin(deg(1.5))})}; coordinates {(1.5,0) (1.5,2.4)}; at (axis cs:1.5, -0.2) {}; coordinates {(-1, {2 + sin(deg(-1))})}; coordinates {(-1,0) (-1,{2 + sin(deg(-1))})}; at (axis cs:-1, -0.2) {}; coordinates {(0, {2 + sin(deg(0))})}; coordinates {(0,0) (0,{2 + sin(deg(0))})}; at (axis cs:0, -0.2) {}; coordinates {(5, {2 + sin(deg(5))})}; coordinates {(5,0) (5,{2 + sin(deg(5))})}; at (axis cs:5, -0.2) {}; {axis} {tikzpicture} {center} {A trained can be explained locally at some point by a linear approximation . For a , this approximation is determined by the . Another form of explanation could be the values for . {fig_explanation_dict}} {figure} See also: , , , , .",
    "color": "orange"
  },
  {
    "id": 127,
    "label": "risk",
    "title": "Consider a used to predict the of a based on its {feature} . We measure the quality of a particular using a . If we interpret {datapoint} as the {realization} of {rv}, the also becomes the of a . The allows us to define the risk of a as the expected . Note that the risk of depends on both the specific choice for the and the of the {datapoint}. \\\\ See also: , , , .",
    "color": "salmon"
  },
  {
    "id": 128,
    "label": "activation function",
    "title": "Each artificial neuron within an is assigned an that maps a weighted combination of the neuron inputs to a single output value . Note that each neuron is parameterized by the . \\\\ See also: , , .",
    "color": "khaki"
  },
  {
    "id": 129,
    "label": "distributed algorithm",
    "title": "A distributed is an designed for a special type of computer, i.e., a collection of interconnected computing devices (or nodes). These devices communicate and coordinate their local computations by exchanging messages over a network , . Unlike a classical , which is implemented on a single , a distributed is executed concurrently on multiple {device} with computational capabilities. Similar to a classical , a distributed can be modeled as a set of potential executions. However, each execution in the distributed setting involves both local computations and message-passing {event}. A generic execution might look as follows: \\[ {array}{l} {Node 1: } { input}_1, \\,s_1^{(1)}, \\,s_2^{(1)}, \\,, \\,s_{T_1}^{(1)}, \\,{ output}_1; \\\\ {Node 2: } { input}_2, \\,s_1^{(2)}, \\,s_2^{(2)}, \\,, \\,s_{T_2}^{(2)}, \\,{ output}_2; \\\\ \\\\ {Node N: } { input}_N, \\,s_1^{(N)}, \\,s_2^{(N)}, \\,, \\,s_{T_N}^{(N)}, \\,{ output}_N. {array} \\] Each starts from its own local input and performs a sequence of intermediate computations at discrete-time instants . These computations may depend on both the previous local computations at the and the messages received from other {device}. One important application of distributed {algorithm} is in where a network of {device} collaboratively trains a personal for each . \\\\ See also: , , , , .",
    "color": "lightblue"
  },
  {
    "id": 130,
    "label": "algorithm",
    "title": "An algorithm is a precise, step-by-step specification for producing an output from a given input within a finite number of well-defined computational steps . For example, an for is an algorithm that explicitly describes how to map a given into {modelparam} through a sequence of {gradstep}. The precise form of an algorithm depends on the available computational infrastructure. For example, if this infrastructure allows to compute a , then we can define a algorithm using the . In contrast, if the computational infrastructe does only allow for basic arithmetic (mulilication and addition), the need to be somehow translated into a sequence of arithmetic operations (e.g., as in {gdmethod}). To study algorithms rigorously, we can represent (or approximate) them by different mathematical structures . One approach is to represent an algorithm as a collection of possible executions. Each individual execution is then a sequence of the form { input}, \\,s_1, \\,s_2, \\,, \\,s_T, \\,{ output}. This sequence starts from an input and progresses via intermediate steps until an output is delivered. Crucially, an algorithm encompasses more than just a mapping from input to output; it also includes intermediate computational steps . \\\\ See also: , , {modelparam}, , , .",
    "color": "violet"
  },
  {
    "id": 131,
    "label": "stochastic algorithm",
    "title": "A uses a random mechanism during its execution. For example, uses a randomly selected subset of {datapoint} to compute an approximation for the of an . We can represent a by a {stochproc}, i.e., the possible execution sequence is the possible outcomes of a , , . \\\\ See also: , , , , , , , , , .",
    "color": "salmon"
  },
  {
    "id": 132,
    "label": "batch learning",
    "title": "In learning (also known as offline learning), the is trained on the entire in a single training iteration, instead of updating it incrementally as arrive. All available are inputted into a learning , resulting in a that can make {prediction}. Since these {dataset} tend to be large, training is computationally expensive and time-consuming, so it is typically performed offline. After learning, the will be static and will not adapt to new automatically. Updating the with new information requires retraining the entirely. Once the has been trained, it is launched into production where it cannot be updated. Training a can take many hours, so many {model} in production settings are updated cyclically on a periodic schedule when the distribution is stable. For example, a retail analytics team could retrain their demand forecast every Sunday using the previous week's sales to predict next week's demand. If a system needs to be constantly updated to rapidly changing , such as in stock price , a more adaptable solution such as is necessary. \\\\ See also: , , , .",
    "color": "violet"
  },
  {
    "id": 133,
    "label": "online learning",
    "title": "Some methods are designed to process in a sequential manner, updating their {modelparam} one at a time, as new {datapoint} become available. A typical example is time-series , such as daily and temperatures recorded by an weather station. These values form a chronological sequence of observations. During each time step , online learning methods update (or refine) the current (or {modelparam} ) based on the newly observed . \\\\ See also: , .",
    "color": "violet"
  },
  {
    "id": 134,
    "label": "online algorithm",
    "title": "An online processes input incrementally, receiving {datapoint} sequentially and making decisions or producing outputs (or decisions) immediately without having access to the entire input in advance , . Unlike an offline , which has the entire input available from the start, an online must handle about future inputs and cannot revise past decisions. Similar to an offline , we represent an online formally as a collection of possible executions. However, the execution sequence for an online has a distinct structure as follows: { in}_{1}, \\,s_1, \\,{ out}_{1}, \\,{ in}_{2}, \\,s_2, \\,{ out}_{2}, \\,, \\,{ in}_{T}, \\,s_T, \\,{ out}_{T}. Each execution begins from an initial state (i.e., \\({in}_{1}\\)) and proceeds through alternating computational steps, outputs (or decisions), and inputs. Specifically, at step \\(\\), the performs a computational step \\(s_{}\\), generates an output \\({out}_{}\\), and then subsequently receives the next input () \\({in}_{+1}\\). A notable example of an online in is , which incrementally updates {modelparam} as new {datapoint} arrive. \\\\ See also: , , , , , , {modelparam}, .",
    "color": "violet"
  },
  {
    "id": 135,
    "label": "transparency",
    "title": "Transparency is a fundamental requirement for . In the context of methods, transparency is often used interchangeably with , . However, in the broader scope of systems, transparency extends beyond and includes providing information about the system\u2019s limitations, reliability, and intended use. In medical diagnosis systems, transparency requires disclosing the confidence level for the {prediction} delivered by a trained . In credit scoring, -based lending decisions should be accompanied by explanations of contributing factors, such as income level or credit history. These explanations allow humans (e.g., a loan applicant) to understand and contest automated decisions. Some methods inherently offer transparency. For example, provides a quantitative measure of reliability through the value . {decisiontree} are another example, as they allow human-readable decision rules . Transparency also requires a clear indication when a user is engaging with an system. For example, -powered chatbots should notify users that they are interacting with an automated system rather than a human. Furthermore, transparency encompasses comprehensive documentation detailing the purpose and design choices underlying the system. For instance, datasheets and system cards help practitioners understand the intended use cases and limitations of an system . \\\\ See also: , .",
    "color": "orange"
  },
  {
    "id": 136,
    "label": "sensitive attribute",
    "title": "revolves around learning a that allows us to predict the of a from its {feature}. In some applications, we must ensure that the output delivered by an system does not allow us to infer sensitive attributes of a . Which part of a is considered a sensitive attribute is a design choice that varies across different application domains. \\\\ See also: , , , , , .",
    "color": "khaki"
  },
  {
    "id": 137,
    "label": "stochastic block model (SBM)",
    "title": "The SBM is a probabilistic generative for an undirected with a given set of nodes . In its most basic variant, the SBM generates a by first randomly assigning each node to a index . A pair of different nodes in the is connected by an edge with that depends solely on the {label} . The presence of edges between different pairs of nodes is statistically independent. \\\\ See also: , , , , .",
    "color": "lightblue"
  },
  {
    "id": 138,
    "label": "deep net",
    "title": "A deep net is an with a (relatively) large number of hidden {layer}. Deep learning is an umbrella term for methods that use a deep net as their . \\\\ See also: , , , .",
    "color": "khaki"
  },
  {
    "id": 139,
    "label": "baseline",
    "title": "Consider some method that produces a learned (or trained ) . We evaluate the quality of a trained by computing the average on a . But how can we assess whether the resulting performance is sufficiently good? How can we determine if the trained performs close to optimal such that there is little point in investing more resources (for collection or computation) to improve it? To this end, it is useful to have a reference (or baseline) level against which we can compare the performance of the trained . \\\\ Such a reference value might be obtained from human performance, e.g., the misclassification rate of dermatologists who diagnose cancer from visual inspection of skin . Another source for a baseline is an existing, but for some reason unsuitable, method. For example, the existing method might be computationally too expensive for the intended application. Nevertheless, its error can still serve as a baseline. Another, somewhat more principled, approach to constructing a baseline is via a . In many cases, given a , we can precisely determine the achievable among any hypotheses (not even required to belong to the ) . \\\\ This achievable (referred to as the ) is the of the for the of a , given its {feature} . Note that, for a given choice of , the (if it exists) is completely determined by the {LC}. However, computing the and presents two main challenges. First, the is unknown and must be estimated from observed . Second, even if were known, computing the exactly may be computationally infeasible . A widely used is the for {datapoint} characterized by numeric {feature} and {label}. Here, for the , the is given by the posterior of the , given the {feature} , . The corresponding is given by the posterior (see Fig. {fig_post_baseline_dict}). {figure}[H] {center} {tikzpicture} (-1,0) -- (7,0) node[right] {}; plot ({}, {2*exp(-0.5*((-3)^2))}); (3,0) -- (3,2.5); at ([yshift=-5pt] 3,2.5) { }; (3-1,1) -- (3+1,1.0); at ([yshift=2pt] 3,1.2) { }; in {0.5} { at (, 0) { }; } at (0.5,-0.2) { }; {tikzpicture} {center} {If the {feature} and the of a are drawn from a , we can achieve the (under ) by using the to predict the of a with {feature} . The corresponding is given by the posterior . We can use this quantity as a baseline for the average of a trained . {fig_post_baseline_dict}} {figure} See also: , .",
    "color": "salmon"
  },
  {
    "id": 140,
    "label": "$k$-fold cross-validation ($k$-fold CV)",
    "title": "A method{-fold cross-} for evaluating the of an -based method. The idea is to divide evenly into subsets (or folds) {figure}[htbp] {tikzpicture}[font=] {5} {1.6} {0.40} {0.18} in {1,...,} { {}{-(-1)*(+)} at (-0.25,+0.5*) {fold }; in {1,...,} { {}{(-1)*} = (,) rectangle ++(,); (,) rectangle ++(,); (,) rectangle ++(,); } } in {1,...,} { {}{(-1)* + 0.5*} at (,+0.2) {}; } {tikzpicture} {In -fold cross-validation, the available is evenly divided into folds . Each fold is used once as a , while the remaining folds form the . {fig_kfoldcv_dict}} {figure} For each fold , train the on the union of all folds except and validate it on . The overall performance is obtained by averaging the results across all folds. \\\\ See also: , .",
    "color": "lightgreen"
  },
  {
    "id": 141,
    "label": "spectrogram",
    "title": "A spectrogram represents the time-frequency distribution of the energy of a time signal . Intuitively, it quantifies the amount of signal energy present within a specific time segment and frequency interval . Formally, the spectrogram of a signal is defined as the squared magnitude of its short-time Fourier transform (STFT) . Fig. {fig:spectrogram_dict} depicts a time signal along with its spectrogram. {figure}[H] {assets/spectrogram.png} {minipage}{} {3ex} { (a) {10em} (b)} {minipage} {(a) A time signal consisting of two modulated Gaussian pulses. (b) An intensity plot of the spectrogram. {fig:spectrogram_dict}} {figure} The intensity plot of its spectrogram can serve as an image of a signal. A simple recipe for audio signal is to feed this signal image into {deepnet} originally developed for image and object detection . It is worth noting that, beyond the spectrogram, several alternative representations exist for the time-frequency distribution of signal energy , . \\\\ See also: , .",
    "color": "khaki"
  },
  {
    "id": 142,
    "label": "graph clustering",
    "title": "aims to cluster {datapoint} that are represented as the nodes of a . The edges of represent pairwise similarities between {datapoint}. We can sometimes quantify the extent of these similarities by an , . \\\\ See also: , , , .",
    "color": "lightblue"
  },
  {
    "id": 143,
    "label": "spectral clustering",
    "title": "Spectral is a particular instance of , i.e., it clusters {datapoint} represented as the nodes of a . Spectral uses the {eigenvector} of the to construct {featurevec} for each node (i.e., for each ) . We can feed these {featurevec} into -based methods, such as or via . Roughly speaking, the {featurevec} of nodes belonging to a well-connected subset (or ) of nodes in are located nearby in the (see Fig. {fig_lap_mtx_specclustering_dict}). {figure}[H] {center} {minipage}{0.4} {tikzpicture} {scope}[every node/.style={circle, fill=black, inner sep=0pt, minimum size=0.3cm}] (1) at (0,0) {}; (2) [below left=of 1, xshift=-0.2cm, yshift=-1cm] {}; (3) [below right=of 1, xshift=0.2cm, yshift=-1cm] {}; (4) [below=of 1, yshift=0.5cm] {}; {scope} (1) -- (2); (1) -- (3); at (1) {}; at (2) {}; at (3) {}; at (4) {}; at (0,-4) {(a)}; {tikzpicture} {minipage} {5mm} {minipage}{0.4} {equation} {}\\!=\\! {pmatrix} 2 & -1 & -1 & 0 \\\\ -1 & 1 & 0 & 0 \\\\ -1 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 0 {pmatrix}\\!=\\!{V} { } {V}\\,^{T} {equation} {minipage}{} {3ex} { (b)} {minipage} {minipage} {20mm}\\\\ {minipage}{0.4} {tikzpicture}[scale=3] (-0.2, 0) -- (1.2, 0) node[right] {}; (0, -0.2) -- (0, 1.2) node[above] {}; (0.577, 0) circle (0.03cm) node[above right] {}; (0.577, 0) circle (0.03cm); (0.577, 0) circle (0.03cm); (0, 1) circle (0.03cm) node[above right] {}; at (0.5,-0.5) {(c)}; {tikzpicture} {minipage} {minipage}{0.4} {align} & {V} = ( ^{(1)},^{(2)},^{(3)},^{(4)} ) \\\\ & {v}^{(1)}\\!=\\!{1}{{3}} {pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 {pmatrix}, \\, {v}^{(2)}\\!=\\!{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 {pmatrix} {align} {minipage}{} {3ex} { (d)} {minipage} {minipage} {{fig_lap_mtx_specclustering_dict} (a) An undirected with four nodes , each representing a . (b) The and its . (c) A of {datapoint} using the {featurevec} . (d) Two {eigenvector} corresponding to the of the . } {center} {figure} See also: , , , .",
    "color": "lightblue"
  },
  {
    "id": 144,
    "label": "flow-based clustering",
    "title": "Flow-based groups the nodes of an undirected by applying to nodewise {featurevec}. These {featurevec} are built from network flows between carefully selected sources and destination nodes . \\\\ See also: , , , .",
    "color": "lightblue"
  },
  {
    "id": 145,
    "label": "estimation error",
    "title": "Consider {datapoint}, each with and . In some applications, we can model the relation between the and the of a as . Here, we use some true underlying and a noise term , which summarizes any modeling or labeling errors. The estimation error incurred by an method that learns a , e.g., using , is defined as , for some . For a parametric , which consists of {map} determined by {modelparam} , we can define the estimation error as , . \\\\ See also: , , , , , , , , {modelparam}.",
    "color": "lightgreen"
  },
  {
    "id": 146,
    "label": "degree of belonging",
    "title": "Degree of belonging is a number that indicates the extent to which a belongs to a {MLBasics}. The degree of belonging can be interpreted as a soft assignment. methods can encode the degree of belonging with a real number in the interval . is obtained as the extreme case when the degree of belonging only takes on values or . \\\\ See also: , , , .",
    "color": "lightblue"
  },
  {
    "id": 147,
    "label": "mean squared estimation error (MSEE)",
    "title": "Consider an method that learns {modelparam} based on some . If we interpret the {datapoint} in as {realization} of a , we define the . Here, denotes the true {modelparam} of the of . The MSEE is defined as the of the squared Euclidean of the , . \\\\ See also: , , , .",
    "color": "salmon"
  },
  {
    "id": 148,
    "label": "generalized total variation minimization (GTVMin)",
    "title": "GTVMin is an instance of using the of local {modelparam} as a . \\\\ See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 149,
    "label": "regression",
    "title": "Regression problems revolve around the of a numeric solely from the {feature} of a {MLBasics}. \\\\ See also: , , , .",
    "color": "orange"
  },
  {
    "id": 150,
    "label": "accuracy",
    "title": "Consider {datapoint} characterized by {feature} and a categorical that takes on values from a finite . The accuracy of a , when applied to the {datapoint} in a , is then defined as using the . \\\\ See also: , , .",
    "color": "lightgreen"
  },
  {
    "id": 151,
    "label": "expert",
    "title": "aims to learn a that accurately predicts the of a based on its {feature}. We measure the error using some . Ideally, we want to find a that incurs minimal on any . We can make this informal goal precise via the and by using the as the for the (average) of a . An alternative approach to obtaining a is to use the learned by an existing method. We refer to this as an expert . minimization methods learn a that incurs a comparable to the best expert , . \\\\ See also: , , .",
    "color": "salmon"
  },
  {
    "id": 152,
    "label": "networked federated learning (NFL)",
    "title": "NFL refers to methods that learn personalized {model} in a distributed fashion. These methods learn from {localdataset} that are related by an intrinsic network structure. \\\\ See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 153,
    "label": "regret",
    "title": "The regret of a relative to another , which serves as a , is the difference between the incurred by and the incurred by . The is also referred to as an . \\\\ See also: , , .",
    "color": "salmon"
  },
  {
    "id": 154,
    "label": "strongly convex",
    "title": "A continuously real-valued is strongly with coefficient if ,{CvxAlgBertsekas}. \\\\ See also: , , .",
    "color": "orange"
  },
  {
    "id": 155,
    "label": "differentiable",
    "title": "A real-valued is differentiable if it can be approximated locally at any point by a linear . The local linear approximation at the point is determined by the . \\\\ See also: , .",
    "color": "violet"
  },
  {
    "id": 156,
    "label": "gradient",
    "title": "For a real-valued , if a exists such that , it is referred to as the gradient of at . If it exists, the gradient is unique and denoted by or . \\\\ See also: , .",
    "color": "violet"
  },
  {
    "id": 157,
    "label": "subgradient",
    "title": "For a real-valued , a such that is referred to as a subgradient of at , . \\\\ See also: , .",
    "color": "lightcoral"
  },
  {
    "id": 158,
    "label": "federated proximal (FedProx)",
    "title": "FedProx refers to an iterative that alternates between separately training {localmodel} and combining the updated local {modelparam}. In contrast to , which uses to train {localmodel}, FedProx uses a for the training . \\\\ See also: , , , {modelparam}, , , .",
    "color": "lightblue"
  },
  {
    "id": 159,
    "label": "rectified linear unit (ReLU)",
    "title": "The ReLU is a popular choice for the of a neuron within an . It is defined as , with being the weighted input of the artificial neuron. \\\\ See also: , .",
    "color": "khaki"
  },
  {
    "id": 160,
    "label": "hypothesis",
    "title": "A hypothesis refers to a (or ) from the to the . Given a with {feature} , we use a hypothesis to estimate (or approximate) the using the . {figure}[htbp] {tikzpicture}[ >=Latex, node distance=2.0cm, box/.style={draw, rounded corners=2pt, inner sep=6pt}, label/.style={font=}, thinline/.style={line width=0.6pt} ] (audio) {}; at (audio.north) [yshift=0mm] {audio samples }; {scope} () .. controls +(.3,.35) and +(-.3,.35) .. ++(0.8,0) .. controls +(.3,-.35) and +(-.3,-.35) .. ++(0.8,0) .. controls +(.3,.25) and +(-.3,.25) .. ++(0.8,0) .. controls +(.3,-.25) and +(-.3,-.25) .. ++(0.8,0); {scope} (model) {}; (audio) -- (model) ; (rating) {}; at () {}; (barL) at (); (barR) at (); {0.82} (ptr) at (); {}; (model) -- (rating); {tikzpicture} {{fig:hypothesis_dict} A hypothesis maps the {feature} of a to a of the . For example, the application {https://freddiemeter.withyoutube.com/} uses the samples of an audio recording as {feature} predict how closely a person\u2019s singing resembles that of Freddie Mercury. } {figure} is all about learning (or finding) a hypothesis such that for any (with {feature} and ). Practical methods, limited by finite computational resources, must restrict learning to a subset of all possible hypothesis maps. This subset is called the or simply the underlying the method. \\\\ See also: , , , .",
    "color": "lightgreen"
  },
  {
    "id": 161,
    "label": "effective dimension",
    "title": "The effective dimension of an infinite is a measure of its size. Loosely speaking, the effective dimension is equal to the effective number of independent tunable {modelparam}. These {parameter} might be the coefficients used in a or the and terms of an . \\\\ See also: , {modelparam}, .",
    "color": "lightgreen"
  },
  {
    "id": 162,
    "label": "label space",
    "title": "In a application, each is described by a set of {feature} together with an associated . The set of all admissible values is called the , denoted by . Importantly, may include values that no observed has as its value. To a large extent, the choice of is up to the engineer and depends on the problem formulation. Fig.~{fig_label_spaces_dict} shows some examples of {labelspace} that are commonly used in applications. {figure}[H] {tikzpicture}[>=Stealth, font=] {scope}[shift={(0,0)}] (-2,0) -- (2,0); at (0,-0.7) {(a) ()}; {scope} {scope}[shift={(7,0)}] (-1,-0.5) rectangle (1,0.5); (-2,0) -- (2,0); (0,-1) -- (0,1); at (0,-0.7) {(b) (multi-label )}; {scope} {scope}[shift={(0,-3)}] (-1,0) circle (1.2pt) node[below=2pt] {{``hot''}}; ( 1,0) circle (1.2pt) node[below=2pt] {{``cold''}}; at (0,-0.7) {(c) Binary }; {scope} {scope}[shift={(7,-3)}] (n1) at (-1.5,0) {}; (n2) at (-0.5,0) {}; (n3) at ( 0.5,0) {}; (n4) at ( 1.5,0) {}; (n1) -- (n2); (n2) -- (n3); (n3) -- (n4); {1}; {2}; {3}; {4}; at (0,-0.7) {(d) (ordinal )}; {scope} {tikzpicture} {{fig_label_spaces_dict} Examples of {labelspace} and corresponding flavours of .} {figure} The choice of determines the flavour of methods appropriate for the application at hand. methods use the while binary methods use a space that consists of two different elements, i.e., . Ordinal methods use a finite, ordered set of values, e.g., with the natural ordering . \\\\ See also: ,, , .",
    "color": "lightgreen"
  },
  {
    "id": 163,
    "label": "prediction",
    "title": "A prediction is an estimate or approximation for some quantity of interest. revolves around learning or finding a that reads in the {feature} of a and delivers a prediction for its . \\\\ See also: , , , , , .",
    "color": "orange"
  },
  {
    "id": 164,
    "label": "histogram",
    "title": "Consider a that consists of {datapoint} , each of them belonging to some cell with side length . We partition this cell evenly into smaller elementary cells with side length . The histogram of assigns each elementary cell to the corresponding fraction of {datapoint} in that fall into this elementary cell. A visual example of such a histogram is provided in Fig. {fig:histogram_dict}.\\\\ {figure}[H] {tikzpicture} {compat=1.18} {axis}[ ybar, ymin=0, ymax=6, bar width=22pt, width=10cm, height=6cm, xlabel={Value}, ylabel={Frequency}, ytick={1,2,3,4,5,6}, xtick={1,2,3,4,5}, xticklabels={{[0,1)}, {[1,2)}, {[2,3)}, {[3,4)}, {[4,5)}}, enlarge x limits=0.15, title={Histogram of Sample Data} ] +[fill=blue!40] coordinates {(1,2) (2,5) (3,4) (4,3) (5,1)}; {axis} {tikzpicture} {A histogram consists of the fractions of {datapoint} that fall within different value ranges (i.e., bins). Each bar height shows the count of {sample} in the corresponding interval.} {fig:histogram_dict} {figure} See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 165,
    "label": "bootstrap",
    "title": "For the analysis of methods, it is often useful to interpret a given set of {datapoint} as {realization} of {rv} drawn from a common . In practice, the is unknown and must be estimated from . The bootstrap approach uses the of as an estimator for , (1/) | : ^{(1)} {A} | {{A}}. By sampling with replacement from according to this empirical distribution, we generate new {dataset} , each containing {datapoint}. We then use each of those {dataset} for (e.g., via ), resulting in the learned {hypothesis} We can use these learned {hypothesis} to estimate the of the method . \\\\ See also: , , , .",
    "color": "salmon"
  },
  {
    "id": 166,
    "label": "feature space",
    "title": "The space of a given application or method is constituted by all potential values that the of a can take on. For {datapoint} described by a fixed number of numerical {feature}, a common choice for the space is the . However, the mere presence of numeric {feature} does not imply that is the most appropriate representation of the space. Indeed, the numerical {feature} might be assigned to {datapoint} in a largely arbitrary or random manner, resulting in {datapoint} that are randomly scattered throughout without any meaningful geometric structure. methods try to learn a transformation of the original (potentially non-numeric) {feature} to ensure a more meaningful arrangement of {datapoint} in . Three examples of spaces are shown in Fig. {fig_featurespace_dict}. {figure}[H] {tikzpicture}[scale=0.6] {scope}[xshift=0cm] (-0.5, 0) -- (3.5, 0) node[right] {}; / in {0.5/, 1.5/, 2.8/} (,0) circle (2pt) node[above] {}; at (1.5, -4.0) {}; at (1.5, -6) {(a)}; {scope} {scope}[xshift=8cm] (0,0) circle (1.8); (0,0) circle (1.8); (0.8, 0.9) circle (2pt) node[anchor=south west] {}; (-1.2, 0.5) circle (2pt) node[anchor=south east] {}; (0.3, -1.4) circle (2pt) node[anchor=north west] {}; at (0.5, -4) {}; at (0.5, -6) {(b)}; {scope} {scope}[xshift=14cm, yshift=0.3cm] (0,0) circle (2pt) node[anchor=north east] {}; (2,1.2) circle (2pt) node[anchor=south west] {}; (1,2.5) circle (2pt) node[anchor=south east] {}; (3,2.5) circle (2pt) node[anchor=south west] {}; (0,0) -- (2,1.2); (2,1.2) -- (1,2.5); (1,2.5) -- (3,2.5); at (1.5, -4.2) {}; at (1.5, -6.2) {(c)}; {scope} {tikzpicture} {Three different spaces. (a) A linear space . (b) A bounded set . (c) A discrete space whose elements are nodes of an undirected . {fig_featurespace_dict}} {figure} See also: , .",
    "color": "lightblue"
  },
  {
    "id": 167,
    "label": "missing data",
    "title": "Consider a constituted by {datapoint} collected via some physical . Due to imperfections and failures, some of the or values of {datapoint} might be corrupted or simply missing. imputation aims to estimate these missing values . We can interpret imputation as an problem where the of a is the value of the corrupted . \\\\ See also: , .",
    "color": "lightblue"
  },
  {
    "id": 168,
    "label": "positive semi-definite (psd)",
    "title": "A (real-valued) symmetric is referred to as psd if for every . The property of being psd can be extended from {matrix} to (real-valued) symmetric {map} (with ) as follows: For any finite set of {featurevec} , the resulting with entries is psd . \\\\ See also: , , , , .",
    "color": "lightblue"
  },
  {
    "id": 169,
    "label": "feature",
    "title": "A feature of a is one of its properties that can be measured or computed easily without the need for human supervision. For example, if a is a digital image (e.g., stored as a {.jpeg} file), then we could use the red\u2013green\u2013blue (RGB) intensities of its pixels as features. {figure} {tikzpicture}[scale=1] plot ({}, {sin( r)}); [count=] in {0,0.5,...,6.28} { (, {sin( r)}) circle (2pt); =1 at (, {sin( r)}) {}; =2 at (, {sin( r)}) {}; } {tikzpicture} {An audio signal (blue waveform) and its discretized signal samples (red dots) which can be used as its features . {fig:audio_features_dict}} {figure} Another example is shown in Fig.\\ {fig:audio_features_dict}, where the the signal samples of a finite-duration audio signal are used as its features. Domain-specific synonyms for the term feature are \"covariate,\" \"explanatory variable,\" \"independent variable,\" \"input (variable),\" \"predictor (variable),\" or \"regressor\" , , . \\\\ See also: .",
    "color": "lightblue"
  },
  {
    "id": 170,
    "label": "feature vector",
    "title": "refers to a whose entries are individual {feature} . Many methods use {vector} that belong to some finite-dimensional . For some methods, however, it can be more convenient to work with {vector} that belong to an infinite-dimensional (e.g., see ). \\\\ See also: , , , , .",
    "color": "lightblue"
  },
  {
    "id": 171,
    "label": "label",
    "title": "A higher-level fact or quantity of interest associated with a . For example, if the is an image, the label could indicate whether the image contains a cat or not. Synonyms for label, commonly used in specific domains, include \"response variable,\" \"output variable,\" and \"target\" , , . \\\\ See also: , .",
    "color": "lightgreen"
  },
  {
    "id": 172,
    "label": "data",
    "title": "In the context of , the term data is often used synonymously with . The ISO/IEC 2382:2015 standard defines data as a . \\\\ See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 173,
    "label": "dataset",
    "title": "A dataset is a set of distinct {datapoint}. Strictly speaking, a dataset is an unordered collection of {datapoint} that does not contain any repetitions. However, in practice, the term dataset is often used as a synonym for a , i.e., a sequence (or finite list) of {datapoint} and may contain repetitions. methods use datasets to train and validate {model}. The notion of a dataset is broad: {datapoint} may represent concrete physical entities (such as humans or animals) or abstract objects (such as numbers). For illustration, Fig.~{fig_cows_dataset_dict} depicts a dataset whose {datapoint} are cows. {figure}[H] {center} {fig:cowsintheswissalps_dict} {assets/CowsAustria.jpg} {center} {{fig_cows_dataset_dict}A cow herd somewhere in the Alps.} {figure} Quite often, an engineer does not have direct access to the underlying dataset. For instance, accessing the dataset in Fig.~{fig_cows_dataset_dict} would require visiting the cow herd. In practice, we work with a more convenient representation (or approximation) of the dataset. Various mathematical {model} have been developed for this purpose , , , . One of the most widely used is the relational , which organizes as a table (or relation) , . A table consists of rows and columns: each row corresponds to a single , while each column represents a specific attribute of a . methods typically interpret these attributes as {feature} or as a of a . As an illustration, Table~{tab:cowdata_dict} shows a relational representation of the dataset from Fig.~{fig_cows_dataset_dict}. In the relational , the order of rows is immaterial, and each attribute (column) is associated with a domain that specifies the set of admissible values. In applications, these attribute domains correspond to the and the . {table}[H] {table} { TABLE \\\\[0.5ex] A Relation (or Table) That Represents the Dataset in Fig. {fig_cows_dataset_dict} } {tab:cowdata_dict} {tabular}{lcccc} & & & & \\\\ Zenzi & 100 & 4 & 100 & 25 \\\\ Berta & 140 & 3 & 130 & 23 \\\\ Resi & 120 & 4 & 120 & 31 \\\\ {tabular} {table} While the relational is useful for the study of many applications, it may be insufficient regarding the requirements for . Modern approaches like datasheets for datasets provide more comprehensive documentation, including details about the collection process, intended use, and other contextual information . \\\\ See also: , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 174,
    "label": "predictor",
    "title": "A predictor is a real-valued . Given a with {feature} , the value is used as a for the true numeric of the . \\\\ See also: , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 175,
    "label": "labeled data point",
    "title": "A whose is known or has been determined by some means that might require human labor. \\\\ See also: , .",
    "color": "lightgreen"
  },
  {
    "id": 176,
    "label": "discrete RV",
    "title": "A , i.e., a that maps the outcomes of a to elements of a space , is referred to as discrete if its value space is a countable set . \\\\ See also: , , .",
    "color": "salmon"
  },
  {
    "id": 177,
    "label": "random variable (RV)",
    "title": "An RV is a that maps the outcomes of a to elements of a space . Mathematically, an RV is a with domain being the of a and co-domain being a space . Different types of RVs include {itemize} {binary RVs}, which map each outcome to an element of a binary set (e.g., or ); {discrete RVs}, which take on values in a countable set (which can be finite or countably infinite); {real-valued RVs}, which take on values in the real numbers ; {-valued RVs}, which map outcomes to the . {itemize} theory uses the concept of spaces to rigorously define and study the properties of collections of RVs . \\\\ See also: , , , , , , , .",
    "color": "salmon"
  },
  {
    "id": 178,
    "label": "probability space",
    "title": "A space is a mathematical structure that allows us to reason about a , e.g., the observation of a physical phenomenon. Formally, a space is a triplet where {itemize} is a containing all possible outcomes of a ; is a , i.e., a collection of subsets of (called {event}) that satisfies certain closure properties under set operations; is a , i.e., a that assigns a to each . This must satisfy and for any countable sequence of pairwise disjoint {event} in . {itemize} spaces provide the foundation of {probmodel} that can be used to study the behavior of methods , , . \\\\ See also: , , , , , , , .",
    "color": "salmon"
  },
  {
    "id": 179,
    "label": "training set",
    "title": "A training set is a that consists of some {datapoint} used in to learn a . The average of on the training set is referred to as the . The comparison of the with the of allows us to diagnose the method and informs how to improve the (e.g., using a different or collecting more {datapoint}) {MLBasics}. \\\\ See also: , , , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 180,
    "label": "networked model",
    "title": "A networked over an assigns a (i.e., a ) to each node of the . \\\\ See also: , , , .",
    "color": "lightblue"
  },
  {
    "id": 181,
    "label": "batch",
    "title": "In the context of , a batch refers to a randomly chosen subset of the overall . We use the {datapoint} in this subset to estimate the of and, in turn, to update the {modelparam}. \\\\ See also: , , , , , {modelparam}.",
    "color": "lightgreen"
  },
  {
    "id": 182,
    "label": "epoch",
    "title": "An epoch represents one complete pass of the entire through some learning . It refers to the point at which a has processed every in the once. Training a usually requires multiple epochs, since each iteration allows the to refine the {parameter} and improve {prediction}. The number of epochs is something predefined by the user, and thus a hyperparameter, which plays a crucial role in determining how the will generalize to unseen . Too few epochs will result in , while too many epochs can result in . \\\\ See also: , , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 183,
    "label": "networked data",
    "title": "Networked consist of {localdataset} that are related by some notion of pairwise similarity. We can represent networked using a whose nodes carry {localdataset} and whose edges encode pairwise similarities. An example of networked can be found in applications where {localdataset} are generated by spatially distributed {device}. \\\\ See also: , , , , .",
    "color": "lightblue"
  },
  {
    "id": 184,
    "label": "training error",
    "title": "The average of a when predicting the {label} of the {datapoint} in a . We sometimes also refer to training error as the minimal average that is achieved by a solution of . \\\\ See also: , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 185,
    "label": "data point",
    "title": "A point is any object that conveys information~. Examples include students, radio signals, trees, images, {rv}, real numbers, or proteins. We describe points of the same type by two categories of properties. The first category includes {feature} that are or computable properties of a point. These attributes can be automatically extracted or computed using sensors, computers, or other collection systems. For a point that represents a patient, one could be the body weight. The second category includes {label} that are higher level facts (or quantities of interest)\u2014that is, facts which typically require human expertise or domain knowledge to determine, rather than being directly measurable\u2014associated with the point. Determining the {label} of a point usually requires human expertise or domain knowledge. For a point that represents a patient, a cancer diagnosis provided by a physician would serve as the . Fig.\\ {fig:datapoint_cowherd_dict} depicts an image as an example of a point along with its {feature} and {label}. Importantly, what constitutes a or a is not inherent to the point itself\u2014it is a design choice that depends on the specific application. {figure}[H] {minipage}[t]{0.95} {assets/CowsAustria.jpg} {A single point.} {5mm} {minipage} {minipage}[t]{0.95} {feature}: {itemize} : Color intensities of all image pixels. : Time-stamp of the image capture. : Spatial location of the image capture. {itemize} {label}: {itemize} : Number of cows depicted. : Number of wolves depicted. : Condition of the pasture (e.g., healthy, overgrazed). {itemize} {minipage} {Illustration of a point consisting of an image. We can use different properties of the image as {feature} and higher level facts about the image as {label}.{fig:datapoint_cowherd_dict}} {figure} The distinction between {feature} and {label} is not always clear-cut. A property that is considered a in one setting (e.g., a cancer diagnosis) may be treated as a in another setting\u2014particularly if reliable automation (e.g., via image analysis) allows it to be computed without human intervention. broadly aims to predict the of a point based on its {feature}. \\\\ See also: , , , .",
    "color": "lightblue"
  },
  {
    "id": 186,
    "label": "validation error",
    "title": "Consider a that is obtained by some method, e.g., using on a . The average of on a , which is different from the , is referred to as the error. \\\\ See also: , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 187,
    "label": "validation",
    "title": "Consider a that has been learned via some method, e.g., by solving on a . {figure}[htbp] {tikzpicture}[scale=1.2,x=1.5cm] plot (,{0.5*}) node[pos=0, above left] {}; (0,0) circle (4pt); (2,2) circle (4pt); at (0,0) {}; (1,3) circle (4pt); at (1,3) {}; {tikzpicture} {Illustration of validation. The blue points represent the {datapoint} in the , while the red point represents a in the . The (black curve) fits the {datapoint} in the perfectly, but incurs a large on the in the .} {figure} Validation refers to the process of evaluating the incurred by the on a set of {datapoint} that are not contained in the . This set of {datapoint} is called the . The average of on the is referred to as the .\\\\ See also: , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 188,
    "label": "quadratic function",
    "title": "A of the form f() = \\,^{T} {Q} {w} + {q}\\,^{T} +a with some , , and scalar . \\\\ See also: , , .",
    "color": "violet"
  },
  {
    "id": 189,
    "label": "validation set",
    "title": "A set of {datapoint} used to estimate the of a that has been learned by some method (e.g., solving ). The average of on the set is referred to as the and can be used to diagnose an method (see {MLBasics}). The comparison between and can inform directions for the improvement of the method (such as using a different ). \\\\ See also: , , , , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 190,
    "label": "test set",
    "title": "A set of {datapoint} that have been used neither to train a (e.g., via ) nor to choose between different {model} in a . \\\\ See also: , , , .",
    "color": "lightgreen"
  },
  {
    "id": 191,
    "label": "model selection",
    "title": "In , selection refers to the process of choosing between different candidate {model}. In its most basic form, selection amounts to: 1) training each candidate ; 2) computing the for each trained ; and 3) choosing the with the smallest {MLBasics}. \\\\ See also: , , .",
    "color": "lightgreen"
  },
  {
    "id": 192,
    "label": "linear classifier",
    "title": "Consider {datapoint} characterized by numeric {feature} and a from some finite . A linear is characterized by having {decisionregion} that are separated by hyperplanes in {MLBasics}. \\\\ See also: , , , , , .",
    "color": "orange"
  },
  {
    "id": 193,
    "label": "empirical risk minimization (ERM)",
    "title": "ERM is the of selecting a that minimizes the average (or ) on a . The is chosen from a (or ) . The is referred to as . A plethora of -based methods is obtained for different design choices for the , , and {MLBasics}. Fig.\\ {fig_erm_dict} illustrates ERM for a and {datapoint} that are characterized by a single and a . The is a that predicts the of a as a linear of its , i.e., , where and are the {modelparam} of the . The ERM problem is to find the {modelparam} and that minimize the average (or ) incurred by the on the . {figure}[H] {center} {tikzpicture}[scale=1] {0.4} {2.0} {0} {6.5} plot ({},{ + }); (hend) at (,{ + }); at (hend) {}; // in {1/1.2/1.8, 2/3.0/2.6, 3/5.0/5.7} { (l) at (, { + }); (n) at (, ); (pt) at (n) {}; (l) -- (n); } at (n1) {}; at (n2) {}; at (n3) {}; {tikzpicture} {ERM learns a out of a by minimizing the average (or ) incurred on a .} {fig_erm_dict} {center} {figure} See also: , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 194,
    "label": "multi-label classification",
    "title": "Multi- problems and methods use {datapoint} that are characterized by several {label}. As an example, consider a representing a picture with two {label}. One indicates the presence of a human in this picture and another indicates the presence of a car. \\\\ See also: , , .",
    "color": "orange"
  },
  {
    "id": 195,
    "label": "training",
    "title": "In the context of , training refers to the process of learning a useful out of a . The training of a is guided by the incurred on a set of {datapoint}, which serve as the . For {parammodel}, where each is characterized by a specific choice for the {modelparam}, training amounts to finding an optimal choice for the {modelparam} . A widely-used approach to training is , which learns a by minimizing the average incurred on a . One of the main challenges in is to control the discrepancy between the incurred on the and the incurred on other (unseen) {datapoint}. \\\\ See also: , , .",
    "color": "lightgreen"
  },
  {
    "id": 196,
    "label": "semi-supervised learning (SSL)",
    "title": "SSL methods use unlabeled {datapoint} to support the learning of a from {labeled datapoint} . This approach is particularly useful for applications that offer a large number of unlabeled {datapoint}, but only a limited number of {labeled datapoint}. \\\\ See also: , , , .",
    "color": "lightgreen"
  },
  {
    "id": 197,
    "label": "objective function",
    "title": "An objective is a that assigns a numeric objective value to each choice of some variable that we want to optimize (see Fig. {fig_obj_func_dict}). In the context of , the optimization variable could be the {modelparam} of a . Common objective {function} include the (i.e., expected ) or the (i.e., average over a ). methods apply optimization techniques, such as {gdmethod}, to find the choice with the optimal value (e.g., the or the ) of the objective . \\\\ {figure}[H] {center} {tikzpicture}[scale=1.0] (-0.5,0) -- (4.5,0) node[right] {}; (0,-0.5) -- (0,3.5); plot ({}, {0.5*(-2)^2 + 0.5}); at (3.5,2.8) {}; {tikzpicture} {center} {An objective maps each possible value of an optimization variable, such as the {modelparam} of an , to a value that measures the usefulness of .{fig_obj_func_dict}} {figure} See also: , , , .",
    "color": "lightgreen"
  },
  {
    "id": 198,
    "label": "regularizer",
    "title": "A regularizer assigns each from a a quantitative measure conveying to what extent its errors might differ on {datapoint} on and outside a . uses the regularizer for linear {map} {MLBasics}. uses the regularizer for linear {map} {MLBasics}. \\\\ See also: , , , .",
    "color": "lightgreen"
  },
  {
    "id": 199,
    "label": "regularization",
    "title": "A key challenge of modern applications is that they often use large {model}, which have an in the order of billions. Training a high-dimensional using basic -based methods is prone to , i.e., the learned performs well on the but poorly outside the . Regularization refers to modifications of a given instance of in order to avoid , i.e., to ensure that the learned does not perform much worse outside the . There are three routes for implementing regularization: {enumerate}[label=)] { pruning:} We prune the original to obtain a smaller . For a parametric , the pruning can be implemented via constraints on the {modelparam} (such as for the weight of in ). { penalization:} We modify the of by adding a penalty term to the . The penalty term estimates how much higher the expected (or ) is compared with the average on the . {:} We can enlarge the by adding perturbed copies of the original {datapoint} in . One example for such a perturbation is to add the of a to the of a . {enumerate} Fig. {fig_equiv_dataaug_penal_dict} illustrates the above three routes to regularization. These routes are closely related and sometimes fully equivalent. using {gaussrv} to perturb the {featurevec} in the of has the same effect as adding the penalty to the (which is nothing but ). The decision on which route to use for regularization can be based on the available computational infrastructure. For example, it might be much easier to implement than pruning. {figure}[H] {center} {tikzpicture}[scale = 1] (0,0.5) -- (7.7,0.5) node[right] { }; (0.5,0) -- (0.5,4.2) node[above] { }; plot ({},{0.4 + 2.0}) ; plot ({},{0.6 + 2.0}) ; (5, 4.5) ellipse [x radius=0.2cm, y radius=1cm]; at (5, 5.8) [text=black, font=] {}; at (6.7,4.5) {}; (l1) at (1.2, 2.48); (l2) at (1.4, 2.56); (l3) at (1.7, 2.68); (l4) at (2.2, 2.2*0.4+2.0); (l5) at (2.4, 2.4*0.4+2.0); (l6) at (2.7, 2.7*0.4+2.0); (l7) at (3.9, 3.9*0.4+2.0); (l8) at (4.2, 4.2*0.4+2.0); (l9) at (4.5, 4.5*0.4+2.0); (n1) at (1.2, 1.8); (n2) at (1.4, 1.8); (n3) at (1.7, 1.8); (n4) at (2.2, 3.8); (n5) at (2.4, 3.8); (n6) at (2.7, 3.8); (n7) at (3.9, 2.6); (n8) at (4.2, 2.6); (n9) at (4.5, 2.6); at (n1) [circle,draw,fill=red,minimum size=6pt,scale=0.6, name=c1] {}; at (n2) [circle,draw,fill=blue,minimum size=6pt, scale=0.6, name=c2] {}; at (n3) [circle,draw,fill=red,minimum size=6pt,scale=0.6, name=c3] {}; at (n4) [circle,draw,fill=red,minimum size=12pt, scale=0.6, name=c4] {}; at (n5) [circle,draw,fill=blue,minimum size=12pt,scale=0.6, name=c5] {}; at (n6) [circle,draw,fill=red,minimum size=12pt, scale=0.6, name=c6] {}; at (n7) [circle,draw,fill=red,minimum size=12pt,scale=0.6, name=c7] {}; at (n8) [circle,draw,fill=blue,minimum size=12pt, scale=0.6, name=c8] {}; at (n9) [circle,draw,fill=red,minimum size=12pt, scale=0.6, name=c9] {}; [<->] () -- () node [pos=0.4, below] {}; ; (l1) -- (c1); (l2) -- (c2); (l3) -- (c3); (l4) -- (c4); (l5) -- (c5); (l6) -- (c6); (l7) -- (c7); (l8) -- (c8); (l9) -- (c9); (6.2, 3.7) circle (0.1cm) node [black,xshift=2.3cm] {original }; (6.2, 3.2) circle (0.1cm) node [black,xshift=1.3cm] {augmented}; at (4.6,1.2) [minimum size=12pt, font={12}{0}, text=blue] {}; at (7.8,1.2) [minimum size=12pt, font={12}{0}, text=red] {}; {tikzpicture} {Three approaches to regularization: 1) ; 2) penalization; and 3) pruning (via constraints on {modelparam}). {fig_equiv_dataaug_penal_dict} } {center} {figure} See also: , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 200,
    "label": "regularized empirical risk minimization (RERM)",
    "title": "Basic learns a (or trains a ) based solely on the incurred on a . To make less prone to , we can implement by including a (scaled) in the learning objective. This leads to RERM such that {equation} {equ_def_rerm_dict} _{ } {}{} + {}. {equation} The controls the strength. For , we recover standard without . As increases, the learned is increasingly biased toward small values of . The component in the of {equ_def_rerm_dict} can be intuitively understood as a surrogate for the increased average that may occur when predicting {label} for {datapoint} outside the . This intuition can be made precise in various ways. For example, consider a trained using and the . In this setting, corresponds to the expected increase in caused by adding {gaussrv} to the {featurevec} in the {MLBasics}. A principled construction for the arises from approximate upper bounds on the error. The resulting RERM instance is known as {ShalevShwartz2009}. \\\\ See also: , , , .",
    "color": "lightgreen"
  },
  {
    "id": 201,
    "label": "generalization",
    "title": "Generalization refers to the ability of a trained on a to make accurate {prediction} on new unseen {datapoint}. This is a central goal of and , i.e., to learn patterns that extend beyond the . Most systems use to learn a by minimizing the average over a of {datapoint} , which is denoted by . However, success on the does not guarantee success on unseen \u2014this discrepancy is the challenge of generalization. \\\\ To study generalization mathematically, we need to formalize the notion of ``unseen'' . A widely used approach is to assume a for generation, such as the . Here, we interpret {datapoint} as independent {rv} with an identical . This , which is assumed fixed but unknown, allows us to define the of a trained as the expected \\[ {}=_{ p()} \\{ (, ) \\}. \\] The difference between and is known as the . Tools from theory, such as {concentrationinequ} and uniform , allow us to bound this gap under certain conditions .\\\\ Generalization without : theory is one way to study how well a generalizes beyond the , but it is not the only way. Another option is to use simple deterministic changes to the {datapoint} in the . The basic idea is that a good should be robust, i.e., its should not change much if we slightly change the {feature} of a . For example, an object detector trained on smartphone photos should still detect the object if a few random pixels are masked . Similarly, it should deliver the same result if we rotate the object in the image . See Fig. {fig:polynomial_fit_dict} for a visual illustration. {figure}[H] {tikzpicture}[scale=0.8] (3, 2) ellipse (6cm and 2cm); at (6, 3) {}; (1, 3) circle (4pt) node[below, xshift=0pt, yshift=0pt] {}; (5, 1) circle (4pt) node[below] {}; (1.6, 3) circle (3pt); (0.4, 3) circle (3pt); (1, 3) -- (1.6, 3); (1, 3) -- (0.4, 3); (5.6, 1) circle (3pt); (4.4, 1) circle (3pt); (5, 1) -- (5.6, 1); (5, 1) -- (4.4, 1); plot (, {- 1* + 5}); at (3, 2.5) [right] {}; {tikzpicture} {Two {datapoint} that are used as a to learn a via . We can evaluate outside either by an with some underlying or by perturbing the {datapoint}.} {fig:polynomial_fit_dict} {figure} See also: , , , .",
    "color": "salmon"
  },
  {
    "id": 202,
    "label": "generalization gap",
    "title": "gap is the difference between the performance of a on the and its performance on {datapoint} outside . We can make this notion precise by using a that allows us to compute the (or expected ) , of a . {figure}[H] {center} {tikzpicture}[x=3cm, y=1cm] {0.2} {0.8} {-2.1} {4.2} (,) rectangle (,); (,) -- (,); (,) -- (,); at ({(+)/2}, {-0.2}) {}; at (2, 4) {}; plot ({+1}, { -0.5}) node[right] {}; plot ({}, {}); (1, -0.5) -- (1, 1) node[midway, right] {gap}; (-1.2,-1) -- (2.2,-1) node[below right] {}; {tikzpicture} {center} {The generalization gap can be defined as the difference between the and the average (or ) computed on a .} {figure} In practice, the underlying this is unknown. Thus, we need to estimate the based on observered {datapoint}. techniques use different constructions of a , which is different from the , to estimate the gap. \\\\ See also: , , , .",
    "color": "salmon"
  },
  {
    "id": 203,
    "label": "concentration inequality",
    "title": "An upper bound on the that a deviates more than a prescribed amount from its . \\\\ See also: , , .",
    "color": "salmon"
  },
  {
    "id": 204,
    "label": "boosting",
    "title": "Boosting is an iterative to learn an accurate (or strong learner) by sequentially combining less accurate {map} (referred to as weak learners) {hastie01statisticallearning}. For example, weak learners are shallow {decisiontree} that are combined to obtain a deep . Boosting can be understood as a of {gdmethod} for using parametric {model} and {lossfunc} . Just as iteratively updates {modelparam} to reduce the , boosting iteratively combines (e.g., by summation) {map} to reduce the (see Fig. {fig_boosting_dict}). A widely used instance of the generic boosting idea is referred to as boosting, which uses {gradient} of the for combining the weak learners . {figure}[H] {center} {tikzpicture}[scale=1.2] (-0.5,0) -- (5.5,0) node[right] {}; (0,-0.5) -- (0,4.5) node[above] {}; plot ({},{(4 - 1.3* + 0.15*)}); / in {0.7/, 1.5/, 2.3/, 3.0/} { (, 0) -- (, {4 - 1.3* + 0.15*}); (, {4 - 1.3* + 0.15*}) circle (2pt); at (, -0.1) {}; } {tikzpicture} {center} {Boosting methods construct a sequence of {map} that are increasingly strong learners (i.e., incurring a smaller ). {fig_boosting_dict}} {figure} See also: , , , .",
    "color": "violet"
  },
  {
    "id": 205,
    "label": "generalized total variation (GTV)",
    "title": "GTV is a measure of the variation of trained {localmodel} (or their {modelparam} ) assigned to the nodes of an undirected weighted with edges . Given a measure for the between {map} , the GTV is {equation} _{{}{'} } _{,'} {{}}{{'}}. {equation} Here, denotes the weight of the undirected edge . \\\\ See also: , {modelparam}, , , , .",
    "color": "lightblue"
  },
  {
    "id": 206,
    "label": "structural risk minimization (SRM)",
    "title": "SRM is an instance of , with which the can be expressed as a countable union of submodels such that . Each submodel permits the derivation of an approximate upper bound on the error incurred when applying to train . These individual bounds\u2014one for each submodel\u2014are then combined to form a used in the objective. These approximate upper bounds (one for each ) are then combined to construct a for {ShalevMLBook}. \\\\ See also: , , , , , .",
    "color": "salmon"
  },
  {
    "id": 207,
    "label": "backdoor",
    "title": "A backdoor attack refers to the intentional manipulation of the training process underlying an method. This manipulation can be implemented by perturbing the (i.e., through ) or via the optimization used by an -based method. The goal of a backdoor attack is to nudge the learned toward specific {prediction} for a certain range of values. This range of values serves as a key (or trigger) to unlock a backdoor in the sense of delivering anomalous {prediction}. The key and the corresponding anomalous are only known to the attacker. \\\\ See also: , , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 208,
    "label": "clustering assumption",
    "title": "The assumption postulates that {datapoint} in a form a (small) number of groups or {cluster}. {datapoint} in the same are more similar to each other than those outside the . We obtain different methods by using different notions of similarity between {datapoint}. \\\\ See also: , , , .",
    "color": "lightblue"
  },
  {
    "id": 209,
    "label": "denial-of-service attack",
    "title": "A denial-of-service aims (e.g., via ) to steer the training of a such that it performs poorly for typical {datapoint}. \\\\ See also: , , , .",
    "color": "khaki"
  },
  {
    "id": 210,
    "label": "networked exponential families (nExpFam)",
    "title": "A collection of exponential families, each of them assigned to a node of an . The {modelparam} are coupled via the network structure by requiring them to have a small . \\\\ See also: , {modelparam}, .",
    "color": "lightblue"
  },
  {
    "id": 211,
    "label": "scatterplot",
    "title": "A visualization technique that depicts {datapoint} using markers in a 2-D plane. Fig. {fig_scatterplot_temp_FMI_dict} depicts an example of a scatterplot. {figure}[H] {center} {tikzpicture}[scale=1] {x=2cm,y=2cm,every path/.style={>=latex},node style/.style={circle,draw}} {axis}[axis x line=none, axis y line=none, ylabel near ticks, xlabel near ticks, enlarge y limits=true, xmin=-5, xmax=30, ymin=-5, ymax=30, width=6cm, height=6cm ] table [x=mintmp, y=maxtmp, col sep = semicolon] {assets/FMIData1.csv}; at (axis cs:26,2) [anchor=west] {}; at (axis cs:0,30) [anchor=west] {}; (axis cs:-5,0) -- (axis cs:30,0); (axis cs:0,-5) -- (axis cs:0,30); {axis} {tikzpicture} {-10mm} {center} {A scatterplot with circle markers, where the {datapoint} represent daily weather conditions in Finland. Each is characterized by its daytime temperature as the and its daytime temperature as the . The temperatures have been measured at the weather station Helsinki Kaisaniemi during 1 September 2024\u201428 October 2024.} {fig_scatterplot_temp_FMI_dict} {-3mm} {figure} A scatterplot can enable the visual inspection of {datapoint} that are naturally represented by {featurevec} in high-dimensional spaces. \\\\ See also: , , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 212,
    "label": "step size",
    "title": "See .",
    "color": "violet"
  },
  {
    "id": 213,
    "label": "learning rate",
    "title": "Consider an iterative method for finding or learning a useful . Such an iterative method repeats similar computational (update) steps that adjust or modify the current to obtain an improved . A key of an iterative method is the learning rate. The learning rate controls the extent to which the current can be modified during a single iteration. Consider, for example, the {MLBasics} {equation} {equ_def_basic_gradstep_lrate_dict} ^{(\\!+\\!1)} = ^{()} - f(^{()}), {equation} of a for where the is the incurred by on a . Given the current {modelparam} at iteration , the produces updated {modelparam} by moving in the opposite direction of the . {figure}[hbtp] {center} {minipage}{0.45} {tikzpicture}[xscale=0.4,yscale=0.6] plot (, {(1/4)*}); (1,0.25) circle [radius=0.1] node [right] (A) {} ; (-2,1) circle [radius=0.1] node [left] (B) {} ; (3,2.25) circle [radius=0.1] node [right] (C) {} ; (-2,1) -- (3,2.25) node [midway,above] {{equ_def_basic_gradstep_lrate_dict}}; (1,0.25) -- (-2,1) node [midway,above] {{equ_def_basic_gradstep_lrate_dict}}; [below] at (0,-0.2) {(a)}; {tikzpicture} {minipage} {minipage}{0.45} {tikzpicture}[xscale=0.4,yscale=0.6] plot (, {(1/4)*}); (4,4) circle [radius=0.1]; [right] at (4,4) {}; (3.8,3.61) circle [radius=0.1]; [left] at (3.8,3.61) {}; (3.65,3.33) circle [radius=0.1]; [right] at (3.65,3.33) {}; [below] at (0,-0.2) {(b)}; {tikzpicture} {minipage} {center} {Effect of using an inadequate {learnrate} in the {equ_def_basic_gradstep_lrate_dict}. (a) If is too large, the {gradstep} can ``overshoot'' such that the iterates diverge away from the optimum, i.e., ! (b) If is too small, the {gradstep} make too little progress towards the optimum within the available number of iterations (due to limited computational budget). {fig_small_large_lrate_dict}} {figure} \\\\ See also: , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 214,
    "label": "feature map",
    "title": "A refers to a : ', ' that transforms a of a into a new , where is typically different from . The transformed representation is often more useful than the original . For instance, the geometry of {datapoint} may become more linear in , allowing the application of a to . This idea is central to the design of {kernelmethod}~. Other benefits of using a include reducing and improving ~. A common use case is visualization, where a with two output dimensions allows the representation of {datapoint} in a 2-D . Some methods employ trainable {map}, whose {parameter} are learned from . An example is the use of hidden {layer} in a , which act as successive {map} . A principled way to train a is through , using a that measures reconstruction quality, e.g., , where is a trainable that attempts to reconstruct from the transformed . \\\\ See also: , , , , .",
    "color": "lightblue"
  },
  {
    "id": 215,
    "label": "least absolute shrinkage and selection operator (Lasso)",
    "title": "The Lasso is an instance of . It learns the of a from a . Lasso is obtained from by adding the scaled - to the average incurred on the . \\\\ See also: , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 216,
    "label": "similarity graph",
    "title": "Some applications generate {datapoint} that are related by a domain-specific notion of similarity. These similarities can be represented conveniently using a similarity . The node represents the th . Two nodes are connected by an undirected edge if the corresponding {datapoint} are similar. \\\\ See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 217,
    "label": "Kullback\u2013Leibler divergence (KL divergence)",
    "title": "The KL divergence is a quantitative measure of how different one is from another . \\\\ See also: .",
    "color": "salmon"
  },
  {
    "id": 218,
    "label": "Laplacian matrix",
    "title": "The structure of a , with nodes , can be analyzed using the properties of special {matrix} that are associated with . One such is the Laplacian , which is defined for an undirected and weighted , . It is defined elementwise as (see Fig. {fig_lap_mtx_dict}) {equation} {}{}{'} {cases} - _{,'}, & { for } ', {}{'}\\!\\!; \\\\ _{'' } _{,''}, & { for } = '; \\\\ 0, & { else.} {cases} {equation} Here, denotes the of an edge . {figure}[H] {center} {minipage}{0.45} {tikzpicture} {scope}[every node/.style={circle, draw, minimum size=1cm}] (1) at (0,0) {1}; (2) [below left=of 1] {2}; (3) [below right=of 1] {3}; (1) -- (2); (1) -- (3); {scope} at (0,-3) {(a)}; {tikzpicture} {minipage} {-15mm} {minipage}{0.45} {equation} {} = {pmatrix} 2 & -1& -1 \\\\ -1& 1 & 0 \\\\ -1 & 0 & 1 {pmatrix} {equation} {minipage}{} {3.5ex} { (b)} {minipage} {minipage} {{fig_lap_mtx_dict} (a) Some undirected with three nodes . (b) The Laplacian of .} {center} {figure} See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 219,
    "label": "algebraic connectivity",
    "title": "The algebraic connectivity of an is the second-smallest of its . {figure}[H] {tikzpicture} (-1, -1) -- (9, -1) node[below] {}; (0, -0.8) -- (0, -1.2) node[below] {{}}; (7, -0.8) -- (7, -1.2) node[below] {}; (A1) at (0, 1.5) {}; {{disconnected}}; (B1) [below right=0.8cm and 0.5cm of A1] {}; (C1) [below left=0.8cm and 0.5cm of A1] {}; [line width=1 pt] (A1) -- (B1); {scope}[xshift=3.5cm] (A2) at (0, 1.5) {}; {{connected}}; (B2) [below right=0.8cm and 0.5cm of A2] {}; (C2) [below left=0.8cm and 0.5cm of A2] {}; [line width=1 pt] (A2) -- (B2); [line width=1 pt] (B2) -- (C2); {scope} {scope}[xshift=7cm] (A3) at (0, 1.5) {}; {{complete }}; (B3) [below right=0.8cm and 0.5cm of A3] {}; (C3) [below left=0.8cm and 0.5cm of A3] {}; [line width=1 pt] (A3) -- (B3); [line width=1 pt] (B3) -- (C3); [line width=1 pt] (A3) -- (C3); {scope} {tikzpicture} {figure} An is connected if and only if . \\\\ See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 220,
    "label": "Courant\u2013Fischer\u2013Weyl min\u2013max characterization",
    "title": "Consider a with (or spectral decomposition), i.e., = _{=1}^{} {} ^{()} ( ^{()} )\\,^{T}. Here, we use the ordered (in ascending order) {eigenvalue} {equation} {1} {}. {equation} The Courant\u2013Fischer\u2013Weyl min\u2013max characterization {GolubVanLoanBook} represents the {eigenvalue} of as the solutions to certain {optproblem}. \\\\ See also: , , , , .",
    "color": "lightblue"
  },
  {
    "id": 221,
    "label": "kernel",
    "title": "Consider a set of {datapoint}, each represented by a , where denotes the . A (real-valued) kernel is a that assigns to every pair of {featurevec} a real number . This value is typically interpreted as a similarity measure between and . The defining property of a kernel is that it is symmetric, i.e., , and that for any finite set of {featurevec} , the {equation} {K} = {pmatrix} {_1}{_1} & {_1}{_2} & & {_1}{_n} \\\\ {_2}{_1} & {_2}{_2} & & {_2}{_n} \\\\ & & & \\\\ {_n}{_1} & {_n}{_2} & & {_n}{_n} {pmatrix} {R}^{n n} {equation} is . A kernel naturally defines a transformation of a into a . The maps an input to the value . We can view the as a new that belongs to a that is typically different from . This new has a particular mathematical structure, i.e., it is a reproducing kernel (RKHS)~, . Since belongs to a RKHS, which is a , we can interpret it as a generalized . Note that a finite-length can be viewed as a that assigns a real value to each index . \\\\ See also: , , , .",
    "color": "lightblue"
  },
  {
    "id": 222,
    "label": "kernel method",
    "title": "A method is an method that uses a to map the original (i.e., raw) of a to a new (transformed) , . The motivation for transforming the {featurevec} is that, by using a suitable , the {datapoint} have a more \"pleasant\" geometry in the transformed . For example, in a binary problem, using transformed {featurevec} might allow us to use {linmodel}, even if the {datapoint} are not linearly separable in the original (see Fig. {fig_linsep_kernel_dict}). {figure}[H] {center} {tikzpicture}[auto,scale=0.6] [thick] (-6,2) circle (0.1cm) node[anchor=west] {{0mm}}; [thick] (-8,1.6) circle (0.1cm) node[anchor=west] {{0mm}}; [thick] (-7.4,-1.7) circle (0.1cm) node[anchor=west] {{0mm}}; [thick] (-6,-1.9) circle (0.1cm) node[anchor=west] {{0mm}}; [thick] (-6.5,0.0) rectangle ++(0.1cm,0.1cm) node[anchor=west,above] {{0mm}}; [thick] (4,0) circle (0.1cm) node[anchor=north] {{0mm}}; [thick] (5,0) circle (0.1cm) node[anchor=north] {{0mm}}; [thick] (6,0) circle (0.1cm) node[anchor=north] {{0mm}}; [thick] (7,0) circle (0.1cm) node[anchor=north] {{0mm}}; [thick] (2,0) rectangle ++(0.1cm,0.1cm) node[anchor=west,above] {{0mm}}; (-3,0) to node[midway,above] {} (1,0); {tikzpicture} {center} { Five {datapoint} characterized by {featurevec} and {label} , for . With these {featurevec}, there is no way to separate the two classes by a straight line (representing the of a ). In contrast, the transformed {featurevec} allow us to separate the {datapoint} using a . {fig_linsep_kernel_dict}} {figure} See also: , , , .",
    "color": "lightblue"
  },
  {
    "id": 223,
    "label": "confusion matrix",
    "title": "Consider {datapoint} characterized by {feature} and corresponding {label} . The {label} take on values in a finite . For a given , the confusion is a where each row corresponds to a different value of the true and each column to a different value of the . The th entry of the confusion represents the fraction of {datapoint} with a true that are predicted as . The main diagonal of the confusion contains the fractions of correctly classified {datapoint} (i.e., those for which ). The off-diagonal entries contain the fractions of {datapoint} that are misclassified by . \\\\ See also: , , , , .",
    "color": "lightblue"
  },
  {
    "id": 224,
    "label": "feature matrix",
    "title": "Consider a with {datapoint} with {featurevec} . It is convenient to collect the individual {featurevec} into a of size . \\\\ See also: , , , , .",
    "color": "lightblue"
  },
  {
    "id": 225,
    "label": "density-based spatial clustering of applications with noise (DBSCAN)",
    "title": "DBSCAN refers to a for {datapoint} that are characterized by numeric {featurevec}. Like and via , DBSCAN also uses the Euclidean distances between {featurevec} to determine the {cluster}. However, in contrast to and , DBSCAN uses a different notion of similarity between {datapoint}. DBSCAN considers two {datapoint} as similar if they are connected via a sequence (i.e., path) of nearby intermediate {datapoint}. Thus, DBSCAN might consider two {datapoint} as similar (and therefore belonging to the same cluster) even if their {featurevec} have a large Euclidean distance. \\\\ See also: , , , , .",
    "color": "lightblue"
  },
  {
    "id": 226,
    "label": "federated learning (FL)",
    "title": "FL is an umbrella term for methods that train {model} in a collaborative fashion using decentralized and computation. \\\\ See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 227,
    "label": "clustered federated learning (CFL)",
    "title": "CFL trains {localmodel} for the {device} in a application by using a , i.e., the {device} of an form {cluster}. Two {device} in the same generate {localdataset} with similar statistical properties. CFL pools the {localdataset} of {device} in the same to obtain a for a -specific . clusters {device} implicitly by enforcing approximate similarity of {modelparam} across well-connected nodes of the .\\\\ See also: , , , , .",
    "color": "lightblue"
  },
  {
    "id": 228,
    "label": "independent and identically distributed (i.i.d.)",
    "title": "A collection of {rv} is referred to as i.i.d. if each follows the same , and the {rv} are mutually independent. That is, for any collection of {event} , we have \\[ { ^{(1)} {A}_1, \\,, \\,^{()} {A}_{}} = _{=1}^{} { ^{()} {A}_}. \\] \\\\ See also: , , , , .",
    "color": "salmon"
  },
  {
    "id": 229,
    "label": "preimage",
    "title": "Consider a between two sets. The preimage of a subset is the set of all inputs that are mapped into by , i.e., \\[ f^{-1}({B}) \\{ u {U} f(u) {B} \\}. \\] The preimage is well defined even if the is non-invertible . \\\\ See also: .",
    "color": "orange"
  },
  {
    "id": 230,
    "label": "measurable",
    "title": "Consider a , such as recording the air temperature at an weather station. The corresponding consists of all possible outcomes (e.g., all possible temperature values in degree Celsius). In many applications, we are not interested in the exact outcome , but only whether it belongs to a subset (e.g., determining whether the temperature is below zero degrees). We call such a subset measurable if it is possible to decide, for any outcome , whether or not (see Fig.\\ {fig_measurable_dict}). \\\\ {figure}[H] {center} {tikzpicture} (0,0) -- (8.5,0) node[right] {temperature (C)}; / in {0/--20, 1/--10, 2/0, 3/10, 4/20, 5/30, 6/40, 7/50, 8/60} { (,0.1) -- (,-0.1); at (,-0.1) {}; } (0,0.3) rectangle (2,0.6); at (1,0.6) {C}; (5.5,0.3) rectangle (7.5,0.6); at (6,0.6) {C C}; {10mm} {tikzpicture} {10mm} {center} {A constituted by all possible temperature values that can occur at an station. Two measurable subsets of temperature values, denoted by and , are highlighted. For any actual temperature value , it is possible to determine (via some equipment) whether and whether . {fig_measurable_dict}} {figure} In principle, measurable sets could be chosen freely (e.g., depending on the resolution of the measuring equipment). However, it is often useful to impose certain completeness requirements on the collection of measurable sets. For example, the itself should be measurable, and the union of two measurable sets should also be measurable. These completeness requirements can be formalized via the concept of a (or -field) , , . A measurable space is a pair that consists of an arbitrary set and a collection of measurable subsets of that form a . \\\\ See also: , , .",
    "color": "salmon"
  },
  {
    "id": 231,
    "label": "$\\sigma$-algebra",
    "title": "Consider a with . A -algebra (or -field) is a collection of sub-sets of with the following properties : {itemize} The empty set and the entire belong to , i.e., and . If a set belongs to , then its complement also belongs to , i.e., implies . If a countable collection of sets belongs to , then their union also belongs to , i.e., implies . {itemize} See also: , , .",
    "color": "salmon"
  },
  {
    "id": 232,
    "label": "$\\sigma$-field",
    "title": "See .",
    "color": "salmon"
  },
  {
    "id": 233,
    "label": "injective",
    "title": "A is if it maps distinct elements of its to distinct elements of its , i.e., if implies for all . Equivalently, no two different inputs are mapped to the same output. \\\\ See also: .",
    "color": "orange"
  },
  {
    "id": 234,
    "label": "event",
    "title": "Consider a , defined on some , which takes values in a space . An event is a subset of such that the is well defined. In other words, the of an event belongs to the underlying . \\\\ See also: , , , .",
    "color": "salmon"
  },
  {
    "id": 235,
    "label": "countable",
    "title": "A set is called countable if its elements can be put into a one-to-one correspondence with the natural numbers or with a finite subset of . Equivalently, a set is countable if there exists an . {figure}[H] {tikzpicture}[>=stealth, node distance=1.0cm, thick] (a1) {}; (a2) {}; (a3) {}; {}; {scope}[on background layer] () rectangle (); {scope} (n1) {}; (n2) {}; (n3) {}; (n4) {}; (ndots) {}; {}; (a1) -- (n3); (a2) -- (n1); (a3) -- (n4); {tikzpicture} {An that maps the elements of a finite set to the natural numbers , which implies that is countable.} {figure} Typical examples include the set of integers and rational numbers . In contrast, the set of real numbers is not countable, meaning no such one-to-one correspondence with exists.\\\\ See also: , .",
    "color": "salmon"
  },
  {
    "id": 236,
    "label": "probability mass function (pmf)",
    "title": "The pmf of a discrete is a that assigns to each possible value of the the . Fig.\\ {fig_pmf_dict} illustrates the pmf of a discrete . {figure}[H] {tikzpicture}[>=stealth, thick,y=2cm] / in {1/0.3, 4/0.7}{ (,0) -- (,); (,) circle (2pt); } at (1,0.3) { }; at (1,0) { }; at (4,0) { }; at (-1.2,-0.80) { }; at (-5.2,1.18) { }; at (3.2,1.56) { }; {tikzpicture} {The pmf of a discrete taking values in the set . Also shown are three {dataset} whose relative frequencies of {datapoint} match this pmf exactly. Such {dataset} could arise as {realization} of {rv} sharing the common pmf . {fig_pmf_dict}} {figure} The pmf satisfies . A pmf defined over the set can be viewed as an idealized description of any sufficiently long , whose relative frequencies of each value are close to the corresponding pmf value , {| \\{1,,\\}: ^{()}= ' |} {} {}{'}. A rigorous treatment of this approximation principle is at the heart information theory and statistics . \\\\ See also: , , , .",
    "color": "salmon"
  },
  {
    "id": 237,
    "label": "coreset",
    "title": "A coreset is a small subset of a larger that approximates certain properties of the original . The construction of a coreset typically involves selecting representative {datapoint} and assigning them weights to reflect their importance in the original (Fig.\\ {fig_coreset_dict}). {figure}[H] {tikzpicture} / in {0.5/0.7, 1.2/1.4, 1.8/0.9, 2.2/1.8, 2.6/1.2, 3.1/1.6} { (,) circle (1.5pt);} / in {1.2/1.4, 2.6/1.2}{ (,) circle (2pt); (,) circle (6pt); } (label) at (0.6,2.2) { coreset}; (label) -- (1.2,1.5); (label) -- (2.6,1.3); {tikzpicture} {A coreset (highlighted in blue) is a small, subset of a larger . {fig_coreset_dict}} {figure} Coresets are particularly useful for applications (such as ) involving large {dataset}, as they allow for efficient computation while preserving the essential characteristics of the . \\\\ See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 238,
    "label": "outlier",
    "title": "Many methods are motivated by the , which interprets {datapoint} as {realization} of {rv} with a common . The is useful for applications where the statistical properties of the generation process are stationary (or time-invariant) . However, in some applications, the consist of a majority of regular {datapoint} that conform with the as well as a small number of {datapoint} that have fundamentally different statistical properties compared with the regular {datapoint}. We refer to a that substantially deviates from the statistical properties of most {datapoint} as an outlier. Different methods for outlier detection use different measures for this deviation. Statistical learning theory studies fundamental limits on the ability to mitigate outliers reliably , . \\\\ See also: , , , .",
    "color": "salmon"
  },
  {
    "id": 239,
    "label": "ensemble",
    "title": "An ensemble method combines multiple methods, referred to as base learners, to improve overall performance. The base learners can be obtained from , using different choices for the , , and . Ensemble methods exploit the diversity among base learners which tend to capture different aspects of the {feature} of a . By aggregating the {prediction} of base learners, ensemble methods can often achieve better performance than any single base learner. Different ensemble methods use different constructions for the base learners and how to aggregate their {prediction}. For example, methods use random sampling to construct different {trainset} for the base learners. A well-known example of a method is a . On the other hand, methods train base learners sequentially, where each new base learner focuses on correcting the errors of the previous ones. A third family of ensemble methods is stacking, where base learners are trained on the same but with potentially different {model}. \\\\ See also: , , .",
    "color": "orange"
  },
  {
    "id": 240,
    "label": "stacking",
    "title": "Stacking is an ensemble method that combines multiple methods, referred to as base learners, to improve overall performance. In stacking, a finite number of base learners are trained on the same using different {model} or {lossfunc} , for {hastie01statisticallearning}, . The -th base learner delivers a learnt . The final for a is obtained by aggregating the {prediction} of the base learners using an aggregation rule , such as majority voting for or averaging for . We can interpret stacking as a form of , where each base learner extracts a new . The aggregation rule is then a map applied to the transformed ()= ( ^{(1)}(), , ^{()}() )^{T}. {figure}[htbp] {center} {tikzpicture}[ font=, scale=1.0, transform shape, node distance=7mm and 10mm, dataset/.style={draw, rounded corners, inner sep=2pt}, learner/.style={draw, rounded corners, minimum width=14mm, minimum height=7mm, inner sep=6pt,align=center}, op/.style={draw, circle, inner sep=1pt}, >=latex ] (D) {}; (L1) { \\\\ }; (L2) { \\\\ }; (L3) { \\\\ }; (D) -- (D1) node[midway, above left=-1pt] {}; (D) -- (D2) node[midway, right] {}; (D) -- (D3) node[midway, above right=-1pt]{}; (agg) {}; (yhat) {}; (L1) -- (agg); (L2) -- (agg); (L3) -- (agg); (agg) -- (yhat); {}; {}; {}; {tikzpicture} { Illustration of the stacking principle: three base learners, trained with different {model} and {lossfunc}, produce {prediction} . These {prediction} serve as new {feature} that are fed into the aggregation rule , which produces the final .} {center} {figure} \\\\ See also: , .",
    "color": "orange"
  },
  {
    "id": 241,
    "label": "sample",
    "title": "In the context of , a sample is a finite (of length ) of {datapoint}, . The number is called the . -based methods use a sample to train a (or learn a ) by minimizing the average (the ) over that sample. Since a sample is defined as a , the same may appear more than once. By contrast, some authors in statistics define a sample as a set of {datapoint}, in which case duplicates are not allowed . These two views can be reconciled by regarding a sample as a of \u2013 pairs, . The -th pair consists of the {feature} and the of an unique underlying . While the underlying {datapoint} are unique, some of them can have identical {feature} and {label}. {figure} {center} {tikzpicture}[>=Latex, font=] (pop) {}; {population}; / [count=] in {-2.0/0.3, -1.6/0.9, -1.2/-0.2, -0.8/0.5, -0.3/-0.6, 0.2/0.1, 0.6/0.8, 1.0/-0.4, 1.4/0.4, 1.8/-0.1} { (p) at (); (p) circle (1.6pt); } (sampleanchor) at ([xshift=1.8cm,yshift=0.5cm]pop.east); {0.55cm} {0.95cm} (s1) at () {}; (s2) at () {}; (s3) at () {}; (s4) at () {}; (s5) at () {}; (s6) at () {}; (seqbox) {}; {a sample}; at () {}; (p2) to[out=0, in=180] (); (p7) to[out=10, in=180] (); (p4) to[out=10, in=180] (); (p5) to[out=-10, in=180] (); (p3) to[out=0, in=180] (); (p3) to[out=-5, in=180] (); {tikzpicture} {center} {A sample viewed as a finite . Each element of this sample consists of the and the of a from an underlying population. The same may occur more than once in the sample. {fig:sample-sequence_dict}} {figure} For the analysis of methods, it is common to interpret (the generation of) a sample as the of a indexed by . A widely used assumption is the , where sample elements , for , are {rv} with a common . \\\\ See also: , , .",
    "color": "lightgreen"
  },
  {
    "id": 242,
    "label": "bagging (or bootstrap aggregation)",
    "title": "Bagging (or bootstrap aggregation) is a technique to improve (the of) a -based method using . The idea is to use random sampling to generate perturbed copies {}^{(1)},,{}^{()} of . Each perturbed copy is used to learn a separate , ^{(1)},,^{()}. We then predict the of a with by combining (or aggregating) the individual {prediction} ^{(1)}(),,^{()}(). For {map} delivering numeric values, this aggregation could be implemented by computing the average of individual {prediction}. Bagging is an example of an method, with base learners using the same but different {trainset}. {figure}[htbp] {center} {tikzpicture}[ font=, scale=0.8, transform shape, node distance=7mm and 10mm, dataset/.style={draw, rounded corners, inner sep=2pt}, learner/.style={draw, rounded corners, minimum width=14mm, minimum height=7mm, inner sep=2pt}, op/.style={draw, circle, inner sep=1pt}, >=latex ] (D) {}; (D1) { }; (D2) { }; (D3) { }; (D) -- (D1) node[midway, above left=-1pt] { resample}; (D) -- (D2) node[midway, right] { resample}; (D) -- (D3) node[midway, above right=-1pt]{ resample}; (L1) { }; (L2) { }; (L3) { }; (D1) -- (L1); (D2) -- (L2); (D3) -- (L3); (agg) { }; (yhat) { }; (L1) -- (agg); (L2) -- (agg); (L3) -- (agg); (agg) -- (yhat); { }; { }; { }; {tikzpicture} {An example of bagging where three base learners use variations of the original to learn the {hypothesis} . The for a with is obtained by applying an aggregation rule to the individual {prediction} .} {center} {figure} See also: , , .",
    "color": "orange"
  },
  {
    "id": 243,
    "label": "decision region",
    "title": "Consider a that delivers values from a finite set . For each value (i.e., category) , the determines a subset of values that result in the same output . We refer to this subset as a decision region of the . \\\\ See also: , , , .",
    "color": "orange"
  },
  {
    "id": 244,
    "label": "decision boundary",
    "title": "Consider a that reads in a and delivers a value from a finite set . The decision boundary of is the set of {vector} that lie between different {decisionregion}. More precisely, a belongs to the decision boundary if and only if each , for any , contains at least two {vector} with different values. \\\\ See also: , , , , , , .",
    "color": "orange"
  },
  {
    "id": 245,
    "label": "Euclidean norm",
    "title": "The Euclidean norm of a = (_{1},,_{} ) {R}^{} defined as {}{2} {_{=1}^{}^2_{}}. The Euclidean norm is distinct among all norms on in the sense that it is induced by the inner-product . In other words, . \\\\ See also: , .",
    "color": "lightcoral"
  },
  {
    "id": 246,
    "label": "Euclidean distance",
    "title": "The Euclidean distance is used synonymously for . \\\\ See also: , .",
    "color": "lightcoral"
  },
  {
    "id": 247,
    "label": "normal equations",
    "title": "The optimality condition for the {modelparam} in are often referred to as normal equations.\\\\ See also: , {modelparam}.",
    "color": "lightcoral"
  },
  {
    "id": 248,
    "label": "Euclidean space",
    "title": "The Euclidean space of dimension consists of {vector} , with real-valued entries . Such a Euclidean space is equipped with a geometric structure defined by the inner product between any two {vector} . \\\\ See also: .",
    "color": "lightcoral"
  },
  {
    "id": 249,
    "label": "explainable empirical risk minimization (EERM)",
    "title": "EERM is an in- stance of that adds a term to the average in the of . The term is chosen to favor {map} that are intrinsically explainable for a specific user. This user is characterized by their {prediction} provided for the {datapoint} in a . \\\\ See also: , , , .",
    "color": "lightgreen"
  },
  {
    "id": 250,
    "label": "$k$-means",
    "title": "The -means principle is an optimization-based approach to the of {datapoint} that are characterized by a numeric {MLBasics}. As a approach, -means partitions a into disjoint subsets (or {cluster}), which are indexed by . Each is characterized by the average of {datapoint} that belong to it. This average (or ) is referred to as the . A visual illustration is provided in Fig. {fig_kmeans_dict}. {figure}[H] {center} {tikzpicture}[scale=1] { data/.style={circle, fill=black, inner sep=1.2pt}, centroid/.style={thick, cross out, draw, minimum size=6pt, inner sep=0pt} } (xi) at (1.0,0.2) {}; {}; in {(-0.3,0.0),(0.2,-0.4),(0.6,0.8),(0.0,0.9),(1.1,-0.2)} at {}; in {(2.5,1.0),(3.7,1.2),(2.6,2.3),(3.8,2.5),(3.0,2.9),(3.6,1.6)} at {}; (mu1) at (0.55,0.4) {}; (mu2) at (3.1,1.85) {}; {}; {}; {tikzpicture} {center} {A of {datapoint}, indexed by and characterized by {featurevec} . The also includes two {clustercentroid} . {fig_kmeans_dict}} {figure} In general, the -means problem is a challenging . However, there is a simple iterative method for finding approximately optimal {clustercentroid}. This method, referred to as Lloyd's method, alternates between: 1) updating the assignments based on the nearest current ; and 2) recalculating the {clustercentroid} given the updated assignments . \\\\ See also: , .",
    "color": "lightblue"
  },
  {
    "id": 251,
    "label": "cluster centroid",
    "title": "methods decompose a given into few {cluster}. Different methods use different representations for these {cluster}. If {datapoint} are characterized by numerical {featurevec} , we can use some vector , referred to as centroid, to represent a . For example, if a consists of a set of {datapoint}, we use the average of their {featurevec} as a centroid. However, there are also other choices for how to construct a centroid. \\\\ See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 252,
    "label": "explainable machine learning (XML)",
    "title": "XML methods aim to complement each with an of how the has been obtained. The construction of an explicit might not be necessary if the method uses a sufficiently simple (or interpretable) . \\\\ See also: , , , .",
    "color": "orange"
  },
  {
    "id": 253,
    "label": "Finnish Meteorological Institute (FMI)",
    "title": "The FMI is a government agency responsible for gathering and reporting weather in Finland. \\\\ See also: .",
    "color": "salmon"
  },
  {
    "id": 254,
    "label": "sample mean",
    "title": "The for a given , with {featurevec} , is defined as = {1}{} _{=1}^{} ^{()}. \\\\ See also: , , , .",
    "color": "lightblue"
  },
  {
    "id": 255,
    "label": "sample covariance matrix",
    "title": "The for a given set of {featurevec} is defined as { } = {1}{} _{=1}^{} (^{()}\\!-\\!{}) (^{()}\\!-\\!{})\\,^{T}. Here, we use the . \\\\ See also: , , , .",
    "color": "lightblue"
  },
  {
    "id": 256,
    "label": "covariance matrix",
    "title": "The of a is defined as . \\\\ See also: , , .",
    "color": "salmon"
  },
  {
    "id": 257,
    "label": "high-dimensional regime",
    "title": "The high-dimensional regime of is characterized by the of the being larger than the , i.e., the number of (labeled) {datapoint} in the . For example, methods operate in the high-dimensional regime whenever the number of {feature} used to characterize {datapoint} exceeds the number of {datapoint} in the . Another example of methods that operate in the high-dimensional regime is large {ann}, which have far more tunable (and bias terms) than the total number of {datapoint} in the . High-dimensional statistics is a recent main thread of theory that studies the behavior of methods in the high-dimensional regime , . \\\\ See also: , , , .",
    "color": "lightgreen"
  },
  {
    "id": 258,
    "label": "covariance",
    "title": "The covariance between two real-valued {rv} and , defined on a common , measures their linear dependence. It is defined as {x}{y} = \\{ (x - \\{ x\\} )(y - \\{y\\} )\\}. A positive covariance indicates that and tend to increase together, while a negative covariance suggests that one tends to increase as the other decreases. If , the {rv} are said to be uncorrelated, though not necessarily statistically independent. See Fig. {fig:covariance-examples_dict} for visual illustrations. {figure}[H] {tikzpicture} {scope}[shift={(0,0)}] {axis}[ width=4.5cm, height=4.5cm, title={}, xlabel={}, ylabel={}, xmin=-3, xmax=3, ymin=-3, ymax=3, xtick=, ytick=, axis lines=middle, enlargelimits ] +[only marks, mark=*, samples=50, domain=-2:2] ({x}, {-x + rand}); {axis} at (1.5,-1) {(a)}; {scope} {scope}[shift={(5.2cm,0)}] {axis}[ width=4.5cm, height=4.5cm, title={}, xlabel={}, ylabel={}, xmin=-3, xmax=3, ymin=-3, ymax=3, xtick=, ytick=, axis lines=middle, enlargelimits ] +[only marks, mark=*, samples=50, domain=-2:2] ({x}, {rand}); {axis} at (1.5,-1) {(b)}; {scope} {scope}[shift={(10.4cm,0)}] {axis}[ width=4.5cm, height=4.5cm, title={}, xlabel={}, ylabel={}, xmin=-3, xmax=3, ymin=-3, ymax=3, xtick=, ytick=, axis lines=middle, enlargelimits ] +[only marks, mark=*, samples=50, domain=-2:2] ({x}, {x + rand}); {axis} at (1.5,-1) {(c)}; {scope} {tikzpicture} {{scatterplot} illustrating {realization} from three different {probmodel} for two {rv} with different covariance values. (a) Negative. (b) Zero. (c) Positive.} {fig:covariance-examples_dict} {figure} See also: , .",
    "color": "salmon"
  },
  {
    "id": 259,
    "label": "Gaussian mixture model (GMM)",
    "title": "A GMM is a particular type of for a numeric (e.g., the {feature} of a ). Within a GMM, the is drawn from a randomly selected with . The index is a with {probability} . Note that a GMM is parameterized by the , the , and the for each . \\\\ See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 260,
    "label": "maximum likelihood",
    "title": "Consider {datapoint} that are interpreted as the {realization} of {rv} with a common , which depends on the {modelparam} . likelihood methods learn {modelparam} by maximizing the probability (density) of the observed . Thus, the likelihood estimator is a solution to the . \\\\ See also: , , .",
    "color": "salmon"
  },
  {
    "id": 261,
    "label": "expectation\u2013maximization (EM)",
    "title": "Consider a for the {datapoint} generated in some application. The estimator for the {modelparam} is obtained by maximizing . However, the resulting might be computationally challenging. EM approximates the estimator by introducing a latent such that maximizing would be easier , , . Since we do not observe , we need to estimate it from the observed using a conditional . The resulting estimate is then used to compute a new estimate by solving . The crux is that the conditional depends on the {modelparam} , which we have updated based on . Thus, we have to recalculate , which, in turn, results in a new choice for the {modelparam}. In practice, we repeat the computation of the conditional (i.e., the E-step) and the update of the {modelparam} (i.e., the M-step) until some is met. \\\\ See also: , , .",
    "color": "salmon"
  },
  {
    "id": 262,
    "label": "probabilistic principal component analysis (PPCA)",
    "title": "PPCA extends basic by using a for {datapoint}. The of PPCA frames the task of as an estimation problem that can be solved using . \\\\ See also: , , , .",
    "color": "lightblue"
  },
  {
    "id": 263,
    "label": "polynomial regression",
    "title": "Polynomial is an instance of that learns a polynomial to predict a numeric based on the numeric {feature} of a . For {datapoint} characterized by a single numeric , polynomial uses the The quality of a polynomial is measured using the average incurred on a set of {labeled datapoint} (which we refer to as the ). \\\\ See also: , , .",
    "color": "lightgreen"
  },
  {
    "id": 264,
    "label": "least squares",
    "title": "Least squares refers to -based methods that use the average {1}{} _{=1}^{} ( ^{()} - (^{()}) )^{2} on a to measure the quality of a . We obtain different least squares methods by using different {model} in . For example, the least squares variant of is a least squares method that uses a . \\\\ See also: , , , .",
    "color": "lightgreen"
  },
  {
    "id": 265,
    "label": "column space",
    "title": "The column space of a , denoted by , is the set of all linear combinations of the columns of . In other words, {} = \\{ : {R}^{} \\}. The column space is a subspace of the . \\\\ See also: , .",
    "color": "lightcoral"
  },
  {
    "id": 266,
    "label": "subspace",
    "title": "A subset of is a subspace of if it is also a with respect to the same operations as . \\\\ See also: .",
    "color": "lightcoral"
  },
  {
    "id": 267,
    "label": "design matrix",
    "title": "The term index matrix is a synonym for the , particularly used in statistics . It collects the {featurevec} of the {datapoint} in a that is used for or .",
    "color": "lightblue"
  },
  {
    "id": 268,
    "label": "data matrix",
    "title": "The term is sometimes used as a synonym for the of a containing {datapoint}, each characterized by a . In particular, when no information is available, the term highlights that the fully characterizes the .\\\\ See also: , , , .",
    "color": "lightblue"
  },
  {
    "id": 269,
    "label": "label vector",
    "title": "Given a of {labeled datapoint}, {^{(1)}}{^{(1)}}, , {^{()}}{^{()}}, it is convenient to collect the corresponding {label} into a single . \\\\ See also: , , , .",
    "color": "lightblue"
  },
  {
    "id": 270,
    "label": "output vector",
    "title": "The term output is used as a synonym for the of a .",
    "color": "lightblue"
  },
  {
    "id": 271,
    "label": "output",
    "title": "The term output is sometimes used as a synonym for the of a .",
    "color": "khaki"
  },
  {
    "id": 272,
    "label": "target vector",
    "title": "The term target is used as a synonym for the of a .",
    "color": "lightblue"
  },
  {
    "id": 273,
    "label": "target",
    "title": "The term target is sometimes used as a synonym for the of a .",
    "color": "lightblue"
  },
  {
    "id": 274,
    "label": "response vector",
    "title": "The term response is used as a synonym for the of a .",
    "color": "lightblue"
  },
  {
    "id": 275,
    "label": "response",
    "title": "The term response is sometimes used as a synonym for the of a .",
    "color": "lightblue"
  },
  {
    "id": 276,
    "label": "linear least squares",
    "title": "Linear least squares refers to the variant of that uses the to measure the quality of a linear . Conversely, it can also be viewed as the variant of that restricts the to a . {figure} {tikzpicture}[scale=1] (-1,-0.333) -- (3,1) node[pos=0.0,below right] {}; (y) at (1,2); (y) circle (1.6pt) node[above] {}; (xw) at (1.5,0.5); (xw) circle (1.6pt) node[below right] {}; (y) -- (xw); {scope}[xshift=6.2cm] at (0,1.2) {}; at (0,0.4) {}; {scope} {tikzpicture} {Linear least squares has both geometric and algebraic interpretations. Left: geometrically, it finds the orthogonal projection of the onto the of the {BoydConvexBook}. Right: algebraically, it solves a linear system known as . {fig_linleastsquares_dict}} {figure} In particular, linear least squares learns the {parameter} of a linear by solving {equation} {eq_linleastsquares_dic} _{ {R}^{}} { - }{2}^2. {equation} Here, the is and the is . Both are constructed from the = \\{ {^{(1)}}{^{(1)}}, \\,, \\, {^{()}}{^{()}} \\}. The optimization problem in {eq_linleastsquares_dic} admits a clear geometric interpretation: we seek the vector in the of that is closest to the label vector (see Fig.~{fig_linleastsquares_dict}) {BoydConvexBook}. A necessary and sufficient condition for to minimize {eq_linleastsquares_dic} are the ^{T} {} = ^{T} . See also: , , , , .",
    "color": "lightcoral"
  },
  {
    "id": 277,
    "label": "weighted least squares",
    "title": "Weighted least squares refers to -based methods that use the weighted average {1}{} _{=1}^{} ^{()} ( ^{()} - (^{()}) )^{2} on a to measure the quality of a . {figure}[H] {tikzpicture}[scale=0.7, y=0.5cm, x=0.5cm] / in { 1/2, 4/3, 7/10 } { (, 0) -- (, ); (, ) circle (2pt); (ptB) at (, ) {}; } (0.5, 3.5) -- (10.5, 3.5) node[right] {}; at (ptB7) {}; (lineStart) at (0.5, 3.5); (lineEnd) at (10.5, 3.5); (lineStart) -- (lineEnd) node[right] {}; let 1 = (ptB7) in coordinate (proj7) at (1, 3.5); (proj7) -- node[right, xshift=2pt, fill=white, inner sep=1pt] {} (ptB7); (proj7) circle (1.2pt); (ptB7) circle (1.2pt); {tikzpicture} {Weighted least squares can be used to mitigate the effect of {datapoint} in a . {fig_weighted_least_squares_dict}} {figure} The weights allow us to emphasize or de-emphasize the contribution of individual {datapoint} in the . Ideally, we assign a small weight (de-empahsize) the -th if it is an (see Figure {fig_weighted_least_squares_dict}). We obtain different weighted least squares methods by using different {model} in . \\\\ See also: , , , .",
    "color": "lightgreen"
  },
  {
    "id": 278,
    "label": "linear regression",
    "title": "Linear methods learn a linear which is used to predict the numeric of a based on its numeric {featurevec} . The least-squares variant of linear measures the quality of a linear via the average incurred on a {^{(1)}}{^{(1)}},, {^{()}}{^{()}}. As an instance of , linear (least-squares) learns the {modelparam} by solving the _{ {R}^{}} {1}{} _{=1}^{} ( ^{()} - ^{T} ^{()} )^{2}. {figure}[H] {tikzpicture}[scale=0.7, y=0.5cm, x=0.5cm] {scope} / in { 1/2, 4/3, 7/4 } { (, 0) -- (, ); (, ) circle (2pt); (ptA) at (, ) {}; } (0.5, 3) -- (10.5, 3) node[right] {}; at (7.5, -4) {(a)}; {scope} {scope}[xshift=10cm] / in { 1/2, 4/3, 7/10 } { (, 0) -- (, ); (, ) circle (2pt); (ptB) at (, ) {}; } (0.5, 7.5) -- (10.5, 7.5) node[right] {}; at (ptB7) {}; at (7.5, -4) {(b)}; {scope} {tikzpicture} {For a with and using the trivial for any , linear reduces to computing the average . (a) A clean and resulting (given by the average). (b) A perturbed (including an ) and the resulting . {fig_linreg_dict}} {figure} We can rewrite the above more compactly using the and the . This allows to rewrite the above as _{ {R}^{}} {1}{} { - }{2}^{2}. By the , a necessary and sufficient condition for a vector to be a solution to the above is the linear system of equations {equation} {eq_linreg_normal_eq_dict} ^{T} {} = ^{T} . {equation} Instead of solving {eq_linreg_normal_eq_dict} directly (via computing the or ), many mehtods use variants of to construct a sequence of increasingly accurate approximations of a solution to {eq_linreg_normal_eq_dict}. These {gdmethod} can be interpreted as a for the following re-formulation of {eq_linreg_normal_eq_dict}, ({I} - ^{T}) {} + ^{T} = {}, { with some invertible } . This equation is solved by a if and only if this also solves {eq_linreg_normal_eq_dict}. The optimality condition {eq_linreg_normal_eq_dict} is also useful for the study of the of linear . Ideally, we would like the solutions of {eq_linreg_normal_eq_dict} to be insensitive to small perturbations of the . We can capture these perturbations via a perturbed and perturbed . Here, and represent small perturbations to the {featurevec} and {label} of the {datapoint} in the original . perturbation theory allows to evaluate how much the solutions of the perturbed linear problem {GolubVanLoanBook} {}^{T} {} {} = {}^{T} {} deviate from the solutions of the original linear problem. \\\\ See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 279,
    "label": "ridge regression",
    "title": "Consider a problem where the goal is to learn a for predicting the numeric of a based on its . Ridge learns the {parameter} by minimizing the penalized average . The average is measured on a set of {labeled datapoint} (i.e., the ) {^{(1)}}{^{(1)}}, \\,, \\,{^{()}}{^{()}}. The penalty term is the scaled squared Euclidean with a . The purpose of the penalty term is , i.e., to prevent in the , where the number of {feature} exceeds the number of {datapoint} in the . For of a , adding to the average is equivalent to computing the average on an augmented . {figure}[H] {center} {tikzpicture}[scale = 1] (0,0.5) -- (7.7,0.5) node[right] { }; (0.5,0) -- (0.5,4.2) node[above] { }; plot ({},{0.4 + 2.0}) ; at (6.7,4.5) {}; (l1) at (1.2, 2.48); (l2) at (1.4, 2.56); (l3) at (1.7, 2.68); (l4) at (2.2, 2.2*0.4+2.0); (l5) at (2.4, 2.4*0.4+2.0); (l6) at (2.7, 2.7*0.4+2.0); (l7) at (3.9, 3.9*0.4+2.0); (l8) at (4.2, 4.2*0.4+2.0); (l9) at (4.5, 4.5*0.4+2.0); (n1) at (1.2, 1.8); (n2) at (1.4, 1.8); (n3) at (1.7, 1.8); (n4) at (2.2, 3.8); (n5) at (2.4, 3.8); (n6) at (2.7, 3.8); (n7) at (3.9, 2.6); (n8) at (4.2, 2.6); (n9) at (4.5, 2.6); at (n1) [circle,draw,fill=red,minimum size=6pt,scale=0.6, name=c1] {}; at (n2) [circle,draw,fill=blue,minimum size=6pt, scale=0.6, name=c2] {}; at (n3) [circle,draw,fill=red,minimum size=6pt,scale=0.6, name=c3] {}; at (n4) [circle,draw,fill=red,minimum size=12pt, scale=0.6, name=c4] {}; at (n5) [circle,draw,fill=blue,minimum size=12pt,scale=0.6, name=c5] {}; at (n6) [circle,draw,fill=red,minimum size=12pt, scale=0.6, name=c6] {}; at (n7) [circle,draw,fill=red,minimum size=12pt,scale=0.6, name=c7] {}; at (n8) [circle,draw,fill=blue,minimum size=12pt, scale=0.6, name=c8] {}; at (n9) [circle,draw,fill=red,minimum size=12pt, scale=0.6, name=c9] {}; [<->] () -- () node [pos=0.4, below] {}; ; (l1) -- (c1); (l2) -- (c2); (l3) -- (c3); (l4) -- (c4); (l5) -- (c5); (l6) -- (c6); (l7) -- (c7); (l8) -- (c8); (l9) -- (c9); (6.2, 3.7) circle (0.1cm) node [anchor=west,black,xshift=0.1cm] {original }; (6.2, 3.2) circle (0.1cm) node [anchor=west,black,xshift=0.1cm] {augmented }; {tikzpicture} {For a , adding the penalty term to the in is equivalent to on an augmented {fig_ridge_regression_dict} } {center} {figure} This augmented is obtained by replacing each in the original by the of infinitely many {rv} whose is centered around . \\\\ See also: , , , .",
    "color": "lightgreen"
  },
  {
    "id": 280,
    "label": "expectation",
    "title": "Consider a numeric that we interpret as the of a with a . The expectation of is defined as the integral . Note that the expectation is only defined if this integral exists, i.e., if the is integrable , , . Fig. {fig_expect_discrete_dict} illustrates the expectation of a scalar discrete that takes on values from a finite set only. {figure}[H] {center} {tikzpicture} {axis}[ ybar, y=5cm, x=2cm, bar width=0.6cm, xlabel={}, clip=false, ylabel={}, y label style={rotate=-90, anchor=west, xshift=-1cm}, xtick={1,2,3,4,5}, ymin=0, ymax=0.6, grid=both, major grid style={gray!20}, tick align=outside, axis line style={black!70}, ] +[ybar, fill=blue!50] coordinates { (1,0.1) (2,0.2) (3,0.4) (4,0.2) (5,0.1) }; at (axis cs:1,0.13) {}; at (axis cs:2,0.23) {}; at (axis cs:3,0.43) {}; at (axis cs:4,0.23) {}; at (axis cs:5,0.13) {}; at (axis cs:3.8,0.53) {}; {axis} {tikzpicture} {center} {-5mm} {The expectation of a discrete is obtained by summing its possible values , weighted by the corresponding . {fig_expect_discrete_dict}} {figure} See also: , , , , .",
    "color": "salmon"
  },
  {
    "id": 281,
    "label": "logistic regression",
    "title": "Logistic learns a linear (or ) to predict a binary based on the numeric of a . The quality of a linear is measured by the average on some {labeled datapoint} (i.e., the ). \\\\ See also: , , , , , , , , , .",
    "color": "orange"
  },
  {
    "id": 282,
    "label": "logistic loss",
    "title": "Consider a characterized by the {feature} and a binary . We use a real-valued to predict the from the {feature} . The logistic incurred by this is defined as {equation} {equ_log_loss_gls_dict} {(,)}{} \\, ( 1 + \\,(- ())). {equation} {figure}[H] {center} {tikzpicture} {axis}[ axis lines=middle, xlabel={}, ylabel={}, xlabel style={at={(axis description cs:1.,0.3)}, anchor=north}, ylabel style={at={(axis description cs:0.5,1.1)}, anchor=center}, xmin=-3.5, xmax=3.5, ymin=-0.5, ymax=2.5, xtick={-3, -2, -1, 0, 1, 2, 3}, ytick={0, 1, 2}, domain=-3:3, samples=100, width=10cm, height=6cm, grid=both, major grid style={line width=.2pt, draw=gray!50}, minor grid style={line width=.1pt, draw=gray!20}, legend pos=south west ] [red, thick] {ln(1 + exp(-x))}; {axis} {tikzpicture} {The logistic incurred by the for a with .} {fig_logloss_dict} {center} {figure} Note that the expression {equ_log_loss_gls_dict} for the logistic applies only for the and when using the thresholding rule {equ_def_threshold_bin_classifier_dict}. \\\\ See also: , , , , , , .",
    "color": "orange"
  },
  {
    "id": 283,
    "label": "hinge loss",
    "title": "Consider a characterized by a and a binary . The hinge incurred by a real-valued is defined as {equation} {equ_hinge_loss_gls_dict} {(,)}{} \\{ 0 , 1 - () \\}. {equation} {figure}[H] {center} {tikzpicture} {axis}[ axis lines=middle, xlabel={}, ylabel={}, xlabel style={at={(axis description cs:1.,0.3)}, anchor=north}, ylabel style={at={(axis description cs:0.5,1.1)}, anchor=center}, xmin=-3.5, xmax=3.5, ymin=-0.5, ymax=2.5, xtick={-3, -2, -1, 0, 1, 2, 3}, ytick={0, 1, 2}, domain=-3:3, samples=100, width=10cm, height=6cm, grid=both, major grid style={line width=.2pt, draw=gray!50}, minor grid style={line width=.1pt, draw=gray!20}, legend pos=south west ] {max(0, 1-x)}; {axis} {tikzpicture} {The hinge incurred by the for a with . A regularized variant of the hinge is used by the .} {fig_hingeloss_dict} {center} {figure} See also: , , .",
    "color": "orange"
  },
  {
    "id": 284,
    "label": "independent and identically distributed assumption (i.i.d.\\ assumption)",
    "title": "The assumption interprets {datapoint} of a as the {realization} of {rv}. \\\\ See also: , , , , .",
    "color": "salmon"
  },
  {
    "id": 285,
    "label": "hypothesis space",
    "title": "A space is a mathematical that characterizes the learning capacity of an method. The goal of such a method is to learn a that maps {feature} of a to a of its . Given a finite amount of computational resources, a practical method typically explores only a restricted set of all possible {map} from the to the . Such a restricted set is referred to as a space underlying the method (see Fig. {fig_hypospace_dict}). For the analysis of a given method, the choice of a space is not unique, i.e., any superset containing all {map} the method can learn is also a valid space. {figure}[H] {center} {tikzpicture}[allow upside down, scale=0.4] [below] at (5,-3) {}; [ultra thick] (5,0) circle (5cm); [ultra thick,fill=black!20] (5,0) circle (1cm); [] at (5,0) {}; {tikzpicture} {center} {The space of an method is a (typically very small) subset of the (typically very large) set of all possible {map} from the into the . {fig_hypospace_dict}} {figure} On the other hand, from an engineering perspective, the space is a design choice for -based methods. This design choice can be guided by the available computational resources and . For instance, if efficient operations are feasible and a roughly linear relation exists between {feature} and {label}, a can be a useful choice for . \\\\ See also: , , , .",
    "color": "lightgreen"
  },
  {
    "id": 286,
    "label": "model",
    "title": "The study and design of methods is often based on a mathematical model . Maybe the most widely used example of a mathematical model for is a . A consists of {map} that are used by an method to predict {label} from the {feature} of {datapoint}. Another important type of mathematical model is a , which consists of {probdist} that describe how {datapoint} are generated. Unless stated otherwise, we use the term model to refer specifically to the underlying an method. We illustrate one example of a and a in Fig. {fig_model_dict}. {figure}[H] {tikzpicture}[scale=1] (-1,0) -- (3,0) node[right] {}; (0,-1) -- (0,3) node[above] {}; (-0.5,0) -- (2.5,2) node[right] {}; (-0.5,1) -- (2.5,1) node[right] {}; (-0.5,2) -- (2.5,0.5) node[right] {}; at (1.5,-1.2) {(a)}; {tikzpicture} {2cm} {tikzpicture}[scale=1] (-1,0) -- (3,0) node[right] {}; (0,-1) -- (0,3) node[above] {}; (1,1) ellipse [x radius=1, y radius=0.5]; (2,2) ellipse [x radius=0.7, y radius=0.3]; at (1,0.3) {}; at (2,2.7) {}; at (1.5,-1.2) {(b)}; {tikzpicture} {Two types of mathematical models used in . (a) A consisting of three {linearmap}. (b) A consisting of over the plane spanned by the and values of a . {fig_model_dict}} {figure} See also: , , .",
    "color": "lightgreen"
  },
  {
    "id": 287,
    "label": "model parameters",
    "title": "The elements of a are specified by quantities that are referred to as {parameter}. In the context of , a consists of maps that are specified by a list of {parameter} . It is often convenient to stack these {parameter} into a . {figure} {tikzpicture}[scale=1] (0,0) ellipse (1.6 and 1.1); at (0,1.5) { }; (-0.6,0.2) circle (1.5pt) node[above left] {}; (0.7,-0.3) circle (1.5pt) node[below right] {}; (6,0) ellipse (1.8 and 1.2); at (6,1.6) { }; (5.4,0.1) circle (1.5pt) node[above left] {}; (6.7,-0.2) circle (1.5pt) node[below right] {}; (-0.6,0.2) .. controls (2,0.9) .. (5.4,0.1); (0.7,-0.3) .. controls (2,-0.9) .. (6.7,-0.2); at (3,1.2) {}; {tikzpicture} {The {parameter} select a well-defined out of the .} {figure} We can think of {parameter} as an identifier for a , similar to how a social security number identifies a person.\\\\ See also: , , , .",
    "color": "lightgreen"
  },
  {
    "id": 288,
    "label": "artificial intelligence (AI)",
    "title": "AI refers to systems that behave rationally in the sense of maximizing a long-term . The -based approach to AI is to train a to predict optimal actions. These {prediction} are computed from observations about the state of the environment. The choice of sets AI applications apart from more basic applications. AI systems rarely have access to a labeled that allows the average to be measured for any possible choice of {modelparam}. Instead, AI systems use observed signals to estimate the incurred by the current choice of {modelparam}. \\\\ See also: , .",
    "color": "lightgreen"
  },
  {
    "id": 289,
    "label": "Markov decision process (MDP)",
    "title": "An MDP is a mathematical structure that can be used to study applications. An MDP formalizes how signals depend on the {prediction} (and corresponding actions) made by an method. Formally, an MDP is a specific type of defined by {itemize} a state space ; an action space (where each action corresponds to a specific made by the method); a transition specifying the over the next state , given the current state and action ; a that assigns a numerical to each state-action pair. {itemize} The defining property of an MDP is the Markov property. That is, the next state and only depend on the current state and action , not on the entire history of interactions. \\\\ See also: , , , , , .",
    "color": "orange"
  },
  {
    "id": 290,
    "label": "clustering error",
    "title": "Consider a method that decomposes a given into {cluster}. The error is a quantitative measure for the usefulness of the {cluster}. Different methods use different choices for the error. {figure} {tikzpicture}[scale=1] (c1) at (0.8,0.7); () rectangle (); at (c1) {}; (c2) at (3.6,1.6); () rectangle (); at (c2) {}; {scope}[shift={(c1)}, xscale=0.9, yscale=0.7, rotate=0] / in {-0.6/-0.4, 0.1/0.9, 0.7/-0.6} { (,) circle (1.5pt); (,) -- (0,0); } {scope} {scope}[shift={(c2)}, xscale=0.8, yscale=0.8, rotate=0] / in {-1.1/-0.5, -0.2/0.6, 0.6/-0.2} { (,) circle (1.5pt); (,) -- (0,0); } {scope} {tikzpicture} {For with numeric {featurevec}, we can use the average squared to the nearest {clustercentroid} as a measure for the error.} {figure} For example, the method measures the error via the average squared between a (its ) and the nearest .",
    "color": "lightblue"
  },
  {
    "id": 291,
    "label": "hard clustering",
    "title": "Hard refers to the task of partitioning a given set of {datapoint} into (a few) non-overlapping {cluster}. This requirement allows to represent a by a subset of {datapoint}, i.e., precisely those belonging to the . In contrast to hard , methods allow for overlapping {cluster} and specify, for each , a numeric to each . Hard is an extreme case of where the {dob} take only two values, indicating either no belonging or full belonging. For {datapoint} characterized by numeric {featurevec} , a widely used hard method is . Any method for numeric {featurevec} can be adapted for non-numerical using methods. One important example of this approach is , where {datapoint} have a similarity structure in the form of an undirected . The nodes of this represent {datapoint} while undirected (possibly weighted) edges represent similarities (and their extend) between {datapoint}. We can then use the entries of the {eigenvector} of the as numeric {feature} for each . \\\\ See also: , , , .",
    "color": "lightblue"
  },
  {
    "id": 292,
    "label": "soft clustering",
    "title": "Soft refers to the task of partitioning a given set of {datapoint} into (a few) overlapping {cluster}. Each is assigned to several different {cluster} with varying {dob}. Soft methods determine the (or soft assignment) for each and each . A principled approach to soft for {datapoint} characterized by numerical {featurevec} is via a such as the . The conditional of a belonging to a specific mixture component is then a natural choice for the . soft methods can be applied to non-numeric by using methods to provide numerical {feature} (such as in ). \\\\ See also: , , , , .",
    "color": "lightblue"
  },
  {
    "id": 293,
    "label": "Kronecker product",
    "title": "The Kronecker product of two {matrix} and is a block denoted by and defined as , \\[ = {pmatrix} a_{11} & & a_{1n} \\\\ & & \\\\ a_{m1} & & a_{mn} {pmatrix} {R}^{mp nq}. \\] The Kronecker product is a special case of the tensor product for {matrix} and is widely used in multivariate statistics, linear algebra, and structured {model}. It satisfies the identity for {vector} and of compatible dimensions. \\\\ See also: , , , .",
    "color": "lightcoral"
  },
  {
    "id": 294,
    "label": "clustering",
    "title": "Clustering methods decompose a given set of {datapoint} into a few subsets, which are referred to as {cluster}. Each consists of {datapoint} that are more similar to each other than to {datapoint} outside the . Different clustering methods use different measures for the similarity between {datapoint} and different forms of representations. The clustering method uses the average of a (i.e., the ) as its representative. A popular method based on represents a by a . \\\\ See also: , , , .",
    "color": "lightblue"
  },
  {
    "id": 295,
    "label": "cluster",
    "title": "A cluster is a subset of {datapoint} that are more similar to each other than to the {datapoint} outside the cluster. The quantitative measure of similarity between {datapoint} is a design choice. If {datapoint} are characterized by Euclidean {featurevec} , we can define the similarity between two {datapoint} via the Euclidean distance between their {featurevec}. An example of such clusters is shown in Fig. {fig:clusters_dict}.\\\\ {figure}[H] {tikzpicture} {compat=1.18} {axis}[ width=10cm, height=8cm, xlabel={}, ylabel={}, title={Clusters of Data Points}, xmin=0, xmax=10, ymin=0, ymax=10, axis lines=left, legend style={at={(0.5,-0.25)}, anchor=north, legend columns=3} ] coordinates { (1,1) (2,1.2) (1.8,2) (2.2,1.5) (1.5,2.5) }; coordinates { (7,8) (8,7.5) (7.5,8.5) (8.2,7.8) (7.7,7) }; coordinates { (5,3) (5.5,3.2) (5.2,2.8) (4.8,3.5) (5.1,3.1) }; {Cluster 1, Cluster 2, Cluster 3} {axis} {tikzpicture} {Illustration of three clusters in a 2-D . Each cluster groups {datapoint} that are more similar to each other than to those in other clusters, based on the Euclidean distance.} {fig:clusters_dict} {figure} See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 296,
    "label": "Huber loss",
    "title": "The Huber unifies the and the . \\\\ See also: , , .",
    "color": "orange"
  },
  {
    "id": 297,
    "label": "support vector machine (SVM)",
    "title": "The SVM is a binary meth\\-od that learns a linear . Thus, like and , it is also an instance of for the . However, the SVM uses a different from the one used in those methods. As illustrated in Fig. {fig_svm_gls_dict}, it aims to maximally separate {datapoint} from the two different classes in the (i.e., margin principle). Maximizing this separation is equivalent to minimizing a regularized variant of the {equ_hinge_loss_gls_dict} , , . {figure}[H] {center} {tikzpicture}[auto,scale=0.8] [thick] (1,2) circle (0.1cm)node[anchor=west] {{0mm}}; [thick] (0,1.6) circle (0.1cm)node[anchor=west] {{0mm}}; [thick] (0,3) circle (0.1cm)node[anchor=west] {{0mm}}; [thick] (2,1) circle (0.1cm)node[anchor=east,above] {{0mm}}; (B) at (-2,0) {support }; (B) to (1.9,1) ; [|<->|,thick] (2.05,0.95) -- (2.75,0.25)node[pos=0.5] {} ; [thick] (1,-1.5) -- (4,1.5) node [right] {} ; [thick] (3,-1.9) rectangle ++(0.1cm,0.1cm) node[anchor=west,above] {{0mm}}; [thick] (4,.-1) rectangle ++(0.1cm,0.1cm) node[anchor=west,above] {{0mm}}; {tikzpicture} {The SVM learns a (or ) with minimal average soft-margin . Minimizing this is equivalent to maximizing the margin between the of and each class of the .} {fig_svm_gls_dict} {center} {figure} The above basic variant of SVM is only useful if the {datapoint} from different categories can be (approximately) linearly separated. For an application where the categories are not derived from a . \\\\ See also: , , , .",
    "color": "orange"
  },
  {
    "id": 298,
    "label": "eigenvalue",
    "title": "We refer to a number as an eigenvalue of a square if there exists a nonzero such that . \\\\ See also: , .",
    "color": "lightblue"
  },
  {
    "id": 299,
    "label": "eigenvector",
    "title": "An eigenvector of a is a nonzero such that with some . \\\\ See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 300,
    "label": "eigenvalue decomposition (EVD)",
    "title": "The EVD for a square is a factorization of the form = {V} { } {V}^{-1}. The columns of the are the {eigenvector} of the . The diagonal contains the {eigenvalue} corresponding to the {eigenvector} . Note that the above decomposition exists only if the is diagonalizable. \\\\ See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 301,
    "label": "singular value decomposition (SVD)",
    "title": "The SVD for a is a factorization of the form = {V} { } {U}\\,^{T} with orthonormal {matrix} and . The is only nonzero along the main diagonal, whose entries are nonnegative and referred to as singular values. \\\\ See also: .",
    "color": "lightblue"
  },
  {
    "id": 302,
    "label": "total variation",
    "title": "See .",
    "color": "lightblue"
  },
  {
    "id": 303,
    "label": "convex clustering",
    "title": "Consider a . learns {vector} by minimizing _{=1}^{} {^{()} - ^{()}}{2}^2 + _{,' } {^{()} - ^{(')}}{p}. Here, denotes the - (for ). It turns out that many of the optimal {vector} coincide. A then consists of those {datapoint} with identical , . \\\\ See also: , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 304,
    "label": "gradient-based method",
    "title": "A -based method is an iterative technique for finding the (or ) of a of the {modelparam} . Such a method constructs a sequence of approximations to an optimal choice for . As the name indicates, a -based method uses the {gradient} of the evaluated during previous iterations to construct new, (hopefully) improved {modelparam}. One important example of a -based method is . \\\\ See also: , , , , .",
    "color": "violet"
  },
  {
    "id": 305,
    "label": "subgradient descent",
    "title": "descent is a of that does not require differentiability of the to be minimized. This generalization is obtained by replacing the concept of a with that of a . Similar to {gradient}, {subgradient} allow us to construct local approximations of an . The might be the viewed as a of the {modelparam} that select a . \\\\ See also: , , , , , , , {modelparam}, .",
    "color": "violet"
  },
  {
    "id": 306,
    "label": "stochastic gradient descent (SGD)",
    "title": "SGD is obtained from by replacing the of the with a approximation. A main application of SGD is to train a parameterized via on a that is either very large or not readily available (e.g., when {datapoint} are stored in a database distributed globally). To evaluate the of the (as a of the {modelparam} ), we need to compute a sum over all {datapoint} in the . We obtain a approximation to the by replacing the sum with a sum over a randomly chosen subset (see Fig. {fig_sgd_approx_dict}). We often refer to these randomly chosen {datapoint} as a . The size is an important of SGD. SGD with is referred to as mini- SGD . {figure}[H] {tikzpicture}[scale=1.5, >=stealth] plot (, {(-1.5)^2 + 1}); at (0.5, 2) {}; plot (, {(-2)^2 + 0.5}); at (3.3, 1.5) {}; {tikzpicture} {SGD for approximates the by replacing the sum over all {datapoint} in the (indexed by ) with a sum over a randomly chosen subset .{fig_sgd_approx_dict}} {figure} See also: , , , , , , , , , , {modelparam}, , .",
    "color": "lightgreen"
  },
  {
    "id": 307,
    "label": "online gradient descent (online GD)",
    "title": "Consider an method that learns {modelparam} from some . The learning process uses {datapoint} that arrive at consecutive time instants . Let us interpret the {datapoint} as copies of a . The of a can then (under mild conditions) be obtained as the limit . We might use this limit as the for learning the {modelparam} . Unfortunately, this limit can only be evaluated if we wait infinitely long in order to collect all {datapoint}. Some applications require methods that learn online, i.e., as soon as a new arrives at time , we update the current {modelparam} . Note that the new contributes the component to the . As its name suggests, online updates via a (projected) such that {equation} {equ_def_ogd_dict} ^{(+1)} {}{^{()} - _{} _{} {^{()}}{}}. {equation} Note that {equ_def_ogd_dict} is a for the current component of the . The update {equ_def_ogd_dict} ignores all previous components , for . It might therefore happen that, compared with , the updated {modelparam} increase the retrospective average . However, for a suitably chosen , online can be shown to be optimal in practically relevant settings. By optimal, we mean that the {modelparam} delivered by online after observing {datapoint} are at least as good as those delivered by any other learning method , . {figure}[H] {center} {tikzpicture}[x=1.5cm,scale=1.5, every node/.style={font=}] (0.5, 0) -- (5.5, 0) node[below] {}; in {1, 2, 3, 4, 5} { (, 0.1) -- (, -0.1) node[below] {}; } / in {1/2.5, 2/1.8, 3/2.3, 4/1.5, 5/2.0} { (, ) circle (2pt) node[above right] {}; } / in {1/1.0, 2/1.6, 3/1.8, 4/2.2, 5/1.9} { (, ) circle (2pt) node[below left] {}; } // in {1/2.5/1.0, 2/1.8/1.6, 3/2.3/2.0, 4/1.5/1.8, 5/2.0/1.9} { (, ) -- (, ); } {tikzpicture} {center} {An instance of online that updates the {modelparam} using the arriving at time . This instance uses the .} {figure} See also: , , , .",
    "color": "violet"
  },
  {
    "id": 308,
    "label": "principal component analysis (PCA)",
    "title": "PCA determines a linear such that the new {feature} allow us to reconstruct the original {feature} with the reconstruction error . \\\\ See also: , , , .",
    "color": "lightblue"
  },
  {
    "id": 309,
    "label": "loss",
    "title": "methods use a to measure the error incurred by applying a specific to a specific . With a slight abuse of notation, we use the term loss for both the itself and the specific value , for a and . \\\\ See also: , .",
    "color": "lightgreen"
  },
  {
    "id": 310,
    "label": "loss function",
    "title": "A is a : {R}_{+}: ( (,), ) {(,)}{}. It assigns a nonnegative real number (i.e., the ) to a pair that consists of a , with {feature} and , and a . The value quantifies the discrepancy between the true and the . Lower (closer to zero) values indicate a smaller discrepancy between and . Fig. {fig_loss_function_gls_dict} depicts a for a given , with {feature} and , as a of the . {figure}[H] {center} {tikzpicture}[scale = 0.7, every axis/.append style={ axis line style={-Latex, thick}, tick style={thick} }] {axis} [axis x line=center, axis y line=center, xlabel={}, xlabel style={below right}, ylabel style={above right}, xtick=, ytick=, xmin=-5, xscale = 1.4, xmax=5, ymin=-0.5, ymax=2.5 ] {ln(1 + exp(-x))}; {0.3*abs(x)}; {0.3*x^2}; {axis} [below] at (10,1) {}; [right] at (4,6) {}; {tikzpicture} {center} {-7mm} {Some for a fixed , with and , and a varying . methods try to find (or learn) a that incurs minimal .} {fig_loss_function_gls_dict} {figure} See also: , , , .",
    "color": "orange"
  },
  {
    "id": 311,
    "label": "decision tree",
    "title": "A decision tree is a flowchart-like representation of a . More formally, a decision tree is a directed containing a root node that reads in the of a . The root node then forwards the to one of its child nodes based on some elementary test on the {feature} . If the receiving child node is not a leaf node, i.e., it has child nodes itself, it represents another test. Based on the test result, the is forwarded to one of its descendants. This testing and forwarding of the is continued until the ends up in a leaf node without any children. See Fig.\\ {fig_decision_tree_dict} for visual illustrations. {figure}[H] {minipage}{.45} {1}{ {tikzpicture} (A) {}; (B) {}; (C) {}; (D) {}; (E) {}; (A) -- (B) node[midway, left] {no}; (A) -- (C) node[midway, right] {yes}; (C) -- (D) node[midway, left] {no}; (C) -- (E) node[midway, right] {yes}; at (0.7,-4.5) { (a)}; {tikzpicture} } {minipage} {15mm} {minipage}{.45} {15mm} {tikzpicture} (-2,2) rectangle (2,-2); {scope} (-0.5,0) circle (1cm); (0.5,0) circle (1cm); (-2,1.5) rectangle (2,-1.5); {scope} (-0.5,0) circle (1cm); (0.5,0) circle (1cm); (-0.5,0) circle [radius=0.025]; [below right, red] at (-0.5,0) {}; [below left, blue] at (-0.7,0) {}; [above left] at (-0.7,1) {}; [left] at (-0.4,0) {}; (0.5,0) circle [radius=0.025]; [right] at (0.6,0) {}; at (0,-3.5) { (b)}; {tikzpicture} {minipage} {(a) A decision tree is a flowchart-like representation of a piecewise constant . Each piece is a . The depicted decision tree can be applied to numeric {featurevec}, i.e., . It is parameterized by the threshold and the {vector} . (b) A decision tree partitions the into {decisionregion}. Each corresponds to a specific leaf node in the decision tree.} {fig_decision_tree_dict} {figure} See also: .",
    "color": "lightblue"
  },
  {
    "id": 312,
    "label": "application programming interface (API)",
    "title": "An API is a formal mechanism that allows software components to interact in a structured and modular way . In the context of , APIs are commonly used to provide access to a trained . Users\u2014whether humans or machines\u2014can submit the of a and receive a corresponding . Suppose a trained is defined as . Through an API, a user can input and receive the output without knowledge of the detailed structure of the or its training. In practice, the is typically deployed on a server connected to the Internet. Clients send requests containing values to the server, which responds with the computed . APIs promote modularity in system design, i.e., one team can develop and train the , while another team handles integration and user interaction. Publishing a trained via an API also offers practical advantages. For instance, the server can centralize computational resources that are required to compute {prediction}. Furthermore, the internal structure of the remains hidden\u2014which is useful for protecting intellectual property or trade secrets. However, APIs are not without . Techniques such as can potentially reconstruct a from its {prediction} using carefully selected {featurevec}. \\\\ See also: , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 313,
    "label": "model inversion",
    "title": "A inversion is a form of on an system. An adversary seeks to infer {sensattr} of individual {datapoint} by exploiting partial access to a trained . This access typically consists of querying the for {prediction} using carefully chosen inputs. Basic inversion techniques have been demonstrated in the context of facial image , where images are reconstructed using the ( of) outputs combined with auxiliary information such as a person\u2019s name (see Fig. {fig_model_inv_dict}). {figure}[H] {center} {tikzpicture}[scale=1.5] (-0.5,0) -- (5.5,0) node[right] {face image }; (0,-0.2) -- (0,2.5) node[above] {name}; plot ({}, {2/(1 + exp(-3*( - 3)))}); {3} {}{2/(1 + exp(-3*( - 3)))} (,0) -- (,); (0,) -- (,); (,) circle (0.1); at (-0.1,) { ``Alexander Jung''}; at (,-0.25) {{assets/AlexanderJung.jpg}}; at (4,2.2) {trained }; {tikzpicture} {center} {Model inversion techniques implemented in the context of facial image classification. {fig_model_inv_dict}} {figure} See also: , , , , , , , , , .",
    "color": "khaki"
  },
  {
    "id": 314,
    "label": "sample size",
    "title": "The number of individual {datapoint} contained in a or . Consider a -based method that uses a with sample size and a with . If the can be well-approximated by the , then the ratio between and can be a useful indicator for the occurence of {MLBasics}. \\\\ See also: , .",
    "color": "lightgreen"
  },
  {
    "id": 315,
    "label": "skip connection",
    "title": "Consider a with neurons that are organized in consecutive {layer}. A skip connection links the output of a neuron in some to the input of a neuron in a non-consecutive .\\\\ See also: , .",
    "color": "khaki"
  },
  {
    "id": 316,
    "label": "random forest",
    "title": "A random forest is a set of different {decisiontree}. Each of these {decisiontree} is obtained by fitting a perturbed copy of the original . \\\\ See also: , .",
    "color": "lightblue"
  },
  {
    "id": 317,
    "label": "gradient descent (GD)",
    "title": "GD is an iterative method for finding the of a . GD generates a sequence of estimates that (ideally) converge to a of . At each iteration , GD refines the current estimate by taking a step in the direction of the steepest descent of a local linear approximation. This direction is given by the negative of the at the current estimate . The resulting update rule is given by {equation} {equ_def_GD_step_dict} ^{(\\!+\\!1)} = ^{()} - f(^{()}) {equation} where is a suitably small . For a suitably choosen , the update typically reduces the value, i.e., . Fig.\\ {fig_basic_GD_step_dict} illustrates a single GD step. {figure}[H] {center} {tikzpicture}[scale=0.9] (-4,0) grid (4,4); plot (, {(1/4)*}); plot (, {2* - 4}); (4,4) -- node[right] {} (4,2); (4,4) -- node[above] {} (1,4); (4,2) -- node[below] {} (3,2) ; (-4.25,0) -- (4.25,0) node[right] {}; (0,-2pt) -- (0,4.25) node[above] {}; (0pt,2pt) -- (0pt,-2pt) node[below] {}; (0pt,2pt) -- (0pt,-2pt) node[below] {}; (0pt,2pt) -- (0pt,-2pt) node[below] {}; / in {1/1, 2/2, 3/3, 4/4} (2pt,0pt) -- (-2pt,0pt) node[left] {}; {tikzpicture} {center} {A single {equ_def_GD_step_dict} toward the minimizer of .} {fig_basic_GD_step_dict} {figure} See also: , , , , .",
    "color": "violet"
  },
  {
    "id": 318,
    "label": "absolute error loss",
    "title": "Consider a with {feature} and numeric . As its name suggests, the absolute error incurred by a is defined as {{}{}}{} = | - ()|. Fig. {fig_abs_err_dict} depicts the absolute error for a fixed with and . It also indicates the values incurred by two different {hypothesis} and . Similar to the , the absolute error is also a of the . However, in contrast to the , the absolute error is , as it is not at the optimal . This property makes -based methods using the absolute error computationally more demanding , . To build intuition, it is useful to consider the two {hypothesis} depicted in Fig. {fig_abs_err_dict}. Just by inspecting the slope of around and , it is impossible to determine whether we are very close to the optimum (at ) or still far away (at ). As a result, any that is based on local approximations of the (such as ) must use a decreasing to avoid overshooting when approaching the optimum. This required decrease in tends to slow down the of the . Besides the increased computational complexity, using absolute error in can be beneficial in the presence of {outlier} in the . In contrast to the , the slope of the absolute error does not increase with increasing error . As a result, the effect of introducing an with large error on the solution of with absolute error is much smaller compared with the effect on the solution of with . {figure}[H] {center} {tikzpicture}[x=3cm,y=1.6cm] {}{0.0} {}{0.6} {}{3.7} {axis}[axis lines=middle,xtick=,ytick=,width=15cm,height=5cm,xmin=-4,xmax=4,ymin=-0.2,ymax=3,domain=-4:4,samples=100,clip=false,enlarge x limits=0.15,enlarge y limits=0.15] {abs(x - )}; coordinates {(, 0)}; at (axis cs:, 0) {}; {}{abs( - )} coordinates {(, )}; at (axis cs:, ) {}; at (axis cs:, 0) {}; coordinates {(, 0) (, )}; {}{abs( - )} coordinates {(, )}; at (axis cs:, 0) {}; coordinates {(, 0) (, )}; at (axis cs:, ) {}; at (axis cs:4,0) {}; {axis} {tikzpicture} {center} {For a with numeric , the absolute error can be used as a to guide the learning of a . {fig_abs_err_dict}} {figure} See also: , , , , , , .",
    "color": "orange"
  },
  {
    "id": 319,
    "label": "device",
    "title": "A physical system that can store and process . In the context of , the term typically refers to a computer capable of reading {datapoint} from different sources and using them to train an . \\\\ See also: , , , .",
    "color": "lightblue"
  },
  {
    "id": 320,
    "label": "Huber regression",
    "title": "Huber refers to -based methods that use the as a measure of the error. Two important special cases of Huber are and . Tuning the threshold of the allows the user to trade the of the against the computational benefits of the . \\\\ See also: , , , .",
    "color": "orange"
  },
  {
    "id": 321,
    "label": "least absolute deviation regression",
    "title": "Least absolute deviation regression is an instance of using the . It is a special case of . {figure}[H] {tikzpicture}[scale=0.7, y=0.5cm, x=0.5cm] {scope} / in { 1/2, 4/3, 7/4 } { (, 0) -- (, ); (, ) circle (2pt); (ptA) at (, ) {}; } (0.5, 3) -- (10.5, 3) node[right] {}; at (7.5, -4) {(a)}; {scope} {scope}[xshift=10cm] / in { 1/2, 4/3, 7/10 } { (, 0) -- (, ); (, ) circle (2pt); (ptB) at (, ) {}; } (0.5, 3) -- (10.5, 3) node[right] {}; at (ptB7) {outlier}; at (7.5, -4) {(b)}; {scope} {tikzpicture} {For the simple , with amounts to computing the . (a) Original . (b) Noisy including an . {fig_lad_dict}} {figure} For the , with is solved by the . Using instead for the same , makes computing the . \\\\ See also: , , .",
    "color": "salmon"
  },
  {
    "id": 322,
    "label": "metric",
    "title": "A metric is a quantitative measure used to compare objects. In mathematics, a metric measures the distance between two points in a space and must follow specific rules, i.e., the distance is always nonnegative, zero only if the points are the same, symmetric, and it satisfies the triangle inequality . In the context of , the term metric refers to a quantitative measure of how well a performs (somewhat similar to a ). Examples include , precision, and the average on a , . The term is typically used in the context of , while the term metric is used in the context of . \\\\ See also: , , .",
    "color": "lightgreen"
  },
  {
    "id": 323,
    "label": "Bayes risk",
    "title": "Consider a for a application where each is interpreted as a . The includes a for the {feature} and of a . The Bayes is the possible that can be achieved by any . Any that achieves the Bayes is referred to as a . \\\\ See also: , , .",
    "color": "salmon"
  },
  {
    "id": 324,
    "label": "Bayes estimator",
    "title": "Consider a with a joint over the {feature} and the of a . For a given , we refer to a as a Bayes estimator if its is the achievable . Note that whether a qualifies as a Bayes estimator depends on the underlying and the choice for the . \\\\ See also: , , .",
    "color": "salmon"
  },
  {
    "id": 325,
    "label": "weights",
    "title": "Consider a parameterized . We use the term weights for numeric {modelparam} that are used to scale {feature} or their transformations in order to compute . A uses weights to compute the linear combination . Weights are also used in {ann} to form linear combinations of {feature} or the outputs of neurons in hidden {layer} (see Fig. {fig_weights_dict}). {figure}[H] {center} {tikzpicture}[neuron/.style={circle, draw, minimum size=1cm}, thick, >=stealth] (h1) at (0, 2) {}; (h2) at (0, 0) {}; (h3) at (0, -2) {}; (outpoint); at ([xshift=0.2cm]outpoint) {}; (h1) -- node[above] {} (outpoint); (h2) -- node[above] {} (outpoint); (h3) -- node[below] {} (outpoint); {tikzpicture} {center} {A section of an that contains a hidden with outputs (or {activation}) ,, and . These outputs are combined linearly to compute , which can be used either as output of the or as input to another .{fig_weights_dict}} {figure} See also: , {modelparam}, , , , , .",
    "color": "khaki"
  },
  {
    "id": 326,
    "label": "probability distribution",
    "title": "To analyze methods, it can be useful to interpret {datapoint} as {realization} of a . The typical properties of such {datapoint} are then governed by the distribution of this . The distribution of a binary is fully specified by the {probability} and . The distribution of a real-valued might be specified by a such that . In the most general case, a distribution is defined by a measure , . \\\\ See also: , , , , .",
    "color": "salmon"
  },
  {
    "id": 327,
    "label": "probability density function (pdf)",
    "title": "The pdf of a continuous real-valued allows to compute the (of the ) via a {BertsekasProb} { {B}} = _{{B}} {}{} d . This definition extends naturally to a (continuous) vector-valued as the is defined for with any . \\\\ See also: , , , , .",
    "color": "salmon"
  },
  {
    "id": 328,
    "label": "parameter",
    "title": "The parameter of an is a tunable (i.e., learnable or adjustable) quantity that allows us to choose between different {map}. For example, the consists of all {map} with a particular choice for the parameters . Another example of a parameter is the assigned to a connection between two neurons of an . \\\\ See also: , , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 329,
    "label": "law of large numbers",
    "title": "The law of large numbers refers to the of the average of an increasing (large) number of {rv} to the of their common . Different instances of the law of large numbers are obtained by using different notions of . \\\\ See also: , , , , .",
    "color": "salmon"
  },
  {
    "id": 330,
    "label": "stopping criterion",
    "title": "Many methods use iterative {algorithm} that construct a sequence of {modelparam} in order to minimize the . For example, {gdmethod} iteratively update the {parameter} of a parametric , such as a or a . Given a finite amount of computational resources, we need to stop updating the {parameter} after a finite number of iterations. A stopping criterion is any well-defined condition for deciding when to stop updating. \\\\ See also: , .",
    "color": "violet"
  },
  {
    "id": 331,
    "label": "Jacobi method",
    "title": "The Jacobi method is an for solving systems of linear equations (i.e., a linear system) of the form . Here, is a square with nonzero main diagonal entries. The method constructs a sequence by updating each entry of according to \\[ x_i^{(+1)} = {1}{a_{ii}} ( b_i - _{j i} a_{ij} x_j^{()} ). \\] Note that all entries are updated simultaneously. The above iteration converges to a solution, i.e., , under certain conditions on the , e.g., being strictly diagonally dominant or symmetric positive definite , , . Jacobi-type methods are appealing for large linear systems due to their parallelizable structure . We can interpret the Jacobi method as a . Indeed, using the decomposition , with being the diagonal of , allows us to rewrite the linear equation as a fixed-point equation \\[ {x} = {^{-1}({b} - {x})}_{ } \\] which leads to the iteration . \\\\ As an example, for the linear equation \\[ = {pmatrix} a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} {pmatrix}, {b} = {pmatrix} b_1 \\\\ b_2 \\\\ b_3 {pmatrix} \\] the Jacobi method updates each component of \\( {x} \\) as follows: \\[ {aligned} x_1^{(k+1)} &= {1}{a_{11}} ( b_1 - a_{12} x_2^{(k)} - a_{13} x_3^{(k)} ); \\\\ x_2^{(k+1)} &= {1}{a_{22}} ( b_2 - a_{21} x_1^{(k)} - a_{23} x_3^{(k)} ); \\\\ x_3^{(k+1)} &= {1}{a_{33}} ( b_3 - a_{31} x_1^{(k)} - a_{32} x_2^{(k)} ). {aligned} \\] See also: , , , .",
    "color": "violet"
  },
  {
    "id": 332,
    "label": "R\\'enyi divergence",
    "title": "The R\\'enyi divergence measures the (dis)similarity between two {probdist} . \\\\ See also: .",
    "color": "salmon"
  },
  {
    "id": 333,
    "label": "non-smooth",
    "title": "We refer to a as non-smooth if it is not . \\\\ See also: , .",
    "color": "orange"
  },
  {
    "id": 334,
    "label": "convex",
    "title": "A subset of the is referred to as convex if it contains the line segment between any two points in that set. A is convex if its is a convex set . We illustrate one example of a convex set and a convex in Fig. {fig_convex_set_function_dict}. {figure}[H] {center} {tikzpicture} (-3,0) ellipse (2 and 1.2); (-3,0) ellipse (2 and 1.2); (-3.7,0.2) circle (2pt) node[left] {}; (-2.3,-0.5) circle (2pt) node[right] {}; (-3.7,0.2) -- (-2.3,-0.5); at (-1.2,-1.0) {}; at (-3,-2.4) {(a)}; {scope}[shift={(5,-1)}] plot ({}, {0.5*}); plot[domain=-1.5:1.5, smooth] ({}, {0.5*}) -- (2, {0.5*2*2}) -- (-2, {0.5*2*2}) -- cycle; at (0,-0.4) {}; at (0,-1.4) {(b)}; {scope} {tikzpicture} {-8mm} {center} {(a) A convex set . (b) A convex .{fig_convex_set_function_dict}} {figure} See also: , , .",
    "color": "orange"
  },
  {
    "id": 335,
    "label": "smooth",
    "title": "A real-valued is smooth if it is and its is continuous at all , . A smooth is referred to as -smooth if the is Lipschitz continuous with Lipschitz constant , i.e., \\| f() - f(') \\| \\| - ' \\| {, for any } ,' {R}^{}. The constant quantifies the smoothness of the : the smaller the , the smoother is. {optproblem} with a smooth can be solved effectively by {gdmethod}. Indeed, {gdmethod} approximate the locally around a current choice using its . This approximation works well if the does not change too rapidly. We can make this informal claim precise by studying the effect of a single with (see Fig. {fig_gd_smooth_dict}). {figure}[H] {center} {tikzpicture}[scale=0.8, x=0.6cm,y=0.05cm] {0.5} plot ({}, {^2}); (w) at (,{}); (wkplus1) at (4+,{(4+)^2}); (wk) at (8+,{(8+)^2}); (wk) -- +(-2, -{4*(8 + )} ) -- +(1, {2*(8 + )}); (w) -- +(-1, {-{2*}} ) -- +(1, {+{2*}}) node[below] {}; (wk) circle (2pt) node[above left] {} node[below right, xshift=-15pt,yshift=-15pt] {} ; (w) circle (2pt) node[above right] {} ; (wkplus1) circle (2pt) node[below right] {}; (wk) -- () ; (wkplus1) -- () ; () -- () node[midway, right] {}; {tikzpicture} {center} {Consider an that is -smooth. Taking a , with , decreases the objective by at least , , . Note that the becomes larger for smaller . Thus, for smoother {objfunc} (i.e., those with smaller ), we can take larger steps. {fig_gd_smooth_dict}} {figure} See also: , , , .",
    "color": "violet"
  },
  {
    "id": 336,
    "label": "parametric model",
    "title": "A parametric is a mathematical characterized by a finite set of variable quantities called {parameter}. An important example is the consisting, for a given , of all {mvndist} (on ) with some and . In the context of , a parametric defines a parameterized by a finite number of {modelparam}. Each is uniquely identified by a list of {modelparam} (see Fig.~{fig_param_space_dict}). We can stack these {parameter} into a . Two widely used examples of parametric {model} are the and the . The corresponding is typically a subset of . {figure}[H] {center} {tikzpicture} (paramspace) {}; { space }; (theta1) at () {}; {}; (theta2) at () {}; {}; (plotcloud) {}; { }; (plot1start) at () {}; (plot1start) .. controls ++(0.8, 1) and ++(-0.8, -0.8) .. () node[anchor=west] {}; (plot2start) at () {}; (plot2start) .. controls ++(0.8, 0.5) and ++(-0.8, -0.8) .. () node[anchor=west] {}; (theta1) to (); (theta2) to (plot2start); {tikzpicture} {center} {The space of an consists of all feasible choices for the {modelparam}. Each choice for the {modelparam} selects a . {fig_param_model_dict}} {figure} See also: , , {modelparam}.",
    "color": "salmon"
  },
  {
    "id": 337,
    "label": "parameter space",
    "title": "The space of an is the set of all feasible choices for the {modelparam} (see Fig. {fig_param_space_dict}). Many important methods use a that is parameterized by {vector} of the . Two widely used examples of parameterized {model} are {linmodel} and {deepnet}. The space is then often a subset , e.g., all {vector} with a smaller than one. {figure}[H] {center} {tikzpicture} (paramspace) {}; { space }; (theta1) at () {}; {}; (theta2) at () {}; {}; (plotcloud) {}; { }; (plot1start) at () {}; (plot1start) .. controls ++(0.8, 1) and ++(-0.8, -0.8) .. () node[anchor=west] {}; (plot2start) at () {}; (plot2start) .. controls ++(0.8, 0.5) and ++(-0.8, -0.8) .. () node[anchor=west] {}; (theta1) to (); (theta2) to (plot2start); {tikzpicture} {center} {The space of an consists of all feasible choices for the {modelparam}. Each choice for the {modelparam} selects a . {fig_param_space_dict}} {figure} See also: , , {modelparam}.",
    "color": "lightgreen"
  },
  {
    "id": 338,
    "label": "data normalization",
    "title": "normalization refers to transformations applied to the {featurevec} of {datapoint} to improve the method's or . For example, in with {gdmethod} using a fixed , depends on controlling the of {featurevec} in the . A common approach is to normalize {featurevec} such that their does not exceed one {MLBasics}. \\\\ See also: , , .",
    "color": "lightblue"
  },
  {
    "id": 339,
    "label": "data augmentation",
    "title": "augmentation methods add synthetic {datapoint} to an existing set of {datapoint}. These synthetic {datapoint} are obtained by perturbations (e.g., adding noise to physical measurements) or transformations (e.g., rotations of images) of the original {datapoint}. These perturbations and transformations are such that the resulting synthetic {datapoint} should still have the same . As a case in point, a rotated cat image is still a cat image even if their {featurevec} (obtained by stacking pixel color intensities) are very different (see Fig. {fig_symmetry_dataaug_dict}). augmentation can be an efficient form of . {figure}[H] {center} {tikzpicture} {}{0.5} {}{2} plot[smooth, tension=1] coordinates {(0,0) (2,1) (4,0) (6,-1) (8,0)}; at (0,0) {}; plot[smooth, tension=1] coordinates {(0 + ,0 + ) (2 + ,1 + ) (4 + ,0 + ) (6 + ,-1 + ) (8 + ,0 + )}; at (8 + ,0 + ) {}; (2,1) circle (2pt) node[above] {}; (6,-1) circle (2pt) node[above] {}; (2,1) to[out=240, in=240] node[midway, below] {} (6,-1); {tikzpicture} {-11mm} {center} { augmentation exploits intrinsic symmetries of {datapoint} in some . We can represent a symmetry by an operator , parameterized by some number . For example, might represent the effect of rotating a cat image by degrees. A with must have the same as a with .{fig_symmetry_dataaug_dict}} {figure} See also: , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 340,
    "label": "local dataset",
    "title": "The concept of a local is in between the concept of a and a . A local consists of several individual {datapoint} characterized by {feature} and {label}. In contrast to a single used in basic methods, a local is also related to other local {dataset} via different notions of similarity. These similarities might arise from {probmodel} or communication infrastructure and are encoded in the edges of an . \\\\ See also: , , , , , , .",
    "color": "lightblue"
  },
  {
    "id": 341,
    "label": "local model",
    "title": "Consider a collection of {device} that are represented as nodes of an . A local is a assigned to a node . Different nodes can have different {hypospace}, i.e., in general, for different nodes . \\\\ See also: , , , .",
    "color": "lightblue"
  },
  {
    "id": 342,
    "label": "mutual information (MI)",
    "title": "The MI between two {rv} , defined on the same is given by {}{} \\{ {p (,)}{p()p()} \\}. It is a measure of how well we can estimate based solely on . A large value of indicates that can be well predicted solely from . This could be obtained by a learned by an -based method. \\\\ See also: , , , , , .",
    "color": "salmon"
  },
  {
    "id": 343,
    "label": "zero-gradient condition",
    "title": "Consider the unconstrained with a and . {figure}[htbp] {tikzpicture}[scale=1.1,x=2cm] {}{1} {}{0.5} {}{-0.2} {}{2.4} {}{(-)^2 + } plot (,{(-)^2 + }); at (,) {}; (xstar) at (,); (xstar) circle (1.6pt); (,0) -- (xstar) node[below,yshift=-18pt] {}; (0,) -- (xstar); (xstar) ++(-0.7,0) -- ++(1.4,0); [xshift=20pt,yshift=-5pt] at () {}; {tikzpicture} {figure} A necessary and sufficient condition for a to solve this problem is that the is the zero . In other words {BoydConvexBook}, f ( {} ) = {0} f ( {} ) = _{ {R}^{}} f(). By defining the operator , we can rewrite the zero-gradient condition as a fixed-point equation ( {I} - f) {} = {}. Here, denotes the identity operator (i.e., ) and is an arbitrary positive number. \\\\ See also: , , , , , .",
    "color": "violet"
  },
  {
    "id": 344,
    "label": "edge weight",
    "title": "Each edge of an is assigned a nonnegative edge weight . A zero edge weight indicates the absence of an edge between nodes . \\\\ See also: .",
    "color": "lightblue"
  },
  {
    "id": 345,
    "label": "data minimization principle",
    "title": "European protection regulation includes a minimization principle. This principle requires a controller to limit the collection of personal information to what is directly relevant and necessary to accomplish a specified purpose. The should be retained only for as long as necessary to fulfill that purpose {GDPR2016}, . \\\\ See also: .",
    "color": "khaki"
  },
  {
    "id": 346,
    "label": "layer",
    "title": "A is an that consists of consecutive layers, indexed by . The -th layer consists of artificial neurons with the layer width . Each of these artificial neurons evaluates an for a weighted sum of the outputs (or {activation}) of the previous layer . The input to layer is formed from weighted sums of the {feature} of the for which the computes a . The outputs of the neurons in layer are then, in turn, used to form the inputs for the neurons in the next layer. The final (output) layer consists of a single neuron whose output is used as the delivered by the . \\\\ See also: , .",
    "color": "khaki"
  },
  {
    "id": 347,
    "label": "activation",
    "title": "The output of an artificial neuron within an is referred to as its activation. In particular, the activation is obtained by applying a (typically nonlinear) to a weighted sum of its inputs. \\\\ See also: , .",
    "color": "khaki"
  },
  {
    "id": 348,
    "label": "concept activation vector (CAV)",
    "title": "Consider a , consisting of several hidden {layer}, trained to predict the of a from its . One way to explain the behavior of the trained is by using the {activation} of a hidden as a new . We then probe the geometry of the resulting new by applying the to {datapoint} that represent a specific concept . By applying the also to {datapoint} that do not belong to this concept, we can train a binary that distinguishes between concept and non-concept {datapoint} based on the {activation} of the hidden . The resulting is a hyperplane whose normal is the CAV for the concept . \\\\ See also: , , , , .",
    "color": "orange"
  },
  {
    "id": 349,
    "label": "backpropagation",
    "title": "Backpropagation is an for computing the of an that depends on the {modelparam} of an . One example of such an is the average incurred by the on a of {datapoint}. This is a direct application of the chain rule from calculus to efficiently compute partial derivatives of the with respect to the {modelparam}. Backpropagation consists of two consecutive phases, also illustrated in Fig. {fig:backprop_schematic_dict}. The first phase includes the forward pass, where a of {datapoint} is fed into the . The processes the input through its {layer} using its current , ultimately producing a at its output. The of the is compared to the true using a , which quantifies the error. The second phase includes the backward pass (i.e., backpropagation), where the error is backpropagated through the {layer}. The obtained partial derivatives with respect to the {parameter} constitute the , which can be used, in turn, to implement a . {figure}[H] {tikzpicture}[ >=Stealth, neuron/.style={circle,draw,minimum size=9mm,inner sep=0pt}, conn/.style={-Stealth, line width=0.7pt}, grad/.style={Stealth-, dashed, line width=0.7pt}, lbl/.style={font=}, box/.style={draw, rounded corners, inner sep=4pt} ] (x1) at (0,1.8) {}; (x2) at (0,0) {}; (x3) at (0,-1.8){}; (h1) {}; (h2) {}; (h3) {}; (yhat) {}; (loss) {}; in {1,2,3}{ in {1,2,3}{ (x) -- (h); } } in {1,2,3}{ (h) -- (yhat); } (yhat) -- (loss); (yhat) to[bend left=10] node[above, lbl] {} (loss); in {1,2,3}{ (h) to[bend left=10] (yhat); } in {1,2,3}{ in {1,2,3}{ (x) to[bend left=10] (h); } } at () {}; at () {}; (legend) { {tikzpicture}[baseline={(0,0)}] (0,0) -- (1.0,0); at (1.5,0) { forward pass (compute )}; (0,-0.5) -- (1.0,-0.5); at (1.5,-0.5) { backward pass (compute )}; {tikzpicture} }; at () {,\\; ,\\; }; {tikzpicture} {Solid arrows show the forward pass (i.e., flow and calculation), while dashed arrows show the correction flow during the backward pass for updating the {parameter} {}.} {fig:backprop_schematic_dict} {figure} See also: , , , .",
    "color": "khaki"
  },
  {
    "id": 350,
    "label": "metric space",
    "title": "A metric space is a set equipped with a function (referred to as a metric) , that satisfies the following requirements for all : {enumerate} , if and only if , , . {enumerate} Formally, a metric space is a pair that satisfies the above requirements. {figure}[H] {tikzpicture}[>=stealth, x=0.5cm, y=1cm] at (2.5,3.6) { }; at (12.5,3.6) { }; {scope}[shift={(0,0)}] (0,0) -- (5.2,0) node[below right] {}; (0,0) -- (0,3.2) node[left] {}; (X) at (1.1,0.9); (Y) at (3.8,2.1); (X) circle (1.2pt) node[below left] {}; (Y) circle (1.2pt) node[above right] {}; (X) -- (Y) node[midway, below right, xshift=1pt] {}; {scope} {scope}[shift={(9.0,0)}] (A) at (1.0,0.6); (B) at (3.1,0.9); (C) at (2.2,2.6); (D) at (4.8,2.2); (E) at (0.4,2.1); (A)--(B)--(C)--(E)--(A); (C)--(D)--(B); (A) circle (1.2pt) node[below left] {}; (D) circle (1.2pt) node[above right] {}; (A)--(B)--(D); at (5.0,0.7) {}; (B) circle (1.2pt); (C) circle (1.2pt); (E) circle (1.2pt); {scope} {tikzpicture} {Examples of metric spaces: Left: with the as metric. Right: with the shortest-path distance as metric.{fig:metric_space_examples_dict}} {figure} A prominent example of a metric space is the equipped with metric given by the Euclidean distance . Another well-known example of a metric space is an , with the metric defined by the length of the shortest path connecting nodes and . \\\\ See also: , , .",
    "color": "lightcoral"
  },
  {
    "id": 351,
    "label": "Rademacher complexity",
    "title": "Similar to the , the Rademacher complexity is a quantitative measure for the size of a . It is defined via the empirical Rademacher complexity, which is defined for a given as {equation} {R}_() = _{_{1},,_{} } _{ } {1}{} _{=1}^ _ (^{()} ). {eq:def_emprical_rademacher_complexity_hypothesis_dict} {equation} Here, the is taken with respect to the {rv} , which are and taking values in with equal . The Rademacher complexity of is then defined as the of the empirical Rademacher complexity of a random that consists of {rv} , for . See also: , , , , .",
    "color": "salmon"
  },
  {
    "id": 352,
    "label": "penalty term",
    "title": "Consider an -based method that learns {modelparam} by minimizing the average (or ) on a . To avoid , and control the , it is common to augment the with a penalty term . We refer to the resulting modified as . {figure}[H] {center} {tikzpicture}[scale = 1] (0,0.5) -- (7.7,0.5) node[right] { }; (0.5,0) -- (0.5,4.2) node[above] { }; plot ({},{0.4 + 2.0}) ; at (6.7,4.5) {}; (l1) at (1.2, 2.48); (l2) at (1.4, 2.56); (l3) at (1.7, 2.68); (l4) at (2.2, 2.2*0.4+2.0); (l5) at (2.4, 2.4*0.4+2.0); (l6) at (2.7, 2.7*0.4+2.0); (l7) at (3.9, 3.9*0.4+2.0); (l8) at (4.2, 4.2*0.4+2.0); (l9) at (4.5, 4.5*0.4+2.0); (n1) at (1.2, 1.8); (n2) at (1.4, 1.8); (n3) at (1.7, 1.8); (n4) at (2.2, 3.8); (n5) at (2.4, 3.8); (n6) at (2.7, 3.8); (n7) at (3.9, 2.6); (n8) at (4.2, 2.6); (n9) at (4.5, 2.6); at (n1) [circle,draw,fill=red,minimum size=6pt,scale=0.6, name=c1] {}; at (n2) [circle,draw,fill=blue,minimum size=6pt, scale=0.6, name=c2] {}; at (n3) [circle,draw,fill=red,minimum size=6pt,scale=0.6, name=c3] {}; at (n4) [circle,draw,fill=red,minimum size=12pt, scale=0.6, name=c4] {}; at (n5) [circle,draw,fill=blue,minimum size=12pt,scale=0.6, name=c5] {}; at (n6) [circle,draw,fill=red,minimum size=12pt, scale=0.6, name=c6] {}; at (n7) [circle,draw,fill=red,minimum size=12pt,scale=0.6, name=c7] {}; at (n8) [circle,draw,fill=blue,minimum size=12pt, scale=0.6, name=c8] {}; at (n9) [circle,draw,fill=red,minimum size=12pt, scale=0.6, name=c9] {}; [<->] () -- () node [pos=0.4, below] {}; ; (l1) -- (c1); (l2) -- (c2); (l3) -- (c3); (l4) -- (c4); (l5) -- (c5); (l6) -- (c6); (l7) -- (c7); (l8) -- (c8); (l9) -- (c9); (6.2, 3.7) circle (0.1cm) node [black,xshift=2.3cm] {original }; (6.2, 3.2) circle (0.1cm) node [black,xshift=1.3cm] {augmented}; at (4.6,1.2) [minimum size=12pt, font={12}{0}, text=blue] {}; at (7.8,1.2) [minimum size=12pt, font={12}{0}, text=red] {}; {tikzpicture} {Adding a penalty term to the in is equivalent to including perturbations of the during . The parameter controls the extend of the perturbations. {fig_penal_term_dict} } {center} {figure} The penalty term depends only on the {modelparam} but not the . It can be interpreted as the average incurred on perturbed copies of the . In other words, adding a penalty term in can be viewed as a form of . \\\\ See also: , , , , , .",
    "color": "lightgreen"
  },
  {
    "id": 353,
    "label": "sequence",
    "title": "A sequence is an ordered collection of values from a set . For example, a sequence of values from the set could be a = ( , , , , , ). Formally, a sequence is a \\[ a: {N} {A}: a_. \\] We denote a sequence as or . Sometimes we also use the notation . Note that the same value can appear multiple times in the sequence at different positions . Sequences are fundamental for the study of methods, for instance when describing successive iterates of an iterative . We can also use a sequence to represent an infinite = \\{ {^{(1)}}{^{(1)}},{^{(2)}}{^{(2)}}, \\}. \\\\ See also: , .",
    "color": "violet"
  },
  {
    "id": 354,
    "label": "convergence",
    "title": "Consider a with numeric values . This is said to converge to a value if the values become arbitrarily close to for sufficiently large indices , Mathematically speaking, the converges to if \\[ > 0, \\; N {N} : > N |a_{} - a^| < . \\] We denote the convergence of a to by \\[ _{ } a_{} = a^. \\] {figure} {tikzpicture}[x=1.2cm, y=2cm, >=stealth] (0.5,0) -- (6.5,0) node[right] {}; (0.5,0) -- (0.5,1.6) node[above] {}; {0.3} (0, {1-}) rectangle (6.3, {1+}); (0,{1+}) -- (6.3,{1+}) node[right] {}; (0,{1-}) -- (6.3,{1-}) node[right] {}; (0,1) -- (6.3,1) node[right] {}; in {1,...,6} { {}{1 - 0.6^()} (,) circle (2pt); } in {1,2,3} { (,0.02) -- (,-0.02) node[below] {}; } {3} (,0) -- (,1.7); at (,1.7) {}; {tikzpicture} {A real-valued sequence converging to the limit .{fig:convergence_dict}} {figure} The concept of convergence of a real-valued (where ) exstends naturally to a in an aribtrary . Indeed, we just need to replace the absolute difference by the . Note that a can only converge if it is a CITE. However, not every is converging (as this requires the underlying to be complete). \\\\ See also: , , .",
    "color": "violet"
  },
  {
    "id": 355,
    "label": "Johnson--Lindenstrauss (JL) lemma",
    "title": "The describes conditions for the existence of a with such that the pairwise between of a finite is approximately preserved . Consider a with {datapoint} characterized by {featurevec} in . Then, for any that satisfies \\[ ' {4 ()}{^2/2 - ^3/3} { for some } 0 < < 1 \\] there is a such that : {equation} {equ_def_approx_norm_JL_lemma_dict} (1\\!-\\!){^{()}\\!-\\!^{(')}}{2} \\!\\!{(^{()})\\!-\\!(^{(')})}{2} \\!\\!(1\\!+\\!){^{()}\\!-\\!^{(')}}{2} {equation} for all . {figure}[hbtp] {tikzpicture} {scope} (x1) at (0.5,-0.6); (x2) at (2.0,0.9); (x3) at (1.1,0.3); in {x1,x2,x3} () circle (1.7pt); at (x1) { }; at (x2) { }; at (x3) { }; [anchor=east] at (1.2,2.2) {}; {scope} {scope}[xshift=1cm] (2.9,2.2) -- (4.1,2.2) node[midway, above] {}; {scope} {scope}[xshift=2cm] (y1) at (4.7,-0.7); (y2) at (6.1,0.5); (y3) at (5.3,-0.1); in {y1,y2,y3} () circle (1.7pt); at (y1) { }; at (y2) { }; at (y3) { }; [anchor=west] at (6.0,2.2) {}; {scope} {tikzpicture} {The JL lemma offers precise conditions that guarantee the existence of a such that pair-wise {eucliddist} between (the {featurevec} of) {datapoint} are approximately preserved. Roughly speaking, maps neighbouring points in the original to neighbouring points in the new .} {figure} The can be obtained from a random whose entries are Gaussian {rv} . It can be shown that the satisfies {equ_def_approx_norm_JL_lemma_dict} with at least . \\\\ See also: , , , , , .",
    "color": "lightblue"
  }
]);
      const edges = new vis.DataSet([
  {
    "from": 1,
    "to": 10
  },
  {
    "from": 1,
    "to": 9
  },
  {
    "from": 1,
    "to": 279
  },
  {
    "from": 1,
    "to": 173
  },
  {
    "from": 1,
    "to": 224
  },
  {
    "from": 1,
    "to": 269
  },
  {
    "from": 1,
    "to": 287
  },
  {
    "from": 2,
    "to": 177
  },
  {
    "from": 2,
    "to": 27
  },
  {
    "from": 2,
    "to": 77
  },
  {
    "from": 2,
    "to": 178
  },
  {
    "from": 2,
    "to": 329
  },
  {
    "from": 2,
    "to": 55
  },
  {
    "from": 2,
    "to": 179
  },
  {
    "from": 2,
    "to": 172
  },
  {
    "from": 2,
    "to": 193
  },
  {
    "from": 2,
    "to": 306
  },
  {
    "from": 2,
    "to": 92
  },
  {
    "from": 2,
    "to": 89
  },
  {
    "from": 2,
    "to": 24
  },
  {
    "from": 2,
    "to": 185
  },
  {
    "from": 2,
    "to": 353
  },
  {
    "from": 3,
    "to": 18
  },
  {
    "from": 3,
    "to": 5
  },
  {
    "from": 4,
    "to": 18
  },
  {
    "from": 4,
    "to": 17
  },
  {
    "from": 4,
    "to": 3
  },
  {
    "from": 4,
    "to": 5
  },
  {
    "from": 5,
    "to": 18
  },
  {
    "from": 5,
    "to": 4
  },
  {
    "from": 5,
    "to": 17
  },
  {
    "from": 6,
    "to": 10
  },
  {
    "from": 7,
    "to": 10
  },
  {
    "from": 8,
    "to": 10
  },
  {
    "from": 9,
    "to": 10
  },
  {
    "from": 9,
    "to": 13
  },
  {
    "from": 9,
    "to": 278
  },
  {
    "from": 9,
    "to": 1
  },
  {
    "from": 10,
    "to": 278
  },
  {
    "from": 10,
    "to": 18
  },
  {
    "from": 10,
    "to": 16
  },
  {
    "from": 10,
    "to": 173
  },
  {
    "from": 10,
    "to": 185
  },
  {
    "from": 10,
    "to": 169
  },
  {
    "from": 10,
    "to": 171
  },
  {
    "from": 10,
    "to": 65
  },
  {
    "from": 10,
    "to": 5
  },
  {
    "from": 10,
    "to": 287
  },
  {
    "from": 11,
    "to": 173
  },
  {
    "from": 11,
    "to": 55
  },
  {
    "from": 11,
    "to": 12
  },
  {
    "from": 11,
    "to": 179
  },
  {
    "from": 11,
    "to": 189
  },
  {
    "from": 11,
    "to": 187
  },
  {
    "from": 11,
    "to": 140
  },
  {
    "from": 11,
    "to": 185
  },
  {
    "from": 11,
    "to": 286
  },
  {
    "from": 12,
    "to": 169
  },
  {
    "from": 12,
    "to": 171
  },
  {
    "from": 12,
    "to": 173
  },
  {
    "from": 12,
    "to": 253
  },
  {
    "from": 12,
    "to": 11
  },
  {
    "from": 12,
    "to": 185
  },
  {
    "from": 13,
    "to": 10
  },
  {
    "from": 13,
    "to": 27
  },
  {
    "from": 13,
    "to": 298
  },
  {
    "from": 13,
    "to": 9
  },
  {
    "from": 13,
    "to": 17
  },
  {
    "from": 14,
    "to": 27
  },
  {
    "from": 14,
    "to": 10
  },
  {
    "from": 14,
    "to": 15
  },
  {
    "from": 14,
    "to": 99
  },
  {
    "from": 14,
    "to": 334
  },
  {
    "from": 14,
    "to": 168
  },
  {
    "from": 14,
    "to": 335
  },
  {
    "from": 14,
    "to": 188
  },
  {
    "from": 14,
    "to": 155
  },
  {
    "from": 14,
    "to": 26
  },
  {
    "from": 14,
    "to": 156
  },
  {
    "from": 15,
    "to": 27
  },
  {
    "from": 15,
    "to": 322
  },
  {
    "from": 15,
    "to": 248
  },
  {
    "from": 16,
    "to": 28
  },
  {
    "from": 16,
    "to": 27
  },
  {
    "from": 16,
    "to": 10
  },
  {
    "from": 16,
    "to": 65
  },
  {
    "from": 16,
    "to": 55
  },
  {
    "from": 16,
    "to": 17
  },
  {
    "from": 16,
    "to": 3
  },
  {
    "from": 17,
    "to": 18
  },
  {
    "from": 17,
    "to": 55
  },
  {
    "from": 17,
    "to": 248
  },
  {
    "from": 17,
    "to": 27
  },
  {
    "from": 17,
    "to": 28
  },
  {
    "from": 17,
    "to": 16
  },
  {
    "from": 17,
    "to": 3
  },
  {
    "from": 17,
    "to": 222
  },
  {
    "from": 18,
    "to": 17
  },
  {
    "from": 18,
    "to": 248
  },
  {
    "from": 18,
    "to": 55
  },
  {
    "from": 18,
    "to": 285
  },
  {
    "from": 18,
    "to": 178
  },
  {
    "from": 18,
    "to": 65
  },
  {
    "from": 18,
    "to": 16
  },
  {
    "from": 18,
    "to": 173
  },
  {
    "from": 18,
    "to": 177
  },
  {
    "from": 19,
    "to": 55
  },
  {
    "from": 19,
    "to": 306
  },
  {
    "from": 19,
    "to": 118
  },
  {
    "from": 19,
    "to": 94
  },
  {
    "from": 20,
    "to": 19
  },
  {
    "from": 20,
    "to": 178
  },
  {
    "from": 20,
    "to": 40
  },
  {
    "from": 20,
    "to": 137
  },
  {
    "from": 20,
    "to": 306
  },
  {
    "from": 20,
    "to": 177
  },
  {
    "from": 20,
    "to": 118
  },
  {
    "from": 20,
    "to": 94
  },
  {
    "from": 20,
    "to": 117
  },
  {
    "from": 20,
    "to": 131
  },
  {
    "from": 20,
    "to": 353
  },
  {
    "from": 21,
    "to": 27
  },
  {
    "from": 21,
    "to": 177
  },
  {
    "from": 21,
    "to": 326
  },
  {
    "from": 22,
    "to": 118
  },
  {
    "from": 22,
    "to": 177
  },
  {
    "from": 22,
    "to": 77
  },
  {
    "from": 22,
    "to": 27
  },
  {
    "from": 22,
    "to": 201
  },
  {
    "from": 22,
    "to": 23
  },
  {
    "from": 22,
    "to": 94
  },
  {
    "from": 22,
    "to": 15
  },
  {
    "from": 23,
    "to": 177
  },
  {
    "from": 23,
    "to": 327
  },
  {
    "from": 23,
    "to": 22
  },
  {
    "from": 23,
    "to": 95
  },
  {
    "from": 23,
    "to": 256
  },
  {
    "from": 23,
    "to": 118
  },
  {
    "from": 23,
    "to": 94
  },
  {
    "from": 25,
    "to": 27
  },
  {
    "from": 25,
    "to": 26
  },
  {
    "from": 25,
    "to": 28
  },
  {
    "from": 26,
    "to": 27
  },
  {
    "from": 26,
    "to": 25
  },
  {
    "from": 26,
    "to": 28
  },
  {
    "from": 27,
    "to": 25
  },
  {
    "from": 27,
    "to": 26
  },
  {
    "from": 27,
    "to": 171
  },
  {
    "from": 27,
    "to": 271
  },
  {
    "from": 28,
    "to": 27
  },
  {
    "from": 29,
    "to": 55
  },
  {
    "from": 29,
    "to": 185
  },
  {
    "from": 29,
    "to": 107
  },
  {
    "from": 29,
    "to": 94
  },
  {
    "from": 29,
    "to": 27
  },
  {
    "from": 29,
    "to": 193
  },
  {
    "from": 29,
    "to": 286
  },
  {
    "from": 29,
    "to": 17
  },
  {
    "from": 29,
    "to": 108
  },
  {
    "from": 29,
    "to": 139
  },
  {
    "from": 29,
    "to": 328
  },
  {
    "from": 30,
    "to": 55
  },
  {
    "from": 30,
    "to": 29
  },
  {
    "from": 30,
    "to": 172
  },
  {
    "from": 30,
    "to": 346
  },
  {
    "from": 30,
    "to": 108
  },
  {
    "from": 30,
    "to": 31
  },
  {
    "from": 30,
    "to": 107
  },
  {
    "from": 30,
    "to": 286
  },
  {
    "from": 31,
    "to": 172
  },
  {
    "from": 31,
    "to": 107
  },
  {
    "from": 31,
    "to": 163
  },
  {
    "from": 31,
    "to": 304
  },
  {
    "from": 31,
    "to": 353
  },
  {
    "from": 32,
    "to": 55
  },
  {
    "from": 32,
    "to": 172
  },
  {
    "from": 32,
    "to": 30
  },
  {
    "from": 32,
    "to": 108
  },
  {
    "from": 32,
    "to": 107
  },
  {
    "from": 32,
    "to": 169
  },
  {
    "from": 32,
    "to": 171
  },
  {
    "from": 32,
    "to": 175
  },
  {
    "from": 32,
    "to": 179
  },
  {
    "from": 32,
    "to": 185
  },
  {
    "from": 32,
    "to": 195
  },
  {
    "from": 32,
    "to": 286
  },
  {
    "from": 32,
    "to": 287
  },
  {
    "from": 33,
    "to": 185
  },
  {
    "from": 33,
    "to": 171
  },
  {
    "from": 33,
    "to": 108
  },
  {
    "from": 33,
    "to": 172
  },
  {
    "from": 33,
    "to": 169
  },
  {
    "from": 33,
    "to": 32
  },
  {
    "from": 33,
    "to": 195
  },
  {
    "from": 34,
    "to": 197
  },
  {
    "from": 34,
    "to": 25
  },
  {
    "from": 34,
    "to": 26
  },
  {
    "from": 35,
    "to": 130
  },
  {
    "from": 35,
    "to": 34
  },
  {
    "from": 35,
    "to": 271
  },
  {
    "from": 36,
    "to": 334
  },
  {
    "from": 36,
    "to": 34
  },
  {
    "from": 36,
    "to": 248
  },
  {
    "from": 36,
    "to": 197
  },
  {
    "from": 36,
    "to": 43
  },
  {
    "from": 36,
    "to": 35
  },
  {
    "from": 36,
    "to": 26
  },
  {
    "from": 36,
    "to": 27
  },
  {
    "from": 37,
    "to": 35
  },
  {
    "from": 37,
    "to": 155
  },
  {
    "from": 37,
    "to": 197
  },
  {
    "from": 37,
    "to": 156
  },
  {
    "from": 37,
    "to": 14
  },
  {
    "from": 37,
    "to": 10
  },
  {
    "from": 37,
    "to": 188
  },
  {
    "from": 37,
    "to": 27
  },
  {
    "from": 37,
    "to": 354
  },
  {
    "from": 37,
    "to": 317
  },
  {
    "from": 37,
    "to": 24
  },
  {
    "from": 37,
    "to": 310
  },
  {
    "from": 37,
    "to": 26
  },
  {
    "from": 37,
    "to": 45
  },
  {
    "from": 37,
    "to": 171
  },
  {
    "from": 37,
    "to": 304
  },
  {
    "from": 37,
    "to": 335
  },
  {
    "from": 38,
    "to": 18
  },
  {
    "from": 38,
    "to": 248
  },
  {
    "from": 38,
    "to": 87
  },
  {
    "from": 38,
    "to": 123
  },
  {
    "from": 38,
    "to": 266
  },
  {
    "from": 39,
    "to": 67
  },
  {
    "from": 40,
    "to": 117
  },
  {
    "from": 40,
    "to": 94
  },
  {
    "from": 40,
    "to": 228
  },
  {
    "from": 40,
    "to": 77
  },
  {
    "from": 40,
    "to": 177
  },
  {
    "from": 41,
    "to": 55
  },
  {
    "from": 41,
    "to": 207
  },
  {
    "from": 41,
    "to": 42
  },
  {
    "from": 41,
    "to": 136
  },
  {
    "from": 41,
    "to": 209
  },
  {
    "from": 41,
    "to": 173
  },
  {
    "from": 41,
    "to": 319
  },
  {
    "from": 42,
    "to": 41
  },
  {
    "from": 42,
    "to": 55
  },
  {
    "from": 42,
    "to": 286
  },
  {
    "from": 42,
    "to": 313
  },
  {
    "from": 42,
    "to": 136
  },
  {
    "from": 42,
    "to": 85
  },
  {
    "from": 42,
    "to": 80
  },
  {
    "from": 43,
    "to": 27
  },
  {
    "from": 43,
    "to": 117
  },
  {
    "from": 43,
    "to": 334
  },
  {
    "from": 43,
    "to": 26
  },
  {
    "from": 44,
    "to": 10
  },
  {
    "from": 44,
    "to": 57
  },
  {
    "from": 44,
    "to": 170
  },
  {
    "from": 44,
    "to": 185
  },
  {
    "from": 44,
    "to": 166
  },
  {
    "from": 44,
    "to": 17
  },
  {
    "from": 44,
    "to": 286
  },
  {
    "from": 44,
    "to": 163
  },
  {
    "from": 44,
    "to": 104
  },
  {
    "from": 44,
    "to": 214
  },
  {
    "from": 45,
    "to": 46
  },
  {
    "from": 47,
    "to": 226
  },
  {
    "from": 47,
    "to": 183
  },
  {
    "from": 47,
    "to": 122
  },
  {
    "from": 47,
    "to": 160
  },
  {
    "from": 47,
    "to": 341
  },
  {
    "from": 47,
    "to": 28
  },
  {
    "from": 47,
    "to": 70
  },
  {
    "from": 48,
    "to": 226
  },
  {
    "from": 48,
    "to": 129
  },
  {
    "from": 49,
    "to": 226
  },
  {
    "from": 49,
    "to": 306
  },
  {
    "from": 49,
    "to": 130
  },
  {
    "from": 49,
    "to": 341
  },
  {
    "from": 49,
    "to": 24
  },
  {
    "from": 49,
    "to": 287
  },
  {
    "from": 50,
    "to": 226
  },
  {
    "from": 50,
    "to": 129
  },
  {
    "from": 50,
    "to": 122
  },
  {
    "from": 50,
    "to": 66
  },
  {
    "from": 50,
    "to": 304
  },
  {
    "from": 51,
    "to": 226
  },
  {
    "from": 51,
    "to": 129
  },
  {
    "from": 51,
    "to": 122
  },
  {
    "from": 51,
    "to": 66
  },
  {
    "from": 51,
    "to": 304
  },
  {
    "from": 51,
    "to": 306
  },
  {
    "from": 52,
    "to": 185
  },
  {
    "from": 52,
    "to": 196
  },
  {
    "from": 52,
    "to": 226
  },
  {
    "from": 52,
    "to": 59
  },
  {
    "from": 52,
    "to": 169
  },
  {
    "from": 52,
    "to": 340
  },
  {
    "from": 53,
    "to": 127
  },
  {
    "from": 53,
    "to": 79
  },
  {
    "from": 53,
    "to": 161
  },
  {
    "from": 53,
    "to": 286
  },
  {
    "from": 53,
    "to": 55
  },
  {
    "from": 53,
    "to": 278
  },
  {
    "from": 53,
    "to": 10
  },
  {
    "from": 53,
    "to": 172
  },
  {
    "from": 53,
    "to": 211
  },
  {
    "from": 53,
    "to": 308
  },
  {
    "from": 53,
    "to": 355
  },
  {
    "from": 53,
    "to": 169
  },
  {
    "from": 53,
    "to": 185
  },
  {
    "from": 53,
    "to": 195
  },
  {
    "from": 54,
    "to": 193
  },
  {
    "from": 54,
    "to": 286
  },
  {
    "from": 54,
    "to": 160
  },
  {
    "from": 54,
    "to": 184
  },
  {
    "from": 54,
    "to": 186
  },
  {
    "from": 54,
    "to": 179
  },
  {
    "from": 54,
    "to": 189
  },
  {
    "from": 54,
    "to": 139
  },
  {
    "from": 54,
    "to": 323
  },
  {
    "from": 54,
    "to": 55
  },
  {
    "from": 54,
    "to": 187
  },
  {
    "from": 54,
    "to": 140
  },
  {
    "from": 54,
    "to": 201
  },
  {
    "from": 55,
    "to": 171
  },
  {
    "from": 55,
    "to": 185
  },
  {
    "from": 55,
    "to": 160
  },
  {
    "from": 55,
    "to": 285
  },
  {
    "from": 55,
    "to": 286
  },
  {
    "from": 55,
    "to": 310
  },
  {
    "from": 55,
    "to": 193
  },
  {
    "from": 55,
    "to": 172
  },
  {
    "from": 55,
    "to": 309
  },
  {
    "from": 55,
    "to": 169
  },
  {
    "from": 56,
    "to": 133
  },
  {
    "from": 56,
    "to": 160
  },
  {
    "from": 56,
    "to": 170
  },
  {
    "from": 56,
    "to": 185
  },
  {
    "from": 56,
    "to": 163
  },
  {
    "from": 56,
    "to": 310
  },
  {
    "from": 56,
    "to": 55
  },
  {
    "from": 56,
    "to": 287
  },
  {
    "from": 57,
    "to": 55
  },
  {
    "from": 57,
    "to": 169
  },
  {
    "from": 57,
    "to": 28
  },
  {
    "from": 57,
    "to": 185
  },
  {
    "from": 57,
    "to": 166
  },
  {
    "from": 57,
    "to": 285
  },
  {
    "from": 57,
    "to": 308
  },
  {
    "from": 57,
    "to": 24
  },
  {
    "from": 57,
    "to": 173
  },
  {
    "from": 58,
    "to": 55
  },
  {
    "from": 58,
    "to": 28
  },
  {
    "from": 58,
    "to": 308
  },
  {
    "from": 58,
    "to": 193
  },
  {
    "from": 58,
    "to": 309
  },
  {
    "from": 58,
    "to": 170
  },
  {
    "from": 58,
    "to": 57
  },
  {
    "from": 58,
    "to": 53
  },
  {
    "from": 58,
    "to": 17
  },
  {
    "from": 58,
    "to": 65
  },
  {
    "from": 58,
    "to": 195
  },
  {
    "from": 58,
    "to": 286
  },
  {
    "from": 59,
    "to": 226
  },
  {
    "from": 59,
    "to": 173
  },
  {
    "from": 59,
    "to": 319
  },
  {
    "from": 59,
    "to": 340
  },
  {
    "from": 59,
    "to": 172
  },
  {
    "from": 59,
    "to": 92
  },
  {
    "from": 59,
    "to": 169
  },
  {
    "from": 59,
    "to": 170
  },
  {
    "from": 59,
    "to": 171
  },
  {
    "from": 59,
    "to": 185
  },
  {
    "from": 60,
    "to": 55
  },
  {
    "from": 60,
    "to": 286
  },
  {
    "from": 60,
    "to": 190
  },
  {
    "from": 60,
    "to": 160
  },
  {
    "from": 60,
    "to": 16
  },
  {
    "from": 60,
    "to": 179
  },
  {
    "from": 60,
    "to": 63
  },
  {
    "from": 60,
    "to": 85
  },
  {
    "from": 60,
    "to": 199
  },
  {
    "from": 60,
    "to": 64
  },
  {
    "from": 60,
    "to": 28
  },
  {
    "from": 60,
    "to": 126
  },
  {
    "from": 60,
    "to": 163
  },
  {
    "from": 60,
    "to": 171
  },
  {
    "from": 60,
    "to": 280
  },
  {
    "from": 61,
    "to": 173
  },
  {
    "from": 61,
    "to": 138
  },
  {
    "from": 61,
    "to": 325
  },
  {
    "from": 61,
    "to": 346
  },
  {
    "from": 61,
    "to": 62
  },
  {
    "from": 61,
    "to": 271
  },
  {
    "from": 62,
    "to": 173
  },
  {
    "from": 62,
    "to": 185
  },
  {
    "from": 62,
    "to": 286
  },
  {
    "from": 62,
    "to": 310
  },
  {
    "from": 62,
    "to": 193
  },
  {
    "from": 62,
    "to": 197
  },
  {
    "from": 62,
    "to": 149
  },
  {
    "from": 62,
    "to": 171
  },
  {
    "from": 62,
    "to": 101
  },
  {
    "from": 62,
    "to": 169
  },
  {
    "from": 62,
    "to": 94
  },
  {
    "from": 62,
    "to": 61
  },
  {
    "from": 62,
    "to": 162
  },
  {
    "from": 63,
    "to": 55
  },
  {
    "from": 63,
    "to": 286
  },
  {
    "from": 63,
    "to": 190
  },
  {
    "from": 63,
    "to": 172
  },
  {
    "from": 63,
    "to": 22
  },
  {
    "from": 63,
    "to": 85
  },
  {
    "from": 63,
    "to": 199
  },
  {
    "from": 63,
    "to": 94
  },
  {
    "from": 63,
    "to": 163
  },
  {
    "from": 64,
    "to": 286
  },
  {
    "from": 64,
    "to": 160
  },
  {
    "from": 64,
    "to": 170
  },
  {
    "from": 64,
    "to": 185
  },
  {
    "from": 64,
    "to": 163
  },
  {
    "from": 64,
    "to": 126
  },
  {
    "from": 64,
    "to": 193
  },
  {
    "from": 64,
    "to": 179
  },
  {
    "from": 64,
    "to": 171
  },
  {
    "from": 64,
    "to": 311
  },
  {
    "from": 64,
    "to": 138
  },
  {
    "from": 64,
    "to": 65
  },
  {
    "from": 64,
    "to": 26
  },
  {
    "from": 65,
    "to": 55
  },
  {
    "from": 65,
    "to": 170
  },
  {
    "from": 65,
    "to": 286
  },
  {
    "from": 65,
    "to": 285
  },
  {
    "from": 65,
    "to": 163
  },
  {
    "from": 65,
    "to": 75
  },
  {
    "from": 65,
    "to": 74
  },
  {
    "from": 65,
    "to": 100
  },
  {
    "from": 65,
    "to": 127
  },
  {
    "from": 65,
    "to": 60
  },
  {
    "from": 65,
    "to": 334
  },
  {
    "from": 65,
    "to": 24
  },
  {
    "from": 65,
    "to": 138
  },
  {
    "from": 65,
    "to": 214
  },
  {
    "from": 65,
    "to": 346
  },
  {
    "from": 65,
    "to": 311
  },
  {
    "from": 65,
    "to": 155
  },
  {
    "from": 65,
    "to": 16
  },
  {
    "from": 65,
    "to": 156
  },
  {
    "from": 65,
    "to": 90
  },
  {
    "from": 65,
    "to": 85
  },
  {
    "from": 65,
    "to": 28
  },
  {
    "from": 65,
    "to": 64
  },
  {
    "from": 65,
    "to": 15
  },
  {
    "from": 65,
    "to": 26
  },
  {
    "from": 65,
    "to": 35
  },
  {
    "from": 65,
    "to": 169
  },
  {
    "from": 65,
    "to": 185
  },
  {
    "from": 65,
    "to": 243
  },
  {
    "from": 65,
    "to": 271
  },
  {
    "from": 66,
    "to": 155
  },
  {
    "from": 66,
    "to": 27
  },
  {
    "from": 66,
    "to": 17
  },
  {
    "from": 66,
    "to": 156
  },
  {
    "from": 66,
    "to": 212
  },
  {
    "from": 66,
    "to": 99
  },
  {
    "from": 66,
    "to": 201
  },
  {
    "from": 66,
    "to": 328
  },
  {
    "from": 66,
    "to": 213
  },
  {
    "from": 66,
    "to": 334
  },
  {
    "from": 66,
    "to": 68
  },
  {
    "from": 68,
    "to": 334
  },
  {
    "from": 68,
    "to": 27
  },
  {
    "from": 68,
    "to": 17
  },
  {
    "from": 68,
    "to": 201
  },
  {
    "from": 68,
    "to": 66
  },
  {
    "from": 68,
    "to": 335
  },
  {
    "from": 68,
    "to": 212
  },
  {
    "from": 69,
    "to": 334
  },
  {
    "from": 69,
    "to": 27
  },
  {
    "from": 69,
    "to": 68
  },
  {
    "from": 70,
    "to": 112
  },
  {
    "from": 70,
    "to": 219
  },
  {
    "from": 71,
    "to": 94
  },
  {
    "from": 71,
    "to": 17
  },
  {
    "from": 71,
    "to": 95
  },
  {
    "from": 71,
    "to": 256
  },
  {
    "from": 71,
    "to": 326
  },
  {
    "from": 71,
    "to": 327
  },
  {
    "from": 71,
    "to": 177
  },
  {
    "from": 71,
    "to": 72
  },
  {
    "from": 71,
    "to": 23
  },
  {
    "from": 71,
    "to": 82
  },
  {
    "from": 71,
    "to": 3
  },
  {
    "from": 71,
    "to": 170
  },
  {
    "from": 72,
    "to": 17
  },
  {
    "from": 72,
    "to": 228
  },
  {
    "from": 72,
    "to": 71
  },
  {
    "from": 72,
    "to": 82
  },
  {
    "from": 72,
    "to": 177
  },
  {
    "from": 73,
    "to": 193
  },
  {
    "from": 73,
    "to": 35
  },
  {
    "from": 73,
    "to": 317
  },
  {
    "from": 73,
    "to": 105
  },
  {
    "from": 73,
    "to": 179
  },
  {
    "from": 73,
    "to": 197
  },
  {
    "from": 73,
    "to": 186
  },
  {
    "from": 73,
    "to": 199
  },
  {
    "from": 73,
    "to": 286
  },
  {
    "from": 73,
    "to": 304
  },
  {
    "from": 73,
    "to": 285
  },
  {
    "from": 73,
    "to": 353
  },
  {
    "from": 73,
    "to": 79
  },
  {
    "from": 73,
    "to": 66
  },
  {
    "from": 73,
    "to": 287
  },
  {
    "from": 74,
    "to": 55
  },
  {
    "from": 74,
    "to": 326
  },
  {
    "from": 74,
    "to": 94
  },
  {
    "from": 74,
    "to": 172
  },
  {
    "from": 74,
    "to": 271
  },
  {
    "from": 75,
    "to": 55
  },
  {
    "from": 75,
    "to": 193
  },
  {
    "from": 75,
    "to": 66
  },
  {
    "from": 75,
    "to": 317
  },
  {
    "from": 75,
    "to": 287
  },
  {
    "from": 76,
    "to": 309
  },
  {
    "from": 76,
    "to": 104
  },
  {
    "from": 76,
    "to": 163
  },
  {
    "from": 76,
    "to": 171
  },
  {
    "from": 76,
    "to": 185
  },
  {
    "from": 76,
    "to": 169
  },
  {
    "from": 77,
    "to": 234
  },
  {
    "from": 77,
    "to": 2
  },
  {
    "from": 78,
    "to": 55
  },
  {
    "from": 78,
    "to": 193
  },
  {
    "from": 78,
    "to": 160
  },
  {
    "from": 78,
    "to": 24
  },
  {
    "from": 78,
    "to": 105
  },
  {
    "from": 78,
    "to": 179
  },
  {
    "from": 78,
    "to": 127
  },
  {
    "from": 79,
    "to": 55
  },
  {
    "from": 79,
    "to": 193
  },
  {
    "from": 79,
    "to": 160
  },
  {
    "from": 79,
    "to": 24
  },
  {
    "from": 79,
    "to": 105
  },
  {
    "from": 79,
    "to": 179
  },
  {
    "from": 79,
    "to": 309
  },
  {
    "from": 79,
    "to": 201
  },
  {
    "from": 79,
    "to": 187
  },
  {
    "from": 79,
    "to": 202
  },
  {
    "from": 80,
    "to": 172
  },
  {
    "from": 80,
    "to": 55
  },
  {
    "from": 80,
    "to": 345
  },
  {
    "from": 80,
    "to": 135
  },
  {
    "from": 80,
    "to": 63
  },
  {
    "from": 81,
    "to": 82
  },
  {
    "from": 82,
    "to": 177
  },
  {
    "from": 82,
    "to": 327
  },
  {
    "from": 82,
    "to": 83
  },
  {
    "from": 82,
    "to": 95
  },
  {
    "from": 82,
    "to": 97
  },
  {
    "from": 82,
    "to": 326
  },
  {
    "from": 82,
    "to": 17
  },
  {
    "from": 82,
    "to": 256
  },
  {
    "from": 82,
    "to": 228
  },
  {
    "from": 82,
    "to": 10
  },
  {
    "from": 82,
    "to": 71
  },
  {
    "from": 82,
    "to": 20
  },
  {
    "from": 82,
    "to": 55
  },
  {
    "from": 82,
    "to": 45
  },
  {
    "from": 82,
    "to": 118
  },
  {
    "from": 82,
    "to": 23
  },
  {
    "from": 82,
    "to": 84
  },
  {
    "from": 82,
    "to": 94
  },
  {
    "from": 82,
    "to": 26
  },
  {
    "from": 83,
    "to": 82
  },
  {
    "from": 84,
    "to": 71
  },
  {
    "from": 84,
    "to": 95
  },
  {
    "from": 84,
    "to": 27
  },
  {
    "from": 84,
    "to": 258
  },
  {
    "from": 84,
    "to": 253
  },
  {
    "from": 84,
    "to": 118
  },
  {
    "from": 84,
    "to": 82
  },
  {
    "from": 84,
    "to": 163
  },
  {
    "from": 84,
    "to": 177
  },
  {
    "from": 85,
    "to": 75
  },
  {
    "from": 85,
    "to": 74
  },
  {
    "from": 85,
    "to": 55
  },
  {
    "from": 85,
    "to": 288
  },
  {
    "from": 85,
    "to": 90
  },
  {
    "from": 85,
    "to": 172
  },
  {
    "from": 85,
    "to": 135
  },
  {
    "from": 85,
    "to": 171
  },
  {
    "from": 86,
    "to": 309
  },
  {
    "from": 86,
    "to": 163
  },
  {
    "from": 86,
    "to": 160
  },
  {
    "from": 86,
    "to": 171
  },
  {
    "from": 86,
    "to": 185
  },
  {
    "from": 86,
    "to": 169
  },
  {
    "from": 87,
    "to": 248
  },
  {
    "from": 87,
    "to": 17
  },
  {
    "from": 87,
    "to": 24
  },
  {
    "from": 88,
    "to": 193
  },
  {
    "from": 88,
    "to": 286
  },
  {
    "from": 88,
    "to": 337
  },
  {
    "from": 88,
    "to": 197
  },
  {
    "from": 88,
    "to": 335
  },
  {
    "from": 88,
    "to": 317
  },
  {
    "from": 88,
    "to": 66
  },
  {
    "from": 88,
    "to": 87
  },
  {
    "from": 88,
    "to": 287
  },
  {
    "from": 89,
    "to": 55
  },
  {
    "from": 89,
    "to": 173
  },
  {
    "from": 89,
    "to": 179
  },
  {
    "from": 89,
    "to": 193
  },
  {
    "from": 89,
    "to": 93
  },
  {
    "from": 89,
    "to": 326
  },
  {
    "from": 89,
    "to": 136
  },
  {
    "from": 89,
    "to": 185
  },
  {
    "from": 89,
    "to": 94
  },
  {
    "from": 89,
    "to": 177
  },
  {
    "from": 89,
    "to": 42
  },
  {
    "from": 89,
    "to": 102
  },
  {
    "from": 89,
    "to": 163
  },
  {
    "from": 89,
    "to": 271
  },
  {
    "from": 89,
    "to": 287
  },
  {
    "from": 90,
    "to": 85
  },
  {
    "from": 90,
    "to": 55
  },
  {
    "from": 90,
    "to": 185
  },
  {
    "from": 90,
    "to": 163
  },
  {
    "from": 90,
    "to": 286
  },
  {
    "from": 90,
    "to": 91
  },
  {
    "from": 90,
    "to": 193
  },
  {
    "from": 90,
    "to": 179
  },
  {
    "from": 90,
    "to": 41
  },
  {
    "from": 90,
    "to": 169
  },
  {
    "from": 91,
    "to": 55
  },
  {
    "from": 91,
    "to": 173
  },
  {
    "from": 91,
    "to": 193
  },
  {
    "from": 91,
    "to": 179
  },
  {
    "from": 91,
    "to": 309
  },
  {
    "from": 91,
    "to": 286
  },
  {
    "from": 91,
    "to": 201
  },
  {
    "from": 91,
    "to": 202
  },
  {
    "from": 91,
    "to": 172
  },
  {
    "from": 91,
    "to": 326
  },
  {
    "from": 91,
    "to": 241
  },
  {
    "from": 91,
    "to": 90
  },
  {
    "from": 91,
    "to": 24
  },
  {
    "from": 91,
    "to": 28
  },
  {
    "from": 91,
    "to": 163
  },
  {
    "from": 91,
    "to": 271
  },
  {
    "from": 91,
    "to": 287
  },
  {
    "from": 92,
    "to": 55
  },
  {
    "from": 92,
    "to": 173
  },
  {
    "from": 92,
    "to": 163
  },
  {
    "from": 92,
    "to": 185
  },
  {
    "from": 92,
    "to": 171
  },
  {
    "from": 92,
    "to": 136
  },
  {
    "from": 92,
    "to": 28
  },
  {
    "from": 92,
    "to": 169
  },
  {
    "from": 92,
    "to": 54
  },
  {
    "from": 92,
    "to": 271
  },
  {
    "from": 92,
    "to": 287
  },
  {
    "from": 93,
    "to": 55
  },
  {
    "from": 93,
    "to": 173
  },
  {
    "from": 93,
    "to": 169
  },
  {
    "from": 93,
    "to": 185
  },
  {
    "from": 93,
    "to": 94
  },
  {
    "from": 93,
    "to": 172
  },
  {
    "from": 93,
    "to": 342
  },
  {
    "from": 93,
    "to": 89
  },
  {
    "from": 93,
    "to": 42
  },
  {
    "from": 93,
    "to": 80
  },
  {
    "from": 93,
    "to": 163
  },
  {
    "from": 93,
    "to": 271
  },
  {
    "from": 94,
    "to": 286
  },
  {
    "from": 94,
    "to": 326
  },
  {
    "from": 94,
    "to": 260
  },
  {
    "from": 94,
    "to": 185
  },
  {
    "from": 94,
    "to": 177
  },
  {
    "from": 94,
    "to": 328
  },
  {
    "from": 95,
    "to": 177
  },
  {
    "from": 95,
    "to": 248
  },
  {
    "from": 95,
    "to": 280
  },
  {
    "from": 95,
    "to": 326
  },
  {
    "from": 95,
    "to": 173
  },
  {
    "from": 95,
    "to": 127
  },
  {
    "from": 95,
    "to": 34
  },
  {
    "from": 95,
    "to": 193
  },
  {
    "from": 95,
    "to": 86
  },
  {
    "from": 96,
    "to": 177
  },
  {
    "from": 96,
    "to": 77
  },
  {
    "from": 96,
    "to": 173
  },
  {
    "from": 96,
    "to": 34
  },
  {
    "from": 96,
    "to": 193
  },
  {
    "from": 96,
    "to": 318
  },
  {
    "from": 96,
    "to": 95
  },
  {
    "from": 96,
    "to": 94
  },
  {
    "from": 96,
    "to": 185
  },
  {
    "from": 96,
    "to": 238
  },
  {
    "from": 96,
    "to": 90
  },
  {
    "from": 96,
    "to": 321
  },
  {
    "from": 96,
    "to": 26
  },
  {
    "from": 96,
    "to": 328
  },
  {
    "from": 96,
    "to": 335
  },
  {
    "from": 97,
    "to": 177
  },
  {
    "from": 97,
    "to": 280
  },
  {
    "from": 97,
    "to": 17
  },
  {
    "from": 98,
    "to": 160
  },
  {
    "from": 98,
    "to": 27
  },
  {
    "from": 98,
    "to": 173
  },
  {
    "from": 98,
    "to": 322
  },
  {
    "from": 98,
    "to": 170
  },
  {
    "from": 98,
    "to": 185
  },
  {
    "from": 99,
    "to": 322
  },
  {
    "from": 99,
    "to": 117
  },
  {
    "from": 100,
    "to": 55
  },
  {
    "from": 100,
    "to": 285
  },
  {
    "from": 100,
    "to": 173
  },
  {
    "from": 100,
    "to": 228
  },
  {
    "from": 100,
    "to": 193
  },
  {
    "from": 100,
    "to": 177
  },
  {
    "from": 100,
    "to": 94
  },
  {
    "from": 100,
    "to": 145
  },
  {
    "from": 100,
    "to": 185
  },
  {
    "from": 100,
    "to": 287
  },
  {
    "from": 101,
    "to": 171
  },
  {
    "from": 101,
    "to": 185
  },
  {
    "from": 101,
    "to": 169
  },
  {
    "from": 102,
    "to": 169
  },
  {
    "from": 102,
    "to": 185
  },
  {
    "from": 102,
    "to": 80
  },
  {
    "from": 102,
    "to": 89
  },
  {
    "from": 102,
    "to": 214
  },
  {
    "from": 103,
    "to": 10
  },
  {
    "from": 103,
    "to": 298
  },
  {
    "from": 103,
    "to": 55
  },
  {
    "from": 103,
    "to": 278
  },
  {
    "from": 103,
    "to": 224
  },
  {
    "from": 103,
    "to": 179
  },
  {
    "from": 103,
    "to": 304
  },
  {
    "from": 103,
    "to": 169
  },
  {
    "from": 103,
    "to": 185
  },
  {
    "from": 104,
    "to": 160
  },
  {
    "from": 104,
    "to": 28
  },
  {
    "from": 104,
    "to": 171
  },
  {
    "from": 104,
    "to": 162
  },
  {
    "from": 104,
    "to": 27
  },
  {
    "from": 104,
    "to": 163
  },
  {
    "from": 104,
    "to": 101
  },
  {
    "from": 104,
    "to": 243
  },
  {
    "from": 105,
    "to": 127
  },
  {
    "from": 105,
    "to": 160
  },
  {
    "from": 105,
    "to": 173
  },
  {
    "from": 105,
    "to": 309
  },
  {
    "from": 105,
    "to": 185
  },
  {
    "from": 106,
    "to": 117
  },
  {
    "from": 107,
    "to": 108
  },
  {
    "from": 107,
    "to": 185
  },
  {
    "from": 107,
    "to": 169
  },
  {
    "from": 107,
    "to": 170
  },
  {
    "from": 107,
    "to": 353
  },
  {
    "from": 108,
    "to": 55
  },
  {
    "from": 108,
    "to": 101
  },
  {
    "from": 108,
    "to": 29
  },
  {
    "from": 108,
    "to": 107
  },
  {
    "from": 108,
    "to": 286
  },
  {
    "from": 109,
    "to": 111
  },
  {
    "from": 109,
    "to": 110
  },
  {
    "from": 109,
    "to": 70
  },
  {
    "from": 109,
    "to": 353
  },
  {
    "from": 110,
    "to": 111
  },
  {
    "from": 110,
    "to": 55
  },
  {
    "from": 110,
    "to": 311
  },
  {
    "from": 110,
    "to": 286
  },
  {
    "from": 110,
    "to": 353
  },
  {
    "from": 111,
    "to": 117
  },
  {
    "from": 111,
    "to": 286
  },
  {
    "from": 112,
    "to": 117
  },
  {
    "from": 113,
    "to": 27
  },
  {
    "from": 113,
    "to": 230
  },
  {
    "from": 113,
    "to": 114
  },
  {
    "from": 114,
    "to": 27
  },
  {
    "from": 114,
    "to": 113
  },
  {
    "from": 114,
    "to": 26
  },
  {
    "from": 115,
    "to": 177
  },
  {
    "from": 115,
    "to": 327
  },
  {
    "from": 115,
    "to": 326
  },
  {
    "from": 116,
    "to": 117
  },
  {
    "from": 116,
    "to": 45
  },
  {
    "from": 116,
    "to": 325
  },
  {
    "from": 117,
    "to": 28
  },
  {
    "from": 117,
    "to": 325
  },
  {
    "from": 118,
    "to": 55
  },
  {
    "from": 118,
    "to": 172
  },
  {
    "from": 118,
    "to": 163
  },
  {
    "from": 118,
    "to": 286
  },
  {
    "from": 118,
    "to": 171
  },
  {
    "from": 118,
    "to": 185
  },
  {
    "from": 118,
    "to": 77
  },
  {
    "from": 118,
    "to": 94
  },
  {
    "from": 118,
    "to": 127
  },
  {
    "from": 118,
    "to": 22
  },
  {
    "from": 118,
    "to": 97
  },
  {
    "from": 118,
    "to": 126
  },
  {
    "from": 119,
    "to": 55
  },
  {
    "from": 119,
    "to": 94
  },
  {
    "from": 119,
    "to": 19
  },
  {
    "from": 119,
    "to": 120
  },
  {
    "from": 119,
    "to": 286
  },
  {
    "from": 119,
    "to": 177
  },
  {
    "from": 119,
    "to": 95
  },
  {
    "from": 119,
    "to": 172
  },
  {
    "from": 119,
    "to": 118
  },
  {
    "from": 119,
    "to": 153
  },
  {
    "from": 119,
    "to": 121
  },
  {
    "from": 120,
    "to": 118
  },
  {
    "from": 120,
    "to": 326
  },
  {
    "from": 120,
    "to": 55
  },
  {
    "from": 120,
    "to": 56
  },
  {
    "from": 120,
    "to": 153
  },
  {
    "from": 121,
    "to": 55
  },
  {
    "from": 121,
    "to": 193
  },
  {
    "from": 121,
    "to": 309
  },
  {
    "from": 121,
    "to": 173
  },
  {
    "from": 121,
    "to": 179
  },
  {
    "from": 121,
    "to": 127
  },
  {
    "from": 121,
    "to": 160
  },
  {
    "from": 121,
    "to": 94
  },
  {
    "from": 121,
    "to": 77
  },
  {
    "from": 121,
    "to": 197
  },
  {
    "from": 121,
    "to": 206
  },
  {
    "from": 121,
    "to": 119
  },
  {
    "from": 121,
    "to": 35
  },
  {
    "from": 121,
    "to": 304
  },
  {
    "from": 121,
    "to": 26
  },
  {
    "from": 121,
    "to": 287
  },
  {
    "from": 122,
    "to": 226
  },
  {
    "from": 122,
    "to": 117
  },
  {
    "from": 122,
    "to": 340
  },
  {
    "from": 122,
    "to": 341
  },
  {
    "from": 122,
    "to": 148
  },
  {
    "from": 122,
    "to": 309
  },
  {
    "from": 122,
    "to": 47
  },
  {
    "from": 122,
    "to": 319
  },
  {
    "from": 122,
    "to": 287
  },
  {
    "from": 123,
    "to": 27
  },
  {
    "from": 123,
    "to": 17
  },
  {
    "from": 123,
    "to": 18
  },
  {
    "from": 124,
    "to": 123
  },
  {
    "from": 124,
    "to": 248
  },
  {
    "from": 124,
    "to": 17
  },
  {
    "from": 125,
    "to": 157
  },
  {
    "from": 125,
    "to": 17
  },
  {
    "from": 126,
    "to": 135
  },
  {
    "from": 126,
    "to": 55
  },
  {
    "from": 126,
    "to": 169
  },
  {
    "from": 126,
    "to": 185
  },
  {
    "from": 126,
    "to": 163
  },
  {
    "from": 126,
    "to": 286
  },
  {
    "from": 126,
    "to": 170
  },
  {
    "from": 126,
    "to": 64
  },
  {
    "from": 126,
    "to": 155
  },
  {
    "from": 126,
    "to": 156
  },
  {
    "from": 126,
    "to": 27
  },
  {
    "from": 126,
    "to": 101
  },
  {
    "from": 126,
    "to": 26
  },
  {
    "from": 126,
    "to": 28
  },
  {
    "from": 127,
    "to": 160
  },
  {
    "from": 127,
    "to": 171
  },
  {
    "from": 127,
    "to": 185
  },
  {
    "from": 127,
    "to": 163
  },
  {
    "from": 127,
    "to": 310
  },
  {
    "from": 127,
    "to": 228
  },
  {
    "from": 127,
    "to": 177
  },
  {
    "from": 127,
    "to": 284
  },
  {
    "from": 127,
    "to": 309
  },
  {
    "from": 127,
    "to": 326
  },
  {
    "from": 127,
    "to": 169
  },
  {
    "from": 128,
    "to": 347
  },
  {
    "from": 128,
    "to": 27
  },
  {
    "from": 128,
    "to": 325
  },
  {
    "from": 128,
    "to": 159
  },
  {
    "from": 128,
    "to": 271
  },
  {
    "from": 129,
    "to": 130
  },
  {
    "from": 129,
    "to": 319
  },
  {
    "from": 129,
    "to": 226
  },
  {
    "from": 129,
    "to": 286
  },
  {
    "from": 129,
    "to": 234
  },
  {
    "from": 129,
    "to": 271
  },
  {
    "from": 129,
    "to": 353
  },
  {
    "from": 130,
    "to": 304
  },
  {
    "from": 130,
    "to": 278
  },
  {
    "from": 130,
    "to": 179
  },
  {
    "from": 130,
    "to": 9
  },
  {
    "from": 130,
    "to": 10
  },
  {
    "from": 130,
    "to": 247
  },
  {
    "from": 130,
    "to": 65
  },
  {
    "from": 130,
    "to": 66
  },
  {
    "from": 130,
    "to": 286
  },
  {
    "from": 130,
    "to": 19
  },
  {
    "from": 130,
    "to": 28
  },
  {
    "from": 130,
    "to": 271
  },
  {
    "from": 130,
    "to": 287
  },
  {
    "from": 130,
    "to": 353
  },
  {
    "from": 131,
    "to": 19
  },
  {
    "from": 131,
    "to": 130
  },
  {
    "from": 131,
    "to": 306
  },
  {
    "from": 131,
    "to": 156
  },
  {
    "from": 131,
    "to": 197
  },
  {
    "from": 131,
    "to": 2
  },
  {
    "from": 131,
    "to": 185
  },
  {
    "from": 131,
    "to": 20
  },
  {
    "from": 131,
    "to": 35
  },
  {
    "from": 131,
    "to": 304
  },
  {
    "from": 131,
    "to": 353
  },
  {
    "from": 132,
    "to": 181
  },
  {
    "from": 132,
    "to": 55
  },
  {
    "from": 132,
    "to": 286
  },
  {
    "from": 132,
    "to": 173
  },
  {
    "from": 132,
    "to": 172
  },
  {
    "from": 132,
    "to": 130
  },
  {
    "from": 132,
    "to": 163
  },
  {
    "from": 132,
    "to": 133
  },
  {
    "from": 132,
    "to": 195
  },
  {
    "from": 133,
    "to": 55
  },
  {
    "from": 133,
    "to": 172
  },
  {
    "from": 133,
    "to": 24
  },
  {
    "from": 133,
    "to": 45
  },
  {
    "from": 133,
    "to": 253
  },
  {
    "from": 133,
    "to": 160
  },
  {
    "from": 133,
    "to": 185
  },
  {
    "from": 133,
    "to": 307
  },
  {
    "from": 133,
    "to": 134
  },
  {
    "from": 133,
    "to": 287
  },
  {
    "from": 133,
    "to": 353
  },
  {
    "from": 134,
    "to": 130
  },
  {
    "from": 134,
    "to": 172
  },
  {
    "from": 134,
    "to": 118
  },
  {
    "from": 134,
    "to": 185
  },
  {
    "from": 134,
    "to": 55
  },
  {
    "from": 134,
    "to": 307
  },
  {
    "from": 134,
    "to": 133
  },
  {
    "from": 134,
    "to": 271
  },
  {
    "from": 134,
    "to": 287
  },
  {
    "from": 134,
    "to": 353
  },
  {
    "from": 135,
    "to": 85
  },
  {
    "from": 135,
    "to": 55
  },
  {
    "from": 135,
    "to": 63
  },
  {
    "from": 135,
    "to": 288
  },
  {
    "from": 135,
    "to": 286
  },
  {
    "from": 135,
    "to": 281
  },
  {
    "from": 135,
    "to": 101
  },
  {
    "from": 135,
    "to": 54
  },
  {
    "from": 135,
    "to": 163
  },
  {
    "from": 135,
    "to": 311
  },
  {
    "from": 136,
    "to": 55
  },
  {
    "from": 136,
    "to": 160
  },
  {
    "from": 136,
    "to": 28
  },
  {
    "from": 136,
    "to": 171
  },
  {
    "from": 136,
    "to": 185
  },
  {
    "from": 136,
    "to": 169
  },
  {
    "from": 136,
    "to": 271
  },
  {
    "from": 137,
    "to": 286
  },
  {
    "from": 137,
    "to": 117
  },
  {
    "from": 137,
    "to": 295
  },
  {
    "from": 137,
    "to": 77
  },
  {
    "from": 137,
    "to": 171
  },
  {
    "from": 137,
    "to": 70
  },
  {
    "from": 138,
    "to": 55
  },
  {
    "from": 138,
    "to": 286
  },
  {
    "from": 138,
    "to": 346
  },
  {
    "from": 139,
    "to": 55
  },
  {
    "from": 139,
    "to": 160
  },
  {
    "from": 139,
    "to": 286
  },
  {
    "from": 139,
    "to": 309
  },
  {
    "from": 139,
    "to": 190
  },
  {
    "from": 139,
    "to": 172
  },
  {
    "from": 139,
    "to": 94
  },
  {
    "from": 139,
    "to": 24
  },
  {
    "from": 139,
    "to": 127
  },
  {
    "from": 139,
    "to": 285
  },
  {
    "from": 139,
    "to": 323
  },
  {
    "from": 139,
    "to": 324
  },
  {
    "from": 139,
    "to": 171
  },
  {
    "from": 139,
    "to": 185
  },
  {
    "from": 139,
    "to": 310
  },
  {
    "from": 139,
    "to": 326
  },
  {
    "from": 139,
    "to": 71
  },
  {
    "from": 139,
    "to": 86
  },
  {
    "from": 139,
    "to": 95
  },
  {
    "from": 139,
    "to": 97
  },
  {
    "from": 139,
    "to": 169
  },
  {
    "from": 140,
    "to": 187
  },
  {
    "from": 140,
    "to": 202
  },
  {
    "from": 140,
    "to": 193
  },
  {
    "from": 140,
    "to": 55
  },
  {
    "from": 140,
    "to": 173
  },
  {
    "from": 140,
    "to": 189
  },
  {
    "from": 140,
    "to": 179
  },
  {
    "from": 140,
    "to": 286
  },
  {
    "from": 140,
    "to": 186
  },
  {
    "from": 141,
    "to": 101
  },
  {
    "from": 141,
    "to": 138
  },
  {
    "from": 142,
    "to": 117
  },
  {
    "from": 142,
    "to": 294
  },
  {
    "from": 142,
    "to": 344
  },
  {
    "from": 142,
    "to": 185
  },
  {
    "from": 142,
    "to": 295
  },
  {
    "from": 143,
    "to": 294
  },
  {
    "from": 143,
    "to": 142
  },
  {
    "from": 143,
    "to": 117
  },
  {
    "from": 143,
    "to": 218
  },
  {
    "from": 143,
    "to": 185
  },
  {
    "from": 143,
    "to": 248
  },
  {
    "from": 143,
    "to": 250
  },
  {
    "from": 143,
    "to": 292
  },
  {
    "from": 143,
    "to": 259
  },
  {
    "from": 143,
    "to": 295
  },
  {
    "from": 143,
    "to": 300
  },
  {
    "from": 143,
    "to": 211
  },
  {
    "from": 143,
    "to": 298
  },
  {
    "from": 143,
    "to": 24
  },
  {
    "from": 143,
    "to": 70
  },
  {
    "from": 143,
    "to": 170
  },
  {
    "from": 143,
    "to": 299
  },
  {
    "from": 144,
    "to": 294
  },
  {
    "from": 144,
    "to": 117
  },
  {
    "from": 144,
    "to": 250
  },
  {
    "from": 144,
    "to": 170
  },
  {
    "from": 145,
    "to": 170
  },
  {
    "from": 145,
    "to": 171
  },
  {
    "from": 145,
    "to": 185
  },
  {
    "from": 145,
    "to": 160
  },
  {
    "from": 145,
    "to": 55
  },
  {
    "from": 145,
    "to": 193
  },
  {
    "from": 145,
    "to": 285
  },
  {
    "from": 145,
    "to": 28
  },
  {
    "from": 145,
    "to": 286
  },
  {
    "from": 145,
    "to": 287
  },
  {
    "from": 146,
    "to": 185
  },
  {
    "from": 146,
    "to": 295
  },
  {
    "from": 146,
    "to": 292
  },
  {
    "from": 146,
    "to": 291
  },
  {
    "from": 147,
    "to": 55
  },
  {
    "from": 147,
    "to": 173
  },
  {
    "from": 147,
    "to": 228
  },
  {
    "from": 147,
    "to": 177
  },
  {
    "from": 147,
    "to": 145
  },
  {
    "from": 147,
    "to": 326
  },
  {
    "from": 147,
    "to": 280
  },
  {
    "from": 147,
    "to": 123
  },
  {
    "from": 147,
    "to": 94
  },
  {
    "from": 147,
    "to": 86
  },
  {
    "from": 147,
    "to": 185
  },
  {
    "from": 147,
    "to": 287
  },
  {
    "from": 148,
    "to": 200
  },
  {
    "from": 148,
    "to": 205
  },
  {
    "from": 148,
    "to": 198
  },
  {
    "from": 148,
    "to": 287
  },
  {
    "from": 149,
    "to": 163
  },
  {
    "from": 149,
    "to": 171
  },
  {
    "from": 149,
    "to": 185
  },
  {
    "from": 149,
    "to": 169
  },
  {
    "from": 150,
    "to": 171
  },
  {
    "from": 150,
    "to": 162
  },
  {
    "from": 150,
    "to": 160
  },
  {
    "from": 150,
    "to": 173
  },
  {
    "from": 150,
    "to": 76
  },
  {
    "from": 150,
    "to": 309
  },
  {
    "from": 150,
    "to": 322
  },
  {
    "from": 150,
    "to": 169
  },
  {
    "from": 150,
    "to": 185
  },
  {
    "from": 151,
    "to": 55
  },
  {
    "from": 151,
    "to": 160
  },
  {
    "from": 151,
    "to": 171
  },
  {
    "from": 151,
    "to": 185
  },
  {
    "from": 151,
    "to": 163
  },
  {
    "from": 151,
    "to": 310
  },
  {
    "from": 151,
    "to": 309
  },
  {
    "from": 151,
    "to": 284
  },
  {
    "from": 151,
    "to": 323
  },
  {
    "from": 151,
    "to": 139
  },
  {
    "from": 151,
    "to": 153
  },
  {
    "from": 151,
    "to": 169
  },
  {
    "from": 152,
    "to": 286
  },
  {
    "from": 152,
    "to": 340
  },
  {
    "from": 152,
    "to": 226
  },
  {
    "from": 153,
    "to": 160
  },
  {
    "from": 153,
    "to": 139
  },
  {
    "from": 153,
    "to": 309
  },
  {
    "from": 153,
    "to": 151
  },
  {
    "from": 154,
    "to": 155
  },
  {
    "from": 154,
    "to": 27
  },
  {
    "from": 154,
    "to": 334
  },
  {
    "from": 155,
    "to": 27
  },
  {
    "from": 155,
    "to": 156
  },
  {
    "from": 156,
    "to": 27
  },
  {
    "from": 156,
    "to": 17
  },
  {
    "from": 157,
    "to": 27
  },
  {
    "from": 157,
    "to": 17
  },
  {
    "from": 158,
    "to": 226
  },
  {
    "from": 158,
    "to": 130
  },
  {
    "from": 158,
    "to": 49
  },
  {
    "from": 158,
    "to": 306
  },
  {
    "from": 158,
    "to": 68
  },
  {
    "from": 158,
    "to": 341
  },
  {
    "from": 158,
    "to": 195
  },
  {
    "from": 158,
    "to": 287
  },
  {
    "from": 159,
    "to": 128
  },
  {
    "from": 160,
    "to": 28
  },
  {
    "from": 160,
    "to": 27
  },
  {
    "from": 160,
    "to": 166
  },
  {
    "from": 160,
    "to": 162
  },
  {
    "from": 160,
    "to": 185
  },
  {
    "from": 160,
    "to": 171
  },
  {
    "from": 160,
    "to": 163
  },
  {
    "from": 160,
    "to": 55
  },
  {
    "from": 160,
    "to": 285
  },
  {
    "from": 160,
    "to": 286
  },
  {
    "from": 160,
    "to": 169
  },
  {
    "from": 161,
    "to": 285
  },
  {
    "from": 161,
    "to": 16
  },
  {
    "from": 161,
    "to": 325
  },
  {
    "from": 161,
    "to": 100
  },
  {
    "from": 161,
    "to": 3
  },
  {
    "from": 161,
    "to": 287
  },
  {
    "from": 161,
    "to": 328
  },
  {
    "from": 162,
    "to": 55
  },
  {
    "from": 162,
    "to": 185
  },
  {
    "from": 162,
    "to": 171
  },
  {
    "from": 162,
    "to": 149
  },
  {
    "from": 162,
    "to": 101
  },
  {
    "from": 162,
    "to": 169
  },
  {
    "from": 163,
    "to": 55
  },
  {
    "from": 163,
    "to": 160
  },
  {
    "from": 163,
    "to": 28
  },
  {
    "from": 163,
    "to": 185
  },
  {
    "from": 163,
    "to": 171
  },
  {
    "from": 163,
    "to": 169
  },
  {
    "from": 164,
    "to": 173
  },
  {
    "from": 164,
    "to": 185
  },
  {
    "from": 164,
    "to": 241
  },
  {
    "from": 165,
    "to": 55
  },
  {
    "from": 165,
    "to": 228
  },
  {
    "from": 165,
    "to": 326
  },
  {
    "from": 165,
    "to": 164
  },
  {
    "from": 165,
    "to": 286
  },
  {
    "from": 165,
    "to": 195
  },
  {
    "from": 165,
    "to": 193
  },
  {
    "from": 165,
    "to": 91
  },
  {
    "from": 165,
    "to": 177
  },
  {
    "from": 165,
    "to": 160
  },
  {
    "from": 165,
    "to": 173
  },
  {
    "from": 165,
    "to": 185
  },
  {
    "from": 166,
    "to": 169
  },
  {
    "from": 166,
    "to": 55
  },
  {
    "from": 166,
    "to": 170
  },
  {
    "from": 166,
    "to": 185
  },
  {
    "from": 166,
    "to": 248
  },
  {
    "from": 166,
    "to": 57
  },
  {
    "from": 166,
    "to": 334
  },
  {
    "from": 166,
    "to": 117
  },
  {
    "from": 167,
    "to": 173
  },
  {
    "from": 167,
    "to": 319
  },
  {
    "from": 167,
    "to": 169
  },
  {
    "from": 167,
    "to": 171
  },
  {
    "from": 167,
    "to": 172
  },
  {
    "from": 167,
    "to": 55
  },
  {
    "from": 167,
    "to": 185
  },
  {
    "from": 168,
    "to": 10
  },
  {
    "from": 168,
    "to": 17
  },
  {
    "from": 168,
    "to": 221
  },
  {
    "from": 168,
    "to": 28
  },
  {
    "from": 168,
    "to": 170
  },
  {
    "from": 169,
    "to": 185
  },
  {
    "from": 169,
    "to": 174
  },
  {
    "from": 170,
    "to": 169
  },
  {
    "from": 170,
    "to": 17
  },
  {
    "from": 170,
    "to": 55
  },
  {
    "from": 170,
    "to": 248
  },
  {
    "from": 170,
    "to": 18
  },
  {
    "from": 170,
    "to": 222
  },
  {
    "from": 171,
    "to": 185
  },
  {
    "from": 171,
    "to": 162
  },
  {
    "from": 171,
    "to": 271
  },
  {
    "from": 171,
    "to": 273
  },
  {
    "from": 171,
    "to": 275
  },
  {
    "from": 172,
    "to": 55
  },
  {
    "from": 172,
    "to": 173
  },
  {
    "from": 172,
    "to": 185
  },
  {
    "from": 172,
    "to": 241
  },
  {
    "from": 173,
    "to": 241
  },
  {
    "from": 173,
    "to": 55
  },
  {
    "from": 173,
    "to": 286
  },
  {
    "from": 173,
    "to": 172
  },
  {
    "from": 173,
    "to": 185
  },
  {
    "from": 173,
    "to": 171
  },
  {
    "from": 173,
    "to": 166
  },
  {
    "from": 173,
    "to": 162
  },
  {
    "from": 173,
    "to": 85
  },
  {
    "from": 173,
    "to": 169
  },
  {
    "from": 173,
    "to": 26
  },
  {
    "from": 173,
    "to": 353
  },
  {
    "from": 174,
    "to": 160
  },
  {
    "from": 174,
    "to": 28
  },
  {
    "from": 174,
    "to": 185
  },
  {
    "from": 174,
    "to": 163
  },
  {
    "from": 174,
    "to": 171
  },
  {
    "from": 174,
    "to": 169
  },
  {
    "from": 175,
    "to": 185
  },
  {
    "from": 175,
    "to": 171
  },
  {
    "from": 176,
    "to": 177
  },
  {
    "from": 176,
    "to": 27
  },
  {
    "from": 176,
    "to": 2
  },
  {
    "from": 176,
    "to": 230
  },
  {
    "from": 176,
    "to": 77
  },
  {
    "from": 176,
    "to": 326
  },
  {
    "from": 176,
    "to": 235
  },
  {
    "from": 177,
    "to": 27
  },
  {
    "from": 177,
    "to": 2
  },
  {
    "from": 177,
    "to": 230
  },
  {
    "from": 177,
    "to": 178
  },
  {
    "from": 177,
    "to": 17
  },
  {
    "from": 177,
    "to": 248
  },
  {
    "from": 177,
    "to": 77
  },
  {
    "from": 177,
    "to": 25
  },
  {
    "from": 177,
    "to": 26
  },
  {
    "from": 177,
    "to": 28
  },
  {
    "from": 177,
    "to": 235
  },
  {
    "from": 178,
    "to": 77
  },
  {
    "from": 178,
    "to": 2
  },
  {
    "from": 178,
    "to": 231
  },
  {
    "from": 178,
    "to": 326
  },
  {
    "from": 178,
    "to": 27
  },
  {
    "from": 178,
    "to": 234
  },
  {
    "from": 178,
    "to": 55
  },
  {
    "from": 178,
    "to": 94
  },
  {
    "from": 178,
    "to": 235
  },
  {
    "from": 178,
    "to": 353
  },
  {
    "from": 179,
    "to": 173
  },
  {
    "from": 179,
    "to": 193
  },
  {
    "from": 179,
    "to": 160
  },
  {
    "from": 179,
    "to": 309
  },
  {
    "from": 179,
    "to": 184
  },
  {
    "from": 179,
    "to": 186
  },
  {
    "from": 179,
    "to": 55
  },
  {
    "from": 179,
    "to": 285
  },
  {
    "from": 179,
    "to": 185
  },
  {
    "from": 179,
    "to": 195
  },
  {
    "from": 180,
    "to": 286
  },
  {
    "from": 180,
    "to": 122
  },
  {
    "from": 180,
    "to": 341
  },
  {
    "from": 180,
    "to": 285
  },
  {
    "from": 181,
    "to": 306
  },
  {
    "from": 181,
    "to": 179
  },
  {
    "from": 181,
    "to": 156
  },
  {
    "from": 181,
    "to": 184
  },
  {
    "from": 181,
    "to": 185
  },
  {
    "from": 181,
    "to": 287
  },
  {
    "from": 182,
    "to": 179
  },
  {
    "from": 182,
    "to": 130
  },
  {
    "from": 182,
    "to": 286
  },
  {
    "from": 182,
    "to": 185
  },
  {
    "from": 182,
    "to": 172
  },
  {
    "from": 182,
    "to": 78
  },
  {
    "from": 182,
    "to": 79
  },
  {
    "from": 182,
    "to": 328
  },
  {
    "from": 182,
    "to": 163
  },
  {
    "from": 183,
    "to": 172
  },
  {
    "from": 183,
    "to": 117
  },
  {
    "from": 183,
    "to": 226
  },
  {
    "from": 183,
    "to": 340
  },
  {
    "from": 183,
    "to": 319
  },
  {
    "from": 184,
    "to": 309
  },
  {
    "from": 184,
    "to": 160
  },
  {
    "from": 184,
    "to": 179
  },
  {
    "from": 184,
    "to": 193
  },
  {
    "from": 184,
    "to": 171
  },
  {
    "from": 184,
    "to": 185
  },
  {
    "from": 184,
    "to": 195
  },
  {
    "from": 185,
    "to": 172
  },
  {
    "from": 185,
    "to": 230
  },
  {
    "from": 185,
    "to": 169
  },
  {
    "from": 185,
    "to": 171
  },
  {
    "from": 185,
    "to": 55
  },
  {
    "from": 185,
    "to": 173
  },
  {
    "from": 185,
    "to": 26
  },
  {
    "from": 185,
    "to": 54
  },
  {
    "from": 185,
    "to": 177
  },
  {
    "from": 186,
    "to": 160
  },
  {
    "from": 186,
    "to": 55
  },
  {
    "from": 186,
    "to": 193
  },
  {
    "from": 186,
    "to": 179
  },
  {
    "from": 186,
    "to": 309
  },
  {
    "from": 186,
    "to": 189
  },
  {
    "from": 186,
    "to": 187
  },
  {
    "from": 187,
    "to": 160
  },
  {
    "from": 187,
    "to": 55
  },
  {
    "from": 187,
    "to": 193
  },
  {
    "from": 187,
    "to": 179
  },
  {
    "from": 187,
    "to": 189
  },
  {
    "from": 187,
    "to": 185
  },
  {
    "from": 187,
    "to": 309
  },
  {
    "from": 187,
    "to": 186
  },
  {
    "from": 187,
    "to": 79
  },
  {
    "from": 187,
    "to": 201
  },
  {
    "from": 188,
    "to": 27
  },
  {
    "from": 188,
    "to": 10
  },
  {
    "from": 188,
    "to": 17
  },
  {
    "from": 189,
    "to": 127
  },
  {
    "from": 189,
    "to": 160
  },
  {
    "from": 189,
    "to": 55
  },
  {
    "from": 189,
    "to": 193
  },
  {
    "from": 189,
    "to": 309
  },
  {
    "from": 189,
    "to": 187
  },
  {
    "from": 189,
    "to": 186
  },
  {
    "from": 189,
    "to": 184
  },
  {
    "from": 189,
    "to": 285
  },
  {
    "from": 189,
    "to": 185
  },
  {
    "from": 190,
    "to": 286
  },
  {
    "from": 190,
    "to": 193
  },
  {
    "from": 190,
    "to": 189
  },
  {
    "from": 190,
    "to": 185
  },
  {
    "from": 191,
    "to": 55
  },
  {
    "from": 191,
    "to": 286
  },
  {
    "from": 191,
    "to": 186
  },
  {
    "from": 191,
    "to": 195
  },
  {
    "from": 192,
    "to": 171
  },
  {
    "from": 192,
    "to": 162
  },
  {
    "from": 192,
    "to": 104
  },
  {
    "from": 192,
    "to": 185
  },
  {
    "from": 192,
    "to": 169
  },
  {
    "from": 192,
    "to": 243
  },
  {
    "from": 193,
    "to": 34
  },
  {
    "from": 193,
    "to": 160
  },
  {
    "from": 193,
    "to": 309
  },
  {
    "from": 193,
    "to": 105
  },
  {
    "from": 193,
    "to": 179
  },
  {
    "from": 193,
    "to": 285
  },
  {
    "from": 193,
    "to": 286
  },
  {
    "from": 193,
    "to": 173
  },
  {
    "from": 193,
    "to": 55
  },
  {
    "from": 193,
    "to": 65
  },
  {
    "from": 193,
    "to": 169
  },
  {
    "from": 193,
    "to": 171
  },
  {
    "from": 193,
    "to": 16
  },
  {
    "from": 193,
    "to": 185
  },
  {
    "from": 193,
    "to": 27
  },
  {
    "from": 193,
    "to": 35
  },
  {
    "from": 193,
    "to": 287
  },
  {
    "from": 194,
    "to": 171
  },
  {
    "from": 194,
    "to": 101
  },
  {
    "from": 194,
    "to": 185
  },
  {
    "from": 195,
    "to": 55
  },
  {
    "from": 195,
    "to": 160
  },
  {
    "from": 195,
    "to": 286
  },
  {
    "from": 195,
    "to": 309
  },
  {
    "from": 195,
    "to": 179
  },
  {
    "from": 195,
    "to": 193
  },
  {
    "from": 195,
    "to": 47
  },
  {
    "from": 195,
    "to": 185
  },
  {
    "from": 195,
    "to": 287
  },
  {
    "from": 195,
    "to": 336
  },
  {
    "from": 196,
    "to": 160
  },
  {
    "from": 196,
    "to": 55
  },
  {
    "from": 196,
    "to": 185
  },
  {
    "from": 196,
    "to": 175
  },
  {
    "from": 197,
    "to": 27
  },
  {
    "from": 197,
    "to": 28
  },
  {
    "from": 197,
    "to": 55
  },
  {
    "from": 197,
    "to": 160
  },
  {
    "from": 197,
    "to": 127
  },
  {
    "from": 197,
    "to": 309
  },
  {
    "from": 197,
    "to": 105
  },
  {
    "from": 197,
    "to": 179
  },
  {
    "from": 197,
    "to": 24
  },
  {
    "from": 197,
    "to": 45
  },
  {
    "from": 197,
    "to": 286
  },
  {
    "from": 197,
    "to": 193
  },
  {
    "from": 197,
    "to": 34
  },
  {
    "from": 197,
    "to": 287
  },
  {
    "from": 197,
    "to": 304
  },
  {
    "from": 198,
    "to": 160
  },
  {
    "from": 198,
    "to": 285
  },
  {
    "from": 198,
    "to": 163
  },
  {
    "from": 198,
    "to": 179
  },
  {
    "from": 198,
    "to": 279
  },
  {
    "from": 198,
    "to": 215
  },
  {
    "from": 198,
    "to": 309
  },
  {
    "from": 198,
    "to": 197
  },
  {
    "from": 198,
    "to": 28
  },
  {
    "from": 198,
    "to": 185
  },
  {
    "from": 199,
    "to": 55
  },
  {
    "from": 199,
    "to": 161
  },
  {
    "from": 199,
    "to": 286
  },
  {
    "from": 199,
    "to": 193
  },
  {
    "from": 199,
    "to": 79
  },
  {
    "from": 199,
    "to": 160
  },
  {
    "from": 199,
    "to": 179
  },
  {
    "from": 199,
    "to": 169
  },
  {
    "from": 199,
    "to": 278
  },
  {
    "from": 199,
    "to": 309
  },
  {
    "from": 199,
    "to": 197
  },
  {
    "from": 199,
    "to": 184
  },
  {
    "from": 199,
    "to": 127
  },
  {
    "from": 199,
    "to": 339
  },
  {
    "from": 199,
    "to": 177
  },
  {
    "from": 199,
    "to": 170
  },
  {
    "from": 199,
    "to": 185
  },
  {
    "from": 199,
    "to": 279
  },
  {
    "from": 199,
    "to": 171
  },
  {
    "from": 199,
    "to": 187
  },
  {
    "from": 199,
    "to": 215
  },
  {
    "from": 199,
    "to": 191
  },
  {
    "from": 199,
    "to": 24
  },
  {
    "from": 199,
    "to": 82
  },
  {
    "from": 199,
    "to": 287
  },
  {
    "from": 200,
    "to": 193
  },
  {
    "from": 200,
    "to": 160
  },
  {
    "from": 200,
    "to": 286
  },
  {
    "from": 200,
    "to": 105
  },
  {
    "from": 200,
    "to": 179
  },
  {
    "from": 200,
    "to": 79
  },
  {
    "from": 200,
    "to": 199
  },
  {
    "from": 200,
    "to": 198
  },
  {
    "from": 200,
    "to": 328
  },
  {
    "from": 200,
    "to": 197
  },
  {
    "from": 200,
    "to": 309
  },
  {
    "from": 200,
    "to": 65
  },
  {
    "from": 200,
    "to": 86
  },
  {
    "from": 200,
    "to": 201
  },
  {
    "from": 200,
    "to": 206
  },
  {
    "from": 200,
    "to": 82
  },
  {
    "from": 200,
    "to": 170
  },
  {
    "from": 200,
    "to": 171
  },
  {
    "from": 200,
    "to": 185
  },
  {
    "from": 201,
    "to": 286
  },
  {
    "from": 201,
    "to": 179
  },
  {
    "from": 201,
    "to": 55
  },
  {
    "from": 201,
    "to": 288
  },
  {
    "from": 201,
    "to": 193
  },
  {
    "from": 201,
    "to": 160
  },
  {
    "from": 201,
    "to": 309
  },
  {
    "from": 201,
    "to": 172
  },
  {
    "from": 201,
    "to": 94
  },
  {
    "from": 201,
    "to": 284
  },
  {
    "from": 201,
    "to": 326
  },
  {
    "from": 201,
    "to": 127
  },
  {
    "from": 201,
    "to": 105
  },
  {
    "from": 201,
    "to": 202
  },
  {
    "from": 201,
    "to": 77
  },
  {
    "from": 201,
    "to": 354
  },
  {
    "from": 201,
    "to": 163
  },
  {
    "from": 201,
    "to": 185
  },
  {
    "from": 201,
    "to": 79
  },
  {
    "from": 201,
    "to": 187
  },
  {
    "from": 201,
    "to": 47
  },
  {
    "from": 201,
    "to": 169
  },
  {
    "from": 201,
    "to": 177
  },
  {
    "from": 201,
    "to": 203
  },
  {
    "from": 202,
    "to": 201
  },
  {
    "from": 202,
    "to": 160
  },
  {
    "from": 202,
    "to": 179
  },
  {
    "from": 202,
    "to": 94
  },
  {
    "from": 202,
    "to": 127
  },
  {
    "from": 202,
    "to": 309
  },
  {
    "from": 202,
    "to": 105
  },
  {
    "from": 202,
    "to": 326
  },
  {
    "from": 202,
    "to": 280
  },
  {
    "from": 202,
    "to": 187
  },
  {
    "from": 202,
    "to": 189
  },
  {
    "from": 202,
    "to": 193
  },
  {
    "from": 202,
    "to": 310
  },
  {
    "from": 202,
    "to": 185
  },
  {
    "from": 203,
    "to": 77
  },
  {
    "from": 203,
    "to": 177
  },
  {
    "from": 203,
    "to": 280
  },
  {
    "from": 204,
    "to": 35
  },
  {
    "from": 204,
    "to": 160
  },
  {
    "from": 204,
    "to": 28
  },
  {
    "from": 204,
    "to": 311
  },
  {
    "from": 204,
    "to": 201
  },
  {
    "from": 204,
    "to": 193
  },
  {
    "from": 204,
    "to": 335
  },
  {
    "from": 204,
    "to": 317
  },
  {
    "from": 204,
    "to": 105
  },
  {
    "from": 204,
    "to": 156
  },
  {
    "from": 204,
    "to": 310
  },
  {
    "from": 204,
    "to": 309
  },
  {
    "from": 204,
    "to": 304
  },
  {
    "from": 204,
    "to": 286
  },
  {
    "from": 204,
    "to": 287
  },
  {
    "from": 204,
    "to": 353
  },
  {
    "from": 205,
    "to": 117
  },
  {
    "from": 205,
    "to": 47
  },
  {
    "from": 205,
    "to": 160
  },
  {
    "from": 205,
    "to": 341
  },
  {
    "from": 205,
    "to": 28
  },
  {
    "from": 205,
    "to": 287
  },
  {
    "from": 206,
    "to": 200
  },
  {
    "from": 206,
    "to": 286
  },
  {
    "from": 206,
    "to": 201
  },
  {
    "from": 206,
    "to": 193
  },
  {
    "from": 206,
    "to": 198
  },
  {
    "from": 206,
    "to": 127
  },
  {
    "from": 206,
    "to": 235
  },
  {
    "from": 207,
    "to": 55
  },
  {
    "from": 207,
    "to": 179
  },
  {
    "from": 207,
    "to": 130
  },
  {
    "from": 207,
    "to": 193
  },
  {
    "from": 207,
    "to": 160
  },
  {
    "from": 207,
    "to": 169
  },
  {
    "from": 207,
    "to": 163
  },
  {
    "from": 207,
    "to": 41
  },
  {
    "from": 207,
    "to": 195
  },
  {
    "from": 208,
    "to": 294
  },
  {
    "from": 208,
    "to": 173
  },
  {
    "from": 208,
    "to": 295
  },
  {
    "from": 208,
    "to": 185
  },
  {
    "from": 209,
    "to": 41
  },
  {
    "from": 209,
    "to": 286
  },
  {
    "from": 209,
    "to": 185
  },
  {
    "from": 209,
    "to": 195
  },
  {
    "from": 210,
    "to": 122
  },
  {
    "from": 210,
    "to": 205
  },
  {
    "from": 210,
    "to": 287
  },
  {
    "from": 211,
    "to": 185
  },
  {
    "from": 211,
    "to": 24
  },
  {
    "from": 211,
    "to": 169
  },
  {
    "from": 211,
    "to": 45
  },
  {
    "from": 211,
    "to": 171
  },
  {
    "from": 211,
    "to": 253
  },
  {
    "from": 211,
    "to": 170
  },
  {
    "from": 211,
    "to": 53
  },
  {
    "from": 212,
    "to": 213
  },
  {
    "from": 213,
    "to": 55
  },
  {
    "from": 213,
    "to": 160
  },
  {
    "from": 213,
    "to": 328
  },
  {
    "from": 213,
    "to": 66
  },
  {
    "from": 213,
    "to": 304
  },
  {
    "from": 213,
    "to": 193
  },
  {
    "from": 213,
    "to": 197
  },
  {
    "from": 213,
    "to": 105
  },
  {
    "from": 213,
    "to": 179
  },
  {
    "from": 213,
    "to": 156
  },
  {
    "from": 213,
    "to": 317
  },
  {
    "from": 213,
    "to": 306
  },
  {
    "from": 213,
    "to": 88
  },
  {
    "from": 213,
    "to": 212
  },
  {
    "from": 213,
    "to": 287
  },
  {
    "from": 214,
    "to": 169
  },
  {
    "from": 214,
    "to": 28
  },
  {
    "from": 214,
    "to": 27
  },
  {
    "from": 214,
    "to": 170
  },
  {
    "from": 214,
    "to": 185
  },
  {
    "from": 214,
    "to": 65
  },
  {
    "from": 214,
    "to": 79
  },
  {
    "from": 214,
    "to": 60
  },
  {
    "from": 214,
    "to": 172
  },
  {
    "from": 214,
    "to": 211
  },
  {
    "from": 214,
    "to": 55
  },
  {
    "from": 214,
    "to": 138
  },
  {
    "from": 214,
    "to": 193
  },
  {
    "from": 214,
    "to": 310
  },
  {
    "from": 214,
    "to": 222
  },
  {
    "from": 214,
    "to": 57
  },
  {
    "from": 214,
    "to": 308
  },
  {
    "from": 214,
    "to": 271
  },
  {
    "from": 214,
    "to": 328
  },
  {
    "from": 214,
    "to": 346
  },
  {
    "from": 215,
    "to": 206
  },
  {
    "from": 215,
    "to": 325
  },
  {
    "from": 215,
    "to": 16
  },
  {
    "from": 215,
    "to": 179
  },
  {
    "from": 215,
    "to": 278
  },
  {
    "from": 215,
    "to": 123
  },
  {
    "from": 215,
    "to": 86
  },
  {
    "from": 216,
    "to": 55
  },
  {
    "from": 216,
    "to": 117
  },
  {
    "from": 216,
    "to": 185
  },
  {
    "from": 216,
    "to": 26
  },
  {
    "from": 216,
    "to": 70
  },
  {
    "from": 217,
    "to": 326
  },
  {
    "from": 218,
    "to": 117
  },
  {
    "from": 218,
    "to": 10
  },
  {
    "from": 218,
    "to": 344
  },
  {
    "from": 218,
    "to": 24
  },
  {
    "from": 219,
    "to": 112
  },
  {
    "from": 219,
    "to": 298
  },
  {
    "from": 219,
    "to": 218
  },
  {
    "from": 219,
    "to": 117
  },
  {
    "from": 219,
    "to": 70
  },
  {
    "from": 220,
    "to": 168
  },
  {
    "from": 220,
    "to": 10
  },
  {
    "from": 220,
    "to": 300
  },
  {
    "from": 220,
    "to": 298
  },
  {
    "from": 220,
    "to": 34
  },
  {
    "from": 221,
    "to": 170
  },
  {
    "from": 221,
    "to": 166
  },
  {
    "from": 221,
    "to": 27
  },
  {
    "from": 221,
    "to": 10
  },
  {
    "from": 221,
    "to": 168
  },
  {
    "from": 221,
    "to": 38
  },
  {
    "from": 221,
    "to": 18
  },
  {
    "from": 221,
    "to": 222
  },
  {
    "from": 221,
    "to": 185
  },
  {
    "from": 222,
    "to": 221
  },
  {
    "from": 222,
    "to": 55
  },
  {
    "from": 222,
    "to": 170
  },
  {
    "from": 222,
    "to": 185
  },
  {
    "from": 222,
    "to": 166
  },
  {
    "from": 222,
    "to": 101
  },
  {
    "from": 222,
    "to": 244
  },
  {
    "from": 222,
    "to": 192
  },
  {
    "from": 222,
    "to": 28
  },
  {
    "from": 222,
    "to": 65
  },
  {
    "from": 222,
    "to": 171
  },
  {
    "from": 223,
    "to": 162
  },
  {
    "from": 223,
    "to": 160
  },
  {
    "from": 223,
    "to": 10
  },
  {
    "from": 223,
    "to": 171
  },
  {
    "from": 223,
    "to": 163
  },
  {
    "from": 223,
    "to": 101
  },
  {
    "from": 223,
    "to": 169
  },
  {
    "from": 223,
    "to": 185
  },
  {
    "from": 224,
    "to": 173
  },
  {
    "from": 224,
    "to": 169
  },
  {
    "from": 224,
    "to": 10
  },
  {
    "from": 224,
    "to": 185
  },
  {
    "from": 224,
    "to": 170
  },
  {
    "from": 225,
    "to": 294
  },
  {
    "from": 225,
    "to": 130
  },
  {
    "from": 225,
    "to": 250
  },
  {
    "from": 225,
    "to": 292
  },
  {
    "from": 225,
    "to": 259
  },
  {
    "from": 225,
    "to": 295
  },
  {
    "from": 225,
    "to": 117
  },
  {
    "from": 225,
    "to": 70
  },
  {
    "from": 225,
    "to": 170
  },
  {
    "from": 225,
    "to": 185
  },
  {
    "from": 225,
    "to": 353
  },
  {
    "from": 226,
    "to": 55
  },
  {
    "from": 226,
    "to": 172
  },
  {
    "from": 226,
    "to": 286
  },
  {
    "from": 227,
    "to": 226
  },
  {
    "from": 227,
    "to": 208
  },
  {
    "from": 227,
    "to": 122
  },
  {
    "from": 227,
    "to": 295
  },
  {
    "from": 227,
    "to": 179
  },
  {
    "from": 227,
    "to": 286
  },
  {
    "from": 227,
    "to": 148
  },
  {
    "from": 227,
    "to": 142
  },
  {
    "from": 227,
    "to": 70
  },
  {
    "from": 227,
    "to": 287
  },
  {
    "from": 227,
    "to": 319
  },
  {
    "from": 227,
    "to": 340
  },
  {
    "from": 227,
    "to": 341
  },
  {
    "from": 228,
    "to": 326
  },
  {
    "from": 228,
    "to": 177
  },
  {
    "from": 228,
    "to": 234
  },
  {
    "from": 228,
    "to": 185
  },
  {
    "from": 228,
    "to": 284
  },
  {
    "from": 229,
    "to": 27
  },
  {
    "from": 230,
    "to": 2
  },
  {
    "from": 230,
    "to": 253
  },
  {
    "from": 230,
    "to": 55
  },
  {
    "from": 230,
    "to": 231
  },
  {
    "from": 230,
    "to": 77
  },
  {
    "from": 231,
    "to": 2
  },
  {
    "from": 231,
    "to": 177
  },
  {
    "from": 231,
    "to": 178
  },
  {
    "from": 231,
    "to": 235
  },
  {
    "from": 232,
    "to": 231
  },
  {
    "from": 233,
    "to": 27
  },
  {
    "from": 233,
    "to": 26
  },
  {
    "from": 233,
    "to": 25
  },
  {
    "from": 233,
    "to": 271
  },
  {
    "from": 234,
    "to": 177
  },
  {
    "from": 234,
    "to": 178
  },
  {
    "from": 234,
    "to": 230
  },
  {
    "from": 234,
    "to": 77
  },
  {
    "from": 234,
    "to": 229
  },
  {
    "from": 234,
    "to": 231
  },
  {
    "from": 234,
    "to": 185
  },
  {
    "from": 234,
    "to": 284
  },
  {
    "from": 234,
    "to": 94
  },
  {
    "from": 235,
    "to": 233
  },
  {
    "from": 235,
    "to": 27
  },
  {
    "from": 235,
    "to": 346
  },
  {
    "from": 236,
    "to": 177
  },
  {
    "from": 236,
    "to": 27
  },
  {
    "from": 236,
    "to": 77
  },
  {
    "from": 236,
    "to": 228
  },
  {
    "from": 236,
    "to": 173
  },
  {
    "from": 236,
    "to": 326
  },
  {
    "from": 236,
    "to": 94
  },
  {
    "from": 236,
    "to": 185
  },
  {
    "from": 237,
    "to": 173
  },
  {
    "from": 237,
    "to": 55
  },
  {
    "from": 237,
    "to": 294
  },
  {
    "from": 237,
    "to": 172
  },
  {
    "from": 237,
    "to": 185
  },
  {
    "from": 237,
    "to": 171
  },
  {
    "from": 237,
    "to": 325
  },
  {
    "from": 238,
    "to": 55
  },
  {
    "from": 238,
    "to": 284
  },
  {
    "from": 238,
    "to": 228
  },
  {
    "from": 238,
    "to": 326
  },
  {
    "from": 238,
    "to": 172
  },
  {
    "from": 238,
    "to": 185
  },
  {
    "from": 238,
    "to": 90
  },
  {
    "from": 238,
    "to": 91
  },
  {
    "from": 238,
    "to": 320
  },
  {
    "from": 238,
    "to": 94
  },
  {
    "from": 238,
    "to": 177
  },
  {
    "from": 239,
    "to": 55
  },
  {
    "from": 239,
    "to": 193
  },
  {
    "from": 239,
    "to": 309
  },
  {
    "from": 239,
    "to": 286
  },
  {
    "from": 239,
    "to": 179
  },
  {
    "from": 239,
    "to": 185
  },
  {
    "from": 239,
    "to": 242
  },
  {
    "from": 239,
    "to": 316
  },
  {
    "from": 239,
    "to": 204
  },
  {
    "from": 239,
    "to": 240
  },
  {
    "from": 239,
    "to": 163
  },
  {
    "from": 239,
    "to": 169
  },
  {
    "from": 240,
    "to": 55
  },
  {
    "from": 240,
    "to": 173
  },
  {
    "from": 240,
    "to": 160
  },
  {
    "from": 240,
    "to": 163
  },
  {
    "from": 240,
    "to": 185
  },
  {
    "from": 240,
    "to": 101
  },
  {
    "from": 240,
    "to": 149
  },
  {
    "from": 240,
    "to": 57
  },
  {
    "from": 240,
    "to": 169
  },
  {
    "from": 240,
    "to": 170
  },
  {
    "from": 240,
    "to": 193
  },
  {
    "from": 240,
    "to": 239
  },
  {
    "from": 240,
    "to": 242
  },
  {
    "from": 240,
    "to": 24
  },
  {
    "from": 240,
    "to": 28
  },
  {
    "from": 240,
    "to": 286
  },
  {
    "from": 240,
    "to": 310
  },
  {
    "from": 241,
    "to": 55
  },
  {
    "from": 241,
    "to": 353
  },
  {
    "from": 241,
    "to": 314
  },
  {
    "from": 241,
    "to": 193
  },
  {
    "from": 241,
    "to": 286
  },
  {
    "from": 241,
    "to": 160
  },
  {
    "from": 241,
    "to": 309
  },
  {
    "from": 241,
    "to": 105
  },
  {
    "from": 241,
    "to": 185
  },
  {
    "from": 241,
    "to": 169
  },
  {
    "from": 241,
    "to": 171
  },
  {
    "from": 241,
    "to": 170
  },
  {
    "from": 241,
    "to": 20
  },
  {
    "from": 241,
    "to": 284
  },
  {
    "from": 241,
    "to": 228
  },
  {
    "from": 241,
    "to": 326
  },
  {
    "from": 241,
    "to": 173
  },
  {
    "from": 241,
    "to": 177
  },
  {
    "from": 242,
    "to": 90
  },
  {
    "from": 242,
    "to": 193
  },
  {
    "from": 242,
    "to": 55
  },
  {
    "from": 242,
    "to": 173
  },
  {
    "from": 242,
    "to": 160
  },
  {
    "from": 242,
    "to": 171
  },
  {
    "from": 242,
    "to": 185
  },
  {
    "from": 242,
    "to": 170
  },
  {
    "from": 242,
    "to": 239
  },
  {
    "from": 242,
    "to": 286
  },
  {
    "from": 242,
    "to": 163
  },
  {
    "from": 242,
    "to": 165
  },
  {
    "from": 242,
    "to": 24
  },
  {
    "from": 242,
    "to": 28
  },
  {
    "from": 242,
    "to": 179
  },
  {
    "from": 243,
    "to": 160
  },
  {
    "from": 243,
    "to": 28
  },
  {
    "from": 243,
    "to": 171
  },
  {
    "from": 243,
    "to": 169
  },
  {
    "from": 243,
    "to": 271
  },
  {
    "from": 244,
    "to": 160
  },
  {
    "from": 244,
    "to": 28
  },
  {
    "from": 244,
    "to": 170
  },
  {
    "from": 244,
    "to": 17
  },
  {
    "from": 244,
    "to": 99
  },
  {
    "from": 244,
    "to": 27
  },
  {
    "from": 244,
    "to": 243
  },
  {
    "from": 245,
    "to": 17
  },
  {
    "from": 245,
    "to": 248
  },
  {
    "from": 245,
    "to": 123
  },
  {
    "from": 246,
    "to": 245
  },
  {
    "from": 246,
    "to": 17
  },
  {
    "from": 246,
    "to": 248
  },
  {
    "from": 247,
    "to": 276
  },
  {
    "from": 247,
    "to": 287
  },
  {
    "from": 248,
    "to": 17
  },
  {
    "from": 248,
    "to": 3
  },
  {
    "from": 249,
    "to": 206
  },
  {
    "from": 249,
    "to": 199
  },
  {
    "from": 249,
    "to": 309
  },
  {
    "from": 249,
    "to": 197
  },
  {
    "from": 249,
    "to": 193
  },
  {
    "from": 249,
    "to": 160
  },
  {
    "from": 249,
    "to": 179
  },
  {
    "from": 249,
    "to": 28
  },
  {
    "from": 249,
    "to": 163
  },
  {
    "from": 249,
    "to": 185
  },
  {
    "from": 250,
    "to": 294
  },
  {
    "from": 250,
    "to": 170
  },
  {
    "from": 250,
    "to": 291
  },
  {
    "from": 250,
    "to": 173
  },
  {
    "from": 250,
    "to": 295
  },
  {
    "from": 250,
    "to": 95
  },
  {
    "from": 250,
    "to": 251
  },
  {
    "from": 250,
    "to": 211
  },
  {
    "from": 250,
    "to": 34
  },
  {
    "from": 250,
    "to": 24
  },
  {
    "from": 250,
    "to": 172
  },
  {
    "from": 250,
    "to": 185
  },
  {
    "from": 251,
    "to": 294
  },
  {
    "from": 251,
    "to": 173
  },
  {
    "from": 251,
    "to": 295
  },
  {
    "from": 251,
    "to": 170
  },
  {
    "from": 251,
    "to": 250
  },
  {
    "from": 251,
    "to": 17
  },
  {
    "from": 251,
    "to": 185
  },
  {
    "from": 252,
    "to": 163
  },
  {
    "from": 252,
    "to": 126
  },
  {
    "from": 252,
    "to": 55
  },
  {
    "from": 252,
    "to": 286
  },
  {
    "from": 253,
    "to": 172
  },
  {
    "from": 254,
    "to": 241
  },
  {
    "from": 254,
    "to": 95
  },
  {
    "from": 254,
    "to": 173
  },
  {
    "from": 254,
    "to": 170
  },
  {
    "from": 255,
    "to": 241
  },
  {
    "from": 255,
    "to": 256
  },
  {
    "from": 255,
    "to": 254
  },
  {
    "from": 255,
    "to": 170
  },
  {
    "from": 256,
    "to": 258
  },
  {
    "from": 256,
    "to": 10
  },
  {
    "from": 256,
    "to": 177
  },
  {
    "from": 257,
    "to": 193
  },
  {
    "from": 257,
    "to": 161
  },
  {
    "from": 257,
    "to": 286
  },
  {
    "from": 257,
    "to": 314
  },
  {
    "from": 257,
    "to": 179
  },
  {
    "from": 257,
    "to": 278
  },
  {
    "from": 257,
    "to": 55
  },
  {
    "from": 257,
    "to": 325
  },
  {
    "from": 257,
    "to": 77
  },
  {
    "from": 257,
    "to": 79
  },
  {
    "from": 257,
    "to": 199
  },
  {
    "from": 257,
    "to": 100
  },
  {
    "from": 257,
    "to": 169
  },
  {
    "from": 257,
    "to": 185
  },
  {
    "from": 258,
    "to": 178
  },
  {
    "from": 258,
    "to": 94
  },
  {
    "from": 258,
    "to": 280
  },
  {
    "from": 258,
    "to": 26
  },
  {
    "from": 258,
    "to": 177
  },
  {
    "from": 258,
    "to": 211
  },
  {
    "from": 259,
    "to": 94
  },
  {
    "from": 259,
    "to": 17
  },
  {
    "from": 259,
    "to": 185
  },
  {
    "from": 259,
    "to": 71
  },
  {
    "from": 259,
    "to": 177
  },
  {
    "from": 259,
    "to": 77
  },
  {
    "from": 259,
    "to": 95
  },
  {
    "from": 259,
    "to": 256
  },
  {
    "from": 259,
    "to": 294
  },
  {
    "from": 259,
    "to": 169
  },
  {
    "from": 260,
    "to": 228
  },
  {
    "from": 260,
    "to": 326
  },
  {
    "from": 260,
    "to": 45
  },
  {
    "from": 260,
    "to": 173
  },
  {
    "from": 260,
    "to": 34
  },
  {
    "from": 260,
    "to": 94
  },
  {
    "from": 260,
    "to": 77
  },
  {
    "from": 260,
    "to": 177
  },
  {
    "from": 260,
    "to": 185
  },
  {
    "from": 260,
    "to": 287
  },
  {
    "from": 261,
    "to": 94
  },
  {
    "from": 261,
    "to": 55
  },
  {
    "from": 261,
    "to": 260
  },
  {
    "from": 261,
    "to": 34
  },
  {
    "from": 261,
    "to": 177
  },
  {
    "from": 261,
    "to": 173
  },
  {
    "from": 261,
    "to": 280
  },
  {
    "from": 261,
    "to": 330
  },
  {
    "from": 261,
    "to": 185
  },
  {
    "from": 261,
    "to": 287
  },
  {
    "from": 262,
    "to": 308
  },
  {
    "from": 262,
    "to": 94
  },
  {
    "from": 262,
    "to": 53
  },
  {
    "from": 262,
    "to": 261
  },
  {
    "from": 262,
    "to": 185
  },
  {
    "from": 263,
    "to": 149
  },
  {
    "from": 263,
    "to": 193
  },
  {
    "from": 263,
    "to": 160
  },
  {
    "from": 263,
    "to": 28
  },
  {
    "from": 263,
    "to": 171
  },
  {
    "from": 263,
    "to": 185
  },
  {
    "from": 263,
    "to": 169
  },
  {
    "from": 263,
    "to": 285
  },
  {
    "from": 263,
    "to": 86
  },
  {
    "from": 263,
    "to": 179
  },
  {
    "from": 263,
    "to": 175
  },
  {
    "from": 264,
    "to": 193
  },
  {
    "from": 264,
    "to": 86
  },
  {
    "from": 264,
    "to": 179
  },
  {
    "from": 264,
    "to": 160
  },
  {
    "from": 264,
    "to": 28
  },
  {
    "from": 264,
    "to": 65
  },
  {
    "from": 264,
    "to": 278
  },
  {
    "from": 264,
    "to": 286
  },
  {
    "from": 265,
    "to": 10
  },
  {
    "from": 265,
    "to": 248
  },
  {
    "from": 265,
    "to": 18
  },
  {
    "from": 265,
    "to": 266
  },
  {
    "from": 266,
    "to": 18
  },
  {
    "from": 267,
    "to": 224
  },
  {
    "from": 267,
    "to": 173
  },
  {
    "from": 267,
    "to": 286
  },
  {
    "from": 267,
    "to": 195
  },
  {
    "from": 267,
    "to": 187
  },
  {
    "from": 267,
    "to": 10
  },
  {
    "from": 267,
    "to": 170
  },
  {
    "from": 267,
    "to": 185
  },
  {
    "from": 268,
    "to": 172
  },
  {
    "from": 268,
    "to": 10
  },
  {
    "from": 268,
    "to": 224
  },
  {
    "from": 268,
    "to": 173
  },
  {
    "from": 268,
    "to": 170
  },
  {
    "from": 268,
    "to": 171
  },
  {
    "from": 268,
    "to": 185
  },
  {
    "from": 269,
    "to": 173
  },
  {
    "from": 269,
    "to": 171
  },
  {
    "from": 269,
    "to": 17
  },
  {
    "from": 269,
    "to": 175
  },
  {
    "from": 269,
    "to": 185
  },
  {
    "from": 270,
    "to": 17
  },
  {
    "from": 270,
    "to": 269
  },
  {
    "from": 270,
    "to": 173
  },
  {
    "from": 270,
    "to": 271
  },
  {
    "from": 271,
    "to": 171
  },
  {
    "from": 271,
    "to": 185
  },
  {
    "from": 272,
    "to": 17
  },
  {
    "from": 272,
    "to": 269
  },
  {
    "from": 272,
    "to": 173
  },
  {
    "from": 272,
    "to": 273
  },
  {
    "from": 273,
    "to": 171
  },
  {
    "from": 273,
    "to": 185
  },
  {
    "from": 274,
    "to": 17
  },
  {
    "from": 274,
    "to": 269
  },
  {
    "from": 274,
    "to": 173
  },
  {
    "from": 274,
    "to": 275
  },
  {
    "from": 275,
    "to": 171
  },
  {
    "from": 275,
    "to": 185
  },
  {
    "from": 276,
    "to": 278
  },
  {
    "from": 276,
    "to": 86
  },
  {
    "from": 276,
    "to": 160
  },
  {
    "from": 276,
    "to": 28
  },
  {
    "from": 276,
    "to": 264
  },
  {
    "from": 276,
    "to": 65
  },
  {
    "from": 276,
    "to": 171
  },
  {
    "from": 276,
    "to": 17
  },
  {
    "from": 276,
    "to": 265
  },
  {
    "from": 276,
    "to": 224
  },
  {
    "from": 276,
    "to": 247
  },
  {
    "from": 276,
    "to": 179
  },
  {
    "from": 276,
    "to": 193
  },
  {
    "from": 276,
    "to": 87
  },
  {
    "from": 276,
    "to": 328
  },
  {
    "from": 277,
    "to": 193
  },
  {
    "from": 277,
    "to": 86
  },
  {
    "from": 277,
    "to": 179
  },
  {
    "from": 277,
    "to": 160
  },
  {
    "from": 277,
    "to": 28
  },
  {
    "from": 277,
    "to": 238
  },
  {
    "from": 277,
    "to": 185
  },
  {
    "from": 277,
    "to": 278
  },
  {
    "from": 277,
    "to": 65
  },
  {
    "from": 277,
    "to": 286
  },
  {
    "from": 277,
    "to": 325
  },
  {
    "from": 278,
    "to": 149
  },
  {
    "from": 278,
    "to": 160
  },
  {
    "from": 278,
    "to": 28
  },
  {
    "from": 278,
    "to": 171
  },
  {
    "from": 278,
    "to": 185
  },
  {
    "from": 278,
    "to": 86
  },
  {
    "from": 278,
    "to": 179
  },
  {
    "from": 278,
    "to": 193
  },
  {
    "from": 278,
    "to": 34
  },
  {
    "from": 278,
    "to": 238
  },
  {
    "from": 278,
    "to": 65
  },
  {
    "from": 278,
    "to": 169
  },
  {
    "from": 278,
    "to": 328
  },
  {
    "from": 278,
    "to": 173
  },
  {
    "from": 278,
    "to": 224
  },
  {
    "from": 278,
    "to": 17
  },
  {
    "from": 278,
    "to": 343
  },
  {
    "from": 278,
    "to": 9
  },
  {
    "from": 278,
    "to": 1
  },
  {
    "from": 278,
    "to": 55
  },
  {
    "from": 278,
    "to": 317
  },
  {
    "from": 278,
    "to": 10
  },
  {
    "from": 278,
    "to": 91
  },
  {
    "from": 278,
    "to": 170
  },
  {
    "from": 278,
    "to": 287
  },
  {
    "from": 278,
    "to": 304
  },
  {
    "from": 278,
    "to": 353
  },
  {
    "from": 279,
    "to": 149
  },
  {
    "from": 279,
    "to": 160
  },
  {
    "from": 279,
    "to": 171
  },
  {
    "from": 279,
    "to": 185
  },
  {
    "from": 279,
    "to": 170
  },
  {
    "from": 279,
    "to": 86
  },
  {
    "from": 279,
    "to": 179
  },
  {
    "from": 279,
    "to": 123
  },
  {
    "from": 279,
    "to": 199
  },
  {
    "from": 279,
    "to": 328
  },
  {
    "from": 279,
    "to": 79
  },
  {
    "from": 279,
    "to": 257
  },
  {
    "from": 279,
    "to": 195
  },
  {
    "from": 279,
    "to": 65
  },
  {
    "from": 279,
    "to": 169
  },
  {
    "from": 279,
    "to": 197
  },
  {
    "from": 279,
    "to": 193
  },
  {
    "from": 279,
    "to": 173
  },
  {
    "from": 279,
    "to": 228
  },
  {
    "from": 279,
    "to": 326
  },
  {
    "from": 279,
    "to": 28
  },
  {
    "from": 279,
    "to": 339
  },
  {
    "from": 279,
    "to": 24
  },
  {
    "from": 279,
    "to": 175
  },
  {
    "from": 279,
    "to": 177
  },
  {
    "from": 280,
    "to": 170
  },
  {
    "from": 280,
    "to": 177
  },
  {
    "from": 280,
    "to": 326
  },
  {
    "from": 280,
    "to": 77
  },
  {
    "from": 280,
    "to": 171
  },
  {
    "from": 281,
    "to": 149
  },
  {
    "from": 281,
    "to": 160
  },
  {
    "from": 281,
    "to": 28
  },
  {
    "from": 281,
    "to": 104
  },
  {
    "from": 281,
    "to": 171
  },
  {
    "from": 281,
    "to": 170
  },
  {
    "from": 281,
    "to": 185
  },
  {
    "from": 281,
    "to": 282
  },
  {
    "from": 281,
    "to": 179
  },
  {
    "from": 281,
    "to": 175
  },
  {
    "from": 282,
    "to": 185
  },
  {
    "from": 282,
    "to": 171
  },
  {
    "from": 282,
    "to": 160
  },
  {
    "from": 282,
    "to": 309
  },
  {
    "from": 282,
    "to": 163
  },
  {
    "from": 282,
    "to": 162
  },
  {
    "from": 282,
    "to": 169
  },
  {
    "from": 282,
    "to": 26
  },
  {
    "from": 283,
    "to": 185
  },
  {
    "from": 283,
    "to": 170
  },
  {
    "from": 283,
    "to": 171
  },
  {
    "from": 283,
    "to": 309
  },
  {
    "from": 283,
    "to": 160
  },
  {
    "from": 283,
    "to": 28
  },
  {
    "from": 283,
    "to": 163
  },
  {
    "from": 283,
    "to": 297
  },
  {
    "from": 283,
    "to": 101
  },
  {
    "from": 283,
    "to": 104
  },
  {
    "from": 283,
    "to": 26
  },
  {
    "from": 284,
    "to": 228
  },
  {
    "from": 284,
    "to": 173
  },
  {
    "from": 284,
    "to": 185
  },
  {
    "from": 284,
    "to": 177
  },
  {
    "from": 285,
    "to": 160
  },
  {
    "from": 285,
    "to": 286
  },
  {
    "from": 285,
    "to": 55
  },
  {
    "from": 285,
    "to": 28
  },
  {
    "from": 285,
    "to": 185
  },
  {
    "from": 285,
    "to": 163
  },
  {
    "from": 285,
    "to": 171
  },
  {
    "from": 285,
    "to": 166
  },
  {
    "from": 285,
    "to": 162
  },
  {
    "from": 285,
    "to": 193
  },
  {
    "from": 285,
    "to": 74
  },
  {
    "from": 285,
    "to": 10
  },
  {
    "from": 285,
    "to": 65
  },
  {
    "from": 285,
    "to": 169
  },
  {
    "from": 286,
    "to": 55
  },
  {
    "from": 286,
    "to": 285
  },
  {
    "from": 286,
    "to": 160
  },
  {
    "from": 286,
    "to": 94
  },
  {
    "from": 286,
    "to": 326
  },
  {
    "from": 286,
    "to": 169
  },
  {
    "from": 286,
    "to": 171
  },
  {
    "from": 286,
    "to": 185
  },
  {
    "from": 286,
    "to": 16
  },
  {
    "from": 286,
    "to": 28
  },
  {
    "from": 287,
    "to": 336
  },
  {
    "from": 287,
    "to": 286
  },
  {
    "from": 287,
    "to": 55
  },
  {
    "from": 287,
    "to": 160
  },
  {
    "from": 287,
    "to": 17
  },
  {
    "from": 287,
    "to": 337
  },
  {
    "from": 287,
    "to": 28
  },
  {
    "from": 287,
    "to": 328
  },
  {
    "from": 288,
    "to": 55
  },
  {
    "from": 288,
    "to": 286
  },
  {
    "from": 288,
    "to": 310
  },
  {
    "from": 288,
    "to": 179
  },
  {
    "from": 288,
    "to": 309
  },
  {
    "from": 288,
    "to": 56
  },
  {
    "from": 288,
    "to": 163
  },
  {
    "from": 288,
    "to": 287
  },
  {
    "from": 289,
    "to": 56
  },
  {
    "from": 289,
    "to": 20
  },
  {
    "from": 289,
    "to": 163
  },
  {
    "from": 289,
    "to": 27
  },
  {
    "from": 289,
    "to": 326
  },
  {
    "from": 290,
    "to": 294
  },
  {
    "from": 290,
    "to": 173
  },
  {
    "from": 290,
    "to": 185
  },
  {
    "from": 290,
    "to": 246
  },
  {
    "from": 290,
    "to": 291
  },
  {
    "from": 290,
    "to": 250
  },
  {
    "from": 290,
    "to": 170
  },
  {
    "from": 290,
    "to": 251
  },
  {
    "from": 290,
    "to": 295
  },
  {
    "from": 291,
    "to": 294
  },
  {
    "from": 291,
    "to": 295
  },
  {
    "from": 291,
    "to": 292
  },
  {
    "from": 291,
    "to": 185
  },
  {
    "from": 291,
    "to": 146
  },
  {
    "from": 291,
    "to": 250
  },
  {
    "from": 291,
    "to": 172
  },
  {
    "from": 291,
    "to": 57
  },
  {
    "from": 291,
    "to": 143
  },
  {
    "from": 291,
    "to": 117
  },
  {
    "from": 291,
    "to": 218
  },
  {
    "from": 291,
    "to": 169
  },
  {
    "from": 291,
    "to": 170
  },
  {
    "from": 291,
    "to": 299
  },
  {
    "from": 292,
    "to": 294
  },
  {
    "from": 292,
    "to": 185
  },
  {
    "from": 292,
    "to": 146
  },
  {
    "from": 292,
    "to": 295
  },
  {
    "from": 292,
    "to": 94
  },
  {
    "from": 292,
    "to": 259
  },
  {
    "from": 292,
    "to": 77
  },
  {
    "from": 292,
    "to": 172
  },
  {
    "from": 292,
    "to": 57
  },
  {
    "from": 292,
    "to": 143
  },
  {
    "from": 292,
    "to": 169
  },
  {
    "from": 292,
    "to": 170
  },
  {
    "from": 293,
    "to": 10
  },
  {
    "from": 293,
    "to": 55
  },
  {
    "from": 293,
    "to": 286
  },
  {
    "from": 293,
    "to": 17
  },
  {
    "from": 294,
    "to": 295
  },
  {
    "from": 294,
    "to": 250
  },
  {
    "from": 294,
    "to": 170
  },
  {
    "from": 294,
    "to": 95
  },
  {
    "from": 294,
    "to": 292
  },
  {
    "from": 294,
    "to": 259
  },
  {
    "from": 294,
    "to": 71
  },
  {
    "from": 294,
    "to": 185
  },
  {
    "from": 295,
    "to": 166
  },
  {
    "from": 295,
    "to": 185
  },
  {
    "from": 295,
    "to": 170
  },
  {
    "from": 296,
    "to": 309
  },
  {
    "from": 296,
    "to": 86
  },
  {
    "from": 296,
    "to": 318
  },
  {
    "from": 297,
    "to": 101
  },
  {
    "from": 297,
    "to": 160
  },
  {
    "from": 297,
    "to": 28
  },
  {
    "from": 297,
    "to": 278
  },
  {
    "from": 297,
    "to": 281
  },
  {
    "from": 297,
    "to": 193
  },
  {
    "from": 297,
    "to": 65
  },
  {
    "from": 297,
    "to": 310
  },
  {
    "from": 297,
    "to": 166
  },
  {
    "from": 297,
    "to": 45
  },
  {
    "from": 297,
    "to": 283
  },
  {
    "from": 297,
    "to": 17
  },
  {
    "from": 297,
    "to": 104
  },
  {
    "from": 297,
    "to": 309
  },
  {
    "from": 297,
    "to": 244
  },
  {
    "from": 297,
    "to": 179
  },
  {
    "from": 297,
    "to": 55
  },
  {
    "from": 297,
    "to": 221
  },
  {
    "from": 297,
    "to": 185
  },
  {
    "from": 298,
    "to": 10
  },
  {
    "from": 298,
    "to": 17
  },
  {
    "from": 299,
    "to": 10
  },
  {
    "from": 299,
    "to": 17
  },
  {
    "from": 299,
    "to": 298
  },
  {
    "from": 300,
    "to": 10
  },
  {
    "from": 300,
    "to": 299
  },
  {
    "from": 300,
    "to": 298
  },
  {
    "from": 301,
    "to": 10
  },
  {
    "from": 302,
    "to": 205
  },
  {
    "from": 303,
    "to": 173
  },
  {
    "from": 303,
    "to": 334
  },
  {
    "from": 303,
    "to": 294
  },
  {
    "from": 303,
    "to": 123
  },
  {
    "from": 303,
    "to": 295
  },
  {
    "from": 303,
    "to": 17
  },
  {
    "from": 303,
    "to": 185
  },
  {
    "from": 304,
    "to": 156
  },
  {
    "from": 304,
    "to": 24
  },
  {
    "from": 304,
    "to": 45
  },
  {
    "from": 304,
    "to": 155
  },
  {
    "from": 304,
    "to": 197
  },
  {
    "from": 304,
    "to": 317
  },
  {
    "from": 304,
    "to": 35
  },
  {
    "from": 304,
    "to": 287
  },
  {
    "from": 304,
    "to": 353
  },
  {
    "from": 305,
    "to": 157
  },
  {
    "from": 305,
    "to": 201
  },
  {
    "from": 305,
    "to": 317
  },
  {
    "from": 305,
    "to": 27
  },
  {
    "from": 305,
    "to": 156
  },
  {
    "from": 305,
    "to": 197
  },
  {
    "from": 305,
    "to": 105
  },
  {
    "from": 305,
    "to": 160
  },
  {
    "from": 305,
    "to": 287
  },
  {
    "from": 306,
    "to": 317
  },
  {
    "from": 306,
    "to": 156
  },
  {
    "from": 306,
    "to": 197
  },
  {
    "from": 306,
    "to": 19
  },
  {
    "from": 306,
    "to": 286
  },
  {
    "from": 306,
    "to": 193
  },
  {
    "from": 306,
    "to": 179
  },
  {
    "from": 306,
    "to": 105
  },
  {
    "from": 306,
    "to": 27
  },
  {
    "from": 306,
    "to": 181
  },
  {
    "from": 306,
    "to": 328
  },
  {
    "from": 306,
    "to": 185
  },
  {
    "from": 306,
    "to": 287
  },
  {
    "from": 307,
    "to": 55
  },
  {
    "from": 307,
    "to": 337
  },
  {
    "from": 307,
    "to": 228
  },
  {
    "from": 307,
    "to": 177
  },
  {
    "from": 307,
    "to": 127
  },
  {
    "from": 307,
    "to": 160
  },
  {
    "from": 307,
    "to": 197
  },
  {
    "from": 307,
    "to": 185
  },
  {
    "from": 307,
    "to": 317
  },
  {
    "from": 307,
    "to": 66
  },
  {
    "from": 307,
    "to": 309
  },
  {
    "from": 307,
    "to": 213
  },
  {
    "from": 307,
    "to": 86
  },
  {
    "from": 307,
    "to": 133
  },
  {
    "from": 307,
    "to": 95
  },
  {
    "from": 307,
    "to": 287
  },
  {
    "from": 308,
    "to": 214
  },
  {
    "from": 308,
    "to": 24
  },
  {
    "from": 308,
    "to": 169
  },
  {
    "from": 308,
    "to": 355
  },
  {
    "from": 309,
    "to": 55
  },
  {
    "from": 309,
    "to": 310
  },
  {
    "from": 309,
    "to": 160
  },
  {
    "from": 309,
    "to": 185
  },
  {
    "from": 309,
    "to": 105
  },
  {
    "from": 310,
    "to": 309
  },
  {
    "from": 310,
    "to": 27
  },
  {
    "from": 310,
    "to": 28
  },
  {
    "from": 310,
    "to": 185
  },
  {
    "from": 310,
    "to": 171
  },
  {
    "from": 310,
    "to": 160
  },
  {
    "from": 310,
    "to": 163
  },
  {
    "from": 310,
    "to": 170
  },
  {
    "from": 310,
    "to": 55
  },
  {
    "from": 310,
    "to": 193
  },
  {
    "from": 310,
    "to": 47
  },
  {
    "from": 310,
    "to": 169
  },
  {
    "from": 311,
    "to": 160
  },
  {
    "from": 311,
    "to": 28
  },
  {
    "from": 311,
    "to": 117
  },
  {
    "from": 311,
    "to": 170
  },
  {
    "from": 311,
    "to": 185
  },
  {
    "from": 311,
    "to": 243
  },
  {
    "from": 311,
    "to": 166
  },
  {
    "from": 311,
    "to": 17
  },
  {
    "from": 311,
    "to": 169
  },
  {
    "from": 312,
    "to": 55
  },
  {
    "from": 312,
    "to": 286
  },
  {
    "from": 312,
    "to": 170
  },
  {
    "from": 312,
    "to": 185
  },
  {
    "from": 312,
    "to": 163
  },
  {
    "from": 312,
    "to": 169
  },
  {
    "from": 312,
    "to": 127
  },
  {
    "from": 312,
    "to": 313
  },
  {
    "from": 312,
    "to": 70
  },
  {
    "from": 312,
    "to": 195
  },
  {
    "from": 312,
    "to": 271
  },
  {
    "from": 313,
    "to": 286
  },
  {
    "from": 313,
    "to": 42
  },
  {
    "from": 313,
    "to": 55
  },
  {
    "from": 313,
    "to": 101
  },
  {
    "from": 313,
    "to": 156
  },
  {
    "from": 313,
    "to": 136
  },
  {
    "from": 313,
    "to": 185
  },
  {
    "from": 313,
    "to": 163
  },
  {
    "from": 313,
    "to": 85
  },
  {
    "from": 313,
    "to": 92
  },
  {
    "from": 314,
    "to": 241
  },
  {
    "from": 314,
    "to": 173
  },
  {
    "from": 314,
    "to": 193
  },
  {
    "from": 314,
    "to": 179
  },
  {
    "from": 314,
    "to": 286
  },
  {
    "from": 314,
    "to": 161
  },
  {
    "from": 314,
    "to": 284
  },
  {
    "from": 314,
    "to": 79
  },
  {
    "from": 314,
    "to": 185
  },
  {
    "from": 315,
    "to": 138
  },
  {
    "from": 315,
    "to": 346
  },
  {
    "from": 315,
    "to": 110
  },
  {
    "from": 315,
    "to": 271
  },
  {
    "from": 316,
    "to": 173
  },
  {
    "from": 316,
    "to": 311
  },
  {
    "from": 317,
    "to": 24
  },
  {
    "from": 317,
    "to": 155
  },
  {
    "from": 317,
    "to": 27
  },
  {
    "from": 317,
    "to": 156
  },
  {
    "from": 317,
    "to": 212
  },
  {
    "from": 317,
    "to": 66
  },
  {
    "from": 317,
    "to": 353
  },
  {
    "from": 318,
    "to": 185
  },
  {
    "from": 318,
    "to": 171
  },
  {
    "from": 318,
    "to": 309
  },
  {
    "from": 318,
    "to": 160
  },
  {
    "from": 318,
    "to": 170
  },
  {
    "from": 318,
    "to": 86
  },
  {
    "from": 318,
    "to": 334
  },
  {
    "from": 318,
    "to": 27
  },
  {
    "from": 318,
    "to": 163
  },
  {
    "from": 318,
    "to": 333
  },
  {
    "from": 318,
    "to": 155
  },
  {
    "from": 318,
    "to": 193
  },
  {
    "from": 318,
    "to": 35
  },
  {
    "from": 318,
    "to": 310
  },
  {
    "from": 318,
    "to": 305
  },
  {
    "from": 318,
    "to": 213
  },
  {
    "from": 318,
    "to": 354
  },
  {
    "from": 318,
    "to": 179
  },
  {
    "from": 318,
    "to": 238
  },
  {
    "from": 318,
    "to": 169
  },
  {
    "from": 318,
    "to": 321
  },
  {
    "from": 318,
    "to": 26
  },
  {
    "from": 319,
    "to": 172
  },
  {
    "from": 319,
    "to": 55
  },
  {
    "from": 319,
    "to": 286
  },
  {
    "from": 319,
    "to": 185
  },
  {
    "from": 320,
    "to": 149
  },
  {
    "from": 320,
    "to": 193
  },
  {
    "from": 320,
    "to": 296
  },
  {
    "from": 320,
    "to": 163
  },
  {
    "from": 320,
    "to": 321
  },
  {
    "from": 320,
    "to": 278
  },
  {
    "from": 320,
    "to": 328
  },
  {
    "from": 320,
    "to": 90
  },
  {
    "from": 320,
    "to": 318
  },
  {
    "from": 320,
    "to": 335
  },
  {
    "from": 320,
    "to": 86
  },
  {
    "from": 321,
    "to": 193
  },
  {
    "from": 321,
    "to": 318
  },
  {
    "from": 321,
    "to": 320
  },
  {
    "from": 321,
    "to": 336
  },
  {
    "from": 321,
    "to": 96
  },
  {
    "from": 321,
    "to": 173
  },
  {
    "from": 321,
    "to": 238
  },
  {
    "from": 321,
    "to": 86
  },
  {
    "from": 321,
    "to": 95
  },
  {
    "from": 321,
    "to": 149
  },
  {
    "from": 322,
    "to": 55
  },
  {
    "from": 322,
    "to": 286
  },
  {
    "from": 322,
    "to": 310
  },
  {
    "from": 322,
    "to": 150
  },
  {
    "from": 322,
    "to": 76
  },
  {
    "from": 322,
    "to": 190
  },
  {
    "from": 322,
    "to": 195
  },
  {
    "from": 322,
    "to": 187
  },
  {
    "from": 322,
    "to": 309
  },
  {
    "from": 323,
    "to": 94
  },
  {
    "from": 323,
    "to": 55
  },
  {
    "from": 323,
    "to": 185
  },
  {
    "from": 323,
    "to": 177
  },
  {
    "from": 323,
    "to": 326
  },
  {
    "from": 323,
    "to": 171
  },
  {
    "from": 323,
    "to": 127
  },
  {
    "from": 323,
    "to": 24
  },
  {
    "from": 323,
    "to": 160
  },
  {
    "from": 323,
    "to": 324
  },
  {
    "from": 323,
    "to": 169
  },
  {
    "from": 324,
    "to": 94
  },
  {
    "from": 324,
    "to": 326
  },
  {
    "from": 324,
    "to": 171
  },
  {
    "from": 324,
    "to": 185
  },
  {
    "from": 324,
    "to": 310
  },
  {
    "from": 324,
    "to": 160
  },
  {
    "from": 324,
    "to": 127
  },
  {
    "from": 324,
    "to": 24
  },
  {
    "from": 324,
    "to": 169
  },
  {
    "from": 325,
    "to": 285
  },
  {
    "from": 325,
    "to": 65
  },
  {
    "from": 325,
    "to": 346
  },
  {
    "from": 325,
    "to": 169
  },
  {
    "from": 325,
    "to": 347
  },
  {
    "from": 325,
    "to": 24
  },
  {
    "from": 325,
    "to": 271
  },
  {
    "from": 325,
    "to": 287
  },
  {
    "from": 326,
    "to": 55
  },
  {
    "from": 326,
    "to": 228
  },
  {
    "from": 326,
    "to": 177
  },
  {
    "from": 326,
    "to": 77
  },
  {
    "from": 326,
    "to": 327
  },
  {
    "from": 326,
    "to": 185
  },
  {
    "from": 327,
    "to": 177
  },
  {
    "from": 327,
    "to": 77
  },
  {
    "from": 327,
    "to": 234
  },
  {
    "from": 327,
    "to": 114
  },
  {
    "from": 327,
    "to": 3
  },
  {
    "from": 327,
    "to": 326
  },
  {
    "from": 327,
    "to": 230
  },
  {
    "from": 327,
    "to": 17
  },
  {
    "from": 327,
    "to": 15
  },
  {
    "from": 328,
    "to": 55
  },
  {
    "from": 328,
    "to": 286
  },
  {
    "from": 328,
    "to": 160
  },
  {
    "from": 328,
    "to": 65
  },
  {
    "from": 328,
    "to": 325
  },
  {
    "from": 328,
    "to": 28
  },
  {
    "from": 329,
    "to": 354
  },
  {
    "from": 329,
    "to": 228
  },
  {
    "from": 329,
    "to": 95
  },
  {
    "from": 329,
    "to": 326
  },
  {
    "from": 329,
    "to": 177
  },
  {
    "from": 330,
    "to": 55
  },
  {
    "from": 330,
    "to": 184
  },
  {
    "from": 330,
    "to": 286
  },
  {
    "from": 330,
    "to": 65
  },
  {
    "from": 330,
    "to": 138
  },
  {
    "from": 330,
    "to": 130
  },
  {
    "from": 330,
    "to": 304
  },
  {
    "from": 330,
    "to": 287
  },
  {
    "from": 330,
    "to": 328
  },
  {
    "from": 330,
    "to": 353
  },
  {
    "from": 331,
    "to": 130
  },
  {
    "from": 331,
    "to": 10
  },
  {
    "from": 331,
    "to": 35
  },
  {
    "from": 331,
    "to": 353
  },
  {
    "from": 332,
    "to": 326
  },
  {
    "from": 333,
    "to": 27
  },
  {
    "from": 333,
    "to": 335
  },
  {
    "from": 334,
    "to": 248
  },
  {
    "from": 334,
    "to": 27
  },
  {
    "from": 334,
    "to": 43
  },
  {
    "from": 334,
    "to": 26
  },
  {
    "from": 334,
    "to": 335
  },
  {
    "from": 335,
    "to": 27
  },
  {
    "from": 335,
    "to": 155
  },
  {
    "from": 335,
    "to": 156
  },
  {
    "from": 335,
    "to": 197
  },
  {
    "from": 335,
    "to": 66
  },
  {
    "from": 335,
    "to": 212
  },
  {
    "from": 335,
    "to": 304
  },
  {
    "from": 335,
    "to": 15
  },
  {
    "from": 335,
    "to": 34
  },
  {
    "from": 336,
    "to": 286
  },
  {
    "from": 336,
    "to": 94
  },
  {
    "from": 336,
    "to": 3
  },
  {
    "from": 336,
    "to": 95
  },
  {
    "from": 336,
    "to": 17
  },
  {
    "from": 336,
    "to": 256
  },
  {
    "from": 336,
    "to": 55
  },
  {
    "from": 336,
    "to": 285
  },
  {
    "from": 336,
    "to": 160
  },
  {
    "from": 336,
    "to": 65
  },
  {
    "from": 336,
    "to": 337
  },
  {
    "from": 336,
    "to": 328
  },
  {
    "from": 336,
    "to": 28
  },
  {
    "from": 336,
    "to": 71
  },
  {
    "from": 336,
    "to": 287
  },
  {
    "from": 337,
    "to": 328
  },
  {
    "from": 337,
    "to": 55
  },
  {
    "from": 337,
    "to": 286
  },
  {
    "from": 337,
    "to": 248
  },
  {
    "from": 337,
    "to": 123
  },
  {
    "from": 337,
    "to": 160
  },
  {
    "from": 337,
    "to": 28
  },
  {
    "from": 337,
    "to": 17
  },
  {
    "from": 337,
    "to": 65
  },
  {
    "from": 337,
    "to": 138
  },
  {
    "from": 337,
    "to": 287
  },
  {
    "from": 338,
    "to": 172
  },
  {
    "from": 338,
    "to": 55
  },
  {
    "from": 338,
    "to": 74
  },
  {
    "from": 338,
    "to": 75
  },
  {
    "from": 338,
    "to": 278
  },
  {
    "from": 338,
    "to": 213
  },
  {
    "from": 338,
    "to": 354
  },
  {
    "from": 338,
    "to": 123
  },
  {
    "from": 338,
    "to": 179
  },
  {
    "from": 338,
    "to": 304
  },
  {
    "from": 338,
    "to": 214
  },
  {
    "from": 338,
    "to": 170
  },
  {
    "from": 338,
    "to": 185
  },
  {
    "from": 339,
    "to": 172
  },
  {
    "from": 339,
    "to": 171
  },
  {
    "from": 339,
    "to": 199
  },
  {
    "from": 339,
    "to": 166
  },
  {
    "from": 339,
    "to": 185
  },
  {
    "from": 339,
    "to": 170
  },
  {
    "from": 339,
    "to": 240
  },
  {
    "from": 339,
    "to": 335
  },
  {
    "from": 340,
    "to": 173
  },
  {
    "from": 340,
    "to": 185
  },
  {
    "from": 340,
    "to": 55
  },
  {
    "from": 340,
    "to": 122
  },
  {
    "from": 340,
    "to": 169
  },
  {
    "from": 340,
    "to": 171
  },
  {
    "from": 340,
    "to": 94
  },
  {
    "from": 341,
    "to": 122
  },
  {
    "from": 341,
    "to": 286
  },
  {
    "from": 341,
    "to": 285
  },
  {
    "from": 341,
    "to": 319
  },
  {
    "from": 342,
    "to": 178
  },
  {
    "from": 342,
    "to": 163
  },
  {
    "from": 342,
    "to": 160
  },
  {
    "from": 342,
    "to": 193
  },
  {
    "from": 342,
    "to": 55
  },
  {
    "from": 342,
    "to": 177
  },
  {
    "from": 343,
    "to": 34
  },
  {
    "from": 343,
    "to": 335
  },
  {
    "from": 343,
    "to": 334
  },
  {
    "from": 343,
    "to": 197
  },
  {
    "from": 343,
    "to": 17
  },
  {
    "from": 343,
    "to": 156
  },
  {
    "from": 344,
    "to": 122
  },
  {
    "from": 345,
    "to": 172
  },
  {
    "from": 346,
    "to": 138
  },
  {
    "from": 346,
    "to": 128
  },
  {
    "from": 346,
    "to": 185
  },
  {
    "from": 346,
    "to": 163
  },
  {
    "from": 346,
    "to": 169
  },
  {
    "from": 346,
    "to": 271
  },
  {
    "from": 346,
    "to": 347
  },
  {
    "from": 347,
    "to": 128
  },
  {
    "from": 347,
    "to": 138
  },
  {
    "from": 347,
    "to": 271
  },
  {
    "from": 348,
    "to": 138
  },
  {
    "from": 348,
    "to": 171
  },
  {
    "from": 348,
    "to": 185
  },
  {
    "from": 348,
    "to": 170
  },
  {
    "from": 348,
    "to": 346
  },
  {
    "from": 348,
    "to": 166
  },
  {
    "from": 348,
    "to": 192
  },
  {
    "from": 348,
    "to": 244
  },
  {
    "from": 348,
    "to": 17
  },
  {
    "from": 348,
    "to": 65
  },
  {
    "from": 348,
    "to": 85
  },
  {
    "from": 348,
    "to": 60
  },
  {
    "from": 348,
    "to": 135
  },
  {
    "from": 348,
    "to": 347
  },
  {
    "from": 349,
    "to": 130
  },
  {
    "from": 349,
    "to": 156
  },
  {
    "from": 349,
    "to": 197
  },
  {
    "from": 349,
    "to": 309
  },
  {
    "from": 349,
    "to": 181
  },
  {
    "from": 349,
    "to": 310
  },
  {
    "from": 349,
    "to": 325
  },
  {
    "from": 349,
    "to": 163
  },
  {
    "from": 349,
    "to": 171
  },
  {
    "from": 349,
    "to": 66
  },
  {
    "from": 349,
    "to": 172
  },
  {
    "from": 349,
    "to": 317
  },
  {
    "from": 349,
    "to": 35
  },
  {
    "from": 349,
    "to": 24
  },
  {
    "from": 349,
    "to": 139
  },
  {
    "from": 349,
    "to": 185
  },
  {
    "from": 349,
    "to": 271
  },
  {
    "from": 349,
    "to": 287
  },
  {
    "from": 349,
    "to": 328
  },
  {
    "from": 349,
    "to": 346
  },
  {
    "from": 350,
    "to": 248
  },
  {
    "from": 350,
    "to": 112
  },
  {
    "from": 350,
    "to": 246
  },
  {
    "from": 350,
    "to": 166
  },
  {
    "from": 350,
    "to": 27
  },
  {
    "from": 350,
    "to": 322
  },
  {
    "from": 351,
    "to": 285
  },
  {
    "from": 351,
    "to": 173
  },
  {
    "from": 351,
    "to": 280
  },
  {
    "from": 351,
    "to": 228
  },
  {
    "from": 351,
    "to": 77
  },
  {
    "from": 351,
    "to": 201
  },
  {
    "from": 351,
    "to": 55
  },
  {
    "from": 351,
    "to": 161
  },
  {
    "from": 351,
    "to": 177
  },
  {
    "from": 352,
    "to": 193
  },
  {
    "from": 352,
    "to": 55
  },
  {
    "from": 352,
    "to": 309
  },
  {
    "from": 352,
    "to": 105
  },
  {
    "from": 352,
    "to": 179
  },
  {
    "from": 352,
    "to": 79
  },
  {
    "from": 352,
    "to": 202
  },
  {
    "from": 352,
    "to": 197
  },
  {
    "from": 352,
    "to": 200
  },
  {
    "from": 352,
    "to": 169
  },
  {
    "from": 352,
    "to": 171
  },
  {
    "from": 352,
    "to": 195
  },
  {
    "from": 352,
    "to": 199
  },
  {
    "from": 352,
    "to": 339
  },
  {
    "from": 352,
    "to": 24
  },
  {
    "from": 352,
    "to": 287
  },
  {
    "from": 352,
    "to": 328
  },
  {
    "from": 353,
    "to": 27
  },
  {
    "from": 353,
    "to": 55
  },
  {
    "from": 353,
    "to": 130
  },
  {
    "from": 353,
    "to": 173
  },
  {
    "from": 354,
    "to": 353
  },
  {
    "from": 354,
    "to": 350
  },
  {
    "from": 354,
    "to": 322
  },
  {
    "from": 355,
    "to": 214
  },
  {
    "from": 355,
    "to": 246
  },
  {
    "from": 355,
    "to": 170
  },
  {
    "from": 355,
    "to": 173
  },
  {
    "from": 355,
    "to": 166
  },
  {
    "from": 355,
    "to": 10
  },
  {
    "from": 355,
    "to": 228
  },
  {
    "from": 355,
    "to": 77
  },
  {
    "from": 355,
    "to": 123
  },
  {
    "from": 355,
    "to": 18
  },
  {
    "from": 355,
    "to": 248
  },
  {
    "from": 355,
    "to": 53
  },
  {
    "from": 355,
    "to": 308
  },
  {
    "from": 355,
    "to": 177
  },
  {
    "from": 355,
    "to": 185
  }
]);
      const container = document.getElementById("mynetwork");
      const data = { nodes: nodes, edges: edges };
      const options = {
        nodes: {
          shape: "dot",
          size: 20,
          font: { size: 14, color: "#000" }
        },
        edges: {
          arrows: "to",
          color: "gray",
          smooth: true
        },
        physics: {
          enabled: true,
          solver: "forceAtlas2Based",
          stabilization: {
            enabled: true,
            iterations: 200,
            fit: true
          }
        },
        interaction: {
          navigationButtons: true,
          keyboard: true,
          zoomView: true,
          dragView: true
        }
      };
      const network = new vis.Network(container, data, options);
    </script>
  </body>
</html>
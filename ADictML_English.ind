\begin{theindex}

  \item $0/1$ loss, \hyperpage{88}
  \item $k$-means, \hyperpage{14}

  \indexspace

  \item absolute error loss, \hyperpage{14}
  \item accuracy, \hyperpage{14}
  \item activation function, \hyperpage{15}
  \item application programming interface, \hyperpage{15}
  \item artificial intelligence, \hyperpage{15}
  \item artificial neural network, \hyperpage{15}
  \item autoencoder, \hyperpage{15}

  \indexspace

  \item backdoor, \hyperpage{16}
  \item bagging, \hyperpage{16}
  \item baseline, \hyperpage{16}
  \item batch, \hyperpage{18}
  \item Bayes estimator, \hyperpage{18}
  \item Bayes risk, \hyperpage{19}
  \item bias, \hyperpage{19}
  \item bootstrap, \hyperpage{19}

  \indexspace

  \item classification, \hyperpage{19}
  \item classifier, \hyperpage{20}
  \item cluster, \hyperpage{20}
  \item clustered federated learning, \hyperpage{20}
  \item clustering, \hyperpage{21}
  \item clustering assumption, \hyperpage{21}
  \item computational aspects, \hyperpage{21}
  \item condition number, \hyperpage{22}
  \item confusion matrix, \hyperpage{22}
  \item connected graph, \hyperpage{22}
  \item convex, \hyperpage{22}
  \item convex clustering, \hyperpage{23}
  \item covariance matrix, \hyperpage{23}

  \indexspace

  \item data, \hyperpage{23}
  \item data augmentation, \hyperpage{24}
  \item data minimization principle, \hyperpage{25}
  \item data normalization, \hyperpage{25}
  \item data point, \hyperpage{25}
  \item data poisoning, \hyperpage{26}
  \item dataset, \hyperpage{26}
  \item DBSCAN, \hyperpage{28}
  \item decision boundary, \hyperpage{27}
  \item decision region, \hyperpage{27}
  \item decision tree, \hyperpage{27}
  \item deep net, \hyperpage{28}
  \item degree of belonging, \hyperpage{28}
  \item denial-of-service attack, \hyperpage{28}
  \item differentiable, \hyperpage{29}
  \item dimensionality reduction, \hyperpage{30}
  \item discrepancy, \hyperpage{30}

  \indexspace

  \item edge weight, \hyperpage{30}
  \item effective dimension, \hyperpage{30}
  \item eigenvalue decomposition, \hyperpage{30}
  \item eigenvector, \hyperpage{31}
  \item empirical risk, \hyperpage{31}
  \item empirical risk minimization, \hyperpage{31}
  \item estimation error, \hyperpage{31}
  \item Euclidean space, \hyperpage{32}
  \item expectation, \hyperpage{32}
  \item expectation-maximization, \hyperpage{32}
  \item expert, \hyperpage{33}
  \item explainability, \hyperpage{33}
  \item explainable AI, \hyperpage{34}
  \item explainable empirical risk minimization, \hyperpage{33}
  \item explanation, \hyperpage{34}

  \indexspace

  \item feature, \hyperpage{34}
  \item feature learning, \hyperpage{34}
  \item feature map, \hyperpage{35}
  \item feature matrix, \hyperpage{36}
  \item feature space, \hyperpage{36}
  \item feature vector, \hyperpage{36}
  \item federated averaging (FedAvg), \hyperpage{36}
  \item federated learning, \hyperpage{36}
  \item federated learning (FL) network, \hyperpage{37}
  \item FedProx, \hyperpage{37}
  \item Finnish Meteorological Institute, \hyperpage{37}
  \item flow-based clustering, \hyperpage{37}

  \indexspace

  \item Gaussian mixture model, \hyperpage{37}
  \item Gaussian random variable, \hyperpage{38}
  \item GDPR, \hyperpage{38}
  \item generalization, \hyperpage{39}
  \item generalized total variation, \hyperpage{41}
  \item gradient, \hyperpage{41}
  \item gradient descent, \hyperpage{41}
  \item gradient step, \hyperpage{42}
  \item gradient-based methods, \hyperpage{43}
  \item graph, \hyperpage{44}
  \item graph clustering, \hyperpage{44}

  \indexspace

  \item hard clustering, \hyperpage{44}
  \item high-dimensional regime, \hyperpage{45}
  \item Hilbert space, \hyperpage{45}
  \item hinge loss, \hyperpage{45}
  \item histogram, \hyperpage{46}
  \item horizontal FL, \hyperpage{46}
  \item Huber loss, \hyperpage{47}
  \item Huber regression, \hyperpage{47}
  \item hypothesis, \hyperpage{47}
  \item hypothesis space, \hyperpage{47}

  \indexspace

  \item i.i.d., \hyperpage{47, 48}
  \item interpretability, \hyperpage{48}

  \indexspace

  \item k-fold cross-validation, \hyperpage{14}
  \item kernel, \hyperpage{48}
  \item kernel method, \hyperpage{48}
  \item KL divergence, \hyperpage{49}

  \indexspace

  \item label, \hyperpage{49}
  \item label space, \hyperpage{50}
  \item labeled data, \hyperpage{50}
  \item Laplacian matrix, \hyperpage{50}
  \item Lasso, \hyperpage{52}
  \item law of large numbers, \hyperpage{51}
  \item learning rate, \hyperpage{51}
  \item learning task, \hyperpage{52}
  \item least absolute deviation regression, \hyperpage{52}
  \item linear classifier, \hyperpage{53}
  \item linear model, \hyperpage{53}
  \item linear regression, \hyperpage{53}
  \item local dataset, \hyperpage{53}
  \item local model, \hyperpage{54}
  \item logistic loss, \hyperpage{54}
  \item logistic regression, \hyperpage{54}
  \item loss, \hyperpage{54}
  \item loss function, \hyperpage{55}

  \indexspace

  \item maximum, \hyperpage{55}
  \item maximum likelihood, \hyperpage{56}
  \item mean, \hyperpage{56}
  \item mean squared estimation error, \hyperpage{56}
  \item minimum, \hyperpage{56}
  \item missing data, \hyperpage{56}
  \item model parameters, \hyperpage{57}
  \item model selection, \hyperpage{57}
  \item multi-label classification, \hyperpage{57}
  \item multitask learning, \hyperpage{57}
  \item multivariate normal distribution, \hyperpage{57}
  \item mutual information, \hyperpage{58}

  \indexspace

  \item nearest neighbour, \hyperpage{58}
  \item neighbourhood, \hyperpage{58}
  \item neighbours, \hyperpage{58}
  \item networked data, \hyperpage{59}
  \item networked exponential families, \hyperpage{59}
  \item networked federated learning, \hyperpage{59}
  \item networked model, \hyperpage{59}
  \item node degree, \hyperpage{59}
  \item non-smooth, \hyperpage{59}
  \item norm, \hyperpage{59}

  \indexspace

  \item objective function, \hyperpage{60}
  \item online GD, \hyperpage{60}
  \item optimism in face of uncertainty, \hyperpage{61}
  \item outlier, \hyperpage{62}
  \item overfitting, \hyperpage{63}

  \indexspace

  \item parameter space, \hyperpage{63}
  \item parameters, \hyperpage{63}
  \item polynomial regression, \hyperpage{64}
  \item positive semi-definite, \hyperpage{64}
  \item prediction, \hyperpage{65}
  \item predictor, \hyperpage{65}
  \item principal component analysis, \hyperpage{65}
  \item privacy leakage, \hyperpage{65}
  \item privacy protection, \hyperpage{66}
  \item probabilistic model, \hyperpage{66}
  \item probabilistic PCA, \hyperpage{66}
  \item probability, \hyperpage{67}
  \item probability density function, \hyperpage{67}
  \item probability distribution, \hyperpage{67}
  \item probability space, \hyperpage{70}
  \item projected gradient descent (projected GD), \hyperpage{67}
  \item projection, \hyperpage{68}
  \item proximable, \hyperpage{68}
  \item proximal operator, \hyperpage{68}

  \indexspace

  \item quadratic function, \hyperpage{69}

  \indexspace

  \item R\'enyi divergence, \hyperpage{70}
  \item random forest, \hyperpage{70}
  \item random variable (RV), \hyperpage{70}
  \item realization, \hyperpage{70}
  \item rectified linear unit (ReLU), \hyperpage{70}
  \item regression, \hyperpage{71}
  \item regret, \hyperpage{71}
  \item regularization, \hyperpage{71}
  \item regularized empirical risk minimization (RERM), \hyperpage{73}
  \item regularizer, \hyperpage{73}
  \item reward, \hyperpage{73}
  \item ridge regression, \hyperpage{73}
  \item risk, \hyperpage{74}

  \indexspace

  \item sample, \hyperpage{74}
  \item sample covariance matrix, \hyperpage{74}
  \item sample mean, \hyperpage{74}
  \item sample size, \hyperpage{75}
  \item scatterplot, \hyperpage{75}
  \item semi-supervised learning, \hyperpage{75}
  \item sensitive attribute, \hyperpage{75}
  \item similarity graph, \hyperpage{76}
  \item singular value decomposition, \hyperpage{76}
  \item smooth, \hyperpage{76}
  \item soft clustering, \hyperpage{77}
  \item spectral clustering, \hyperpage{78}
  \item spectrogram, \hyperpage{80}
  \item squared error loss, \hyperpage{80}
  \item statistical aspects, \hyperpage{81}
  \item step size, \hyperpage{81}
  \item stochastic block model, \hyperpage{81}
  \item stochastic gradient descent, \hyperpage{81}
  \item stopping criterion, \hyperpage{82}
  \item strongly convex, \hyperpage{82}
  \item structural risk minimization, \hyperpage{83}
  \item subgradient, \hyperpage{83}
  \item subgradient descent, \hyperpage{83}
  \item support vector machine, \hyperpage{83}

  \indexspace

  \item test set, \hyperpage{84}
  \item total variation minimization, \hyperpage{44}
  \item training error, \hyperpage{84}
  \item training set, \hyperpage{84}
  \item transparency, \hyperpage{85}
  \item trustworthy AI, \hyperpage{85}

  \indexspace

  \item underfitting, \hyperpage{86}
  \item upper confidence bound, \hyperpage{86}

  \indexspace

  \item validation, \hyperpage{86}
  \item validation error, \hyperpage{86}
  \item validation set, \hyperpage{86}
  \item variance, \hyperpage{87}
  \item VC dimension, \hyperpage{87}
  \item vertical FL, \hyperpage{87}

  \indexspace

  \item weights, \hyperpage{87}

  \indexspace

  \item zero-gradient condition, \hyperpage{88}

\end{theindex}

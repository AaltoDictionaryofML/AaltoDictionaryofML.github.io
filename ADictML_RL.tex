\newglossaryentry{mdp}
{name={Markov decision process (MDP)},
	description={An MDP\index{Markov decision process (MDP)} is a mathematical 
	        structure for the study of \gls{reinforcementlearning}. 
		Formally, an MDP is a \gls{stochproc} that is defined by a specific choice for 
		\begin{itemize}
    			\item a \gls{statespace} $\statespace$;
    			\item an \gls{actionspace} $\actionspace$;
    			\item a transition \gls{function} $\prob{\state' \mid \state, \arm}$ 
				specifying the \gls{condprobdist} $\probdist^{(\state' \mid \state, \arm)}$
				over the next \gls{state} $\state' \in \statespace$, given the current \gls{state} 
				$\state \in \statespace$ and \gls{action} $\arm \in \actionspace$;
    			\item a \gls{reward} \gls{function} $\reward(\state, \arm) \in \mathbb{R}$ 
				that assigns a numerical \gls{reward} to each \gls{state}-\gls{action}
				pair $(\state,\arm)$.
		\end{itemize}
		For a given \gls{policy} $\policy$, these components define the \gls{probdist} of a \gls{sequence}
		$$ \state_{1},\,\arm_{1},\,\reward_{1},\,\state_{2},\,\arm_{2},\,\reward_{2},\,\ldots,\,\state_{\timeidx},\,\arm_{\timeidx},\,\reward_{\timeidx}$$ 
		of \glspl{rv}. The defining property of an MDP is the \gls{markovprop}. 
		That is, at time instant $\timeidx$, the \gls{condprobdist} of the next \gls{state} 
		$\state_{\timeidx+1}$ and \gls{reward} $\reward_{\timeidx}$ depends on the 
		past only via the current \gls{state} $\state_{\timeidx}$ and \gls{action} $\arm_{\timeidx}$. 
		\Gls{reinforcementlearning} methods try to learn a \gls{policy} $\policy$ that 
		maximizes the expected return:
		\[
			\expect\!\left\{ 
			 \sum_{\timeidx=1}^{\infty} 
			 \discountfac^{\timeidx-1} \reward_{\timeidx} 
			 \;\middle|\; \state_{1} 
			 \right\}. 
		\]
		The conditioning on the initial \gls{state} $\state_{1}$ indicates that the expected 
		return is evaluated by following the \gls{policy} $\policy$ from a given initial \gls{state}.
		The expected return involves the discount factor $\discountfac \in (0,1)$ 
		that determines the relative importance of future \glspl{reward} compared 
		to the immediate \gls{reward}. 
		The discount factor $\discountfac$ is typically fixed for a given MDP and 
		controls the trade-off between short-term and long-term \gls{reward}. 
		MDPs are widely used in robotics, game playing, and autonomous systems to model 
		decision-making problems where an agent interacts with an environment to 
		achieve a goal \cite{SuttonEd2}, \cite{BertsekasDynOptII}, \cite{BertsekasDynProgVolI}.
		\\
		See also: \gls{reinforcementlearning}, \gls{stochproc}, \gls{function}, \gls{reward}.},
 	first={Markov decision process (MDP)},
	type=reinflearning, 
	plural={MDPs}, 
	firstplural={Markov decision processes (MDPs)}, 
 	text={MDP} 
}

\newglossaryentry{statevaluefunction}
{name={state-value function},
	description={For a given \gls{mdp}, any \gls{policy} $\policy$ naturally induces 
		a \gls{valuefunction}\index{state-value function} 
		$\statevaluefunc{\policy}:\statespace \rightarrow \mathbb{R}$. 
		The value $\statevaluefunc{\policy}(\state)$ is the expected 
		return when the \gls{mdp} starts in a given \gls{state} $\state \in \statespace$ 
		and \glspl{action} are selected according to $\policy$.
		\\
		See also: \gls{mdp}, \gls{valuefunction}, \gls{state}. },
	first={state-value function},
	type=reinflearning,
	plural={state-value functions},
	firstplural={state-value functions},
	text={state-value function}
}

\newglossaryentry{valuefunction}
{name={value function},
	description={In the context of an \gls{mdp}, a value \gls{function}\index{value function} 
		$\valuefunc: \statespace \rightarrow \mathbb{R}$ assigns to each \gls{state} 
		$\state \in \statespace$ a real number $\valuefunc(\state)$ that quantifies 
		the long-term desirability of being in \gls{state} $\state$.
		\\
		See also: \gls{state}. },
	first={value function},
	type=reinflearning,
	plural={value functions},
	firstplural={value functions},
	text={value function}
}

\newglossaryentry{policy}
{name={policy (reinforcement learning)},
	description={A policy\index{policy} is a \gls{function} that specifies how 
	 	the next \gls{action} $\arm_{\timeidx}$ in an \gls{mdp} 
		is chosen when the current \gls{state} is $\state_{\timeidx}$. 
		Typically, a policy is \gls{stochastic}, meaning that it defines a 
		\gls{condprobdist} $\probdist^{(\arm \mid \state)}$ over the 
		\glspl{action} for a given current \gls{state}. We can view 
		a policy also as a \gls{hypothesis} that uses \glspl{feature} 
		derived from the current \gls{state} to predict the best next 
		\gls{action} \cite{SuttonEd2}.
		\\
		See also: \gls{action}, \gls{mdp}, \gls{state}.},
	first={policy},
	type=reinflearning,
	plural={policies},
	firstplural={policies},
	text={policy}
}

\newglossaryentry{bellmanoperator}
{name={Bellman operator},
	description={The Bellman \gls{operator}\index{Bellman operator} $\fixedpointop$ 
		associated with an \gls{mdp} is defined on the space of all 
		\glspl{valuefunction}. In particular, it maps a 
		\gls{valuefunction} $\valuefunc: \statespace \rightarrow \mathbb{R}$ 
		to another \gls{valuefunction} $\valuefunc': \statespace \rightarrow \mathbb{R}$ 
		as follows:
		\[
		       	\valuefunc'(\state) =\max_{\arm \in \actionspace}
			\Bigl(
			\expect\{ \reward(\state,\arm)\mid \state,\arm \}
			+ \discountfac \, \expect\{ \valuefunc(\state') \mid \state,\arm \}
			\Bigr),
		\]
		where $\discountfac \in (0,1)$ is a discount factor and $\state'$ is the 
		next \gls{state} generated according to the transition \gls{function} 
		$\state' \sim \prob{\state' \mid \state,\arm}$.
		The \gls{statevaluefunction} $\valuefunc^\star$ of the optimal 
		\gls{policy} $\policy^\star$ is a \gls{fixedpoint} of the Bellman \gls{operator}, 
		$\valuefunc^\star = \fixedpointop \valuefunc^\star$. This \gls{fixedpointeq} 
		naturally lends itself to the \gls{valueiteration} method for computing the \gls{statevaluefunction} 
		of an optimal \gls{policy}. Besides the Bellman \gls{operator} 
		associated with an \gls{mdp}, there is also a Bellman \gls{operator} 
		$\fixedpointop^{(\policy)}$ associated with a \gls{policy} $\policy$. 
		In this case, the Bellman \gls{operator} is defined as
		\[
			\fixedpointop^{(\policy)} \valuefunc(\state) = \expect\{ \reward(\state,\arm) \mid \state,\arm \}
			+ \discountfac \, \expect\{ \valuefunc(\state') \mid \state,\arm \},
		\]
		where $\state' \sim \prob{\state' \mid \state,\arm}$ and $\arm$ is selected
		according to $\policy$. The \gls{statevaluefunction} $\statevaluefunc{\policy}$ is a 
		\gls{fixedpoint} of $\fixedpointop^{(\policy)}$, $\statevaluefunc{\policy}= \fixedpointop^{(\policy)}\statevaluefunc{\policy}$. 
		This \gls{fixedpointeq} can be solved by a \gls{fixedpointiter} that is known as \gls{policyevaluation}. 
		The Bellman \gls{operator} is named after Richard Bellman, who introduced it in the context of
		dynamic programming \cite{Bellman1957}. The Bellman \gls{operator} is a key
		concept in \gls{reinforcementlearning} and is used to derive \glspl{algorithm} for
		solving \glspl{mdp}, such as \gls{valueiteration} and \gls{policy} \gls{iteration}\cite{SuttonEd2}.
		\\
		See also: \gls{mdp}, \gls{valuefunction}, \gls{policy}, \gls{valueiteration}, \gls{contractop}, \gls{banachfixedpoint}. }, 
	first={Bellman operator},
	type=reinflearning,
	plural={Bellman operators},
	firstplural={Bellman operators},
	text={Bellman operator}
}

\newglossaryentry{policyevaluation}
{name={policy evaluation (reinforcement learning)},
	description={\Gls{policy} evaluation\index{policy evaluation} refers to computing 
		the \gls{statevaluefunction} $\statevaluefunc{\policy}$ 
		of a given \gls{policy} $\policy$ in an \gls{mdp}. One widely used method, referred to 
		as iterative \gls{policy} evaluation, is based on the characterization of 
		$\statevaluefunc{\policy}$ as a \gls{fixedpoint} of the \gls{bellmanoperator} 
		$\fixedpointop^{(\policy)}$. In particular, starting from an initial 
		\gls{valuefunction} $\valuefunc_{0}$, we iteratively apply the \gls{bellmanoperator}
		$\fixedpointop^{(\policy)}$ to obtain a \gls{sequence} of \glspl{valuefunction} 
		$\valuefunc_{1}, \,\valuefunc_{2}, \,\ldots$ as follows:
		\[
		\valuefunc_{\iteridx+1} = \fixedpointop^{(\policy)} \valuefunc_{\iteridx}, \qquad \iteridx=0,\,1,\,2,\,\ldots.
		\]
		Under mild conditions, this \gls{fixedpointiter} converges to 
		$\statevaluefunc{\policy}$ as $\iteridx \rightarrow \infty$ \cite[Sec. 4.2]{SuttonEd2}.
		\\
		See also: \gls{policy}, \gls{statevaluefunction}, \gls{mdp}. }, 
	first={policy evaluation},
	type=reinflearning,
	text={policy evaluation}	 
}

\newglossaryentry{valueiteration}
{name={value iteration},
	description={Consider an \gls{mdp} with the associated \gls{bellmanoperator} $\fixedpointop$. The 
		\gls{statevaluefunction} $\valuefunc^\star$ of the optimal \gls{policy} 
		is a \gls{fixedpoint} of $\fixedpointop$, i.e., $\valuefunc^\star = \fixedpointop \valuefunc^\star$. 
		Value \gls{iteration}\index{value iteration} is the \gls{fixedpointiter} for computing
		$\valuefunc^\star$ by repeatedly applying $\fixedpointop$ to an initial 
		\gls{valuefunction} $\valuefunc_{0}$ \cite[Sec. 4.4]{SuttonEd2}.
		\\
		See also: \gls{statevaluefunction}, \gls{fixedpointiter}, \gls{valuefunction}. }, 
	first={value iteration},
	type=reinflearning,
	plural={value iterations},
	firstplural={value iterations},
	text={value iteration} 
}

\newglossaryentry{reinforcementlearning}
{name={reinforcement learning (RL)},
	description={RL\index{reinforcement learning (RL)} refers to an \gls{onlinelearning} setting where 
		we can only evaluate the usefulness of a single \gls{hypothesis} (i.e., a specific
		choice of \glspl{modelparam}) at each time step $\timeidx$. In particular, RL 
		methods apply the current \gls{hypothesis} $\hypothesis^{(\timeidx)}$ to the 
		\gls{featurevec} $\featurevec^{(\timeidx)}$ of the newly received \gls{datapoint}
		to predict the next \gls{action}. 
		The usefulness of the resulting \gls{prediction} $\hypothesis^{(\timeidx)}(\featurevec^{(\timeidx)})$ 
		is quantified by a \gls{reward} signal $\reward^{(\timeidx)}$ (see Fig. \ref{fig_reinforcementlearning_dict}). 
		\begin{figure}[H]
		\begin{center}
			\begin{tikzpicture}[scale=1]
			\draw[->] (-2, 0) -- (6, 0);
			\node at (6.3, 0) {$\hypothesis$};
	        		% loss at time t 
			\draw[thick, blue, domain=0:3, samples=20] plot (\x-3, {-0.2*(\x)^2 + 2});
			\node[anchor=west,yshift=4pt] at (0-3, {-0.2*(0)^2 + 2}) {$-\loss^{(\timeidx)}(\hypothesis)$};
			% Marker and hypothesis label for h^(t)
			\filldraw[blue] (1.5-3, {-0.2*(1.5)^2 + 2}) circle (2pt);
			\node[anchor=north] at (1.5-3, -0.3) {$\hypothesis^{(\timeidx)}$};		
			\draw[dotted] (1.5-3, 0) -- (1.5-3, {-0.2*(1.5)^2 + 2});
			%%% time t+1
			\draw[thick, red, domain=0:5, samples=20, dashed] plot (\x, {-0.15*(\x - 2)^2 + 3});
			\node[anchor=west,yshift=4pt] at (3, {-0.15*(3 - 2)^2 + 3}) {$-\loss^{(\timeidx+1)}(\hypothesis)$};
			\filldraw[red] (2, {-0.15*(2 - 2)^2 + 3}) circle (2pt);
			\node[anchor=north] at (2, -0.3) {$\hypothesis^{(\timeidx+1)}$};
			\draw[dotted] (2, 0) -- (2, {-0.15*(3 - 2)^2 + 3});
			%%% time t+2
			\draw[thick, green!60!black, domain=3:5, samples=20, dotted] plot (\x+2, {-0.1*(\x - 4)^2 + 1.5});
			\node[anchor=west,yshift=4pt] at (4.5+2, {-0.1*(4.5 - 4)^2 + 1.5}) {$-\loss^{(\timeidx+2)}(\hypothesis)$};
			\filldraw[green!60!black] (3.5+2, {-0.1*(3.5 - 4)^2 + 1.5}) circle (2pt);
			\node[anchor=north] at (3.5+2, -0.3) {$\hypothesis^{(\timeidx+2)}$};
			\draw[dotted] (3.5+2, 0) -- (3.5+2, {-0.1*(3.5 - 4)^2 + 1.5});
			\end{tikzpicture}
		\caption{Three consecutive time steps $\timeidx,\timeidx+1,\timeidx+2$ with corresponding \glspl{lossfunc} $\loss^{(\timeidx)},
			\loss^{(\timeidx+1)}, \loss^{(\timeidx+2)}$. During time step $\timeidx$, an RL method can evaluate the 
			\gls{lossfunc} only for one specific \gls{hypothesis} $\hypothesis^{(\timeidx)}$, resulting in the \gls{reward} 
			signal $\reward^{(\timeidx)}=-\loss^{(\timeidx)}(\hypothesis^{(\timeidx)})$. \label{fig_reinforcementlearning_dict}}
		\end{center}
		\end{figure}
		In general, the \gls{reward} depends also on the 
		previous \glspl{prediction} $\hypothesis^{(\timeidx')}\big(\featurevec^{(\timeidx')}\big)$ 
		for $\timeidx' < \timeidx$. The goal of RL is to learn $\hypothesis^{(\timeidx)}$, for 
		each time step $\timeidx$, such that the (possibly discounted) cumulative \gls{reward} 
		is maximized \cite{MLBasics}, \cite{SuttonEd2}.
		\\
		See also: \gls{reward}, \gls{lossfunc}, \gls{ml}.},
	first={reinforcement learning (RL)},
	type=reinflearning,
	text={RL}
}

\newglossaryentry{action}
{name={action},
	description={An action\index{action} refers to a decision taken by an \gls{aisystem}
		at a given time step $\timeidx$ that influences the observed 
		\gls{reward} signal. The actions are elements of an 
		\gls{actionspace} $\actionspace$ and are typically denoted by 
		$\arm_{\timeidx} \in \actionspace$. The action $\arm_{\timeidx}$ is selected 
		based on the \gls{featurevec} $\featurevec^{(\timeidx)}$ (which collects 
		all available observations) and the current 
		\gls{hypothesis} $\hypothesis^{(\timeidx)}$. 
		\Gls{reinforcementlearning} uses \gls{onlinelearning} methods 
		to learn a \gls{hypothesis} $\hypothesis^{(\timeidx)}$ that predicts 
		a (nearly) optimal action. The usefulness of the 
		\gls{prediction} $\arm_{\timeidx}$ is evaluated indirectly 
		through the resulting \gls{reward} signal $\reward^{(\timeidx)}$.
		In the special case of an \gls{mab}, the set of possible actions is 
		finite and each action corresponds to selecting one arm. In more
		general \gls{reinforcementlearning} settings, the \gls{actionspace} may be \gls{continuous}.
		\\
		See also: \gls{reward}, \gls{hypothesis}, \gls{reinforcementlearning}, \gls{mab}, \gls{lossfunc}.},
	first={action},
	type=reinflearning,
	plural={actions},
	firstplural={actions},
	text={action}
}

\newglossaryentry{actionspace}
{name={action space},
	description={See\index{action space} \gls{action}.} ,
	first={action space},
	type=reinflearning,
	plural={action spaces},
	firstplural={action spaces},
	text={action space}
}

\newglossaryentry{mab}
{name={multiarmed bandit (MAB)},
	description={An MAB\index{multiarmed bandit (MAB)} is a precise 
		formulation of a sequential decision-making task under \gls{uncertainty}. 
		At each time step $\iteridx$, one must choose an \gls{action} from a finite 
		\gls{actionspace} $\actionspace$. Choosing \gls{action} 
		$\arm$ at time $\iteridx$ yields a \gls{reward} $\reward^{(\arm,\iteridx)}$. 
		Each MAB induces an \gls{ml} problem, i.e., to learn a \gls{hypothesis} that 
		predicts the optimal \gls{action} $\arm_{\timeidx}$ at time $\timeidx$. 
		This \gls{prediction} must be based on the \glspl{action} 
		and \glspl{reward} received up to time $\timeidx-1$ 
		\cite{SuttonEd2}, \cite{Bubeck2012}.
			\\ 
		See also: \gls{reward}, \gls{regret}.},
	first={MAB},
	type=reinflearning,
	text={MAB}
}

\newglossaryentry{stochmab}
{name={stochastic multiarmed bandit (stochastic MAB)},
	description={A stochastic \gls{mab}\index{stochastic multiarmed bandit (stochastic MAB)} 
		is a \gls{stochproc} that is obtained from an \gls{mab}. In particular, 
		the \gls{reward} $\reward^{(\arm,\iteridx)}$ is modeled as an \gls{rv}		 
		with an unknown \gls{probdist} $\probdist^{\big(\reward^{(\arm,\iteridx)}\big)}$ 
		\cite{HazanOCO}, \cite{Bubeck2012}. 
		In the simplest setting, the \gls{probdist} $\probdist^{\big(\reward^{(\arm,\iteridx)}\big)}$ 
		does not depend on $\timeidx$, i.e., it is time invariant. 
		\\ 
		See also: \gls{reward}, \gls{regret}.},
	first={stochastic multiarmed bandit (stochastic MAB)},
	type=reinflearning,
	text={stochastic MAB}
}

\newglossaryentry{regret}
{name={regret},
	description={The regret\index{regret} of a \gls{hypothesis} $\hypothesis$ relative to 
		another \gls{hypothesis} $\hypothesis'$, which serves as a \gls{baseline}, 
		is the difference between the \gls{loss} incurred by $\hypothesis$ and the \gls{loss} 
		incurred by $\hypothesis'$ \cite{PredictionLearningGames}. 
		The \gls{baseline} \gls{hypothesis} $\hypothesis'$ is also referred to as an \gls{expert}.
					\\ 
		See also: \gls{baseline}, \gls{loss}, \gls{expert}.},
	first={regret},
	type=reinflearning,
	text={regret} 
}
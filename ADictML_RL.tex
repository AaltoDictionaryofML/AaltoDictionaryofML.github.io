\newglossaryentry{mdp}
{name={Markov decision process (MDP)},
	description={An MDP \index{Markov decision process (MDP)} is a mathematical 
	        structure for the study of \gls{reinforcementlearning}. 
			Formally, an MDP is a \gls{stochproc} that is defined by a specific choice for 
			\begin{itemize}
    			\item a \gls{statespace} $\statespace$;
    			\item an \gls{actionspace} $\actionspace$;
    			\item a transition \gls{function} $\prob{\state' \mid \state, \arm}$ 
			          specifying the \gls{condprobdist} $\probdist^{(\state' \mid \state, \arm)}$ 
					  over the next \gls{state} $\state' \in \statespace$, given the current \gls{state} 
					  $\state \in \statespace$ and action $\arm \in \actionspace$;
    			\item a \gls{reward} \gls{function} $\reward(\state, \arm) \in \mathbb{R}$ 
					  that assigns a numerical \gls{reward} to each \gls{state}-\gls{action} 
					  pair $(\state,\arm)$.
			\end{itemize}
			For a given policy $\policy$, these components define the \gls{probdist} of a 
			\gls{sequence} $$ \state_{1},\,\arm_{1},\,\reward_{1},\,\state_{2},\,\arm_{2},\,\reward_{2},\,\ldots,\,\state_{\timeidx},\,\arm_{\timeidx},\,\reward_{\timeidx}$$ 
			of \glspl{rv}. The defining property of an MDP is the \gls{markovprop}. 
			That is, at time $\timeidx$, the \gls{condprobdist} of the next state 
			$\state_{\timeidx+1}$ and \gls{reward} $\reward_{\timeidx}$ depends on the 
			past only via the current \gls{state} $\state_{\timeidx}$ and action $\arm_{\timeidx}$. 
			\Gls{reinforcementlearning} methods try to learn a \gls{policy} $\policy$ that 
			maximizes the expected return 
			 \[
			 	\expect\!\left\{ 
			 	\sum_{\timeidx=1}^{\infty} 
			 	\discountfac^{\timeidx-1} \reward_{\timeidx} 
			 	\;\middle|\; \state_{1} 
			 	\right\}. 
			 \]
			 The conditioning on the initial \gls{state} $\state_{1}$  indicates that the expected 
			 return is evaluated by following the \gls{policy} $\policy$ from a given initial \gls{state}.
			 The expected return involves the discount factor $\discountfac \in (0,1)$ 
			 that determines the relative importance of future \glspl{reward} compared 
			 to the immediate \gls{reward}. 
			 The discount factor $\discountfac$ is typically fixed for a given \gls{mdp} and 
			 controls the trade-off between short-term and long-term \gls{reward}. 
			 MDPs are widely used in robotics, game playing, and autonomous systems, to model 
			 decision-making problems where an agent interacts with an environment to 
			 achieve a goal\cite{SuttonEd2}, \cite{Bertsekas2012}.
		\\
		See also: \gls{reinforcementlearning}, \gls{stochproc}, \gls{function},  \gls{reward}.},
 	first={Markov decision process (MDP)},
	type=reinflearning, 
	plural={MDPs}, 
	firstplural={MDPs}, 
 	text={MDP} 
}

\newglossaryentry{statevaluefunction}
{name={state-value function},
	description={For a given \gls{mdp}, any \gls{policy} $\policy$ naturally induces 
	             a value \gls{function} \index{state-value function} 
				 $\statevaluefunc{\policy}:\statespace \rightarrow \mathbb{R}$. 
				 The value $\statevaluefunc{\policy}(\state)$ is the expected 
				 return when the \gls{mdp} starts in given state $\state \in \statespace$ 
				  and \glspl{action} are selected according to $\policy$.},
	first={state-value function},
	type=reinflearning,
	plural={state-value functions},
	firstplural={state-value functions},
	text={state-value function}
}

\newglossaryentry{valuefunction}
{name={value function},
	description={In the context of a \gls{mdp}, a value \gls{function} \index{value function} 
				 $\valuefunc: \statespace \rightarrow \mathbb{R}$ assigns to each state 
				 $\state \in \statespace$ a real number $\valuefunc(\state)$ that quantifies 
				 the long-term desirability of being in state $\state$.},
	first={value function},
	type=reinflearning,
	plural={value functions},
	firstplural={value functions},
	text={value function}
}

\newglossaryentry{policy}
{name={policy (reinforcement learning)},
	description={A policy \index{policy} is a \gls{function} specifies how 
	 			 the next \gls{action} $\arm_{\timeidx}$ in a \gls{mdp} 
				 is chosen when the current \gls{state} is $\state_{\timeidx}$. 
				 Typically, a policy is stochastic, meaning that it defines a 
				 \gls{condprobdist} $\probdist^{(\arm \mid \state)}$ over the 
				 \glspl{action} for a given current \gls{state}. We can view 
				 a policy also as a \gls{hypothesis} that uses \glspl{feature} 
				 derived from the current \gls{state} to predict the best next 
				 \gls{action} \cite{SuttonEd2}.
		\\
		See also: \gls{mdp}, \gls{action}, \gls{state}.},
	first={policy},
	type=reinflearning,
	plural={policies},
	firstplural={policies},
	text={policy}
}


\newglossaryentry{bellmanoperator}
{name={Bellman operator},
	description={The Bellman operator \index{Bellman operator} $\fixedpointop$ 
	             associated with an \gls{mdp} is defined on the space of all 
				 \glspl{valuefunction}. In particular, it maps a 
				 \gls{valuefunction} $\valuefunc: \statespace \rightarrow \mathbb{R}$ 
				 to another \gls{valuefunction} $\valuefunc': \statespace \rightarrow \mathbb{R}$ 
				according to
				\[
		       	\valuefunc'(\state) =\max_{\arm \in \actionspace}
					\Bigl(
						\expect\{ \reward(\state,\arm)\mid \state,\arm \}
					+ \discountfac \, \expect\{ \valuefunc(\state') \mid \state,\arm \}
				\Bigr),
					\]
				where $\discountfac \in (0,1)$ is a discount factor and $\state'$ is the 
				next state generated according to the transition \gls{function}. 
				$\state' \sim \prob{\state' \mid \state,\arm}$.
				The \gls{statevaluefunction} $\valuefunc^\star$ of the optimal 
				\gls{policy} $\policy^\star$ is a \gls{fixedpoint} of the Bellman operator, 
				$\valuefunc^\star = \fixedpointop \valuefunc^\star$. This \gls{fixedpointeq} 
			    lends naturally to the value iteration method for computing the \gls{statevaluefunction} 
			    of an optimal \gls{policy}. Beside the Bellman operator 
				associated with an \gls{mdp}, there is also a Bellman operator 
				$\fixedpointop^{(\policy)}$ associated with a \gls{policy} $\policy$. 
				In this case, the Bellman operator is defined as
				\[
				\fixedpointop^{(\policy)} \valuefunc(\state) = \expect\{ \reward(\state,\arm) \mid \state,\arm \}
				+ \discountfac \, \expect\{ \valuefunc(\state') \mid \state,\arm \},
				\]
				where $\state' \sim \prob{\state' \mid \state,\arm}$ and $\arm$ is selected
				according to $\policy$. The \gls{statevaluefunction} $\statevaluefunc{\policy}$ is a 
				\gls{fixedpoint} of $\fixedpointop^{(\policy)}$, $\statevaluefunc{\policy}= \fixedpointop^{(\policy)}\statevaluefunc{\policy}$. This \gls{fixedpointeq} can 
				be solved by a \gls{fixedpointiter} which is know as \gls{policyevaluation}. 
				The Bellman operator is named after Richard Bellman, who introduced it in the context of
				dynamic programming \cite{Bellman1957}. The Bellman operator is a key
				concept in \gls{reinforcementlearning} and is used to derive algorithms for
				solving \glspl{mdp}, such as value iteration and policy iteration\cite{SuttonEd2}.}, 
	first={Bellman operator},
	type=reinflearning,
	plural={Bellman operators},
	firstplural={Bellman operators},
	text={Bellman operator}
}

\newglossaryentry{policyevaluation}
{name={policy evaluation (reinforcement learning)},
 description={Policy evaluation refers to computing the \gls{statevaluefunction} $\statevaluefunc{\policy}$ 
		      of a given \gls{policy} $\policy$ in a \gls{mdp}. One widely used method, referred to 
			  as iterative policy evaluation, is based on the characterization of 
			  $\statevaluefunc{\policy}$ as a \gls{fixedpoint} of the \gls{bellmanoperator} 
			  $\fixedpointop^{(\policy)}$. In particular, starting from an initial 
			  \gls{valuefunction} $\valuefunc_{0}$, we iteratively apply the \gls{bellmanoperator}
			  $\fixedpointop^{(\policy)}$ to obtain a sequence of \glspl{valuefunction} 
			  $\valuefunc_{1}, \valuefunc_{2}, \ldots$ according to 
			  \[
			  	\valuefunc_{\iteridx+1} = \fixedpointop^{(\policy)} \valuefunc_{\iteridx}, \quad \iteridx=0,1,2,\ldots
			  \]
			  Under mild conditions, this \gls{fixedpointiter} converges to 
			  $\statevaluefunc{\policy}$ as $\iteridx \rightarrow \infty$ \cite[Sec. 4.2]{SuttonEd2}.}, 
	text = {policy evaluation}, 
	type=reinflearning,
	first = {policy evaluation} 
}

\newglossaryentry{valueiteration}
{name={value iteration},
 description={Consider a \gls{mdp} with associated \gls{bellmanoperator} $\fixedpointop$. The 
              \gls{statevaluefunction} $\valuefunc^\star$ of the optimal \gls{policy} 
			  is a \gls{fixedpoint} of $\fixedpointop$, i.e., $\valuefunc^\star = \fixedpointop \valuefunc^\star$. 
			  Value iteration\index{value iteration} is the \gls{fixedpointiter} for computing
			  $\valuefunc^\star$ by repeatedly applying $\fixedpointop$ to an initial 
			  \gls{valuefunction} $\valuefunc_{0}$ \cite[Sec. 4.4]{SuttonEd2}}, 
	first={value iteration},
	type=reinflearning,
	plural={value iterations},
	firstplural={value iterations},
	text={value iteration} 
}


\newglossaryentry{reinforcementlearning}
{name={reinforcement learning (RL)},
	description={RL\index{reinforcement learning (RL)} refers to an \gls{onlinelearning} setting where 
		we can only evaluate the usefulness of a single \gls{hypothesis} (i.e., a specific 
		choice of \glspl{modelparam}) at each time step $\timeidx$. In particular, RL 
		methods apply the current \gls{hypothesis} $\hypothesis^{(\timeidx)}$ to the 
		\gls{featurevec} $\featurevec^{(\timeidx)}$ of the newly received \gls{datapoint} 
		to predict the next \gls{action}. 
		The usefulness of the resulting \gls{prediction} $\hypothesis^{(\timeidx)}(\featurevec^{(\timeidx)})$ 
		is quantified by a \gls{reward} signal $\reward^{(\timeidx)}$ (see Fig. \ref{fig_reinforcementlearning_dict}). 
		\begin{figure}[H]
		\begin{center}
			\begin{tikzpicture}[scale=1]
			\draw[->] (-2, 0) -- (6, 0);
			\node at (6.3, 0) {$\hypothesis$};
	        		% loss at time t 
			\draw[thick, blue, domain=0:3, samples=20] plot (\x-3, {-0.2*(\x)^2 + 2});
			\node[anchor=west,yshift=4pt] at (0-3, {-0.2*(0)^2 + 2}) {$-\loss^{(\timeidx)}(\hypothesis)$};
			% Marker and hypothesis label for h^(t)
			\filldraw[blue] (1.5-3, {-0.2*(1.5)^2 + 2}) circle (2pt);
			\node[anchor=north] at (1.5-3, -0.3) {$\hypothesis^{(\timeidx)}$};		
			\draw[dotted] (1.5-3, 0) -- (1.5-3, {-0.2*(1.5)^2 + 2});
			%%% time t+1
			\draw[thick, red, domain=0:5, samples=20, dashed] plot (\x, {-0.15*(\x - 2)^2 + 3});
			\node[anchor=west,yshift=4pt] at (3, {-0.15*(3 - 2)^2 + 3}) {$-\loss^{(\timeidx+1)}(\hypothesis)$};
			\filldraw[red] (2, {-0.15*(2 - 2)^2 + 3}) circle (2pt);
			\node[anchor=north] at (2, -0.3) {$\hypothesis^{(\timeidx+1)}$};
			\draw[dotted] (2, 0) -- (2, {-0.15*(3 - 2)^2 + 3});
			%%% time t+2
			\draw[thick, green!60!black, domain=3:5, samples=20, dotted] plot (\x+2, {-0.1*(\x - 4)^2 + 1.5});
			\node[anchor=west,yshift=4pt] at (4.5+2, {-0.1*(4.5 - 4)^2 + 1.5}) {$-\loss^{(\timeidx+2)}(\hypothesis)$};
			\filldraw[green!60!black] (3.5+2, {-0.1*(3.5 - 4)^2 + 1.5}) circle (2pt);
			\node[anchor=north] at (3.5+2, -0.3) {$\hypothesis^{(\timeidx+2)}$};
			\draw[dotted] (3.5+2, 0) -- (3.5+2, {-0.1*(3.5 - 4)^2 + 1.5});
			\end{tikzpicture}
		\caption{Three consecutive time steps $\timeidx,\timeidx+1,\timeidx+2$ with corresponding \glspl{lossfunc} $\loss^{(\timeidx)},
			\loss^{(\timeidx+1)}, \loss^{(\timeidx+2)}$. During time step $\timeidx$, an RL method can evaluate the 
			\gls{lossfunc} only for one specific \gls{hypothesis} $\hypothesis^{(\timeidx)}$, resulting in the \gls{reward} 
			signal $\reward^{(\timeidx)}=-\loss^{(\timeidx)}(\hypothesis^{(\timeidx)})$. \label{fig_reinforcementlearning_dict}}
		\end{center}
		\end{figure}
		In general, the \gls{reward} depends also on the 
		previous \glspl{prediction} $\hypothesis^{(\timeidx')}\big(\featurevec^{(\timeidx')}\big)$ 
		for $\timeidx' < \timeidx$. The goal of RL is to learn $\hypothesis^{(\timeidx)}$, for 
		each time step $\timeidx$, such that the (possibly discounted) cumulative \gls{reward} 
		is maximized \cite{MLBasics}, \cite{SuttonEd2}.
		\\
		See also: \gls{reward}, \gls{lossfunc}, \gls{ml}.},
	first={reinforcement learning (RL)},
	type=reinflearning,
	text={RL}
}

\newglossaryentry{action}
{name={action},
	description={An action\index{action} refers to a decision taken by an \gls{aisystem}
		at a given time step $\timeidx$ that influences the observed 
		\gls{reward} signal. The actions are elements of an 
		\gls{actionspace} $\actionspace$ and are typically denoted by 
		$\arm_{\timeidx} \in \actionspace$. The action $\arm_{\timeidx}$ is selected 
		based on the \gls{featurevec} $\featurevec^{(\timeidx)}$ (which collects 
		all available observations) and the current 
		\gls{hypothesis} $\hypothesis^{(\timeidx)}$. 
		\Gls{reinforcementlearning} uses \gls{onlinelearning} methods 
		to learn a \gls{hypothesis} $\hypothesis^{(\timeidx)}$ that predicts 
		a (nearly) optimal action. The usefulness of the 
		\gls{prediction} $\arm_{\timeidx}$ is evaluated indirectly 
		through the resulting \gls{reward} signal $\reward^{(\timeidx)}$.
		In the special case of an \gls{mab}, the set of possible actions is 
		finite and each action corresponds to selecting one arm. In more
		general \gls{reinforcementlearning} settings, the \gls{actionspace} may be \gls{continuous}.
		\\
		See also: \gls{reward}, \gls{hypothesis}, \gls{reinforcementlearning}, \gls{mab}, \gls{lossfunc}.},
	first={action},
	type=reinflearning,
	plural={actions},
	firstplural={actions},
	text={action}
}

\newglossaryentry{actionspace}
{name={action space},
	description={See\index{action space} \gls{action}.} ,
	first={action space},
	type=reinflearning,
	plural={action spaces},
	firstplural={action spaces},
	text={action space}
}

\newglossaryentry{mab}
{name={multiarmed bandit (MAB)},
	description={A MAB \index{multiarmed bandit (MAB)} is a precise 
		         formulation of a sequential decision-making task under \gls{uncertainty}. 
				 At each time step $\iteridx$, one must choose an \gls{action} from an finite 
				 \gls{actionspace} $\actionspace$. Choosing \gls{action} 
				 $\arm$ at time $\iteridx$ yields a \gls{reward} $\reward^{(\arm,\iteridx)}$. 
				 Each MAB induces a \gls{ml} problem: Learn a \gls{hypothesis} that 
				 predicts the optimal \gls{action} $\arm_{\timeidx}$ at time $\timeidx$. 
				 This \gls{prediction} must be based on the \glspl{action} 
				 and \glspl{reward} received up to time $\timeidx-1$ 
				  \cite{Bubeck2012}, \cite{SuttonEd2}. 
		See also: \gls{reward}, \gls{regret}.},
	first={MAB},
	type=reinflearning,
	text={MAB}
}

\newglossaryentry{stochmab}
{name={stochastic multiarmed bandit (MAB)},
	description={An stochastic \gls{mab} \index{stochastic multiarmed bandit (MAB)} 
	             is a \gls{stochproc} that is obtained from a \gls{mab}. In particular, 
				 the \gls{reward} $\reward^{(\arm,\iteridx)}$ is modelled as a \gls{rv}		 
				 with an unknown \gls{probdist} $\probdist^{\big(\reward^{(\arm,\iteridx)}\big)}$ \cite{Bubeck2012,HazanOCO}. 
				 In the simplest setting, the \gls{probdist} $\probdist^{\big(\reward^{(\arm,\iteridx)}\big)}$ 
				 does not depend on $\timeidx$, i.e., it is time invariant. \\ 
		See also: \gls{reward}, \gls{regret}.},
	first={stochastic MAB},
	type=reinflearning,
	text={stochastic MAB}
}

\newglossaryentry{regret}
{name={regret},
	description={The regret\index{regret} of a \gls{hypothesis} $\hypothesis$ relative to 
		another \gls{hypothesis} $\hypothesis'$, which serves as a \gls{baseline}, 
		is the difference between the \gls{loss} incurred by $\hypothesis$ and the \gls{loss} 
		incurred by $\hypothesis'$ \cite{PredictionLearningGames}. 
		The \gls{baseline} \gls{hypothesis} $\hypothesis'$ is also referred to as an \gls{expert}.
					\\ 
		See also: \gls{baseline}, \gls{loss}, \gls{expert}.},
	first={regret},
	type=reinflearning,
	text={regret} 
}
\newglossaryentry{mdp}
{name={Markov decision process (MDP)},
	description={An MDP \index{Markov decision process (MDP)} is a mathematical 
	        structure for the study of \gls{reinforcementlearning}. 
		Formally, an MDP is a \gls{stochproc} that is defined by a specific choice for 
		\begin{itemize}
    			\item a state space $\statespace$;
    			\item an action space $\mathcal{A}$;
    			\item a transition \gls{function} $\prob{\state' \mid \state, \arm}$ 
			specifying the \gls{probdist} over the next state 
			$\state' \in \statespace$, given the current state 
			$\state \in \statespace$ and action $\arm \in \mathcal{A}$;
    			\item a \gls{reward} \gls{function} $\reward(\state, \arm) \in \mathbb{R}$ 
			that assigns a numerical \gls{reward} to each 
			state-action pair $(\state,\arm)$.
		\end{itemize}
		These components define the \gls{probdist} of a \gls{sequence}
		$$ \state_{1},\,\arm_{1},\,\reward_{1},\,\state_{2},\,\arm_{2},\,\reward_{2},\,\ldots,\,\state_{\timeidx},\,\arm_{\timeidx},\,\reward_{\timeidx}$$ 
		of \glspl{rv}. The defining property of an MDP is the \gls{markovprop}. That is, at 
		time instant $\timeidx$, the \gls{condprobdist} of the 
		next state $\state_{\timeidx+1}$ and \gls{reward} $\reward_{\timeidx}$ 
		depends on the past only via the current state $\state_{\timeidx}$ and action $\arm_{\timeidx}$. 
		\\
		See also: \gls{reinforcementlearning}, \gls{stochproc}, \gls{function}, \gls{probdist}, \gls{reward}, \gls{prediction}.},
 	first={Markov decision process (MDP)},
	type=reinflearning, 
	plural={MDPs}, 
	firstplural={MDPs}, 
 	text={MDP} 
}
\newglossaryentry{mdp}
{name={Markov decision process (MDP)},
	description={An MDP \index{Markov decision process (MDP)} is a mathematical 
	        structure for the study of \gls{reinforcementlearning}. 
			Formally, an MDP is a \gls{stochproc} 
			which is defined by a specific choice for 
			\begin{itemize}
    			\item a \gls{statespace} $\statespace$;
    			\item an \gls{actionspace} $\actionspace$;
    			\item a transition \gls{function} $\prob{\state' \mid \state, \arm}$ 
				     specifying the \gls{probdist} over the next state 
					 $\state' \in \statespace$, given the current state 
					 $\state \in \statespace$ and action $\arm \in \mathcal{A}$;
    			\item a \gls{reward} \gls{function} $\reward(\state, \arm) \in \mathbb{R}$ 
				      that assigns a numerical \gls{reward} to each 
					  state-action pair $(\state,\arm)$.
			\end{itemize}
			These components define the \gls{probdist} of a sequence
			$$ \state_{1},\arm_{1},\reward_{1},\state_{2},\arm_{2},\reward_{2},\ldots,\state_{\timeidx},\arm_{\timeidx},\reward_{\timeidx}$$ 
			of \glspl{rv}. The defining property of an MDP is the \gls{markovprop}. That is, at 
			time instant $\timeidx$, the \gls{condprobdist} of the 
			next state $\state_{\timeidx+1}$ and \gls{reward} $\reward_{\timeidx}$ 
			dpends on the past only via the current state $\state_{\timeidx}$ and action $\arm_{\timeidx}$. 
		\\
		See also: \gls{reinforcementlearning}, \gls{reward}, \gls{prediction}, \gls{stochproc}, \gls{function}, \gls{probdist}.},
 	first={MDP},
	type=reinflearning, 
	plural={MDPs}, 
	firstplural={MDPs}, 
 	text={MDP} 
}

\newglossaryentry{reinforcementlearning}
{name={reinforcement learning (RL)},
	description={RL\index{reinforcement learning (RL)} refers to an \gls{onlinelearning} setting where 
		we can only evaluate the usefulness of a single \gls{hypothesis} (i.e., a choice of \glspl{modelparam}) 
		at each time step $\timeidx$. In particular, RL methods apply the current \gls{hypothesis} 
		$\hypothesis^{(\timeidx)}$ to the \gls{featurevec} $\featurevec^{(\timeidx)}$ of the 
		newly received \gls{datapoint}. The usefulness of the resulting \gls{prediction} 
		$\hypothesis^{(\timeidx)}(\featurevec^{(\timeidx)})$ is quantified by a \gls{reward} 
		signal $\reward^{(\timeidx)}$ (see Fig. \ref{fig_reinforcementlearning_dict}). 
		\begin{figure}[H]
		\begin{center}
			\begin{tikzpicture}[scale=1]
			\draw[->] (-2, 0) -- (6, 0);
			\node at (6.3, 0) {$\hypothesis$};
	        		% loss at time t 
			\draw[thick, blue, domain=0:3, samples=20] plot (\x-3, {-0.2*(\x)^2 + 2});
			\node[anchor=west,yshift=4pt] at (0-3, {-0.2*(0)^2 + 2}) {$-\loss^{(\timeidx)}(\hypothesis)$};
			% Marker and hypothesis label for h^(t)
			\filldraw[blue] (1.5-3, {-0.2*(1.5)^2 + 2}) circle (2pt);
			\node[anchor=north] at (1.5-3, -0.3) {$\hypothesis^{(\timeidx)}$};		
			\draw[dotted] (1.5-3, 0) -- (1.5-3, {-0.2*(1.5)^2 + 2});
			%%% time t+1
			\draw[thick, red, domain=0:5, samples=20, dashed] plot (\x, {-0.15*(\x - 2)^2 + 3});
			\node[anchor=west,yshift=4pt] at (3, {-0.15*(3 - 2)^2 + 3}) {$-\loss^{(\timeidx+1)}(\hypothesis)$};
			\filldraw[red] (2, {-0.15*(2 - 2)^2 + 3}) circle (2pt);
			\node[anchor=north] at (2, -0.3) {$\hypothesis^{(\timeidx+1)}$};
			\draw[dotted] (2, 0) -- (2, {-0.15*(3 - 2)^2 + 3});
			%%% time t+2
			\draw[thick, green!60!black, domain=3:5, samples=20, dotted] plot (\x+2, {-0.1*(\x - 4)^2 + 1.5});
			\node[anchor=west,yshift=4pt] at (4.5+2, {-0.1*(4.5 - 4)^2 + 1.5}) {$-\loss^{(\timeidx+2)}(\hypothesis)$};
			\filldraw[green!60!black] (3.5+2, {-0.1*(3.5 - 4)^2 + 1.5}) circle (2pt);
			\node[anchor=north] at (3.5+2, -0.3) {$\hypothesis^{(\timeidx+2)}$};
			\draw[dotted] (3.5+2, 0) -- (3.5+2, {-0.1*(3.5 - 4)^2 + 1.5});
			\end{tikzpicture}
		\caption{Three consecutive time steps $\timeidx,\timeidx+1,\timeidx+2$ with corresponding \glspl{lossfunc} $\loss^{(\timeidx)},
		\loss^{(\timeidx+1)}, \loss^{(\timeidx+2)}$. During time step $\timeidx$, an RL method can evaluate the 
		\gls{lossfunc} only for one specific \gls{hypothesis} $\hypothesis^{(\timeidx)}$, resulting in the \gls{reward} 
		signal $\reward^{(\timeidx)}=-\loss^{(\timeidx)}(\hypothesis^{(\timeidx)})$. \label{fig_reinforcementlearning_dict}}
		\end{center}
		\end{figure}
		In general, the \gls{reward} depends also on the 
		previous \glspl{prediction} $\hypothesis^{(\timeidx')}\big(\featurevec^{(\timeidx')}\big)$ 
		for $\timeidx' < \timeidx$. The goal of RL is to learn $\hypothesis^{(\timeidx)}$, for 
		each time step $\timeidx$, such that the (possibly discounted) cumulative \gls{reward} 
		is maximized \cite{MLBasics}, \cite{SuttonEd2}.
		\\
		See also: \gls{reward}, \gls{lossfunc}, \gls{ml}.},
	first={reinforcement learning (RL)},
	type=reinflearning, 
	text={RL}
}

\newglossaryentry{action}
{name={action},
	description={An action\index{action} refers to a decision taken by an \gls{ai} 
	             system at a given time step $\timeidx$ that influences the observed 
				 \gls{reward} signal. The actions are elements of an 
				 action space $\actionspace$ and are typically denoted by 
				 $\arm_{\timeidx} \in \actionspace$. The action $\arm_{\timeidx}$ is selected 
				 based on the \gls{featurevec} $\featurevec^{(\timeidx)}$ (that collects 
				 all available observations) and the current 
				 \gls{hypothesis} $\hypothesis^{(\timeidx)}$. 
		         RL uses \gls{onlinelearning} methods 
				 to learn a \gls{hypothesis} $\hypothesis^{(\timeidx)}$ that predicts 
		         an (nearly) optimal action. The usefulness of the 
				 \gls{prediction} $\arm_{\timeidx}$ is evaluated indirectly 
				 through the resulting \gls{reward} signal $\reward^{(\timeidx)}$.
		         In the special case of a \gls{mab}, the set of possible actions is 
				 finite and each action corresponds to selecting one arm. In more
				 general RL settings, the action space may be continuous.
		\\
		See also: \gls{reinforcementlearning}, \gls{reward}, \gls{lossfunc}, \gls{mab}, \gls{hypothesis}.},
	first={action},
	type=reinflearning,
	plural={actions},
	firstplural={actions},
	text={action}
}


\newglossaryentry{actionspace}
{name={action space},
	description={See\index{action space} \gls{action}.} ,
	first={action space},
	type=reinflearning,
	plural={action spaces},
	firstplural={action spaces},
	text={action space}
}

\newglossaryentry{mab}
{name={multiarmed bandit (MAB)},
	description={An MAB \index{multiarmed bandit (MAB)} problem is a precise 
	formulation of a sequential decision-making task under \gls{uncertainty}. At each 
	discrete time step $\iteridx$, a learner selects one of several possible 
	actions—called arms—from a finite set $\actionset$. Pulling arm $\arm$ at time 
	$\iteridx$ yields a \gls{reward} $\reward^{(\arm,\iteridx)}$ that is drawn from an unknown 
	\gls{probdist} $\prob{\reward^{(\arm,\iteridx)}}$. We obtain different classes 
	of MAB problems by placing different restrictions on this \gls{probdist}. In the simplest 
	setting, the \gls{probdist} $\prob{\reward^{(\arm,\iteridx)}}$ does not depend on $\timeidx$. 
		Given an MAB problem, the goal is to construct \gls{ml} methods that maximize the cumulative 
		\gls{reward} over time by strategically balancing exploration (i.e., gathering information 
		about uncertain arms) and exploitation (i.e., selecting arms known to perform well). 
		MAB problems form an important special case of \gls{reinforcementlearning} problems \cite{Bubeck2012}, \cite{SuttonEd2}.
					\\ 
		See also: \gls{reward}, \gls{regret}.},
	first={MAB},
	type=reinflearning, 
	text={MAB}
}

\newglossaryentry{regret}
{name={regret},
	description={The regret\index{regret} of a \gls{hypothesis} $\hypothesis$ relative to 
		another \gls{hypothesis} $\hypothesis'$, which serves as a \gls{baseline}, 
		is the difference between the \gls{loss} incurred by $\hypothesis$ and the \gls{loss} 
		incurred by $\hypothesis'$ \cite{PredictionLearningGames}. 
		The \gls{baseline} \gls{hypothesis} $\hypothesis'$ is also referred to as an \gls{expert}.
					\\ 
		See also: \gls{baseline}, \gls{loss}, \gls{expert}.},
	first={regret},
	type=reinflearning, 
	text={regret} 
}

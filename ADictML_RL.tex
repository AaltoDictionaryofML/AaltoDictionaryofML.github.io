\newglossaryentry{mdp}
{name={Markov decision process (MDP)},
	description={An MDP \index{Markov decision process (MDP)} is a mathematical structure that can 
		be used to study \gls{reinforcementlearning} applications. An MDP formalizes how \gls{reward} 
		signals depend on the \glspl{prediction} (and corresponding actions) made by an \gls{reinforcementlearning} 
		method. Formally, an MDP is a specific type of \gls{stochproc} defined by
		\begin{itemize}
			\item a state space $\mathcal{S}$;
			\item an action space $\mathcal{A}$ (where each action $a \in \mathcal{A}$ corresponds to a specific 
			\gls{prediction} made by the \gls{reinforcementlearning} method);
			\item a transition \gls{function} $\prob{s' \mid s, a}$ specifying the \gls{probdist} over the 
			next state $s' \in \mathcal{S}$, given the current state $s \in \mathcal{S}$ and action $a \in \mathcal{A}$;
			\item a \gls{reward} \gls{function} $\reward(s, a) \in \mathbb{R}$ that assigns a numerical \gls{reward} to each 
			state-action pair.
		\end{itemize}
		An MDP is defined by the \gls{markovproperty}.
		\\
		See also: \gls{reinforcementlearning}, \gls{reward}, \gls{prediction}, \gls{stochproc}, \gls{function}, \gls{probdist}.},
	first={Markov decision process},
	text={MDP} 
}

\newglossaryentry{markovproperty}
{name={Markov property},
	description={Markov property \index{Markov property} is the memoryless property of a \gls{stochproc}. A \gls{stochproc} has the Markov property iff its future is independent from its future given the present. That is, the next state and \gls{reward} only depend on the current state and action, not on the entire history of interactions. A \gls{stochproc} with this property is a \gls{mdp}.
		%The defining property of an MDP is the Markov property. That is, the next state $s'$ and \gls{reward} only depend on the current state $s$ and action $a$, not on the entire history of interactions. 
		\\
		See also: \gls{mdp}, \gls{stochproc}, \gls{reinforcementlearning}.},
	first={Markov property},
	text={Markov property} 
}
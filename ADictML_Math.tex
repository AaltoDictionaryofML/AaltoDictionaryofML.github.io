\newglossaryentry{psd}
{name={positive semi-definite (psd)},
	description={A\index{positive semi-definite (psd)} (real-valued) 
	             symmetric \gls{matrix} $\mQ = \mQ\,^{T} \in \mathbb{R}^{\featuredim \times \featuredim}$ 
	 	         is referred to as psd if $\featurevec\,^{T} \mQ \featurevec \geq 0$ 
				 for every \gls{vector} $\featurevec \in \mathbb{R}^{\featuredim}$. 
	 	         The property of being psd can be extended from \glspl{matrix} to (real-valued) 
	 	         symmetric \gls{kernel} \glspl{map} $\kernel: \featurespace \times \featurespace \rightarrow \mathbb{R}$ 
	 	         (with $\kernel(\featurevec,\featurevec') = \kernel(\featurevec',\featurevec)$)
	 	         as follows: For any finite set of \glspl{featurevec} $\featurevec^{(1)}, \,\ldots, \,\featurevec^{(\samplesize)}$, 
	 	         the resulting \gls{matrix} $\mQ \in \mathbb{R}^{\samplesize \times \samplesize}$ with 
		        entries $Q_{\sampleidx,\sampleidx'} = \kernelmap{\featurevec^{(\sampleidx)}}{\featurevec^{(\sampleidx')}}$ 
		        is psd \cite{LearningKernelsBook}.
			\\
		See also: \gls{matrix}, \gls{kernel}.},
	first={positive semi-definite (psd)},
	type=math, 
	text={psd}  
}

\newglossaryentry{normalmatrix}
{name={normal matrix},
	description={A square matrix $\mA \in \mathbb{C}^{\nrfeatures \times \nrfeatures}$ 
                 that commutes with its conjugate transpose, i.e., $\mA\mA^{H}=\mA^{H}\mA$. 
                 Normal matrices admit an orthonormal basis of \glspl{eigenvector} and are 
                 unitarily \gls{diagonalizable}.},
	first={normal matrix},
    type=math, 
	plural={normal matrices},
	firstplural={normal matrices},
	text={normal matrix}
}

\newglossaryentry{spectraldecomp}
{name={spectral decomposition},
	description={Every \gls{normalmatrix} $\mA \in \mathbb{C}^{\nrfeatures \times \nrfeatures}$ 
                 admits\index{spectral decomposition} a spectral decomposition of the form \cite{HornMatAnalysis,Axler2025}
       \begin{align}
        \nonumber
            \mA 
            % & = 
            % \big[ \vu^{(1)}\ \cdots\ \vu^{(\nrfeatures)}\big]
            % \pmatrix{
            % \eigval{1} &        &        & 0\\
            %         & \eigval{2} &     &  \\
            %         &        & \ddots &  \\
            % 0         &        &        & \eigval{\nrfeatures}
            % }
            % \pmatrix{
            % \big(\vu^{(1)}\big)^{H}\\
            % \vdots\\
            % \big(\vu^{(\nrfeatures)}\big)^{H}
            % } \nonumber \\
                & = \sum_{\featureidx=1}^{\nrfeatures} \eigval{\featureidx} \vu^{(\featureidx)} \big(\vu^{(\featureidx)})^{H} \nonumber \\ 
       \end{align}
      with an orthonormal basis $\vu^{(1)},\ldots,\vu^{(\nrfeatures)}$. 
       \begin{figure}[t]
            \centering
            \begin{tikzpicture}[scale=2.1, line cap=round, line join=round]
            % two eigenvectors with different lengths
            \draw[->, thick] (0,0) -- (-0.5,1) node[above right] {$\eigval{1} \vu^{(1)}$};
            \draw[->, thick] (0,0) -- (2.0,1) node[above left]  {$\eigval{2} \vu^{(2)}$};
            \end{tikzpicture}
            \caption{The spectral decomposition of a \gls{normalmatrix} $\mA$ provides 
                     an orthonormal basis $\vu^{(1)}, \vu^{(2)}$. Applying $\mA$ 
                     amounts to a scaling of the basis \glspl{vector} by the 
                     \glspl{eigenvalue} $\eigval{1},\eigval{2}$.\label{fig:eigenvectors-length_dict}}
            \end{figure}
       Each basis element $\vu^{(\featureidx)}$ 
       is an \gls{eigenvector} of $\mA$ with corresponding \gls{eigenvalue} $\eigval{\featureidx}$, 
       for $\featureidx=1,\ldots,\nrfeatures$.
       },
	first={spectral decomposition},
    type=math,
	text={spectral decomposition}
}


\newglossaryentry{symmetricmatrix}
{name={symmetric matrix},
	description={A square\index{symmetric matrix} \gls{matrix} $\mA$ with real-valued 
                 entries that is equal to its transpose, i.e., $\mA=\mA^{T}$. Every 
                 symmetric \gls{matrix} is a \gls{normalmatrix}.},
	first={symmetric matrix},
	plural={symmetric matrices},
    type=math,
	firstplural={symmetric matrices},
	text={symmetric matrix}
}

\newglossaryentry{transpose}
{name={transpose},
 description={The transpose\index{transpose} of a real-valued \gls{matrix} is obtained by exchanging 
                  rows and columns. For a \gls{matrix} $\mA \in \mathbb{R}^{\samplesize \times \nrfeatures}$, 
				  its transpose is denoted $\mA^{T}$ and satisfies $\big(\mA^{T}\big)_{\featureidx,\featureidx'}=\mA_{featureidx',\featureidx}$.},
 	first={transpose},
    type=math, 
 	text={transpose}
 }

 \newglossaryentry{conjugatetranspose}
{name={conjugate transpose},
 description={The conjugate transpose\index{conjugate transpose} of a 
              \gls{matrix} is obtained by transposing the \gls{matrix} 
			  and taking the complex conjugate of each entry.
              For a matrix $\mA \in \mathbb{C}^{\samplesize \times \nrfeatures}$, its
              conjugate transpose is denoted by $\mA^{H} \in 
              \mathbb{C}^{\nrfeatures \times \samplesize}$ and is defined entrywise by
              \[
                 (\mA^{H})_{\featureidx,\sampleidx}
                 = \complexconjugate{\big(\mA\big)_{\sampleidx,\featureidx}},
              \]
              where $\complexconjugate{(\cdot)}$ denotes complex conjugation.},
 	first={conjugate transpose},
    type=math, 
 	text={conjugate transpose}
 }

\newglossaryentry{hermitian}
 {name={Hermitian (matrix)},
 	description={A square\index{Hermitian} matrix $\mA \in \mathbb{C}^{\nrfeatures \times \nrfeatures}$ is 
	             Hermitian if it coincides with its conjugate transpose, i.e., $\mA=\mA^{H}$. 
                 Trivally, a Hermitian matrix is also a \gls{normalmatrix}.},
 	first={Hermitian},
     type=math, 
 	plural={Hermitian},
 	firstplural={Hermitian},
 	text={Hermitian}
}

\newglossaryentry{dimension}
{name={dimension},
	description={The\index{dimension} dimension $\dim \mathcal{A}$ of a 
		\gls{vectorspace} $\mathcal{A}$ is the cardinality of any 
		\gls{basis} of $\mathcal{A}$ \cite{StrangLinAlg2016}. 
		Strictly speaking, this definition applies only to finite-dimensional \glspl{vectorspace}, 
		i.e., those that possess a finite \gls{basis}. 
		\begin{figure}[H]
			\begin{tikzpicture}[scale=1]
  			% Axes (optional; remove if you want it even more minimal)
  			%	\draw[->, thin, gray] (-0.2,0) -- (3.2,0) node[right] {$\vw^{(1)}$};
  			%	\draw[->, thin, gray] (0,-0.2) -- (0,3.2) node[above] {$\vw^{(1)}$};
  			\coordinate (O) at (0,0);
  			% Basis 1: standard (solid)
  			\draw[->, thick] (O) -- (1.8,0) node[below right] {$\ve^{(1)}$};
  			\draw[->, thick] (O) -- (0,1.6) node[above left] {$\ve^{(2)}$};
  			% Basis 2: rotated by ~45° (dashed)
			\draw[->, thick, dashed, shift={(3.5,0.5)}] (0,0) -- (1.2,1.2) node[above right] {$\vu^{(1)}$};
			\draw[->, thick, dashed, shift={(3.5,0.5)}] (0,0) -- (-1.2,1.2) node[above left] {$\vu^{(2)}$};
  			% Basis 3: non-orthogonal / skewed (dotted)
  			\draw[->, thick, dotted, shift={(-2.5,-2.5)}] (O) -- (2.0,0.6) node[above right] {$\vw^{(1)}$};
  			\draw[->, thick, dotted, shift={(-2.5,-2.5)}] (O) -- (0.4,1.8) node[left] {$\vw^{(2)}$};
  			% Simple legend
 			% \node[anchor=west] at (1.6,-0.6) {\footnotesize \textbf{Bases:} solid = standard,\; 
 			% dashed = rotated,\; dotted = skewed};
			\end{tikzpicture}
		\caption{Three \glspl{basis}, $\big\{\ve^{(1)},\ve^{(2)} \big\}, \big\{\vu^{(1)},\vu^{(2)} \big\},
		\big\{\vw^{(1)},\vw^{(2)} \big\}$, for the \gls{vectorspace} $\mathbb{R}^{2}$.} 
		\end{figure}
		For such spaces, all \glspl{basis} have the same cardinality, which is the dimension of the space 
		\cite[Ch.~2]{Axler2025}.	
   			 \\
		See also: \gls{vectorspace}, \gls{basis}. }, 
	text={dimension}, 
	type=math,
	first={dimension}  
}

\newglossaryentry{linearlyindep}
{name={linearly independent},
	description={A subset $\{\va^{(1)}, \,\ldots, \,\va^{(\nrfeatures)}\} \in \mathcal{V}$ 
		of a \gls{vectorspace} is linearly independent\index{linearly independent} 
		if there is no nontrivial linear combination of these \glspl{vector} that 
		equals the zero \gls{vector} \cite{StrangLinAlg2016}. 
		In other words, $$\sum_{\featureidx=1}^{\nrfeatures} \alpha_{\featureidx} \va^{(\featureidx)} = \mathbf{0}	
		\quad \text{ implies } \quad \alpha_{1} = \alpha_{2} = \ldots = \alpha_{k} = 0.$$ 
			\\ 
		See also: \gls{vectorspace}, \gls{vector}, \gls{dimension}, \gls{basis}.}, 
	text={linearly independent}, 
	type=math,
	first={linearly independent}  
}

\newglossaryentry{basis}
{name={basis},
	description={A basis\index{basis} of a \gls{vectorspace} $\mathcal{V}$ 
		is a set of \gls{linearlyindep} \glspl{vector} $\{\va^{(1)}, \,\ldots, \,\va^{(\nrfeatures)}\}$ such 
		that any \gls{vector} $\va \in \mathcal{A}$ can be expressed as a linear combination 
		of the basis \glspl{vector}, i.e.,	
		$$ \va = \sum_{\featureidx=1}^{\nrfeatures} \alpha_{\featureidx} \va^{(\featureidx)} 
		\quad \text{ for some } \alpha_{1}, \,\ldots, \,\alpha_{\nrfeatures} \in \mathbb{R}. $$
			\\
		See also: \gls{vectorspace}, \gls{linearlyindep}, \gls{vector}. },
	text={basis}, 
	firstplural={bases}, 
	plural={bases}, 
	type=math,
	first={basis} 
}


\newglossaryentry{widematrix}
{name={wide matrix},
 description={A\index{wide matrix} \gls{matrix} 
   		$\featuremtx \in \mathbb{R}^{\samplesize \times \nrfeatures}$ 
		is referred to as wide if it has more columns than rows, 
		i.e., when $\nrfeatures > \samplesize$.
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}
				\def\matHeight{2}
				\def\matWidth{5} 
				\draw[thick, fill=gray!20] (0,0) rectangle (\matWidth, \matHeight);
				\node at (0.5*\matWidth, 0.5*\matHeight) {\Large $\featuremtx \in \mathbb{R}^{\samplesize \times \nrfeatures}$};
				% vertical brace for samplesize (LEFT, correct orientation)
				\draw[decorate, decoration={brace, amplitude=5pt}, thick] 
  					(-0.2, 0) -- (-0.2, \matHeight)
  					node[midway, left=8pt] {$\samplesize$};
				\draw[decorate, decoration={brace, amplitude=5pt, mirror}, thick] 
        			(0, -0.2) -- (\matWidth, -0.2) 
        			node[midway, below=8pt] {$\nrfeatures \quad (\nrfeatures > \samplesize)$};
			\end{tikzpicture}
		\end{figure}
		See also: \gls{matrix}. },
	text={wide matrix}, 
	firstplural={wide matrices}, 
	plural={wide matrices}, 
	type=math,
   	first={wide matrix} 
}

\newglossaryentry{randomexperiment}
{name={random experiment},
	description={A random experiment\index{random experiment} is a physical (or abstract) process 
    	 	that produces an outcome $\outcome$ from a set $\samplespace$ of possibilities. 
	 	This set of all possible outcomes is referred to as the \gls{samplespace} of 
	 	the experiment. The key characteristic of a random experiment is that its 
	 	outcome is unpredictable (or uncertain). Any measurement or observation 
	 	of the outcome is a \gls{rv}, i.e., a \gls{function} of the outcome $\outcome \in \samplespace$. 
	 	\Gls{probability} theory uses a \gls{probspace} as a mathematical structure for the study of 
	 	random experiments. A key conceptual property of a random experiment is that it can 
	 	be repeated under identical conditions. Strictly speaking, repeating a random experiment 
	 	a given number of $\samplesize$ times defines a new random experiment. The outcomes 
	 	of this new experiment are length-$\samplesize$ sequences of outcomes 
	 	from the original experiment (see Fig. \ref{fig_randomexperiment_dict}). While the outcome of a single experiment is 
	 	uncertain, the long-run behaviour of the outcomes of repeated experiments 
	 	tends to become increasingly predictable. This informal claim can be made 
	 	precise via fundamental results of \gls{probability} theory, such as the \gls{lln} 
	 	and the \gls{clt}.
	 	\begin{figure}[H]
		\begin{center}
	 		\begin{tikzpicture}[>=Stealth, node distance=1.5cm and 2cm, every node/.style={font=\small}]
			\node (experiment) [draw, rectangle, rounded corners, minimum width=2.6cm, align=center] {random\\experiment};
			\node (omega) [right=of experiment] {$\outcome \in \samplespace$};
			\coordinate (rightpad) at ($(omega.east) + (0.2,0)$);
			\draw[->] (experiment) -- (omega);
			\node (sequence) [below=of experiment, yshift=-0.5cm] {$(\outcome^{(1)}, \,\outcome^{(2)}, \,\dots, \,\outcome^{(\samplesize)})$};
			\node (sequence1) [below=of sequence, yshift=-0.5cm] {$(\datapoint^{(1)}, \,\datapoint^{(2)}, \,\dots, \,\datapoint^{(\samplesize)})$};
			\draw[->, thick] (experiment.south) -- node[midway, right, xshift=3pt] {repeat $\samplesize$ times} (sequence.north);
			\draw[->, thick] (sequence.south) -- node[midway, right, xshift=3pt] {\glspl{rv}} (sequence1.north);
			% Anchor node ~60% along the repeat arrow
			\path (experiment.south) -- (sequence.north) coordinate[pos=0.6] (repeatpoint);
			% Dotted rounded box enclosing experiment and part of repeat arrow
        			\node[draw=black, rounded corners, dotted, fit={(experiment) (repeatpoint) (rightpad)}, inner sep=8pt, label=above:{new random experiment with $\samplespace' = \samplespace \times \ldots \times \samplespace$}] {};
	 		\end{tikzpicture}
	     	\end{center}
		\caption{A random experiment produces an outcome $\outcome \in \samplespace$ from a set 
		of possibilities (i.e., a \gls{samplespace}) 
		$\samplespace$. Repeating the experiment $\samplesize$ times yields another random 
		experiment, whose outcomes are sequences 
		$(\outcome^{(1)}, \,\outcome^{(2)}, \,\dots, \,\outcome^{(\samplesize)}) \in \samplespace\times\ldots\times \samplespace$. 
		One example of a random experiment arising in many \gls{ml} applications is the gathering 
		of a \gls{trainset} $\datapoint^{(1)},\,\ldots,\,\datapoint^{(\samplesize)}$. \label{fig_randomexperiment_dict}}
	 	\end{figure} 
	 	Examples for random experiments arising in \gls{ml} applications include the following: 
	 	\begin{itemize} 
			\item \Gls{data} collection: The \glspl{datapoint} collected in \gls{erm}-based methods 
			can be interpreted as \glspl{rv}, i.e., as \glspl{function} of the outcome $\outcome \in \samplespace$ 
			of a random experiment. 
			\item \Gls{stochGD} uses a random experiment at each iteration to select a subset of 
			the \gls{trainset}. 
			\item \Gls{privprot} methods use random experiments to perturb  
			the outputs of an \gls{ml} method to ensure \gls{diffpriv}. 
	 	\end{itemize} 
		See also: \gls{samplespace}, \gls{rv}, \gls{probability}, \gls{probspace}.},
 	firstplural={random experiments},
 	plural={random experiments},
	type=math, 
 	first={random experiment},
 	text={random experiment}
}

\newglossaryentry{pseudoinverse}
{name={pseudoinverse},
  description={The \index{pseudoinverse}Moore–Penrose pseudoinverse $\mA^{+}$ 
 	of a \gls{matrix} $\featuremtx \in \mathbb{R}^{\samplesize \times \nrfeatures}$ 
	generalizes the notion of an \gls{inverse} \cite{GolubVanLoanBook}. 
	The pseudoinverse arises naturally in \gls{ridgeregression} for a 
	\gls{dataset} with \gls{featuremtx} $\featuremtx$ and \gls{labelvec} 
	$\labelvec$ \cite[Ch.\ 3]{hastie01statisticallearning}. 
	The \glspl{modelparam} learned by \gls{ridgeregression} 
  	are given by
  	\[
  	\widehat{\weights}^{(\regparam)}  = \big(\featuremtx^{T} \featuremtx + \regparam \mI \big)^{-1} \featuremtx^{T} \vy, \quad \regparam > 0.
  	\]
  	We can then define the pseudoinverse $\featuremtx^{+} \in \mathbb{R}^{\nrfeatures \times \samplesize}$ via 
  	the limit \cite[Ch. 3]{benisrael2003generalized}
  	\[
  	\lim_{\regparam \to 0^+} \widehat{\weights}^{(\regparam)} = \featuremtx^+ \vy.
  	\]
	\\
	See also: \gls{matrix}, \gls{inverse}, \gls{ridgeregression}. },
 first={pseudoinverse},
 type=math, 
 text={pseudoinverse}
} 


\newglossaryentry{tallmatrix}
{name={tall matrix},
 description={A\index{tall matrix} \gls{matrix} $\featuremtx \in \mathbb{R}^{\samplesize \times \nrfeatures}$ 
			  is referred to as tall if it has more rows than columns, i.e., 
			  when $\samplesize > \nrfeatures$.
			  \begin{figure}[H]
			\centering
			\begin{tikzpicture}
				\def\matHeight{5}
				\def\matWidth{3} 
				\draw[thick, fill=gray!20] (0,0) rectangle (\matWidth, \matHeight);
				\node at (0.5*\matWidth, 0.5*\matHeight) {\Large $\featuremtx \in \mathbb{R}^{\samplesize \times \nrfeatures}$};
				% vertical brace for samplesize (LEFT, correct orientation)
				\draw[decorate, decoration={brace, amplitude=5pt}, thick] 
  					(-0.2, 0) -- (-0.2, \matHeight)
  					node[midway, left=8pt] {$\samplesize$};
				\draw[decorate, decoration={brace, amplitude=5pt, mirror}, thick] 
        			(0, -0.2) -- (\matWidth, -0.2) 
        			node[midway, below=8pt] {$\nrfeatures$};
			\end{tikzpicture}
		\end{figure}
		See also: \gls{matrix}. },
	text={tall matrix}, 
	firstplural={tall matrices}, 
	plural={tall matrices}, 
	type=math,
   	first={tall matrix} 
}

\newglossaryentry{mgf}
{name={moment generating function (MGF)}, 
 description={Consider the\index{moment generating function (MGF)} MGF $\mgf{x}{t}$ 
	 	of a real-valued \gls{rv} $x$, which is defined as
	 	$\mgf{x}{t} = \expect \{ \exp(t \cdot x) \}$ for any $t \in \mathbb{R}$ 
	 	for which this \gls{expectation} exists \cite[Sec. 21]{BillingsleyProbMeasure}. 
		As its name indicates, the MGF allows us to compute the moments 
	 	$\expect\{ x^{k} \}$ for $k \in \mathbb{N}$. 
	 	In particular, the $k$th moment is obtained by evaluating the $k$th 
	 	derivative of $\mgf{x}{t}$ for $t=0$, i.e., $\expect\{ x^{k} \} = \mgfder{x}{k}{0}$. 
	 	This fact can be verified by the following identities: 
	 	\begin{align}
			\mgf{x}{t} & =\expect\{ \exp(t \cdot x)  \} \nonumber \\ 
			& \stackrel{(a)}{=} \expect\!\bigg\{\sum_{k=0}^{\infty} \frac{t^{k}}{k!} x^{k}\bigg\}  \nonumber \\ 
			& \stackrel{(b)}{=}  \sum_{k=0}^{\infty} \frac{t^{k}}{k!}\, \expect\!\big\{ x^{k} \big\}. \nonumber
	 	\end{align}
	 	Here, step $(a)$ is due to the Taylor series expansion of 
	 	$\exp\,(t \cdot x)$ and step $(b)$ is valid when the MGF exists 
	 	for all $t$ in some interval $(-t_{0},t_{0})$ \cite[p. 278]{BillingsleyProbMeasure}.
	 	\begin{figure}[H]
			\centering
			\begin{tikzpicture}
			\begin{axis}[
				width=9cm, height=4.2cm,
				domain=-1:1,
				samples=200,
				%axis lines*=left,        % removes bounding box, keeps left axis
				xlabel={$x$},
				ylabel={},
				ytick=\empty,
				ytick={-1,-0.5,0,0.5,1}, % manually set x-ticks
            			yticklabels={$-1$,$-0.5$,$0$,$0.5$,$1$},
				xtick={-1,-0.5,0,0.5,1}, % manually set x-ticks
            			xticklabels={$-1$,$-0.5$,$0$,$0.5$,$1$},
				xmin=-1, xmax=1,
				legend style={at={(1.5,0.02)},anchor=south east}
				]
				% f1 = x
				\addplot[thick, dashed] {x};
				\addlegendentry{$x$}
				% f2 = x^2
				\addplot[thick, dotted] {x^2};
				\addlegendentry{$x^{2}$}
				% f3 = x^3
				\addplot[thick, dashdotdotted] {x^3};
				\addlegendentry{$x^{3}$}
			\end{axis}
			\end{tikzpicture}
		\caption{The first few powers of an \gls{rv} $x$. The MGF 
		encodes the moments of $x$, which are the \glspl{expectation} 
		of the powers $x^{k}$ for $k=1,\,2,\,\ldots$.}
		\end{figure}
		The MGF is a useful tool for the study of sums of independent 
		\glspl{rv}. As a case in point, if $x$ and $y$ are independent 
		\glspl{rv}, then the MGF of their sum $z = x + y$ typically 
		satisfies $\mgf{z}{t} = \mgf{x}{t}\,\mgf{y}{t}$,
        		i.e., the MGF of the sum is typically the pointwise product of the 
		individual MGFs \cite[p.~280]{BillingsleyProbMeasure}.
		\\
 		See also: \gls{rv}, \gls{expectation}. }, 
 	first={moment generating function (MGF)}, 
 	firstplural={moment generating functions (MGFs)}, 
 	plural={MGFs}, 
 	type=math, 
 	text={MGF}
} 

\newglossaryentry{chernoffbound}
{name={Chernoff bound}, 
	description={TBC\index{Chernoff bound}.}, 
 	first={Chernoff bound}, 
 	type=math, 
 	text={Chernoff bound}
}

\newglossaryentry{rankdeficient}
{name={rank-deficient},
	description={A \gls{matrix} $\mA \in \mathbb{R}^{\samplesize \times \nrfeatures}$ 
         	is rank-deficient\index{rank-deficient} if it is not \gls{fullrank}, i.e., 
         	when $\rank{\mA} < \min\{\samplesize,\nrfeatures\}$.
 		\begin{figure}[H]
			\begin{center}
			\begin{tikzpicture}[x=2cm]
				% LEFT: Standard basis vectors and unit square
				\begin{scope}
					\draw[->, thick] (0,0) -- (1,0) node[below] {$\vu^{(1)}$};
					\draw[->, thick] (0,0) -- (0,1) node[above] {$\vu^{(2)}$};
					%\draw[fill=gray!15] (0,0) -- (1,0) -- (1,1) -- (0,1) -- cycle;
					%\node at (0.5,0.5) {\small unit square};
					%\node at (0.5,-0.6) {standard basis};
				\end{scope}
				% RIGHT: Transformed basis vectors and parallelogram
				\begin{scope}[shift={(3.2,0)}]
				%\draw[->, thick] (0,0) -- (1,0) node[below] {$\vv^{(1)}$};
				%	\draw[->, thick] (0,0) -- (0,1) node[above] {$\vv^{(2)}$};
					\coordinate (A) at (0.2,0.0);
					\coordinate (B) at (2.0,0.0);
					\draw[->, very thick, red] (0,0) -- (A) node[below,yshift=-2pt] {$\mA \vu^{(1)}$};
					\draw[->, very thick, red] (0,0) -- (B) node[above,yshift=2pt] {$\mA \vu^{(2)}$};
					%	\node[blue] at (0.25,1.25) {};
					%	\node at (0.8,-0.6) {transformed basis};
				\end{scope}
				% Arrow between plots
				\draw[->, thick] (1.6,0.5) to[bend left] node[midway, above] {$\mA$} (2.7,0.5);
				%	\draw[->, thick] (1.3,0.5) -- (2.4,0.5) node[midway, above] {$\mA$};
			\end{tikzpicture}
			\end{center}
		\caption{Example of a rank-deficient \gls{matrix} 
		$\mA \in \mathbb{R}^{2 \times 2}$.	\label{fig_matrix_rank_defdict}} 
		\end{figure} 
		In \gls{linreg}, the solution of the \gls{erm} problem is not 
		unique whenever the \gls{featuremtx} $\featuremtx$ is such that 
		the \gls{matrix} $\featuremtx^{\top}\featuremtx$ is rank-deficient.
		\\
   		See also: \gls{fullrank}, \gls{dimension}, \gls{vectorspace}. }, 
	first={rank-deficient}, 
   	type=math,
   	text={rank-deficient}
}

\newglossaryentry{fullrank}
 {name={full-rank},
 	description={A \gls{matrix} $\mA \in \mathbb{R}^{\samplesize \times \nrfeatures}$ 
  		is\index{full-rank} full-rank if it has \gls{maximum} \gls{rank} \cite{StrangLinAlg2016}. 
  		For a \gls{tallmatrix}, i.e., when $\nrfeatures < \samplesize$, being 
  		full-rank means that its \gls{rank} is equal to $\nrfeatures$. 
 		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[every node/.style={font=\small}]
			% --- Full-rank square ---
			\node at (0,2) {$
			\mA =
			\begin{pmatrix}
			1 & 2\\
			3 & 4
			\end{pmatrix}
			$};
			\node[below=0.8cm of {$(0,2)$}] {\small full-rank square};
			% --- Rank-deficient square ---
			\node at (4.5,2) {$
			\mB =
			\begin{pmatrix}
			1 & 2\\
			2 & 4
			\end{pmatrix}
			$};
			\node[below=0.8cm of {$(4.5,2)$}] {\small \gls{rankdeficient} square};
			% --- Full-rank tall (3x2) ---
			\node at (0,-1.0) {$
			\mC =
			\begin{pmatrix}
			1 & 0\\
			0 & 1\\
			1 & 1
			\end{pmatrix}
			$};
			\node[below=1.2cm of {$(0,-1.0)$}] {\small full-rank \gls{tallmatrix}};
			% --- Rank-deficient wide (2x3) ---
			\node at (4.5,-1.0) {$
			\mD =
			\begin{pmatrix}
			1 & 2 & 3\\
			2 & 4 & 6
			\end{pmatrix}
			$};
			\node[below=1.2cm of {$(4.5,-1.0)$}] {\small \gls{rankdeficient} \gls{widematrix}};
			\end{tikzpicture}
		\caption{Examples of full-rank and \gls{rankdeficient} \glspl{matrix}.}
		\end{figure} 
  		A square \gls{matrix} is full-rank if and only if it is invertible. 
		\\ 
  		See also: \gls{matrix}, \gls{dimension}, \gls{linearmap}, \gls{columnspace}.}, 
	text = {full-rank}, 
  	type=math,
  	first={full-rank} 
 }

\newglossaryentry{rank}
{name={rank},
	description={The rank\index{rank} of a \gls{matrix} $\mA \in \mathbb{R}^{\samplesize \times \nrfeatures}$, 
 		denoted as $\rank{\mA}$, is the \gls{maximum} number of \gls{linearlyindep} columns 
 		of $\mA$ \cite{StrangLinAlg2016}. Equivalently, the rank can be defined as the 
 		\gls{dimension} of the \gls{columnspace} $\linspan{\mA} = \big\{ \mA \weights \mbox{ for some } 
 		\weights \in \mathbb{R}^{\nrfeatures} \big\}$. The rank of a \gls{matrix} 
 		$\mA \in \mathbb{R}^{\samplesize \times \nrfeatures}$ can neither exceed the 
 		number of rows nor the number of columns of $\mA$ \cite{Horn91}, \cite{MeyerMatrixAnalysis}, 
		i.e., $\rank{\mA} \leq \min \{ \samplesize, \nrfeatures \}$.  
		\\ 
 		See also: \gls{matrix}, \gls{dimension}, \gls{columnspace}, \gls{linearmap}.}, 
	text={rank}, 
	type=math,
	first={rank} 
}

\newglossaryentry{inverse}
{name={inverse matrix},
 description={An inverse \gls{matrix}\index{inverse matrix} $\mA^{-1}$ is defined for a 
 	square \gls{matrix} $\mA \in \mathbb{R}^{n \times n}$ that is of full rank, meaning its 
 	columns are linearly independent. In this case, $\mA$ is said to be invertible, 
 	and its inverse satisfies 
 	\[
 	\mA \mA^{-1} = \mA^{-1} \mA = \mI.
 	\]  	
     	A square \gls{matrix} is invertible if and only if its \gls{det} is non-zero. Inverse \glspl{matrix} are 
     	fundamental in solving systems of linear equations and in the closed-form solution of 
     	\gls{linreg} \cite{Strang2007}, \cite{Horn91}.  The concept of an inverse \gls{matrix} can be extended 
     	to \glspl{matrix} that are not square or does not have full \gls{rank}. One may define a ``left inverse'' $\mB$ 
     	satisfying $\mB \mA = \mI$ or a ``right inverse'' $\mC$ satisfying $\mA \mC = \mI$. 
     	For general rectangular or singular \glspl{matrix}, the Moore–Penrose \gls{pseudoinverse}
     	$\mA^{+}$ provides a unified concept of a generalized inverse \gls{matrix} \cite{GolubVanLoanBook}.
 	 \begin{figure}[H]
 		\centering
 		\begin{tikzpicture}[x=2cm,y=2cm]
 			% LEFT: Standard basis
 			\begin{scope}
 				\draw[->, thick] (0,0) -- (1,0) node[below right] {$\vx$};
 				\draw[->, thick] (0,0) -- (0,1) node[above left] {$\vy$};
 			\end{scope}
 			% CENTER: Transformed basis (by A)
 			\begin{scope}[shift={(2.0,0)}]
 				\coordinate (A) at (1.5,0.5);
 				\coordinate (B) at (-0.2,1.2);
				\draw[->, very thick, red] (0,0) -- (A) node[pos=0.5, below right] {$\mA \vx$};
 				\draw[->, very thick, red] (0,0) -- (B) node[above right] {$\mA \vy$};
 			\end{scope}
 			% RIGHT: Inverse transformation
 			\begin{scope}[shift={(4.9,0)}]
 				\draw[->, very thick, blue] (0,0) -- (1,0) node[pos=0.5, below] {$\mA^{-1} (\mA \vx) = \vx$};
 				\draw[->, very thick, blue] (0,0) -- (0,1) node[above] {$\mA^{-1} (\mA \vy) = \vy$};
 			\end{scope}
 			% Curved arrows between stages
 			\draw[->, thick, bend left=20] (1.2,0.4) to node[above] {$\mA$} (1.8,0.4);
 			\draw[->, thick, bend left=20] (3.8,0.4) to node[below] {$\mA^{-1}$} (4.4,0.4);
 		\end{tikzpicture}
 		\caption{A \gls{matrix} $\mathbf{A}$ represents a linear transformation of $\mathbb{R}^{2}$. The inverse \gls{matrix} $\mathbf{A}^{-1}$ 
 			represents the inverse transformation. \label{fig_matrix_inverse_dict}} 
 		\end{figure}
		See also: \gls{matrix}, \gls{det}, \gls{linreg}, \gls{pseudoinverse}.},
	first={inverse matrix},
		type=math,
	text={inverse matrix}
}

\newglossaryentry{matrix}
{name={matrix},
	description={A matrix\index{matrix} of size $\samplesize \times \nrfeatures$ is a 2-D array of numbers, 
 		which is denoted by 
		$$
  		\mA = \begin{pmatrix}
   		A_{1,1} & A_{1,2} & \dots  & A_{1,\nrfeatures} \\
		A_{2,1} & A_{2,2} & \dots  & A_{2,\nrfeatures} \\
		\vdots  & \vdots  & \ddots & \vdots \\
		A_{\samplesize,1} & A_{\samplesize,2} & \dots  & A_{\samplesize,\nrfeatures}
		\end{pmatrix} \in \mathbb{R}^{\samplesize \times \nrfeatures}.
		$$
		Here, $A_{\sampleidx,\featureidx}$ denotes the matrix entry in the $\sampleidx$th row and the 
		$\featureidx$th column. Matrices are useful representations of various mathematical objects \cite{StrangLinAlg2016},
		including the following:
		\begin{itemize}
			\item Systems of linear equations: We can use a matrix to represent a system of linear equations 
			$$ \begin{pmatrix}
			A_{1,1} & A_{1,2} \\
			A_{2,1} & A_{2,2}
			\end{pmatrix}
			\begin{pmatrix}
				w_1 \\
				w_2
			\end{pmatrix}
			=\begin{pmatrix}
				y_1 \\
				y_2
			\end{pmatrix}
			\quad \text{ compactly as } \quad \mA \vw = \vy.
			$$
    			One important example of systems of linear equations is the optimality condition for the 
    			\glspl{modelparam} within \gls{linreg}. 
			\item \Glspl{linearmap}:
			Consider a $\nrfeatures$-dimensional \gls{vectorspace} $\mathcal{U}$ and a $\samplesize$-dimensional \gls{vectorspace} $\mathcal{V}$. 
			If we fix a \gls{basis} $\mathbf{u}^{(1)},\,\ldots,\,\mathbf{u}^{(\nrfeatures)}$ for $\mathcal{U}$ and a \gls{basis} $\mathbf{v}^{(1)},\,\ldots,\,\mathbf{v}^{(\samplesize)}$ 
			for $\mathcal{V}$, each matrix $\mA \in \mathbb{R}^{\samplesize \times \nrfeatures}$ naturally defines a 
			\gls{linearmap} $\alpha: \mathcal{U} \rightarrow \mathcal{V}$ (see Fig. \ref{fig_matrix_dict}) such that
   			$$\vu^{(\featureidx)} \mapsto \sum_{\sampleidx=1}^{\samplesize} A_{\sampleidx,\featureidx} \vv^{(\sampleidx)}.$$
			\item \Glspl{dataset}: We can use a matrix to represent a \gls{dataset}. Each row 
			corresponds to a single \gls{datapoint}, and each column corresponds to a specific 
			\gls{feature} or \gls{label} of a \gls{datapoint}. 
		\end{itemize}
		\begin{figure}[H]
		\begin{center}
		\begin{tikzpicture}[x=2cm]
			% LEFT: Standard basis vectors and unit square
			\begin{scope}
				\draw[->, thick] (0,0) -- (1,0) node[below] {$\vu^{(1)}$};
				\draw[->, thick] (0,0) -- (0,1) node[above] {$\vu^{(2)}$};
				%\draw[fill=gray!15] (0,0) -- (1,0) -- (1,1) -- (0,1) -- cycle;
				%\node at (0.5,0.5) {\small unit square};
				%\node at (0.5,-0.6) {standard basis};
			\end{scope}
			% RIGHT: Transformed basis vectors and parallelogram
			\begin{scope}[shift={(3.2,0)}]
				\draw[->, thick] (0,0) -- (1,0) node[below] {$\vv^{(1)}$};
				\draw[->, thick] (0,0) -- (0,1) node[above] {$\vv^{(2)}$};
				\coordinate (A) at (0.2,-1.0);
				\coordinate (B) at (0.4,1.2);
				\draw[->, very thick, red] (0,0) -- (A) node[below,right] {$\mA \vu^{(1)}\!=\!A_{1,1}\vv^{(1)}\!+\!A_{2,1}\vv^{(2)}$};
				\draw[->, very thick, red] (0,0) -- (B) node[right,xshift=1pt] {$\mA \vu^{(2)}$};
				%	\node[blue] at (0.25,1.25) {};
				%	\node at (0.8,-0.6) {transformed basis};
			\end{scope}
			% Arrow between plots
			\draw[->, thick] (1.6,0.5) to[bend left] node[midway, above] {$\mA$} (2.7,0.5);
		%	\draw[->, thick] (1.3,0.5) -- (2.4,0.5) node[midway, above] {$\mA$};
		\end{tikzpicture}
		\end{center}
		\caption{A matrix $\mA$ defines a \gls{linearmap} between two \glspl{vectorspace}. \label{fig_matrix_dict}} 
		\end{figure}
		See also: \gls{linearmap}, \gls{dataset}, \gls{linmodel}. },
	first={matrix},
	firstplural={matrices},
	type=math,
	plural={matrices},
	text={matrix}
}

\newglossaryentry{hyperplane}
{name={hyperplane},
	description={A hyperplane\index{hyperplane} is an $(\nrfeatures-1)$-dimensional affine 
				 subspace of a $\nrfeatures$-dimensional \gls{vectorspace}. In the 
				 context of an \gls{euclidspace} $\mathbb{R}^{\nrfeatures}$, a 
				 hyperplane is a set of the form
 				 \[ \{ \vx \in \mathbb{R}^\nrfeatures : \vw^\top \vx = b \},
  				 \]
                 where $\vw \in \mathbb{R}^d \setminus \{0\}$ is a normal \gls{vector} 
                 and $b \in \mathbb{R}$ is an offset. Such a hyperplane partitions 
				 $\mathbb{R}^\nrfeatures$ into two halfspaces
				\[\{ \vx \in \mathbb{R}^\nrfeatures : \vw^\top \vx \leq b \} \quad 
				\text{and} \quad \{ \vx \in \mathbb{R}^\nrfeatures : \vw^\top \vx \geq b \}.\]
 				Hyperplanes arise as the \glspl{decisionboundary} of \glspl{linclass}.},
	first={hyperplane},
	type=math,
	plural={hyperplanes}, 
	firstplural={hyperplanes},
	text={hyperplane}
}

\newglossaryentry{normalvector}
{name={normal vector},
	description={See\index{normal vector} \gls{hyperplane}.},
	first={normal vector},
	type=math,
	plural={normal vectors}, 
	firstplural={normal vectors},
	text={normal vector}
}

\newglossaryentry{halfspace}
{name={halfspace},
	description={See\index{halfspace} \gls{hyperplane}.},
	first={halfspace},
	type=math,
	plural={halfspaces}, 
	firstplural={halfspaces},
	text={halfspace}
}



\newglossaryentry{subspace}
{name={subspace},
	description={A subset of a \gls{vectorspace} $\mathcal{V}$ is a subspace\index{subspace} of $\mathcal{V}$ if it is also a 
		\gls{vectorspace} with respect to the same operations as $\mathcal{V}$.
			   \\
		See also: \gls{vectorspace}.},
	type=math, 
	first={subspace},
	text={subspace}
}

\newglossaryentry{columnspace}
{name={column space},
	description={The column space\index{column space} of a \gls{matrix} 
		$\mA \in \mathbb{R}^{\samplesize \times \nrfeatures}$,
		denoted by $\linspan{\mA}$, is the set of all linear combinations of the 
		columns of $\mA$. In other words, 
		$$ \linspan{\mA} = \{ \mA \weights : \weights \in \mathbb{R}^{\nrfeatures} \}. $$
		The column space $\linspan{\mA}$ of the \gls{matrix} $\mA$ 
		is a \gls{subspace} of the \gls{euclidspace} $\mathbb{R}^{\samplesize}$.
			   \\
		See also: \gls{matrix}, \gls{vectorspace}.},
	type=math,
	first={column space},
	text={column space}
}

\newglossaryentry{mvndist}
{name={multivariate normal distribution}, 
	description={The\index{multivariate normal distribution} multivariate normal distribution, 
		which is denoted by $\mvnormal{\meanvecgeneric}{\covmtxgeneric}$, is a fundamental 
		\gls{probmodel} for numerical \glspl{featurevec} of fixed dimension $\nrfeatures$. 
		It defines a family of \glspl{probdist} over \gls{vector}-valued \glspl{rv} 
		$\featurevec \in \mathbb{R}^{\nrfeatures}$~\cite{BertsekasProb}, \cite{GrayProbBook}, \cite{Lapidoth09}. 
		Each distribution in this family is fully specified by its \gls{mean} \gls{vector} 
		$\meanvecgeneric \in \mathbb{R}^{\nrfeatures}$ and \gls{covmtx} 
		$\covmtxgeneric \in \mathbb{R}^{\nrfeatures \times \nrfeatures}$. When the 
		\gls{covmtx} $\covmtxgeneric$ is invertible, the corresponding \gls{probdist} is 
		characterized by the following \gls{pdf}:
		\[p(\featurevec) = 
 		\frac{1}{\sqrt{(2\pi)^{\nrfeatures} \det\,(\covmtxgeneric)}} 
 		\exp\left[ -\frac{1}{2} 
 		(\featurevec - \meanvecgeneric)\,^{T}\, \covmtxgeneric^{-1} 
 		(\featurevec - \meanvecgeneric) \right].
 		\]
		Note that this \gls{pdf} is only defined when $\covmtxgeneric$ is invertible.
   		More generally, any \gls{rv} $\featurevec \sim \mvnormal{\meanvecgeneric}{\covmtxgeneric}$ 
   		admits the following representation:
  		\[
    		\featurevec = \mA \vz + \meanvecgeneric
   		\]
   		where $\vz \sim \mvnormal{\mathbf{0}}{\mathbf{I}}$ is a \gls{stdnormvec} 
   		and $\mA \in \mathbb{R}^{\nrfeatures \times \nrfeatures}$ satisfies $\mA \mA^\top = \covmtxgeneric$. 
   		This representation remains valid even when $\covmtxgeneric$ is singular, in which case $\mA$ 
   		is not full rank~\cite[Ch. 23]{Lapidoth2017}.
   		The family of multivariate normal distributions is exceptional among \glspl{probmodel} for numerical 
   		quantities, at least for the following reasons. First, the family is closed under affine 
   		transformations, i.e.,
		\[ 
		\featurevec \sim \mathcal{N}(\meanvecgeneric,\covmtxgeneric) \mbox{ implies } 
		\mB\featurevec\!+\!\vc \sim \mathcal{N}\big( \mB\meanvecgeneric+\vc,\mB \covmtxgeneric \mB\,^{T} \big). 
		\]
		Second, the \gls{probdist} $\mathcal{N}(\mathbf{0},\covmtxgeneric)$ maximizes the 
		\gls{diffentropy} among all distributions with the same \gls{covmtx} $\covmtxgeneric$~\cite{coverthomas}. 
		\\ 
		See also: \gls{probmodel}, \gls{probdist}, \gls{stdnormvec}, \gls{diffentropy}, \gls{gaussrv}.}, 
	first={multivariate normal distribution},
	type=math, 
	text={multivariate normal distribution}
}

\newglossaryentry{stdnormvec}
{name={standard normal random vector}, 
	description={A\index{standard normal vector} standard normal random \gls{vector} is a random 
		\gls{vector} $\vx=\big(x_{1}, \,\ldots, \,x_{\nrfeatures}\big)\,^{T}$ 
		whose entries are \gls{iid} \glspl{gaussrv} $x_{\featureidx} \sim \mathcal{N}(0,1)$. 
		It is a special case of a \gls{mvndist}, $\vx \sim \mathcal(\mathbf{0},\mathbf{I})$.
		\\ 
		See also: \gls{vector}, \gls{iid}, \gls{gaussrv}, \gls{mvndist}, \gls{rv}.}, 
	first={standard normal random vector},
	type=math, 
	text={standard normal random vector}
}

\newglossaryentry{continuous}
{name={continuous}, 
	description={A \gls{function}\index{continuous} $f: \mathbb{R}^{\nrfeatures} \to \mathbb{R}$ is 
		continuous at a point $\featurevec' \in \mathbb{R}^{\nrfeatures}$ if, for 
	 	every $\epsilon > 0$, there is a $\delta > 0$ such that, for all 
	 	$\featurevec \in \mathbb{R}^{\nrfeatures}$ with $\normgeneric{\featurevec - \featurevec'}{2} < \delta$, 
	 	it holds that $|f(\featurevec) - f(\featurevec')| < \epsilon$ \cite{RudinBookPrinciplesMatheAnalysis}. 
	 	In other words, we can make $f(\featurevec)$ arbitrarily close to $f(\featurevec')$ 
	 	by choosing $\featurevec$ sufficiently close to $\featurevec'$. 
		\begin{figure}[htbp]
		\centering
	 	\begin{tikzpicture}[
		>=stealth, 
    	thick,
		%f(x) = 0.5(x-2)^3 + 2
    	declare function={f(\x) = 0.3*(\x-2)^3 + 2.5;}
			]
		\def\xprime{2}      % x'
    	\def\epsilonval{1.2}   % epsilon
		\def\deltaval{1.3}     % delta
		\def\xmax{5.5}
		\def\ymax{5}
		\fill[blue!5] (0, {f(\xprime)-\epsilonval}) rectangle (\xmax, {f(\xprime)+\epsilonval});
		\draw[blue!30, dashed] (0, {f(\xprime)-\epsilonval}) -- (\xmax, {f(\xprime)-\epsilonval});
		\draw[blue!30, dashed] (0, {f(\xprime)+\epsilonval}) -- (\xmax, {f(\xprime)+\epsilonval});
		\fill[red!10] ({\xprime-\deltaval}, 0) rectangle ({\xprime+\deltaval}, \ymax);
		\draw[red!40, dashed] ({\xprime-\deltaval}, 0) -- ({\xprime-\deltaval}, \ymax);
		\draw[red!40, dashed] ({\xprime+\deltaval}, 0) -- ({\xprime+\deltaval}, \ymax);
		\fill[purple!20] ({\xprime-\deltaval}, {f(\xprime)-\epsilonval}) rectangle ({\xprime+\deltaval}, {f(\xprime)+\epsilonval});
		\draw[purple, thick] ({\xprime-\deltaval}, {f(\xprime)-\epsilonval}) rectangle ({\xprime+\deltaval}, {f(\xprime)+\epsilonval});
		\draw[->] (-0.5,0) -- (\xmax,0) node[right] {$\feature$};
		\draw[->] (0,-0.5) -- (0,\ymax) node[above] {};
		% The Function 
		\draw[line width=1.5pt, black!80] plot[domain=0.2:4.5, samples=100] (\x, {f(\x)}) 
			node[right] {$f(\feature)$};
		\draw[dashed] (\xprime, 0) -- (\xprime, {f(\xprime)}) -- (0, {f(\xprime)});
		\fill[black] (\xprime, {f(\xprime)}) circle (2pt);
		\node[below,yshift=-5pt] at (\xprime, 0) {$\feature'$};
		\node[left] at (0, {f(\xprime)}) {$f(\feature')$};
		\draw[decorate, decoration={brace, amplitude=4pt}, blue!80] 
		(-0.2, {f(\xprime)}) -- (-0.2, {f(\xprime)+\epsilonval}) 
		node[midway, left=4pt] {$\varepsilon$};
		\draw[decorate, decoration={brace, amplitude=4pt}, blue!80] 
		(-0.2, {f(\xprime)-\epsilonval}) -- (-0.2, {f(\xprime)}) 
		node[midway, left=4pt] {$\varepsilon$};
		\draw[decorate, decoration={brace, amplitude=4pt, mirror}, red!80] 
		(\xprime, -0.2) -- ({\xprime+\deltaval}, -0.2) 
		node[midway, below=4pt] {$\delta$};
		\draw[decorate, decoration={brace, amplitude=4pt, mirror}, red!80] 
		({\xprime-\deltaval}, -0.2) -- (\xprime, -0.2) 
		node[midway, below=4pt] {$\delta$};
		\node[align=left, purple!40!black, font=\small] at (6.2, 1.5) 
		{If $| \feature - \feature' | < \delta$\\
		then $| f(\feature) - f(\feature') | < \varepsilon$};
		\end{tikzpicture}
		\caption{The function $f(\feature) = 0.3(\feature-2)^3 + 2.5$ is continuous 
		         at every $\feature'$.}
		\end{figure}
		If $f$ is continuous 
	 	at every point $\featurevec' \in \mathbb{R}^{\nrfeatures}$, then $f$ is said to be 
	 	continuous on $\mathbb{R}^{\nrfeatures}$. The notion of a continuous 
	 	\gls{function} can be naturally extended to \glspl{function} between general \glspl{metricspace} 
		\cite{RudinBookPrinciplesMatheAnalysis}.
		\\
		See also: \gls{euclidspace}, \gls{metric}.},
	first={continuous},
	type=math,
	text={continuous}
}

\newglossaryentry{minimum}
{name={minimum},
	description={Given a set of real numbers, the minimum\index{minimum} is the smallest of those numbers.
		Note that for some sets, such as the set of negative real numbers, the minimum does not exist.},
	firstplural={minima}, 
 	plural={minima},
	type=math, 
	first={minimum},
	text={minimum}
}

\newglossaryentry{co-domain}
{name={co-domain}, 
	description={The co-\gls{domain}\index{co-domain} of a \gls{function} 
		$f: \mathcal{U} \rightarrow \mathcal{V}$ is the set $\mathcal{V}$ 
		into which $f$ maps elements of its \gls{domain} $\mathcal{U}$.  
		\begin{figure}[H]
			\centering
		\begin{tikzpicture}[
			>=stealth, 
			node distance=2cm,
			scale=1.0, every node/.style={transform shape} % Scales the whole diagram down slightly
		]
			% Domain A
			\draw[thick, fill=blue!5] (0,0) ellipse (0.8cm and 1.4cm);
			\node[] at (0, 1.6) {\gls{domain}};
			% Codomain B
			\draw[thick, fill=red!5] (3.5,0) ellipse (1cm and 1.6cm);
			\node[] at (3.5, 1.8) {co-\gls{domain}};
			\draw[dashed, fill=green!10, thick, green!60!black] (3.5, -0.3) circle (0.6cm);
			\node[] at (3.5, -1.1) {range};
			\fill (0, 0.5) circle (1.5pt) coordinate (a1);
			\fill (0, -0.5) circle (1.5pt) coordinate (a2);
			% Output points
			\fill (3.5, 0) circle (1.5pt) coordinate (b1);
			\fill (3.5, -0.5) circle (1.5pt) coordinate (b2);
			% Unreached point
			\fill (3.5, 1.0) circle (1.5pt) coordinate (b_miss) node[right, font=\tiny, gray] {unused};
			\draw[->, semithick] (a1) -- (b1);
			\draw[->, semithick] (a2) -- (b2);
			% Function Label
			\node[font=\footnotesize] at (1.75, 0.5) {$f$};
		\end{tikzpicture}
	\end{figure}
		See also: \gls{domain}, \gls{function}, \gls{map}.},
	first={co-domain},
	firstplural={co-domains}, 
	type=math, 
	plural={co-domains},
	text={co-domain}
}


\newglossaryentry{cdf}
{name={cumulative distribution function (cdf)},
	description={The \index{cumulative distribution function (cdf)} cdf 
		$\cdf{\feature}{\eta}$ of a real-valued \gls{rv} $\feature$ is \cite{AshProbMeasure}, \cite{papoulis}
		$$\cdf{\feature}{\eta} \defeq \prob{\feature \leq \eta}.$$
					\\ 
		See also: \gls{rv}, \gls{pdf}, \gls{probdist}.},
	first={cumulative distribution function (cdf)},
	firstplural={cumulative distribution functions (cdfs)}, 
	plural={cdfs}, 
	type=math,
	text={cdf} 
}

\newglossaryentry{weightedgraph}
{name={weighted graph},
	description={A \gls{graph}\index{weighted graph} whose edges 
	are assigned numeric weights. Typically, these edge weights 
	are nonnegative real numbers. For example, if a \gls{graph} represents 
	a road network with nodes being intersections and edges representing 
	road segments, the edge weight could represent the capacity (measured 
	in maximum vehicles per hour) of the road segment \cite{NewmannBook}.  
	\begin{figure}[htbp] 
				\centering
			  \begin{tikzpicture}[scale=1.5,
					node/.style={circle, fill=black, inner sep=1.9pt},
					lab/.style={anchor=west, xshift=3pt}
					]
					% Nodes (points)
					\node[node] (v1) at (0,0) {};
					\node[node] (v2) at (2,0) {};
					\node[node] (v3) at (1,1.5) {};
					\node[node] (v4) at (3,1.5) {};
					% Labels
					\node[anchor=north] at (v1) {$\nodeidx_1$};
					\node[anchor=north] at (v2) {$\nodeidx_2$};
					\node[anchor=south] at (v3) {$\nodeidx_3$};
					\node[anchor=south] at (v4) {$\nodeidx_4$};
					% Undirected edges
					\draw [line width=1pt] (v1) -- node[midway, above] {$\edgeweight_{\nodeidx_1,\nodeidx_2}$} (v2);
					\draw (v2) -- node[midway, right] {$\edgeweight_{\nodeidx_2,\nodeidx_3}$} (v3);
					\draw (v3) -- node[midway, left] {$\edgeweight_{\nodeidx_3,\nodeidx_1}$} (v1);
					\draw (v2) -- node[midway, right] {$\edgeweight_{\nodeidx_2,\nodeidx_4}$} (v4);
				\end{tikzpicture}
				\caption{A weighted graph with four nodes 
				$\nodes = \{\nodeidx_1, \nodeidx_2, \nodeidx_3, \nodeidx_4\}$ 
				and four edges $\edges = \{\{\nodeidx_1,\nodeidx_2\},
				\{\nodeidx_2,\nodeidx_3\}, \{\nodeidx_3,\nodeidx_1\}, 
				\{\nodeidx_2,\nodeidx_4\}\}$. Each edge is assigned a weight.}
			\end{figure}		
		See also: \gls{graph}.},
	first={weighted graph},
	type=math,
	firstplural={weighted graphs}, 
	plural={weighted graphs}, 
	text={weighted graph} 
}

\newglossaryentry{graph}
{name={graph},
 description={A graph\index{graph} $\graph = \pair{\nodes}{\edges}$ 
              consists of a node set $\nodes$ and an edge set $\edges$.
			  Each edge $\edgeidx \in \edges$ is characterized by the nodes to which 
			  it is connected and in what precise sense. For example, 
			  an edge of a \gls{directedgraph} is leaving one node 
			  and pointing to another node. An edge of an undirected 
			  graph connects two nodes without any sense of 
			  direction \cite{NewmannBook,RockNetworks}. 
			  In principle, there can also be several (parallel) edges that are 
			  connected to the same nodes in the same way \cite{RockNetworks}. 
			  Moreover, edges may connect a node to itself, resulting in 
			  so-called self-loops \cite{NewmannBook}. 
			  A simple undirected graph contains no 
			  parallel edges and no self-loops \cite{WilsonGraph2010}. 
			  Each edge $\edgeidx \in \edges$ of a simple undirected 
			  graph can be identified with a set of two nodes ${\nodeidx,\nodeidx'}$. 
			  \begin{figure}[htbp] 
				\centering
			  \begin{tikzpicture}[scale=1,
					node/.style={circle, fill=black, inner sep=1.9pt},
					lab/.style={anchor=west, xshift=3pt}
					]
					% Nodes (points)
					\node[node] (v1) at (0,0) {};
					\node[node] (v2) at (2,0) {};
					\node[node] (v3) at (1,1.5) {};
					\node[node] (v4) at (3,1.5) {};
					% Labels
					\node[anchor=north] at (v1) {$\nodeidx_1$};
					\node[anchor=north] at (v2) {$\nodeidx_2$};
					\node[anchor=south] at (v3) {$\nodeidx_3$};
					\node[anchor=south] at (v4) {$\nodeidx_4$};
					% Undirected edges
					\draw [line width=1pt] (v1) -- (v2);
					\draw (v2) -- (v3);
					\draw (v3) -- (v1);
					\draw (v2) -- (v4);
				\end{tikzpicture}
				\caption{A simple undirected graph with four nodes 
				$\nodes = \{\nodeidx_1, \nodeidx_2, \nodeidx_3, \nodeidx_4\}$ 
				and four edges $\edges = \{\{\nodeidx_1,\nodeidx_2\},
				\{\nodeidx_2,\nodeidx_3\}, \{\nodeidx_3,\nodeidx_1\}, 
				\{\nodeidx_2,\nodeidx_4\}\}$.}
			\end{figure}
		      \Glspl{weightedgraph} assign a numerical value $\edgeweight_{\edgeidx}$, 
			  referred to as edge weight, to each edge $\edgeidx \in \edges$.
					\\ 
		See also: \gls{map}, \gls{weights}.},
 first={graph},
 text={graph}, 
 firstplural={graphs}, 
 plural={graphs}, 
 type=math
}

\newglossaryentry{markovchain}
{name={Markov chain},
  description={A Markov chain\index{Markov chain} is a \gls{stochproc} $\{X_\timeidx \}_{\timeidx \in \mathbb{N}}$, 
               defined on a common \gls{probspace} and using the index set $\mathbb{N}$. The 
			   \gls{rv} $X_\timeidx$ might represent (the generation of) a state of a physical system 
			   at the time instant $\timeidx$. The defining property of a Markov chain 
			   is the Markov property \cite{NorrisMarkovChains1997,durrett2010probability,papoulis}: 
			   For all $\timeidx \in \mathbb{N}$,
 			   \begin{equation}
 				\nonumber \probdist^{(X_{\timeidx+1} \mid X_\timeidx,\ldots,X_1)} = \probdist^{(X_{\timeidx+1} \mid X_\timeidx)}. 
 				\end{equation}
 			    In other words, the \gls{condprobdist} of the next state $X_{\timeidx+1}$ 
				depends on the past $X_{\timeidx},X_{\timeidx-1},\dots,X_{1}$ 
				only through the current state $X_{\timeidx}$. The concept of a 
				Markov chain can be generalized from discrete time (with index set $\mathbb{N}$) 
				to continuous time (with index set $\mathbb{R}$) \cite{NorrisMarkovChains1997}. \\ 
				See also: \gls{stochproc}, \gls{condprobdist}.},
    first={Markov chain},
    type=math,
    text={Markov chain}, 
	plural={Markov chains}, 
	firstplural={Markov chains}
}

\newglossaryentry{markovprop}
{name={Markov property},
  description={See \gls{markovchain}\index{Markov property}.},
    first={Markov property},
    type=math,
    text={Markov property}
}

\newglossaryentry{em}
{name={expectation–maximization (EM)}, 
	description={\index{expectation–maximization (EM)}
	    The EM \gls{algorithm} \cite{dempster1977maximum} is an iterative \gls{optmethod} for 
		approximately solving certain \gls{maxlikelihood} \glspl{optproblem} 
		that are difficult to solve directly \cite[Sec. 9.4]{BishopBook}, \cite[Sec. 11.4.7]{Murphy2012}.
	    To motivate the EM \gls{algorithm} and explain its construction, 
		consider a \gls{ml} application involving a single observed \gls{datapoint} 
		with \gls{feature} $\feature \in \featurespace$, where $\featurespace$ 
		is a finite \gls{featurespace}. The \gls{data} generation 
		is modeled via a \gls{probmodel} that consists of a \gls{rv} $\feature'$ 
		with \gls{pmf} $\pmf{\feature'}{\cdot;\weights}$. Here, 
		the actual \glspl{modelparam} $\weights \in \paramspace$ - used for the \gls{data} 
		generation via sampling from $\pmf{\feature'}{\cdot;\weights}$ - are unknown. A widely 
		used approach for estimating these \glspl{modelparam} is via the solutions of the \gls{maxlikelihood} 
		problem
		\begin{equation}
			\label{equ_def_ML_EM_dict}
			\min_{\weights \in \paramspace} - \log \pmf{\feature'}{\feature;\weights}.
		\end{equation}
		For some \glspl{probmodel}, such as a \gls{gmm}, this \gls{optproblem} can be 
		difficult to solve directly. As a work-around, one can often introduce an auxiliary 
		attribute $\truelabel \in \labelspace$, generated via some \gls{rv} $\truelabel'$, 
		such that the corresponding \gls{probmodel} $\pmf{\feature',\truelabel'}{\cdot,\cdot;\weights}$ 
		yields a much easier \gls{maxlikelihood} problem 
		\begin{equation}
			\label{equ_def_complete_EM_dict}
			\min_{\weights \in \paramspace} 
			- \log \pmf{\feature',\truelabel'}{\feature,\truelabel;\weights}.
		\end{equation}
		The attribute $\truelabel$ is introduced solely to simplify 
		\eqref{equ_def_complete_EM_dict}, but it is not observed in practice—only the 
		feature $\feature$ is available. Thus, we cannot solve \eqref{equ_def_complete_EM_dict} 
		directly as we do not know which value $\truelabel$ to plug into the \gls{pmf} 
		$ \pmf{\feature',\truelabel'}{\feature,\truelabel;\weights}$. 
		The EM method resolves this dilemma by alternating between two steps:
		\begin{itemize}
			\item {\bf (E-step)} computing a “soft’’ estimate of the auxiliary 
			attribute $\truelabel$ in the form of the \gls{posterior} 
			$\pmf{\truelabel'|\feature'}{\cdot;\widehat{\weights}}$
			using the current choice $\widehat{\weights}$ for the \glspl{modelparam}, and 
			\item {\bf (M-step)} minimizing a surrogate \gls{objfunc} derived from this \gls{posterior}.
		\end{itemize}
		The completion of these two steps constitutes one full \gls{iteration} of 
		the EM method. 
		In more detail, the E-step produces the \gls{function}
		\[
		Q(\weights \mid \widehat{\weights})
		\defeq 
		- \sum_{\truelabel \in \labelspace} 
		\pmf{\truelabel'|\feature'}{\truelabel;\widehat{\weights}}
		\log\!\Big(
		\pmf{\feature',\truelabel'}{\feature,\truelabel;\weights}
		/\pmf{\truelabel'|\feature'}{\truelabel;\widehat{\weights}}
		\Big),
		\]
		and the M-step minimizes $Q(\weights \mid \widehat{\weights})$ over 
		$\weights \in \paramspace$.
		This \gls{function} satisfies two key properties \cite[Sec. 9.4]{BishopBook},\cite[Sec. 11.4.7]{Murphy2012}:
		\begin{itemize}
			\item {\bf Upper bound:}
			\begin{equation}
				\label{equ_upper_bound_EM_dict}
				Q(\weights\mid \widehat{\weights})
				\geq
				- \log \pmf{\feature'}{\feature;\weights}
				\quad \text{for all } \weights \in \paramspace.
			\end{equation}
			\item {\bf Tightness:}
			\begin{equation}
				\label{equ_upper_bound_EM_dict_tight}
				Q(\widehat{\weights}\mid \widehat{\weights})
				=- \log \pmf{\feature'}{\feature;\widehat{\weights}}.
			\end{equation}
		\end{itemize}
		To summarize, during each \gls{iteration}, EM minimizes an upper-bounding 
		surrogate \gls{objfunc} that is tight at the current iterate $\widehat{\weights}$. 
		Thus, EM is a \gls{majmin} method for approximately solving \eqref{equ_def_ML_EM_dict}. 
		The above construction and analysis of EM can be extended to more 
		general settings involving multiple \glspl{datapoint} 
		and infinite \glspl{featurespace} such as $\mathbb{R}^{\featuredim}$; see \cite[Sec. 11.4.7]{Murphy2012} for details. 
			\\
		See also: \gls{probmodel}, \gls{maxlikelihood}, \gls{optproblem}. },
	first={expectation-maximization (EM)},
	type=math, 
	text={EM}
}

\newglossaryentry{ppca}
{name={probabilistic principal component analysis (PPCA)}, 
	description={PPCA\index{probabilistic principal component analysis (PPCA)} 
		extends\linebreak basic \gls{pca} by using a \gls{probmodel} for \glspl{datapoint}. 
		The \gls{probmodel} of PPCA frames the task of \gls{dimred} 
		as an estimation problem that can be solved using \gls{em} \cite{TippingProbPCA}.
				\\
		See also: \gls{pca}, \gls{probmodel}, \gls{dimred}, \gls{em}.},
	first={probabilistic principal component analysis (PPCA)},
	type=math, 
	text={PPCA}
}

\newglossaryentry{condprobdist}
{name={conditional probability distribution}, 
	description={Consider\index{conditional probability distribution} 
     		a \gls{stochproc} consisting of two \glspl{rv} $\featurevec$ and $\truelabel$ 
    		with \gls{probdist} $\probdist^{(\featurevec,\truelabel)}$. The conditional 
    		\gls{probdist} of $\truelabel$ given (or conditioned on) $\featurevec$ is 
    		denoted by $\probdist^{(\truelabel\mid\featurevec)}$. It is defined via the 
    		\glspl{conditionalexpect} of the indicator \glspl{function} of  
    		\gls{measurable} sets in the \gls{sigmaalgebra} generated by 
    		the \gls{rv} $\truelabel$ \cite{BillingsleyProbMeasure}, \cite{KallenbergBook}.
		\\
		See also: \gls{probdist}, \gls{conditionalexpect}. },
  	first={conditional probability distribution}, 
  	plural={conditional probability distributions},
  	type=math, 
  	text={conditional probability distribution}
}

\newglossaryentry{linearmap}
{name={linear map}, plural={linear maps}, 
	description={A\index{linear map} linear \gls{map} 
	             $f: \mathbb{R}^\nrfeatures \rightarrow \mathbb{R}^\samplesize$ 
	    is a \gls{function} that satisfies additivity, i.e.,
		$f(\vx + \vy) = f(\vx) + f(\vy)$, and homogeneity, i.e.,
		$f(c\vx) = c f(\vx)$, for all \glspl{vector} $\vx, \vy \in \mathbb{R}^\nrfeatures$ 
		and scalars $c \in \mathbb{R}$. In particular, $f(\mathbf{0}) = \mathbf{0}$. 
		Any linear \gls{map} can be represented as a \gls{matrix} multiplication 
		$f(\vx) = \mA \vx$ for some \gls{matrix} $\mA \in \mathbb{R}^{m \times n}$. 
		The collection of real-valued linear \glspl{map} (where $\samplesize=1$) 
		for a given dimension $\nrfeatures$ constitute a \gls{linmodel}. The notion 
		of a linear \gls{map} can be generalized from \gls{domain} $\mathbb{R}^{\nrfeatures}$ 
		and \gls{co-domain} $\mathbb{R}^{\samplesize}$ to arbirary \glspl{vectorspace}.
		\\
		See also: \gls{map}, \gls{function}, \gls{vector}, \gls{matrix}, \gls{linmodel}.},
	first={linear map},
	type=math, 
	plural={linear maps}, 
	firstplural={linear maps}, 
	text={linear map}
}

\newglossaryentry{vector}
{name={vector},
	description={A\index{vector} vector is an element of a \gls{vectorspace}. 
		In the context of \gls{ml}, a particularly important example of a \gls{vectorspace} 
		is the \gls{euclidspace} $\mathbb{R}^{\nrfeatures}$, where $\nrfeatures \in \mathbb{N}$ 
		is the (finite) dimension of the space. A vector $\vx \in \mathbb{R}^{\nrfeatures}$ 
		can be represented as a list or one-dimensional (1-D) array of real numbers, i.e., 
		$x_1, \,\ldots, \,x_{\nrfeatures}$ with $x_\featureidx \in \mathbb{R}$ for 
		$\featureidx = 1, \,\ldots, \,\nrfeatures$. The value $x_\featureidx$ is the $\featureidx$th 
		entry of the vector $\vx$. It can also be useful to view a vector $\vx \in \mathbb{R}^{\nrfeatures}$ 
		as a \gls{function} that maps each index $\featureidx \in \{1, \,\ldots, \,\nrfeatures\}$ 
		to a value $x_\featureidx \in \mathbb{R}$, i.e., $\vx: \featureidx \mapsto x_\featureidx$. 
		This perspective is particularly useful for the study of \glspl{kernelmethod}. See Fig. 
		\ref{fig:vector-function-dual_dict} for the two views of a vector.
		\begin{figure}[H]
			% Left: Stem plot
			\begin{minipage}[c]{0.48\textwidth}
				\centering 
				2, --1, 3, 0, --2, 1
				\begin{minipage}{\textwidth}
				\vspace{5ex}
				\centering
				{\selectfont (a)}
				\end{minipage}
			\end{minipage}
			\hfill
			% Right: Column vector
			\begin{minipage}{0.48\textwidth}
			\centering
			\begin{tikzpicture}
			\begin{axis}[
    				width=6.5cm,
    				height=5cm,
    				title={},
    				xlabel={index $\featureidx$},
    				ylabel={$x_\featureidx$},
   		 		ymin=-3.5, ymax=3.5,
    				xmin=0.5, xmax=6.5,
   	 			xtick={1,2,3,4,5,6},
    				ytick={-3,-2,-1,0,1,2,3},
    				axis x line=bottom,        % <-- horizontal axis at y=0
    				axis y line=left,          % <-- vertical axis on the left
    				grid=both,
    				major grid style={dotted, gray!60},
    				enlargelimits=0.1
			]
			\addplot+[ycomb, thick, mark=*]
    			coordinates {
        				(1,2)
        				(2,-1)
       	 			(3,3)
        				(4,0)
        				(5,-2)
        				(6,1)
    			};
			\end{axis}
			\node at (2,-2.5) {(b)};
			\end{tikzpicture}
			\end{minipage}
			\caption{Two equivalent views of a vector $\vx= \big( 2, -1, 3, 0, -2, 1 \big)^{T} \in \mathbb{R}^{6}$.
			(a) As a numeric array. (b) As a \gls{map} $\featureidx \mapsto x_\featureidx$.}
			\label{fig:vector-function-dual_dict}
		\end{figure}
		See also: \gls{vectorspace}, \gls{euclidspace}, \gls{linearmap}.},
	first={vector},
	firstplural={vectors},
	type=math,
	plural={vectors},
	text={vector}
}

\newglossaryentry{vectorspace}
{name={vector space},
	description={A\index{vector space} \gls{vector} space $\mathcal{V}$ (also called linear space) 
		is a collection of elements, called \glspl{vector}, along with the following two operations 
		(see also Fig. \ref{fig:vector-ops_dict}): 
    		1) addition (denoted by $\vv+\vw$) of two \glspl{vector} $\vv,\vw$; and 2) multiplication 
		(denoted by $c \,\cdot \,\vv$) of a \gls{vector} $\vv$ with a scalar $c$ that belongs to some 
		number field (such as the real numbers $\mathbb{R}$ or the complex numbers $\mathbb{C}$). The defining 
		property of a \gls{vector} space is that it is closed under two specific operations. First, 
		if $\vv, \vw \in \mathcal{V}$, then $\vv + \vw \in \mathcal{V}$. Second, if $\vv \in \mathcal{V}$ 
		and $c \in \mathbb{R}$, then $c \vv \in \mathcal{V}$.
		\begin{figure}[H]
		\centering
			\begin{tikzpicture}[>=Stealth, scale=1.2]
			% Coordinates
  			\coordinate (O) at (0,0);            % Origin
  			\coordinate (V) at (2,1.5);          % vector v
  			\coordinate (W) at (1,3);            % vector w
  			\coordinate (VplusW) at (3,4.5);     % v + w
  			\coordinate (HalfV) at (1,0.75);     % 0.5 * v
  			\draw[->, thick, blue] (O) -- (V) node[pos=1, right] {$\vv$};
  			\draw[->, thick, red] (O) -- (W) node[pos=1, left] {$\vw$};
  			\draw[->, thick, purple] (O) -- (VplusW) node[pos=0.99, above right] {$\vv+\vw$};
  			\draw[dashed, red] (V) -- (VplusW);
  			\draw[dashed, blue] (W) -- (VplusW);
  			\draw[->, thick, orange] (O) -- (HalfV) node[midway, right] {$ \alpha \vv$};
			% Filled dots
  			\filldraw[black] (O) circle (2pt) node[below left] {$\mathbf{0}$};  % origin
  			\filldraw[blue] (V) circle (2pt);         % v
  			\filldraw[red] (W) circle (2pt);          % w
  			\filldraw[purple] (VplusW) circle (2pt);  % v + w
  			\filldraw[orange] (HalfV) circle (2pt);   % 0.5v
			\end{tikzpicture}
			\caption{A \gls{vector} space $\mathcal{V}$ is a collection of \glspl{vector} such that 
			scaling and adding them always yields another \gls{vector} in $\mathcal{V}$.}
			%In \gls{ml}, we use vector spaces to represent \glspl{rv}, \glspl{datapoint} 
			%(or their \glspl{featurevec}) as well as invariances (or symmetries) of \glspl{model}.}
			\label{fig:vector-ops_dict}
		\end{figure}
		A common example of a \gls{vector} space is the \gls{euclidspace} $\mathbb{R}^n$, which is 
		widely used in \gls{ml} to represent \glspl{dataset}. We can also use $\mathbb{R}^n$ 
		to represent, either exactly or approximately, the \gls{hypospace} used by an \gls{ml} method.  
		Another example of a \gls{vector} space, which is naturally associated with every \gls{probspace} 
		$\big(\samplespace,\eventspace,\prob{\cdot} \big)$, is the collection of all 
		real-valued \glspl{rv} $x: \samplespace \rightarrow \mathbb{R}$ \cite{RudinBook}, \cite{folland1999real}.  
		\\
		See also: \gls{vector}, \gls{euclidspace}, \gls{linmodel}, \gls{linearmap}.},
	first={vector space},
	plural={vector spaces}, 
	firstplural={vector spaces}, 
	type=math,
	text={vector space}
}


\newglossaryentry{stochastic}
{name={stochastic},
	description={We refer to a\index{stochastic} method as stochastic if it involves a 
		random component or is governed by probabilistic laws. \Gls{ml} methods use randomness 
		to reduce computational complexity (e.g., see \gls{stochGD}) or 
		to capture \gls{uncertainty} in \glspl{probmodel}.
		\\
		See also: \gls{stochGD}, \gls{uncertainty}, \gls{probmodel}.},
	first={stochastic},
	type=math, 
	text={stochastic}
}

\newglossaryentry{stochproc}
{name={stochastic process},
	description={A \gls{stochastic} process\index{stochastic process} is a collection of 
		\glspl{rv} defined on a common \gls{probspace} and indexed by some set 
		$\mathcal{I}$ \cite{GrayProbBook}, \cite{papoulis}, \cite{Brockwell91}. The index set 
		$\mathcal{I}$ typically represents time or space, allowing us to represent 
		random phenomena that evolve across time or spatial dimensions—for example, 
		sensor noise or financial time series. \Gls{stochastic} processes are not limited 
		to temporal or spatial settings. For instance, random \glspl{graph} such as 
		the \gls{ergraph} or the \gls{sbm} can also be viewed as \gls{stochastic} processes. 
		Here, the index set $\mathcal{I}$ consists of node pairs that index \glspl{rv} whose values 
		encode the presence or weight of an edge between two nodes. Moreover, \gls{stochastic} 
		processes naturally arise in the analysis of \glspl{stochalgorithm}, 
		such as \gls{stochGD}, which construct a sequence of \glspl{rv}. 
		\\
		See also:  \gls{rv}, \gls{sbm}, \gls{stochGD}, \gls{uncertainty}, \gls{probmodel}.},
	first={stochastic process},
	firstplural={stochastic processes},
	type=math, 
	plural={stochastic processes},
	text={stochastic process}
}

\newglossaryentry{characteristicfunc}
{name={characteristic function},
	description={The characteristic \gls{function}\index{characteristic function} 
		of a real-valued \gls{rv} $x$ is the \gls{function} \cite[Sec. 26]{BillingsleyProbMeasure}
		$$ \phi_{x}(t) \defeq \expect { \exp\,(j t x) } \mbox{ with } j = \sqrt{-1}.$$
	 	The characteristic \gls{function} uniquely determines the \gls{probdist} of $x$. 
		\\
		See also: \gls{rv}, \gls{probdist}.},
	first={characteristic function},
	firstplural={characteristic functions}, 
	type=math, 
	plural={characteristic functions},
	text={characteristic function}
}

\newglossaryentry{entropy}
{name={entropy},
	description={Entropy\index{entropy} quantifies the \gls{uncertainty} or 
		unpredictability associated with an \gls{rv} \cite{coverthomas}. 
		For a \gls{discreteRV} $x$ taking on values in a finite set 
		$\mathcal{S} = \{x_1, \,\ldots, \,x_\nrcluster\}$ with 
		a \gls{pmf} $\pmf{x}{x_{\clusteridx}} (=\prob{x = x_{\clusteridx}})$, 
		the entropy is defined as \cite{coverthomas}
		\[
		   \entropy{x} \defeq -\sum_{\clusteridx=1}^{\nrcluster} \pmf{x}{x_{\clusteridx}}  \log \pmf{x}{x_{\clusteridx}} .
		\]
		For a given set of values $\mathcal{S}$, the entropy is maximized for a 
		uniformly distributed \gls{rv}, where $\pmf{x}{x_{\clusteridx}}=1/\nrcluster$. 
		The minimal entropy, which is zero, is obtained when $\pmf{x}{x_{\clusteridx}}=1$ 
		for some $x_{\clusteridx} \in \mathcal{S}$.
		\Gls{diffentropy} generalizes the concept of \gls{entropy} from \glspl{discreteRV} to 
		\gls{continuous} \glspl{rv}. 
		\\
		See also: \gls{uncertainty}, \gls{probmodel}.},
	first={entropy},
	type=math, 
	text={entropy}
}

\newglossaryentry{diffentropy}
{name={differential entropy},
	description={For\index{differential entropy} a 
	\gls{rv} $\featurevec \in \mathbb{R}^{\nrfeatures}$ 
	with a \gls{pdf} $\pdf{x}{\cdot}$, the differential \gls{entropy} 
	is defined as \cite{coverthomas}
		\[
		h(\featurevec) \defeq - \int_{\featurevec' \in \mathbb{R}^{\nrfeatures}} \log p(\featurevec') \, d \pdf{\featurevec}{\featurevec'} .
		\]
	Differential \gls{entropy} can be negative and lacks some properties of 
	\gls{entropy} for discrete-valued \glspl{rv}, such as invariance under 
	a change of variables \cite{coverthomas}. Among all \glspl{rv} with a 
	given \gls{mean} $\meanvecgeneric$ and \gls{covmtx} $\covmtxgeneric$, 
	$h(\featurevec)$ is maximized by $\featurevec \sim \mvnormal{\meanvecgeneric}{\covmtxgeneric}$. 
		\\
		See also: \gls{uncertainty}, \gls{probmodel}.},
	first={differential entropy},
	type=math,
	text={differential entropy}
}


\newglossaryentry{domain}
{name={domain}, 
	description={The domain\index{domain} of a \gls{function} 
		$f: \mathcal{U} \rightarrow \mathcal{V}$ is the set $\mathcal{U}$ 
		from which $f$ takes its inputs.  
		\\
		See also: \gls{function}, \gls{co-domain}, \gls{map}.},
	first={domain},
	firstplural={domains}, 
	type=math, 
	plural={domains},
	text={domain}
}

\newglossaryentry{function}
{name={function}, 
	description={A function\index{function} between two sets $\mathcal{U}$ and $\mathcal{V}$ assigns  
		each element $u \in \mathcal{U}$ exactly one element $f(u) \in \mathcal{V}$ \cite{RudinBookPrinciplesMatheAnalysis}.
		We write this as $$f: \mathcal{U} \rightarrow \mathcal{V}: u \mapsto f(u)$$ 
		where $\mathcal{U}$ is the domain and $\mathcal{V}$ the co-domain of $f$. 
		That is, a function $f$ defines a unique output $f(u) \in \mathcal{V}$ for every 
		input $u \in \mathcal{U}$ (see Fig. \ref{fig_function_dict}).
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[>=stealth, node distance=1.2cm and 2.5cm]
				\tikzset{dot/.style={circle, fill=black, inner sep=1.2pt}}
				\node (A) [dot, label=left:$a$] {};
				\node (B) [dot, below=of A, label=left:$b$] {};
				\node (C) [dot, below=of B, label=left:$c$] {};
				\node (1) [dot, right=4cm of A, label=right:$\star$] {};
				\node (2) [dot, below=of 1, label=right:$\circ$] {};
				\node (3) [dot, below=of 2, label=right:$\otimes$] {};
				\node[draw=blue!70, thick, ellipse, inner sep=0.5cm, fit=(A)(B)(C), label=above:$\mathcal{U}$] {};
				\node[draw=green!70!black, thick, ellipse, inner sep=0.5cm, fit=(1)(2)(3), label=above:$\mathcal{V}$] {};
				\draw[->] (A) -- (2);
				\draw[->] (B) -- (1);
				\draw[->] (C) -- (2);
			\end{tikzpicture}
			\caption{A function \( f \colon \mathcal{U} \to \mathcal{V} \) mapping each element 
			of the domain $\mathcal{U} =  \{a,b,c\}$ to exactly one element of 
				the co-domain $\mathcal{V} = \{\star,\circ,\otimes\}$. \label{fig_function_dict}}
		\end{figure} },
	first={function},
	firstplural={functions}, 
	type=math, 
	plural={functions},
	text={function}
}

\newglossaryentry{map}
{name={map}, 
	description={We\index{map} use the term map as a synonym for \gls{function}.
		\\
		See also: \gls{function}.},
	first={map},
	firstplural={maps},	
	type=math, 
	plural={maps},
	text={map}
}

\newglossaryentry{event}
{name={event}, 
	description={Consider\index{event} an \gls{rv} $\featurevec$, defined on some \gls{probspace}, 
				 which takes values in a \gls{measurable} space $\featurespace$. An 
				 event $\mathcal{A} \subseteq \featurespace$ is a subset of $\featurespace$ 
				 such that the \gls{probability} $\prob{\featurevec \in \mathcal{A}}$ is well 
				 defined. In other words, the \gls{preimage} $\featurevec^{-1}(\mathcal{A})$ 
				 of an event belongs to the underlying \gls{sigmaalgebra}, i.e., the \gls{preimage} 
				 is a \gls{measurable} subset of the \gls{samplespace} \cite{RudinBook}, \cite{BillingsleyProbMeasure}, \cite{durrett2010probability}.	
		 		 Roughly speaking, an event represents a set of possible outcomes of some 
				 process. One example of such a process could also be the treatment of a 
				 health-care patient. 
				 \\
		See also: \gls{rv}, \gls{datapoint}, \gls{iidasspt}, \gls{probmodel}.},
	first={event},
	firstplural={events},
	plural={events},
	type=math,
	text={event} 
}

\newglossaryentry{countable}
{name={countable},
	description={A set is called countable\index{countable} if its 
		elements can be put into a one-to-one correspondence with the natural numbers 
		$\mathbb{N}=\{1,\,2,\,3,\,\ldots\}$ or with a finite subset of $\mathbb{N}$ \cite{HalmosSet}. 
		Equivalently, a set $\mathcal{A}$ is countable if there exists an \gls{injective} 
		\gls{function} $f:\mathcal{A}\rightarrow\mathbb{N}$. 
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[>=stealth, node distance=1.0cm, thick]
  			%--- Left: elements of set A ---
  			\node (a1) {$a_1$};
  			\node[below=of a1] (a2) {$a_2$};
  			\node[below=of a2] (a3) {$a_3$};
  			\node[left=0.4cm of a2, align=center] {$\mathcal{A}$};
  			% Ellipse enclosing set A
  			\begin{scope}[on background layer]
    			\draw[rounded corners, dashed, gray] ($(a1)+(-0.5,0.4)$) rectangle ($(a3)+(0.5,-0.4)$);
  			\end{scope}
  			%--- Right: natural numbers ---
  			\node[right=3.0cm of a1] (n1) {$1$};
  			\node[below=of n1] (n2) {$2$};
  			\node[below=of n2] (n3) {$3$};
  			\node[below=of n3] (n4) {$4$};
  			\node[below=of n4] (ndots) {$\vdots$};
  			\node[right=0.4cm of n2, align=center] {$\mathbb{N}$};
  			%--- Arrows (injective mapping) ---
  			\draw[->] (a1) -- (n3);
  			\draw[->] (a2) -- (n1);
  			\draw[->] (a3) -- (n4);
			\end{tikzpicture}
		\caption{An \gls{injective} \gls{function} that maps the elements of a finite set 
			$\mathcal{A}$ to the natural numbers $\mathbb{N}$, which implies that $\mathcal{A}$ is countable.}
		\end{figure}
		Typical examples include the set of integers $\mathbb{Z}$ and rational 
		numbers $\mathbb{Q}$. In contrast, the set of real numbers $\mathbb{R}$ 
		is not countable, meaning no such one-to-one correspondence with $\mathbb{N}$ exists.
		\\ 
		See also: \gls{injective}, \gls{function}.}, 
	first={countable}, 
	type=math, 
	text={countable}
}

\newglossaryentry{pmf}
{name={probability mass function (pmf)}, 
	description={The pmf\index{probability mass function (pmf)} 
		of a discrete \gls{rv} $\feature$ is a \gls{function} 
		$\pmf{\feature}{\cdot}: \featurespace \rightarrow [0,1]$ that assigns to each 
		possible value $\feature' \in \featurespace$ of the \gls{rv} $\feature$ 
		the \gls{probability} $\pmf{\feature}{\feature'} = \prob{\feature' = \feature}$ \cite{papoulis}. 
		Fig.\ \ref{fig_pmf_dict} illustrates the pmf of a discrete \gls{rv} $\feature$. 
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[>=stealth, thick,y=2cm]
  			\foreach \x/\p in {1/0.3, 4/0.7}{
			% Stem lines
			\draw[gray] (\x,0) -- (\x,\p);
			% Dots at the end of stems
			\fill[blue] (\x,\p) circle (2pt);
			% Labels next to each value
			% \node[anchor=west, blue] at (\x+0.1,\p) {\small $\pmf{\feature}{\cdot}$};
			}
  			\node[anchor=south,align=center] at (1,0.3) {\small $\pmf{\feature}{\star}=\frac{3}{10}$};
  			\node[anchor=north] at (1,0) {\small $\star$};
  			\node[anchor=north] at (4,0) {\small $\otimes$};
    			%--- Example datasets (same length, same frequencies, different permutations) ---
  			\node[anchor=west,text width=11cm] at (-1.2,-0.80) {
    			$\dataset = (\star,\,\star,\,\star,\,\otimes,\,\otimes,\,\otimes,\,\otimes,\,\otimes,\,\otimes,\,\otimes)$
  			};
  			\node[anchor=west,text width=11cm] at (-5.2,1.18) {\small
    			$\dataset' = (\otimes,\,\star,\,\otimes,\,\star,\,\otimes,\,\otimes,\,\star,\,\otimes,\,\otimes,\,\otimes)$
  			};
  			\node[anchor=west,text width=11cm] at (3.2,1.56) {
    			$\dataset'' = (\otimes,\,\otimes,\,\otimes,\,\star,\,\otimes,\,\star,\,\otimes,\,\otimes,\,\star,\,\otimes)$
  			};
			\end{tikzpicture}
		\caption{The pmf $\pmf{\feature}{\cdot}$ of a discrete \gls{rv} $\feature$ 
			taking values in the set $\featurespace = \{\star,\otimes\}$. Three \glspl{dataset} 
			are also shown whose relative frequencies of \glspl{datapoint} match 
			this pmf exactly. Such \glspl{dataset} could arise as \glspl{realization} of 
			\gls{iid} \glspl{rv} sharing the common pmf $\pmf{\feature}{\cdot}$. 
			\label{fig_pmf_dict}}
		\end{figure}
		A pmf always satisfies $\sum_{\feature' \in \featurespace} \pmf{\feature}{\feature'} = 1$. 
		We can view a pmf as representing a collection of (sufficiently long)  
		\glspl{dataset}. This collection contains any 
		$\dataset = \{\feature^{(1)}, \,\ldots, \,\feature^{(\samplesize)}\}$, 
		with the relative frequencies of every value $\feature' \in \featurespace$ 
		being close to the corresponding pmf value $\pmf{\feature}{\feature'}$, 
		$$ \frac{\big|\sampleidx \in \{1,\,\ldots,\,\samplesize\}: \feature^{(\sampleidx)}= \feature' \big|}
		{\samplesize} \approx \pmf{\feature}{\feature'}.$$ 
		Note that requiring relative frequencies to be close to the pmf values 
		implies that the empirical \gls{entropy} of such a \gls{dataset} is close to the 
		\gls{entropy} of the pmf $\pmf{\feature}{\cdot}$. Information theory refers 
		to the collection of such \glspl{dataset} as the typical set corresponding to the pmf 
		$\pmf{\feature}{\cdot}$ \cite{coverthomas}. A main result of information theory states 
		that a \gls{dataset} generated by \gls{iid} sampling from $\pmf{\feature}{\cdot}$ 
		belongs, with high \gls{probability}, to the typical set with respect to $\pmf{\feature}{\cdot}$ \cite[Th. 3.1.2]{coverthomas}.
				\\
		See also: \gls{rv}, \gls{probability}, \gls{probdist}, \gls{probmodel}.},
	first={probability mass function (pmf)},
	firstplural={probability mass functions (pmfs)},
	plural={pmfs},
	type=math,
	text={pmf} 
}

\newglossaryentry{discreteRV}
{name={discrete random variable (discrete RV)}, 
 	description={A\index{discrete random variable (discrete RV)} \gls{rv}, i.e., 
	             a \gls{function} that maps the \glspl{outcome} of a \gls{randomexperiment} 
				 to elements of a \gls{measurable} space $\featurespace$, 
		         is referred to as discrete if its value space 
				 $\featurespace$ \gls{countable} \cite{BillingsleyProbMeasure}. 
			\\
		See also: \gls{probability}, \gls{rv}, \gls{probdist}.},
 	first={discrete random variable (discrete RV)},
	firstplural={discrete random variables (discrete RVs)}, 
	plural={discrete RVs},
	type=math, 
 	text={discrete RV}  
}

\newglossaryentry{rv}
{name={random variable (RV)}, plural={RVs},
 	description={A RV\index{random variable (RV)} is a \gls{function} that maps the 
		outcomes of a \gls{randomexperiment} to elements of a \gls{measurable} space 
		\cite{BillingsleyProbMeasure,GrayProbBook}. 
 		Mathematically, a RV is a \gls{function} $x: \samplespace \rightarrow \featurespace$ 
		with domain being the \gls{samplespace} $\samplespace$ of a \gls{probspace} and 
		co-domain being a \gls{measurable} space $\featurespace$. 
 		Different types of RVs include  
 		\begin{itemize} 
 			\item {binary RVs}, which map each outcome to an element of a binary 
			set (e.g., $\{-1,1\}$ or $\{\text{cat}, \text{no cat}\}$); 
			\item {discrete RVs}, which take on values in a \gls{countable} set (which can 
			be finite or countably infinite); 
 			\item {real-valued RVs}, which take on values in the real numbers $\mathbb{R}$;  
 			\item {\gls{vector}-valued RVs}, which map outcomes to the \gls{euclidspace} $\mathbb{R}^{\featuredim}$.  
 		\end{itemize} 
 		\Gls{probability} theory uses the concept of \gls{measurable} spaces to rigorously define 
 		and study the properties of collections of RVs \cite{BillingsleyProbMeasure}.
			\\
		See also: \gls{function}, \gls{randomexperiment}, \gls{samplespace}, \gls{probspace}, \gls{vector}, \gls{euclidspace}, \gls{probability}, \gls{measurable}.}, 
	first={random variable (RV)},
	firstplural={random variables (RVs)},
	plural={RVs},
	type=math, 
	text={RV}  
}
 
 

\newglossaryentry{outcome}
{name={outcome}, 
  	description={One possible\index{outcome} result of a physical process.  
				 Such a process could be the observation of a physical phenomenon,  
				 a computation performed by an \gls{algorithm}, or a \gls{randomexperiment}  
				\cite{BillingsleyProbMeasure}.   
		\\
 		See also: \gls{samplespace}.},  
	type=math, 
  	first={outcome}, 
 	firstplural={outcomes},
 	plural={outcomes},
  	text={outcome}
} 

\newglossaryentry{probspace}
 {name={probability space}, 
 	description={A\index{probability space} \gls{probability} space is a mathematical 
 		structure that allows us to reason about a \gls{randomexperiment}, e.g., 
		the observation of a physical phenomenon. 
 	   	Formally, a \gls{probability} space $\mathcal{P}$ is a triplet 
		$(\samplespace, \eventspace, \prob{\cdot})$ where
 		\begin{itemize} 
 			\item  $\samplespace$ is a \gls{samplespace} containing all possible outcomes 
			of a \gls{randomexperiment};
 			\item  $\eventspace$ is a \gls{sigmaalgebra}, i.e., a collection of subsets of 
			$\samplespace$ (called \glspl{event}) that satisfies certain closure properties 
			under set operations;
 			\item $\prob{\cdot}$ is a \gls{probdist}, i.e., a \gls{function} that assigns 
			a \gls{probability} $\prob{\mathcal{A}} \in [0,1]$ to each \gls{event} $\mathcal{A} 
			\in \eventspace$. This \gls{function} must satisfy $\prob{\samplespace} = 1$ and 
			$\prob{\bigcup_{i=1}^{\infty} \mathcal{A}_i} = \sum_{i=1}^{\infty} \prob{\mathcal{A}_i}$ 
			for any \gls{countable} \gls{sequence} of pairwise disjoint \glspl{event} $\mathcal{A}_1, \,\mathcal{A}_2, \,\ldots$ in $\mathcal{F}$.
 		\end{itemize}
 		\Gls{probability} spaces provide the foundation of \glspl{probmodel} 
		that can be used to study the behavior of \gls{ml} methods \cite{BillingsleyProbMeasure}, \cite{GrayProbBook}, \cite{ross2013first}.
				\\
		See also: \gls{probability}, \gls{randomexperiment}, \gls{samplespace}, \gls{event}, \gls{probdist}, \gls{function}, \gls{probmodel}, \gls{ml}.},  
 	first={probability space}, 
	plural={probability spaces},
	firstplural={probability spaces},
	type=math, 
 	text={probability space}
 }

 \newglossaryentry{integrable}
{name={integrable},
	description={A \gls{measurable} \gls{function} $f\!:\!\samplespace \!\to\! \mathbb{R}$ 
		defined on a \gls{measurespace} $(\samplespace, \,\sigmaalgebra, \,\mu)$ 
		is called integrable\index{integrable} if the \gls{LebesgueIntegral} 
		of its absolute value is finite, i.e.,
		\[
		\int_{\samplespace} |f(x)|\,\mathrm{d}\mu < \infty.
		\]
		In this case, the \gls{LebesgueIntegral} $\int_{\samplespace} f(x)\,\mathrm{d}\mu$ 
		is well-defined and finite. An \gls{rv} $x$ defined on the \gls{samplespace} 
		of a \gls{probspace} $(\samplespace, \,\sigmaalgebra, \,\probdist)$ 
		is integrable if 
		\[
		\expect\{|x|\}
		= \int_{\samplespace} |x(\omega)|\,\mathrm{d}\probdist 
		< \infty
		\]
		which ensures that the \gls{expectation} $\expect\{|x|\}$ 
		exists and is finite. 
		\\ 
		See also: \gls{measurespace}, \gls{measure}.},
	first={integrable},
	type=math, 
	text={integrable}
}

\newglossaryentry{measurespace}
{name={measure space},
	description={A \gls{measure} space\index{measure space} is a triple $(\samplespace, \,\sigmaalgebra, \,\mu)$ consisting of 
        		a set $\samplespace$, a \gls{sigmaalgebra} $\sigmaalgebra$ of subsets 
		of $\samplespace$, and a \gls{measure} $\mu\!:\sigmaalgebra\!\to\![0,\infty)$. 
        		The \gls{measure} $\mu$ assigns a nonnegative number to each \gls{measurable} 
		set $\mathcal{A} \in \sigmaalgebra$, generalizing the 
		notions of length, area, or volume in \glspl{euclidspace} \cite{RudinBookPrinciplesMatheAnalysis}, \cite{HalmosMeasure}. 
		\Gls{measure} spaces provide the mathematical foundation for the 
		\gls{LebesgueIntegral} or the definition of \glspl{rv} as \gls{measurable} mappings 
        		between \gls{measure} spaces.  
        		A \gls{probspace} is a special case of a \gls{measure} space 
		where the total \gls{measure} of the \gls{samplespace} is normalized to one, 
        		i.e., $\mu(\samplespace) = 1$. In this case, $\mu$ is called a \gls{probdist}.
		\\ 
		See also: \gls{measurable}, \gls{probspace}, \gls{probdist}. },
    	first={measure space},
	plural={measure spaces}, 
	firstplural={measure spaces}, 
	type=math, 
    text={measure space}
}

\newglossaryentry{measure}
{name={measure},
	description={A measure\index{measure} $\mu$ on a set $\samplespace$ equipped with a 
		\gls{sigmaalgebra} $\sigmaalgebra$ is a \gls{function} $\mu: \sigmaalgebra \to [0, \infty)$ 
		that assigns a nonnegative value to each \gls{measurable} set 
		$\mathcal{A} \in \sigmaalgebra$ such that 
		\cite{RudinBookPrinciplesMatheAnalysis}, \cite{BillingsleyProbMeasure}, \cite{HalmosMeasure}: 
		1) $\mu(\emptyset) = 0$; and 
		2) for any \gls{countable} collection $\{\mathcal{A}_i\}_{i=1}^{\infty}$ of 
		pairwise disjoint sets in $\sigmaalgebra$,
		\[
		\mu\!\left(\bigcup_{i=1}^{\infty} A_i\right) = \sum_{i=1}^{\infty} \mu(A_i)
		\]
		which is referred to as ``\gls{countable} additivity''.
		\\
		See also: \gls{measurable}, \gls{countable}. },
	first={measure},
	type=math, 
	plural={measures}, 
	firstplural={measures}, 
	text={measure}
}

\newglossaryentry{LebesgueIntegral}
{name={Lebesgue integral},
	description={The Lebesgue integral\index{Lebesgue integral} 
		assigns each \gls{integrable} \gls{function} 
		$f: \mathbb{R}^{\nrfeatures} \rightarrow \mathbb{R}$ a number 
		$\int_{\featurevec} f(\featurevec) d\featurevec$ 
		that is referred to as the integral of $f$. 
		The integral of $f$ can be interpreted as the volume that 
		is enclosed by the \gls{function} $f$ in the space 
		$\mathbb{R}^{\nrfeatures+1}$. We can compute it be increasingly 
		accurate approximations by \glspl{simplefunction} 
		\cite[Ch. 1]{RudinBook}. 
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[scale=1.5]
  			% Axes
  			\draw[->] (-0.2,0) -- (3.4,0) node[right] {$x$};
 			% \draw[->] (0,-0.1) -- (0,2.6) node[above] {$y$};
  			% Continuous function f(x)
  			\draw[thick,domain=0:3,smooth] plot(\x,{0.5+0.3*\x+0.1*\x*\x}) node[right] {$f(x)$};
  			% Shade area under f(x)
  			\fill[gray!20] (0,0) -- plot[domain=0:3] (\x,{0.5+0.3*\x+0.1*\x*\x}) -- (3,0) -- cycle;
  			% Lower simple function: left-endpoint rectangles for [0,1], [1,2], [2,3]
  			\draw[fill=gray!50,opacity=0.35] (0,0) rectangle (1,0.5);
  			\node at (0.5,-0.2) {$\cluster^{(1)}$}; 
  			\draw[fill=gray!50,opacity=0.35] (1,0) rectangle (2,0.9);
  			\draw[fill=gray!50,opacity=0.35] (2,0) rectangle (3,1.5);
  			% Step lines of the simple function
  			\draw[thick] (0,0.5)--(1,0.5);
  			\draw[thick] (1,0.9)--(2,0.9);
  			\draw[thick] (2,1.5)--(3,1.5);
 	 		\node[above left] at (0.8,0.5) {};
  			% Dashed partition lines
  			\foreach \x in {0,1,2,3} \draw[dashed,gray] (\x,0) -- (\x,2.5);
  			% Labels
  			\node[anchor=north] at (0,-0.05) {$a$};
  			\node[anchor=north] at (1,-0.05) {$b$};
  			\node[anchor=north] at (2,-0.05) {$c$};
  			\node[anchor=north] at (3,-0.05) {$d$};
  			\node at (1.6,3.0) {$\displaystyle \int_a^d f(x)\,dx \approx \underbrace{f(a)(b-a) + f(b)(c-b)+f(c)(d-c)}_{=:\int_a^d s(x)dx}$};
  			\node [anchor=west] at (3.2,1.5) {\gls{simplefunction} $s(x)$};
			\end{tikzpicture}
		\end{figure}
 		It is useful to think of the Lebesgue integral as a \gls{function} that maps 
 		an \gls{integrable} \gls{function} $f$ to the value of its integral, 
		$$ f \mapsto \int_{\featurevec} f(\featurevec) d\featurevec.$$ 
		The precise definition of this \gls{function}, whose \gls{domain} 
		consists of the \gls{integrable} \glspl{function}, is a cornerstone of 
		\gls{measure} theory \cite[Ch. 1]{RudinBook}.
					\\ 
		See also: \gls{function}.},
	first={Lebesgue integral},
	text={Lebesgue integral},
	type=math, 
	plural={Lebesgue integrals},
	firstplural={Lebesgue integrals}
}

\newglossaryentry{conditionalexpect}
{name={conditional expectation}, 
	description={Consider a numeric \gls{rv} $\featurevec \in \mathbb{R}^{\nrfeatures}$ defined on a 
                 \gls{probspace} $\mathcal{P}=(\samplespace,\eventspace,\probmeasure)$. 
                 Let $\sigmaalgebra' \subseteq \sigmaalgebra$ be a (sub-) \gls{sigmaalgebra} that 
                  represents partial information about the outcome of a \gls{randomexperiment}. 
                  The conditional expectation\index{conditional expectation} of 
                  $\featurevec$ given (or conditioned on) $\sigmaalgebra'$, denoted 
                  $\expect\{ \featurevec \mid \sigmaalgebra'\}$, is a numeric \gls{rv} 
				  that \cite{BillingsleyProbMeasure,durrett2010probability}
                  \begin{itemize}
                   \item is \gls{measurable} with respect to $\sigmaalgebra'$, and
                    \item satisfies
			  		$$
    		  		\int_{\mathcal{A}} \expect\{\featurevec \mid \sigmaalgebra'\} {\rm d} \probmeasure =
    		  		\int_{\mathcal{A}} \featurevec {\rm d} \probmeasure \quad \text{for any } \mathcal{A} \in \sigmaalgebra'.
			  		$$
                \end{itemize}
               Intuitively, $\expect\{\featurevec \mid \eventspace'\}$ summarizes 
               the average value of $\featurevec$ using only information contained 
               in the (typically smaller) \gls{sigmaalgebra} $\sigmaalgebra'$ 
			   \cite{BillingsleyProbMeasure,GrayProbBook,ross2013first}. \\
              See also: \gls{expectation}, \gls{sigmaalgebra}, \gls{probspace}.}, 
 	first={conditional expectation},
 	plural={conditional expectations}, 
 	type=math, 
 	firstplural={conditional expectations},  
 	text={conditional expectation}
}

\newglossaryentry{conditionalpmf}
{name={conditional probability mass function (conditional pmf)}, 
	description={Consider two \glspl{discreteRV} $\truelabel$ and $\feature$ defined on 
                  the same \gls{probspace} $(\samplespace,\eventspace,\prob{\cdot})$. 
                  The conditional pmf\index{conditional pmf} of $\truelabel$ given 
				  (or conditioned on) $\feature$ is denoted 
                $\pmf{\truelabel \mid \feature}{\cdot \mid \cdot}$ and is defined by
                \[
                  \pmf{\truelabel \mid \feature}{\truelabel' \mid \feature'}
                  :=
                  \prob{\truelabel=\truelabel' \mid \feature=\feature'},
               \]
                for all realizations $\truelabel',\feature'$ with 
                $\prob{\feature=\feature'}>0$. 
                Equivalently, the conditional pmf can be expressed using 
                \gls{conditionalexpect} as
                \[
                  \pmf{\truelabel \mid \feature}{\truelabel' \mid \feature'}
                  =
                  \expect\{\indicatorfunc{\truelabel=\truelabel'}\mid \sigmaalgebra(\feature)\}(\feature'),
                \]
                where $\sigmaalgebra(\feature)$ denotes the \gls{sigmaalgebra} generated by 
                 the \gls{rv} $\feature$. \\
              See also: \gls{conditionalexpect}, \gls{pmf}, \gls{probspace}.}, 
 	first={conditional probability mass function (conditional pmf)},
 	firstplural={conditional probability mass functoins (conditional pmfs)}, 
 	type=math, 
 	plural={conditional pmfs},  
 	text={conditional pmf}
}


\newglossaryentry{iid}
{name={independent and identically distributed (i.i.d.)}, 
	description={A collection of \glspl{rv}\linebreak $\datapoint^{(1)}, \,\ldots, \,\datapoint^{(\samplesize)}$ is 
		referred to as i.i.d.\index{independent and identically distributed (i.i.d.)} 
		if each $\datapoint^{(\sampleidx)}$ follows the same \gls{probdist}, and 
		the \glspl{rv} are mutually independent. That is, for any collection of 
		\glspl{event} $\mathcal{A}_1, \,\ldots, \,\mathcal{A}_\samplesize$, we have
       		\[
          		\prob{ \datapoint^{(1)} \in \mathcal{A}_1, \,\ldots, \,\datapoint^{(\samplesize)} \in \mathcal{A}_{\samplesize}} 
         		= \prod_{\sampleidx=1}^{\samplesize} \prob{ \datapoint^{(\sampleidx)} \in \mathcal{A}_\sampleidx}.
         	\]
				\\
		See also: \gls{rv}, \gls{probdist}, \gls{event}, \gls{datapoint}, \gls{iidasspt}.},
	first={independent and identically distributed (i.i.d.)},
	type=math, 
	text={{i.i.d.}} 
}

\newglossaryentry{preimage}
{name={preimage}, 
	description={Consider a \gls{function}\index{preimage} $f\colon \mathcal{U} \rightarrow \mathcal{V}$ 
		between two sets. The preimage $f^{-1}(\mathcal{B})$ of a subset $\mathcal{B} \subseteq \mathcal{V}$ is the set 
		of all inputs $u \in \mathcal{U}$ that are mapped into $\mathcal{B}$ by $f$, i.e.,
		\[
		f^{-1}(\mathcal{B}) \defeq \{ u \in \mathcal{U} \mid f(u) \in \mathcal{B} \}.
		\]
		The preimage is well defined even if the \gls{function} $f$ is non-invertible \cite{RudinBookPrinciplesMatheAnalysis}.
		\\
		See also: \gls{function}. },
	first={preimage},
	type=math, 
	text={preimage}
}

\newglossaryentry{measurable}
{name={measurable}, 
	description={Consider\index{measurable} a \gls{randomexperiment}, such as recording 
		the air temperature at an \gls{fmi} weather station. The corresponding \gls{samplespace} 
		$\samplespace$ consists of all possible outcomes $\outcome$ (e.g., 
		all possible temperature values in degree Celsius). In many \gls{ml} 
		applications, we are not interested in the exact outcome $\outcome$, but only 
		whether it belongs to a subset $\mathcal{A} \subseteq \samplespace$ 
		(e.g., determining whether the temperature is below zero degrees). 
		We call such a subset $\mathcal{A}$ measurable if it is possible to 
		decide, for any outcome $\outcome$, whether $\outcome \in \mathcal{A}$ 
		or not (see Fig.\ \ref{fig_measurable_dict}). \\
		\begin{figure}[H]
		\begin{center}
		\begin{tikzpicture}
			% Draw temperature axis
			\draw[->] (0,0) -- (8.5,0) node[right] {temperature ($^\circ$C)};
			% Add tick marks and labels every 20 degrees from -20 to 100
			\foreach \x/\label in {0/--20, 1/--10, 2/0, 3/10, 4/20, 5/30, 6/40, 7/50, 8/60} {
				\draw (\x,0.1) -- (\x,-0.1);
				\node[below] at (\x,-0.1) {\label};
			}
			% Shade measurable set: Temperature < 0°C
			\fill[blue!20] (0,0.3) rectangle (2,0.6);
			\node[above] at (1,0.6) {$\outcome < 0\;^\circ$C};
			% Shade measurable set: 5°C < omega < 10°C
			\fill[red!20] (5.5,0.3) rectangle (7.5,0.6);
			\node[above] at (6,0.6) {$35\;^\circ$C $< \outcome < 55\;^\circ$C};
			\vspace*{10mm}
			\end{tikzpicture}
			\vspace*{10mm}
			\end{center}
			\caption{A \gls{samplespace} constituted by all possible temperature values $\outcome$ 
			that can occur at an \gls{fmi} station. Two measurable subsets of temperature 
			values, denoted by $\mathcal{A}^{(1)}$ and $\mathcal{A}^{(2)}$, are highlighted. For any 
			actual temperature value $\outcome$, it is possible to determine (via some equipment) 
			whether $\outcome \in \mathcal{A}^{(1)}$ and whether $\outcome \in \mathcal{A}^{(2)}$. 
			\label{fig_measurable_dict}} 
		\end{figure}
		In principle, measurable sets could be chosen freely (e.g., depending on the resolution of the 
		measuring equipment). However, it is often useful to impose certain completeness requirements 
		on the collection of measurable sets. For example, the \gls{samplespace} itself should be 
		measurable, and the union of two measurable sets should also be measurable. These completeness 
		requirements can be formalized via the concept of a \gls{sigmaalgebra} (or \gls{sigmafield}) 
		\cite{RudinBook}, \cite{BillingsleyProbMeasure}, \cite{durrett2010probability}. 
		A measurable space is a pair $\big(\featurespace,\eventspace\big)$ that consists of an arbitrary 
		set $\featurespace$ and a collection $\eventspace$ of measurable subsets of $\featurespace$ 
		that form a \gls{sigmaalgebra}. 
		\\
		See also: \gls{samplespace}, \gls{sigmaalgebra}, \gls{probability}.},
	first={measurable},
	type=math, 
	text={measurable} 
}

\newglossaryentry{sigmaalgebra}
{name={$\sigma$-algebra}, 
sort={sigma-algebra},
	description={Consider a \gls{randomexperiment} with a \gls{samplespace} $\samplespace$. 
		A $\sigma$-algebra\index{$\sigma$-algebra} (or \gls{sigmafield}) $\sigmaalgebra$ 
		is a collection of subsets of $\samplespace$ with the following properties 
		\cite{RudinBook}, \cite{BillingsleyProbMeasure}, \cite{durrett2010probability}:
		 \begin{itemize}
		 	\item The empty set $\emptyset$ and the entire \gls{samplespace} 
		 	$\samplespace$ belong to $\sigmaalgebra$, i.e., $\emptyset \in \sigmaalgebra$ and $\samplespace \in \sigmaalgebra$.
		 	\item If a set $\mathcal{A}$ belongs to $\sigmaalgebra$, then its complement 
		 	$\samplespace \setminus \mathcal{A}$ also belongs to $\sigmaalgebra$, i.e., 
		 	$\mathcal{A} \in \sigmaalgebra$ implies $\samplespace \setminus \mathcal{A} \in \sigmaalgebra$.
		 	\item If a countable collection of sets $\mathcal{A}_1, \,\mathcal{A}_2, \,\ldots$ belongs 
			to $\sigmaalgebra$, 
		 	then their union also belongs to $\sigmaalgebra$, i.e.,
		 	$\mathcal{A}_1, \,\mathcal{A}_2, \,\ldots \in \sigmaalgebra$ implies 
		 	$\bigcup_{i=1}^{\infty} \mathcal{A}_i \in \sigmaalgebra$.	
		 \end{itemize}			 
		See also: \gls{samplespace}, \gls{rv}, \gls{probspace}.},
	first={$\sigma$-algebra},
	type=math, 
	text={$\sigma$-algebra} 
}

\newglossaryentry{sigmafield}
{name={$\sigma$-field}, 
sort={sigma-field},
	description={See \gls{sigmaalgebra}\index{$\sigma$-field}.}, 
	first={$\sigma$-field},
	type=math,
	text={$\sigma$-field} 
}

\newglossaryentry{injective}
{name={injective}, 
	description={A \gls{function} $f: \mathcal{U} \rightarrow \mathcal{V}$ is injective\index{injective} 
		if it maps distinct elements of its \gls{domain} to distinct elements 
		of its \gls{co-domain}, 
    		i.e., if $f(u_1) = f(u_2)$ implies $u_1 = u_2$ for all $u_1, u_2 \in \mathcal{U}$ 
        		\cite{HalmosSet}. 
    		Equivalently, no two different \gls{function} inputs are mapped to the same \gls{function} \gls{output}.
				\\
		See also: \gls{function}.},
	first={injective},
	type=math,
	text={injective} 
}


\newglossaryentry{typicalset}
{name={typical set}, 
	description={See \gls{pmf}\index{typical set}.}, 
 	first={typical set},
 	firstplural={typical sets},
 	type=math,
 	plural={typical sets},
 	text={typical set} 
}
